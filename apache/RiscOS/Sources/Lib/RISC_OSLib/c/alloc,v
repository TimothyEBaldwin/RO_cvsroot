head	4.10;
access;
symbols
	RISC_OSLib-5_97:4.10
	RISC_OSLib-5_96:4.10
	RISC_OSLib-5_95:4.10
	RISC_OSLib-5_94:4.10
	RISC_OSLib-5_93:4.10
	RISC_OSLib-5_92:4.10
	RISC_OSLib-5_91:4.10
	RISC_OSLib-5_90:4.10
	RISC_OSLib-5_89:4.10
	RISC_OSLib-5_88:4.10
	RISC_OSLib-5_87:4.9
	RISC_OSLib-5_86-1:4.9
	RISC_OSLib-5_86:4.9
	RISC_OSLib-5_85:4.9
	RISC_OSLib-5_84:4.9
	RISC_OSLib-5_83-2:4.9
	RISC_OSLib-5_83-1:4.9
	RISC_OSLib-5_83:4.9
	RISC_OSLib-5_82:4.9
	RISC_OSLib-5_81:4.9
	RISC_OSLib-5_75-2:4.9
	RISC_OSLib-5_80:4.9
	RISC_OSLib-5_79:4.9
	RISC_OSLib-5_78:4.9
	RISC_OSLib-5_75-1:4.9
	RISC_OSLib-5_77:4.9
	RISC_OSLib-5_76:4.9
	RISC_OSLib-5_75:4.9
	RISC_OSLib-5_74:4.9
	RISC_OSLib-5_73:4.9
	RISC_OSLib-5_72:4.9
	RISC_OSLib-5_71:4.9
	RISC_OSLib-5_70:4.9
	RISC_OSLib-5_69:4.9
	RISC_OSLib-5_68:4.9
	RISC_OSLib-5_67:4.9
	RISC_OSLib-5_66:4.9
	RISC_OSLib-5_65:4.9
	RISC_OSLib-5_64:4.9
	RISC_OSLib-5_63:4.9
	RISC_OSLib-5_62:4.9
	RISC_OSLib-5_61:4.9
	RISC_OSLib-5_60:4.9
	RISC_OSLib-5_59:4.9
	RISC_OSLib-5_58:4.9
	RISC_OSLib-5_57:4.9
	RISC_OSLib-5_56:4.9
	RISC_OSLib-5_55:4.9
	RISC_OSLib-5_54:4.9
	RISC_OSLib-5_53:4.9
	RISC_OSLib-5_52:4.9
	RISC_OSLib-5_51:4.9
	RO_5_07:4.9
	RISC_OSLib-5_50:4.9
	RISC_OSLib-5_49:4.9
	RISC_OSLib-5_46-4_64_2_1:4.6
	NoInlineAsm:4.6.0.2
	RISC_OSLib-5_48:4.9
	RISC_OSLib-5_47:4.8
	RISC_OSLib-5_46:4.6
	RISC_OSLib-5_45:4.6
	RISC_OSLib-5_44:4.6
	RISC_OSLib-5_43:4.5
	RISC_OSLib-5_42:4.5
	RISC_OSLib-5_41:4.5
	RISC_OSLib-5_40:4.4
	RISC_OSLib-5_39:4.4
	RISC_OSLib-5_38:4.4
	RISC_OSLib-5_37:4.4
	RISC_OSLib-5_36:4.4
	RISC_OSLib-5_35:4.4
	RISC_OSLib-5_34:4.4
	RISC_OSLib-5_33-4_50_2_1:4.4
	sbrodie_dev:4.4.0.2
	sbrodie_dev_bp:4.4
	RISC_OSLib-5_33:4.4
	RISC_OSLib-5_32:4.4
	RISC_OSLib-5_31:4.4
	RISC_OSLib-5_30:4.4
	RISC_OSLib-5_29:4.4
	RISC_OSLib-5_28:4.4
	RISC_OSLib-5_27:4.4
	RISC_OSLib-5_26:4.4
	RISC_OSLib-5_25:4.4
	RISC_OSLib-5_24:4.4
	RISC_OSLib-5_01-4_16_2_5:4.3
	RISC_OSLib-5_23:4.4
	RISC_OSLib-5_22:4.4
	RISC_OSLib-5_21:4.4
	RISC_OSLib-5_20:4.4
	RISC_OSLib-5_19:4.4
	RISC_OSLib-5_18:4.4
	RISC_OSLib-5_17:4.4
	RISC_OSLib-5_16:4.4
	RISC_OSLib-5_15:4.4
	dellis_autobuild_BaseSW:4.4
	RISC_OSLib-5_14:4.4
	RISC_OSLib-5_13:4.4
	RISC_OSLib-5_12:4.4
	RISC_OSLib-5_01-4_16_2_4:4.3
	RISC_OSLib-5_11:4.4
	RISC_OSLib-5_01-4_16_2_3:4.3
	RISC_OSLib-5_01-4_16_2_2:4.3
	RISC_OSLib-5_10:4.4
	RISC_OSLib-5_01-4_16_2_1:4.3
	Bethany:4.3.0.4
	RISC_OSLib-5_09:4.4
	RISC_OSLib-5_08:4.4
	RISC_OSLib-5_07:4.4
	RISC_OSLib-5_06:4.4
	RISC_OSLib-4_97-4_12_2_8:4.3.2.1
	RISC_OSLib-5_05:4.3
	RISC_OSLib-5_04:4.3
	sbrodie_sedwards_16Mar2000:4.3
	RISC_OSLib-5_03:4.3
	RISC_OSLib-5_02:4.3
	RISC_OSLib-4_97-4_12_2_7:4.3.2.1
	RISC_OSLib-5_01:4.3
	RISC_OSLib-5_00:4.3
	RISC_OSLib-4_99:4.3
	RISC_OSLib-4_98:4.3
	RISC_OSLib-4_97-4_12_2_6:4.3.2.1
	RISC_OSLib-4_97-4_12_2_5:4.3
	RISC_OSLib-4_97-4_12_2_4:4.3
	RISC_OSLib-4_97-4_12_2_3:4.3
	RISC_OSLib-4_97-4_12_2_2:4.3
	sbrodie_RISC_OSLib-4_97-4_12_2_1:4.3
	kbracey_32bit:4.3.0.2
	kbracey_32bit_bp:4.3
	dcotton_autobuild_BaseSW:4.4
	RISC_OSLib-4_97:4.3
	RISC_OSLib-4_96:4.3
	RISC_OSLib-4_95:4.3
	RISC_OSLib-4_94:4.3
	RISC_OSLib-4_93:4.3
	RISC_OSLib-4_92:4.3
	mstphens_UrsulaRiscPCBuild_20Nov98:4.2
	Ursula_RiscPC:4.2.0.8
	sforrest_daytona_appflash-0_31:4.2
	RISC_OSLib-4_91:4.3
	RISC_OSLib-4_90:4.3
	RISC_OSLib-4_89:4.3
	Ursula_merge:4.2
	RISC_OSLib-4_88:4.3
	RISC_OSLib-4_87:4.3
	blaughto_daytona_appflash-0_30:4.2
	rmanby_clib-4_86:4.2
	rthornb_UrsulaBuild-19Aug1998:4.2
	UrsulaBuild_FinalSoftload:4.2
	rthornb_UrsulaBuild-12Aug1998:4.2
	aglover_UrsulaBuild-05Aug1998:4.2
	rthornb_UrsulaBuild-29Jul1998:4.2
	rthornb_UrsulaBuild-22Jul1998:4.2
	rthornb_UrsulaBuild-15Jul1998:4.2
	rthornb_UrsulaBuild-07Jul1998:4.2
	rthornb_UrsulaBuild-17Jun1998:4.2
	rthornb_UrsulaBuild-03Jun1998:4.2
	rthornb_UrsulaBuild-27May1998:4.2
	rthornb_UrsulaBuild-21May1998:4.2
	rthornb_UrsulaBuild_01May1998:4.2
	afrost_NC2_Generic:4.1.7.1
	Spinner_B20_2:4.1.7.1
	Spinner_19_3:4.1.7.1
	Spinner_B18:4.1.7.1
	Spinner_B17:4.1.7.1
	Spinner_B15:4.1.7.1
	Spinner_B14:4.1.7.1
	Spinner_B13:4.1.7.1
	Spinner_B12:4.1.7.1
	Spinner_B10:4.1.7.1
	Daytona:4.2.0.6
	Daytona_bp:4.2
	Ursula:4.2.0.2
	Ursula_bp:4.2
	Spinner_B7:4.1.7.1
	RO_3_71:4.1.3.1
	ARTtmp_merge:4.1.7.1
	Spin_3Apr97:4.1.7.1
	ARTtmp:4.1.7.1.0.2
	Spin_merge:4.1.7.1
	MergeFiles:4.1.3.1
	RO_3_70:4.1.3.1
	NC_1_06:4.1.7.1
	Spinner:4.1.7
	Spin_xx:4.1.5
	NC_xx:4.1.5.1
	RO_3_60:4.1.1.1
	StrongARM:4.1.3
	Black:4.1.1;
locks; strict;
comment	@# @;


4.10
date	2016.02.29.10.24.11;	author bavison;	state Exp;
branches;
next	4.9;
commitid	4Cb775DX43yNvNWy;

4.9
date	2003.12.02.16.51.29;	author bavison;	state Exp;
branches;
next	4.8;

4.8
date	2003.10.13.15.41.23;	author bavison;	state Exp;
branches;
next	4.7;

4.7
date	2003.10.09.17.39.47;	author bavison;	state Exp;
branches;
next	4.6;

4.6
date	2002.11.15.15.00.26;	author kbracey;	state Exp;
branches;
next	4.5;

4.5
date	2002.05.22.10.45.23;	author kbracey;	state Exp;
branches;
next	4.4;

4.4
date	2000.05.09.14.09.21;	author kbracey;	state Exp;
branches;
next	4.3;

4.3
date	98.08.27.10.01.09;	author smiddle;	state Exp;
branches
	4.3.2.1;
next	4.2;

4.2
date	97.01.21.17.42.37;	author nturton;	state Exp;
branches;
next	4.1;

4.1
date	96.11.05.09.24.46;	author nturton;	state Exp;
branches
	4.1.1.1
	4.1.3.1
	4.1.5.1
	4.1.7.1;
next	;

4.3.2.1
date	99.11.23.13.42.40;	author sbrodie;	state Exp;
branches;
next	;

4.1.1.1
date	96.11.05.09.24.46;	author nturton;	state Exp;
branches;
next	;

4.1.3.1
date	96.11.05.19.51.36;	author nturton;	state Exp;
branches;
next	;

4.1.5.1
date	96.11.21.11.26.48;	author nturton;	state Exp;
branches;
next	;

4.1.7.1
date	96.11.29.19.47.19;	author nturton;	state Exp;
branches;
next	;


desc
@@


4.10
log
@  Support for ARMv8
Detail:
  The SWP and SWPB instructions have finally been removed in ARMv8, after
  having been deprecated for a very long time. This version adds alternative
  versions of code that used to use them with ones that use LDREX/STREX and
  LDREXB/STREXB instead. Soft-loadable C libraries will choose between
  implementations at runtime using OS_PlatformFeatures; ROM builds only
  include the appropriate version for the target hardware.
Admin:
  Tested on Raspberry Pi 3.

Version 5.88. Tagged as 'RISC_OSLib-5_88'
@
text
@/* Copyright 1996 Acorn Computers Ltd
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
/*
  Title:        alloc - Storage management (dynamic allocation/deallocation)

  Copyright (C) Acorn Computers Ltd., 1988
*/

/* ***** IMPORTANT ** IMPORTANT ** IMPORTANT ** IMPORTANT ** IMPORTANT *****
 * The #defines which control a large part of this source file are decribed
 * in the header file.
 */

/*
 * NOTES:
 *  Non-implemented (possible) functionality is described under ASSUMPTIONS
 *   and marked with a '!'.
 *  Heap extensions inside the current heap (in a previous heap hole) has not
 *   been tested, but the code is there.
 *  A certain percentage (FRACTION_OF_HEAP_NEEDED_FREE) of the heap is always
 *   kept free, this is a bit wasteful but the number of coalesces and garbage
 *   collections goes down as this percentage rises. It has been found by
 *   experimentation that this fraction should be approximately between 1/8 and
 *   1/4 (currently at 1/6). Large blocks are allocated from the start of the
 *   overflow list ie the low memory addresses and small and medium sized
 *   blocks are allocated from the end of the overflow list. For this reason
 *   the overflow list is a doubly linked list with a head at both ends. A
 *   pointer to the last free block on the heap is also kept so that when the
 *   heap is extended and the old bitmap is returned to the free list (and
 *   merged with any adjacent free block), the last heap block, if it is free,
 *   can be merged with it also.
 * ASSUMPTIONS:
 *  Address units are in bytes.
 *  There are an exact number of address units (bytes) per word.
 *  All target machines are either word aligned or run slower with non word
 *   alignment (so word aligning is a good and right thing to do).
 *  The heap can not grow downwards (all heap extensions must be above the
 *   heap base determined by the first block claimed from OSStorage), and if
 *   two consecutive (in time) blocks are doled out by OSStorage they are only
 *   assumed to be contiguous if the lower limit (arithmetically) of the second
 *   block is equal to the higher limit (arithmetically) of the first block
 *   plus one.
 *  Blocks may be doled out in unspecified address order (but note that
 *   every time a heap extension, which is inside my heap bounds, is given out
 *   the heap has to be scanned in order to find and modify the heap hole in
 *   which the extension has been given.
!*  The range of address units to be found in a single bin can only be the
 *   number of address units in a word, extra code will have to be written to
 *   manage bin ranges other than this size (more trouble than its worth, if
 *   its worth anything at all).
 *  MAXCARD is the largest number representable in a word (ie all bits set).
 * ALLOCATE:
 *  An array of lists of free blocks of similar sizes (bins) is kept so that
 *   when an ALLOCATE of size n is requested the list starting at array entry
 *   n DIV BINRANGE will automatically have as the first element of the list
 *   a block of the correct size (plus the OVERHEADWORDS) or no block at all
 *   (or the block requested may be too big to be in the allocate bins). if
 *   there is no block available in the bin, then bins containing lists of
 *   larger blocks are checked and the block allocated from one of these (if
 *   the bin block is big enough, it is split). if there is still no block
 *   available then the overflow list is checked and if available, the block
 *   is cut from here (the block required is cut from the end of the larger
 *   block if the size required is not large (size < LARGEBLOCK) otherwise it
 *   is taken from the start of the large block). if the remainder of the block
 *   is greater than the largest bin block then it remains in the overflow
 *   list, otherwise it is removed to the correct bin. if the overflow list
 *   does not have a block large enough then the heap is either extended (more
 *   memory claimed from OSStorage), coalesced or garbage collected, depending
 *   on the state of the heap etc and whether garbage collection is enabled.
 *   After coalescing or garbage collection the allocate algorithm is executed
 *   again in order to allocate the block.
 * COALESCE:
 *  if the overflow list does not contain a block large enough and a
 *   reasonable amount of storage has been deallocated since the last coalesce,
 *   (reasonable is difficult to define and is only deducable by
 *   experimentation) then all allocatable blocks (by storage) and all blocks
 *   on the overflow deallocate list are marked free, the heap is scanned and
 *   the blocks scattered into bins and overflow list in increasing address
 *   order.
 * DEALLOCATE:
 *  When a block is DEALLOCATED, if it will fit in a bin then it is put at
 *   the start of the relevant bin list otherwise it is conceptually released
 *   to the overflow deallocate list (there is no need for a list, set the
 *   block's header bits to indicate it is free and it will automatically be
 *   sucked in at the next coalesce).
 * HEAP EXTENSIONS:
 *  Whenever the heap is extended, a certain amount (if available) is allocated
 *   for the garbage collection bit maps (even if garbage collection has not
 *   been enabled.
 */

#include <stdlib.h>
#include <stddef.h>
#include <string.h>             /* for memset(...), memcpy(...) */
#include <setjmp.h>
#include <signal.h>

#include "hostsys.h"
#include "alloc.h"
#include "kernel.h"             /* for _alloc_chunk   */
#include "swis.h"               /* used only by free */

#if defined(VERBOSE)||defined(DEBUG)||defined(STACKCHECK)||defined(ANALYSIS)
#include <stdio.h>
#endif

#if defined(VERBOSE)||defined(DEBUG)||defined(STACKCHECK)||defined(ANALYSIS)
static int n, d, last, iw;
#define dbmsg(f, a, b, c, d) {char v[128]; sprintf(v, f, a, b, c, d); last = 0;\
        for(iw=0;v[iw];_kernel_oswrch(last=v[iw++])); \
        if (last == 10)_kernel_oswrch(13);}
#define LOW_OVERHEAD_F0(v) {last = 0; \
        for(iw=0;v[iw];_kernel_oswrch(last=v[iw++])); \
        if (last == 10)_kernel_oswrch(13);}
#define LOW_OVERHEAD_FD(i, b) {n = i; d = 1; \
        while (n >= b) {d *= b; n /= b;} n = i; \
        while(d) {if ((n/d) > 9) _kernel_oswrch(n/d+'A'-10); \
                  else _kernel_oswrch(n/d+'0'); \
                  n = n-(n/d)*d; d /= b;}}
#else
#define dbmsg(f, a, b, c, d)
#define LOW_OVERHEAD_F0(v)
#define LOW_OVERHEAD_FD(i, b)
#endif
/* put this here too */
#ifdef VERBOSE
#define F0(f)             LOW_OVERHEAD_F0(f)
#define FD(i, b)          LOW_OVERHEAD_FD(i, b)
#else
#define F0(f)
#define FD(i, b)
#endif
#ifdef DEBUG
#define D0(f)             LOW_OVERHEAD_F0(f)
#define DD(i, b)          LOW_OVERHEAD_FD(i, b)
#define D1(f, a)          dbmsg(f, a, 0, 0, 0)
#define D2(f, a, b)       dbmsg(f, a, b, 0, 0)
#define D3(f, a, b, c)    dbmsg(f, a, b, c, 0)
#define D4(f, a, b, c, d) dbmsg(f, a, b, c, d)
#else
#define D0(f)
#define DD(i, b)
#define D1(f, a)
#define D2(f, a, b)
#define D3(f, a, b, c)
#define D4(f, a, b, c, d)
#endif

#define IGNORE(param) param = param

#define FALSE 0
#define TRUE  1

/*
 * FRACTION_OF_HEAP_NEEDED_FREE is used when deciding whether to coalesce, GC
 * or extend the heap. An attempt is made to keep this amount free, if it is
 * not free then the heap is extended. The amount of free space is the total of
 * all free blocks (without overheads). if there is a bitmap at the end of the
 * heap, it is not included in the heap size.
 */
#define FRACTION_OF_HEAP_NEEDED_FREE 6
/* initialisation for blocks on allocation */

static BlockP heapLow;  /* address of the base of the heap */
static BlockP heapHigh; /* address of heap hole guard at the top of heap */
static BlockP sys_heap_top; /* address of top of system heap, should = heapLow
                               after _init_user_alloc is called          */
static void *RMABase;   /* base address of RMA */
static void *RMAEnd;      /* end address of RMA */
/*
 * amount of heap that user can actually write to, does not include bitmaps
 * and block overheads
 */
static size_t totalFree;
static size_t userHeap;  /* size of heap (bytes) excluding gc bitmaps */
static size_t totalHeap; /* size of heap (bytes) including gc bitmaps */
/*
 * The overflow list is a chain of large blocks ready for use, the chain is a
 * doubly linked list of blocks in increasing address order.
 * bin[0] is the start of the overflow list.
 * bin[NBINS+1] is end of the overflow list.
 *
 * bin is an array of pointers to lists of free small blocks ( <= MAXBINSIZE)
 * of the same size. Last deallocated block is at the start of the list.
 */
static BlockP bin[NBINS+2];

static BlockP endOfLastExtension;

static struct {
  char allocates, deallocates;
} check;

static int lookInBins;

static BlockP lastFreeBlockOnHeap;
static int enoughMemoryForGC;
static char *mapForExistingHeap;
static char *mapForNewHeap;
static BlockP endOfExistingHeap;
static BlockP startOfNewHeap;

#define MAXEVENTS 64 /* remember the last MAXEVENTS events */

typedef struct StatsStruct {
  StorageInfo stats;
  EventInfo events[MAXEVENTS];
  int nextEvent;
  /* ShowStats variables */
  unsigned guard;
  size_t size;
  unsigned firstWord;
  BlockP elementBase;
  BlockP nextBase;
  int freeBlk;
  int heapHole;
  int bitmap;
  unsigned totFree;
  unsigned totUsed;
  unsigned totMaps;
  unsigned holeBlocks;
  unsigned totHole;
  unsigned freeBlocks;
  unsigned usedBlocks;
  unsigned mapsBlocks;
  unsigned largestFreeBlock;
  StorageInfo stat;
  EventInfo eventInfo;
  int eventNo;
} StatsRec, *StatsPtr;

/* static StatsPtr statsP; */

static char sys_message[60];

/* Mutex-y things */

static int heapMutex;
extern int _swp_available;

#define INITMUTEX heapMutex = 1

#ifndef __APCS_32

static int arm2_swp(int newValue, int *location);

static inline int swp(int newValue, int *location)
{
  if (!_swp_available) return arm2_swp(newValue, location);
  int oldValue;
  __asm { SWP oldValue, newValue, [location] }
  return oldValue;
}

#define ACQUIREMUTEX \
  do { \
    while (swp(0, &heapMutex) == 0) \
      _swix(OS_UpCall, _INR(0,1), 6, &heapMutex); \
  } while (0)

#else

extern void AcquireMutex(volatile int *p);

#define ACQUIREMUTEX \
  do { \
    AcquireMutex(&heapMutex); \
  } while (0)

#endif

#define RELEASEMUTEX { heapMutex = 1; }
#define RELEASEANDRETURN(value) {RELEASEMUTEX return (value);}

/*
 * This code will use a maximum of 32 words of stack excluding any used by
 * the system storage wholesaler.
 *
 * Turn off stack overflow checking.
 */
#pragma -s1

#if defined(VERBOSE) || defined(STACKCHECK)
static int *stackOnEntryToAlloc;
#define ENTRYTOALLOC(local) stackOnEntryToAlloc = (int *)(&local)
#define STACKDEPTH(local, depth) \
        {if (stackOnEntryToAlloc-(int *)(&local) > (depth))\
         {LOW_OVERHEAD_F0("!! stack ") \
          LOW_OVERHEAD_FD(stackOnEntryToAlloc-(int *)(&local), 10) \
          LOW_OVERHEAD_F0(" words\n")}}
#else
#define ENTRYTOALLOC(local)
#define STACKDEPTH(local, depth)
#endif

static void _alloc_die(const char *message, int rc)
{
  const char *ct;
  char *cs;

  /* nb rc is here so that it can be examined - otherwise the C compiler
   * tends to lose a useful value.
   */
  IGNORE(rc);
  cs = sys_message;
  ct = message;
  while ((*cs++ = *ct++) >= ' ');
  if (rc == CORRUPT) {
    cs--;
    ct = _kernel_getmessage(", (heap corrupt)", "C10");
    while ((*cs++ = *ct++) >= ' ');
  }
  _sysdie(sys_message);
}

/*static void bad_size(size_t size)
{
  IGNORE(size);
  _alloc_die(_kernel_getmessage("Over-large or -ve size request", "C11"), FAILED);
}
*/
#ifdef STATS
void print_event(int event)
{
  switch (event) {
    case COALESCE:                      D0("Coalesce success:"); break;
    case EXTENSION:                     D0("Heap Extension  :"); break;
    case COALESCE_AND_EXTENSION:        D0("Coalesce-Extend :"); break;
  }
}

static void MakeEventRec(int thisEvent, Event type, size_t size)
{
  statsP->nextEvent = thisEvent + 1;
  thisEvent %= MAXEVENTS;
  statsP->events[thisEvent].event = type;
  statsP->events[thisEvent].blockThatCausedEvent = size;
  statsP->events[thisEvent].userHeap = statsP->stats.userHeap;
  statsP->events[thisEvent].totalFree = totalFree;
  statsP->events[thisEvent].allocates = statsP->stats.blocksAllocated;
  statsP->events[thisEvent].deallocates = statsP->stats.blocksDeallocated;
  statsP->events[thisEvent].bytesAllocated = statsP->stats.bytesAllocated;
  statsP->events[thisEvent].bytesDeallocated = statsP->stats.bytesDeallocated;
  D0("!!MakeEventRec ");
  print_event(statsP->events[thisEvent].event);
  D1(" blockThatCausedEvent %u\n",
                               statsP->events[thisEvent].blockThatCausedEvent);
  D1("  userHeap %u, ", statsP->events[thisEvent].userHeap);
  D1("totalFree %u, ", statsP->events[thisEvent].totalFree);
  D1("allocates %u, ", statsP->events[thisEvent].allocates);
  D1("deallocates %u\n", statsP->events[thisEvent].deallocates);
  D1("  bytesAllocated %u, ", statsP->events[thisEvent].bytesAllocated);
  D1("bytesDeallocated %u, ", statsP->events[thisEvent].bytesDeallocated);
}

/* ------------------------- Statistics reporting --------------------------*/

extern void _GetStorageInfo(StorageInfoP info)
{
  statsP->stats.currentHeapRequirement = totalHeap - totalFree;
  *info = statsP->stats;
}

extern void _NextHeapElement(BlockP *nextBase, unsigned int *guard, size_t *size,
                             int *free, int *heapHole, int *bitmap, unsigned int *firstWord)
{ BlockP junkBlock;
  if (*nextBase == NULL) {junkBlock = heapLow;} else {junkBlock = *nextBase;}
#ifdef BLOCKS_GUARDED
  *guard = junkBlock->guard;
#else
  *guard = 0;
#endif
  *firstWord = (unsigned int) junkBlock->next;
  *free = FREE(junkBlock);
  if (!*free) {
    if (HEAPHOLEBIT & junkBlock->size) {
      *bitmap = FALSE;
      *heapHole = TRUE;
    } else {
      *heapHole = FALSE; *bitmap = FALSE;
    }
  }
  *size = SIZE(junkBlock);
  ADDBYTESTO(junkBlock, OVERHEAD + *size);
  *nextBase = junkBlock;
  if (*nextBase > heapHigh) *nextBase = NULL;
}

extern int _GetEventData(int event, EventInfoP info)
{ int index;
  int previous;
   if ((event >= statsP->nextEvent) || (event < statsP->nextEvent-MAXEVENTS)
                                                              || (event < 1))
     return FALSE;
   index = event % MAXEVENTS;
   previous = (event-1) % MAXEVENTS;
   *info = statsP->events[index];
   info->allocates -= statsP->events[previous].allocates;
   info->deallocates -= statsP->events[previous].deallocates;
   info->bytesAllocated -= statsP->events[previous].bytesAllocated;
   info->bytesDeallocated -= statsP->events[previous].bytesDeallocated;

   return TRUE;
}

extern int _GetLastEvent(void)
{
  return (statsP->nextEvent-1);
}

static void ShowStats(void)
{
  _GetStorageInfo(&statsP->stat);

  statsP->nextBase = NULL;
  statsP->totFree = 0; statsP->totUsed = 0;
  statsP->totHole = 0; statsP->totMaps = 0;
  statsP->holeBlocks = 0; statsP->freeBlocks = 0;
  statsP->usedBlocks = 0; statsP->mapsBlocks = 0;
  statsP->largestFreeBlock = 0;
  D0("Storage description. (All sizes in bytes)\n");
  D0("Current storage analysis (by traversing heap):");
  do {
    statsP->elementBase = statsP->nextBase;
    _NextHeapElement(&statsP->nextBase, &statsP->guard, &statsP->size,
                    &statsP->freeBlk, &statsP->heapHole, &statsP->bitmap,
                                                       &statsP->firstWord);
    if (statsP->heapHole)
      { statsP->holeBlocks++; statsP->totHole += statsP->size; }
    else if (statsP->freeBlk) {
      if (statsP->size > statsP->largestFreeBlock)
        statsP->largestFreeBlock = statsP->size;
      statsP->freeBlocks++; statsP->totFree += statsP->size;
    } else if (statsP->bitmap)
        {statsP->mapsBlocks++; statsP->totMaps += statsP->size;}
    else {statsP->usedBlocks++; statsP->totUsed += statsP->size;}
  } while (statsP->nextBase != NULL);

  D0("\n");
  D4("Free memory of %d in %d blocks + overhead of %d = %d\n",
     statsP->totFree, statsP->freeBlocks, statsP->freeBlocks*OVERHEAD,
                           statsP->totFree+statsP->freeBlocks*OVERHEAD);
  D1("Largest free block = %d\n", statsP->largestFreeBlock);
  D4("Used memory of %d in %d blocks + overhead of %d = %d\n",
     statsP->totUsed, statsP->usedBlocks, statsP->usedBlocks*OVERHEAD,
                           statsP->totUsed+statsP->usedBlocks*OVERHEAD);
  D4("Memory taken by heap holes = %d in %d blocks + overhead of %d = %d\n",
     statsP->totHole, statsP->holeBlocks, statsP->holeBlocks*OVERHEAD,
                         statsP->totHole + statsP->holeBlocks*OVERHEAD);
  D4("Memory taken by GC bitmaps = %d in %d blocks + overhead of %d = %d\n",
     statsP->totMaps, statsP->mapsBlocks, statsP->mapsBlocks*OVERHEAD,
                         statsP->totMaps + statsP->mapsBlocks*OVERHEAD);
  D1("Current heap requirement (all except user free blocks) = %d\n",
                      (statsP->totHole+statsP->holeBlocks*OVERHEAD) +
                      (statsP->totUsed+statsP->usedBlocks*OVERHEAD) +
                      (statsP->totMaps+statsP->mapsBlocks*OVERHEAD) +
                             (statsP->freeBlocks*OVERHEAD) - OVERHEAD);
  D1("total heap usage = %d\n",
      (statsP->totHole+statsP->holeBlocks*OVERHEAD) +
      (statsP->totUsed+statsP->usedBlocks*OVERHEAD) +
      (statsP->totMaps+statsP->mapsBlocks*OVERHEAD) +
      (statsP->totFree+statsP->freeBlocks*OVERHEAD) - OVERHEAD);
  D0("\n");
  D0("Current storage statistics:\n");
  D3("%d coalesces, %d heap extensions, %d garbage collects\n",
      statsP->stat.coalesces, statsP->stat.heapExtensions,
                                   statsP->stat.garbageCollects);
  D3("Heap base = &%X, heap top = &%X, size of user heap = %d\n",
      (unsigned) statsP->stat.heapLow, (unsigned) statsP->stat.heapHigh,
                                                   statsP->stat.userHeap);
  D2("Maximum storage requested = %d, current storage requested = %d\n",
      statsP->stat.maxHeapRequirement, statsP->stat.currentHeapRequirement);
  D4("total allocated = %d in %d blocks, deallocated = %d in %d\n",
      statsP->stat.bytesAllocated, statsP->stat.blocksAllocated,
      statsP->stat.bytesDeallocated, statsP->stat.blocksDeallocated);
  D0("\n");

  statsP->eventNo = _GetLastEvent();
  D0("Description of past events in storage (most recent first):\n");
  while (_GetEventData(statsP->eventNo, &statsP->eventInfo)) {
    print_event(statsP->eventInfo.event);
    D3(" block size = %d, user heap size %d, %d usable\n",
       statsP->eventInfo.blockThatCausedEvent, statsP->eventInfo.userHeap,
                                               statsP->eventInfo.totalFree);
    D4("   allocated %d in %d, deallocated %d in %d since last event\n",
       statsP->eventInfo.bytesAllocated, statsP->eventInfo.allocates,
       statsP->eventInfo.bytesDeallocated, statsP->eventInfo.deallocates);
    statsP->eventNo--;
  }
}
#endif

#ifdef BLOCKS_GUARDED
extern void __heap_checking_on_all_deallocates(int on)
{
  check.deallocates = on;
}

extern void __heap_checking_on_all_allocates(int on)
{
  check.allocates = on;
}
#endif

static int internal_coalesce(void)
{ BlockP block;
  BlockP previous;
  BlockP tail;
#ifndef BLOCKS_GUARDED
  BlockP bin_copy[NBINS+2];
#endif
  size_t size;
  /* where size is used to specify an element of an array it should really be
   * called index, but to generate better code I got rid of the index variable
   */

  F0("!!internal_coalesce...");
#ifdef STATS
  statsP->stats.coalesces++;
#endif

  lookInBins = FALSE;
  totalFree = 0;
  /* set bins and overflow lists to empty */
  for (size = 0; size <= NBINS+1; size++)
  { bin[size] = NULL;
#ifndef BLOCKS_GUARDED
    bin_copy[size] = NULL;
#endif
  }

  block = heapLow;

  /* NULL indicates previous doesn't point to start of free block */
  previous = NULL; tail = NULL;

  while (block <= heapHigh) {
    if (INVALID(block)) return CORRUPT;
    if (FREE(block)) { /* free block */
      if (previous == NULL) previous = block;
    } else if (previous != NULL) {
      size = PTRDIFF(block, previous) - OVERHEAD;
      /* set flags to Free */
      totalFree += size;
      previous->size = (size | FREEBIT);
      if (size <= MAXBINSIZE) { /* return to bin */
        size /= BINRANGE;
        if (bin[size] == NULL) bin[size] = previous;
        else {
          /* if not BLOCKS_GUARDED use guard word of first block in bin to hold
           * a pointer to the last block in the list for this bin otherwise
           * use the bin_copy array. This allows me to keep the list in
           * ascending address order. Remember to put back the guard words at
           * the end of coalescing if BLOCKS_GUARDED.
           */
#ifdef BLOCKS_GUARDED
          ((BlockP) bin[size]->guard)->next = previous;
#else
          (bin_copy[size])->next = previous;
#endif
        }
#ifdef BLOCKS_GUARDED
        bin[size]->guard = (int) previous;
#else
        bin_copy[size] = previous;
#endif
      } else { /* put block on overflow list */
        if (bin[0] == NULL)
          {bin[0] = previous; previous->previous = NULL;}
        else
          {tail->next = previous; previous->previous = tail;}
        tail = previous;
      }
      previous = NULL;
    }
    ADDBYTESTO(block, SIZE(block) + OVERHEAD);
  }

  /* replace the guard words at the start of the bins lists */
  for (size = 1; size <= NBINS; size++) {
    if (bin[size] != NULL) {
      lookInBins = TRUE;
#ifdef BLOCKS_GUARDED
      ((BlockP) bin[size]->guard)->next = NULL;
      bin[size]->guard = GUARDCONSTANT;
#else
      (bin_copy[size])->next = NULL;
#endif
    }
  }

  /* do both ends of overflow list */
  if (bin[0] != NULL) {
    tail->next = NULL;
    bin[NBINS+1] = tail;
  } else { bin[NBINS+1] = NULL; }
  lastFreeBlockOnHeap = bin[NBINS+1];

  F0(" ... complete\n");
  return OK;
}

static int InsertBlockInOverflowList(BlockP block)
{
#if HEAP_ALLOCATED_IN_ASCENDING_ADDRESS_ORDER
  F0("!!InsertBlockInOverflowList &")
  FD((unsigned)block, 16)
  F0(" at end of list\n")
  /* OK to add remainder of block to tail of overflow list */
  if (bin[0] == NULL) {bin[0] = block; block->previous = NULL;}
  else {bin[NBINS+1]->next = block; block->previous = bin[NBINS+1];}
  bin[NBINS+1] = block; block->next = NULL;
#else
  BlockP previous;
  BlockP tail;
  F0("!!InsertBlockInOverflowList &")
  FD((unsigned)block, 16);
  if (bin[0] == NULL) {
    F0(" at end of list\n");
    /* OK to add remainder of block to tail of overflow list */
    if (bin[0] == NULL) {bin[0] = block; block->previous = NULL;}
    else {bin[NBINS+1]->next = block; block->previous = bin[NBINS+1];}
    bin[NBINS+1] = block; block->next = NULL;
  } else {
    /* insert remainder block at right position in overflow list */
    F0(" walk chain to determine where\n");
    tail = bin[0];
    while (tail != NULL && tail < block) {
      if (INVALID(tail)) return CORRUPT;
      previous = tail; tail = tail->next;
    }
    if (tail == bin[0]) {
      block->next = bin[0]; block->previous = NULL;
      bin[0]->previous = block; bin[0] = block;
    } else {
      block->next = previous->next; block->previous = previous;
      previous->next = block;
      if (tail == NULL) bin[NBINS+1] = block; else tail->previous = block;
    }
  }
#endif
  return OK;
}

static int GetMoreOSHeap(size_t minSize, BlockP *base_ptr, size_t *size_ptr)
{ size_t size = *size_ptr;
  BlockP base = *base_ptr;
#if !HEAP_ALLOCATED_IN_ASCENDING_ADDRESS_ORDER
  BlockP tempBlock;
#endif
  BlockP bitmap;
  int gotWhatWasWanted;
#ifdef STACKCHECK
  LOW_OVERHEAD_F0("stack on entry to GetMoreOSHeap = &")
  LOW_OVERHEAD_FD((unsigned) &gotWhatWasWanted, 16)
  LOW_OVERHEAD_F0("\n");
  STACKDEPTH(gotWhatWasWanted, 20);
#endif

#ifdef STATS
  if (statsP != NULL) statsP->stats.heapExtensions++;
#endif
  minSize += OVERHEAD + HOLEOVERHEAD;
  if (userHeap/FRACTION_OF_HEAP_NEEDED_FREE > totalFree)
    minSize += userHeap / FRACTION_OF_HEAP_NEEDED_FREE - totalFree;
  F0("!!GetMoreOSHeap: ") FD(minSize, 10)
  F0(" bytes, old heap top ")
  FD((unsigned)endOfLastExtension, 16) F0("\n")

  base = endOfLastExtension;

  size = _kernel_alloc(BYTESTOWORDS(minSize),(void **)&base) * BYTESPERWORD;
  F0("!!size = ") FD(size, 10)
  F0(" bytes, base = ")
  FD((unsigned)base, 16) F0("\n")
  if (base == ADDBYTES(endOfLastExtension, HOLEOVERHEAD)) {
    base = endOfLastExtension;
    size += HOLEOVERHEAD;
  }
  gotWhatWasWanted = (size >= minSize);
  if (size <= HOLEOVERHEAD) {size = 0; base = NULL;}
  else size -= HOLEOVERHEAD;
  F0("  got ") FD(size, 10)
  F0(" at &")
  FD((unsigned)base, 16) F0("\n")

  bitmap = base;
  if (base == endOfLastExtension) {
    /* extension contiguous with last block on heap. */
    if (lastFreeBlockOnHeap != NULL &&
            ADDBYTES(lastFreeBlockOnHeap,
                          SIZE(lastFreeBlockOnHeap)+OVERHEAD) == bitmap) {
      /* so do the merge of the extension and last block on the heap */
      lastFreeBlockOnHeap->size = SIZE(lastFreeBlockOnHeap) + OVERHEAD;
      totalFree -= lastFreeBlockOnHeap->size;
      size += lastFreeBlockOnHeap->size;

      bitmap = lastFreeBlockOnHeap;
      if (lastFreeBlockOnHeap == bin[NBINS+1]) {
        /* remove block from end of overflow list */
        if (lastFreeBlockOnHeap->previous == NULL) bin[0] = NULL;
        else lastFreeBlockOnHeap->previous->next = NULL;
        bin[NBINS+1] = lastFreeBlockOnHeap->previous;
      } /* else it is not in any list ie waiting for coalesce */
    }
  }

  /* SEE WHAT TO DO WITH NEW BLOCK (IF THERE IS ONE) */
  if (size > MAXBINSIZE+OVERHEAD) {
    F0("\n");
    /* block is big enough to do something with */
    /* HANDLE BEING DROPPED INTO A HEAP HOLE, AND CREATING THE HEAP HOLE
       MARKER AT THE END OF THE NEW EXTENSION BLOCK. */
    if (base >= heapHigh) {
      if (endOfLastExtension != NULL && base != endOfLastExtension) {
        /* heap hole, mark it as allocated */
        F0("  extension not contiguous with heap, heap hole created\n");
        endOfLastExtension->size =
                 (PTRDIFF(base, endOfLastExtension) - HOLEOVERHEAD) | HEAPHOLEBIT;
      } else F0("  extension contiguous with heap\n");
      endOfLastExtension = ADDBYTES(bitmap, size);
#ifdef BLOCKS_GUARDED
      endOfLastExtension->guard = GUARDCONSTANT;
#endif
      endOfLastExtension->size = 0; /* as an end marker for Coalesce */
    }
#if !HEAP_ALLOCATED_IN_ASCENDING_ADDRESS_ORDER
      else { /* find the heap hole I've been dropped in and modify it */
      BlockP holeStart;
      BlockP hole;
      F0("  extension is in a heap hole\n");
      hole = heapLow; holeStart = NULL;
      while (hole <= base) {
        if (HEAPHOLE(hole)) holeStart = hole;
        ADDBYTESTO(hole, SIZE(hole)+OVERHEAD);
      }
      if (holeStart != base) /* extension is NOT at start of heap hole */
        holeStart->size = PTRDIFF(base, holeStart) - HOLEOVERHEAD | HEAPHOLEBIT;
      else if (ADDBYTES(holeStart, HOLEOVERHEAD) == base) {
        base = holeStart;
        size += HOLEOVERHEAD;
      }
      if (ADDBYTES(base, size+HOLEOVERHEAD) == hole) size += HOLEOVERHEAD;
      else { /* create a new hole at the end of the extension */
        tempBlock = ADDBYTES(base ,size);
#ifdef BLOCKS_GUARDED
        tempBlock->guard = GUARDCONSTANT;
#endif
        tempBlock->size = (PTRDIFF(hole, tempBlock) - HOLEOVERHEAD) | HEAPHOLEBIT;
      }
    }
#endif /* EXTENSIONS_IN_HEAP_HOLES */

    /* INITIALISE HEADER OF NEW BLOCK */
    base = bitmap;
    if (base > lastFreeBlockOnHeap) lastFreeBlockOnHeap = base;
    size -= OVERHEAD;
#ifdef BLOCKS_GUARDED
    base->guard = GUARDCONSTANT;
#endif
    /* set flags to Free */
    base->size = size | FREEBIT;
    totalFree += size;
    if (!gotWhatWasWanted) {
      F0("  extension too small, ");
      if (InsertBlockInOverflowList(base) != OK) return FAILED;
    }
  } else /* block is not big enough to worry about, throw it away */
    F0(", no heap extension\n");

  /* endOfLastExtension is the address of the storage after the end of the
     block (used to handle heap holes) */
  if (endOfLastExtension > heapHigh) heapHigh = endOfLastExtension;
  if (base < heapLow && base != NULL) heapLow = base;
  totalHeap = PTRDIFF(heapHigh, heapLow);
  userHeap = totalHeap;
#ifdef STATS
  if (statsP != NULL) {
    statsP->stats.userHeap = userHeap;
    statsP->stats.heapLow = heapLow;
    statsP->stats.heapHigh = heapHigh;
  }
#endif

  *size_ptr = size;
  *base_ptr = base;
  if (gotWhatWasWanted) return OK; else return FAILED;
}

#ifdef BLOCKS_GUARDED
static int check_heap(void)
{ BlockP block;
  if (userHeap > 0) {
    for (block = heapLow; ; ) {
      if (block >= heapHigh) {
        if (block > ADDBYTES(heapHigh,OVERHEAD)) return CORRUPT;
        else return OK;
      }
      if (INVALID(block)) return CORRUPT;
      ADDBYTESTO(block, SIZE(block)+OVERHEAD);
    }
  }
  return OK;
}
#endif

#define COALESCED     (1U<<31)
#define FORCECOALESCE (1U<<30)

static int _primitive_alloc(size_t size/*words*/)
{ BlockP block;
  size_t actualSize;
  register int index;
  int fromHighMemory;
  unsigned status = 0;

  ACQUIREMUTEX;
#ifdef BLOCKS_GUARDED
  if (check.allocates && check_heap() != OK) RELEASEANDRETURN(CORRUPT)
#endif
  /* convert size from words to addresss units */
  size *= BYTESPERWORD;
  F0("!!primitive_alloc: size ")
  FD(size, 10)
  F0(" bytes")
  if (size >= MAXBYTES) RELEASEANDRETURN(FAILED)
  else if (size == 0) size = BYTESPERWORD;

  index = 0;
  fromHighMemory = ((size <= LARGEBLOCK) && sys_heap_top);
  for (;;) {
    if (size <= MAXBINSIZE && lookInBins) { /* get from bin (if not empty) */
      F0("  looking in bins");
      index = size / BINRANGE;
      do {
        block = bin[index];
        if (block != NULL) { /* got a block */
          if (INVALID(block)) RELEASEANDRETURN(CORRUPT)
          bin[index] = block->next;
          actualSize = SIZE(block);
          F0(" ");
          FD(index, 10);
          goto got_block;
        } /* else try other bins */
      } while (++index <= NBINS);
    }

    /* block bigger than largest bin / bin is empty, check overflow list */
    /* if large block required, take it from high memory otherwise from low */
get_from_overflow:
    F0("  looking in overflow ");
    if (fromHighMemory) {block = bin[NBINS+1]; F0("<");}
    else {block = bin[0]; F0(">");}

    while (block != NULL) {
      if (INVALID(block)) RELEASEANDRETURN(CORRUPT)
      actualSize = SIZE(block);
      if (actualSize >= size) {
        /* got a block big enough, now see if it needs splitting */
        if (actualSize-size <= MAXBINSIZE+OVERHEAD) {
          /* remove all of block from overflow list */
          if (block == lastFreeBlockOnHeap) lastFreeBlockOnHeap = NULL;
          if (block->previous == NULL) bin[0] = block->next;
          else block->previous->next = block->next;
          if (block->next == NULL) bin[NBINS+1] = block->previous;
          else block->next->previous = block->previous;
          goto got_block;
        } else { /* split and leave unwanted part of the block in list */
          goto split_block;
        }
      } else {
          if (fromHighMemory) {block = block->previous; F0("<");}
          else {block = block->next; F0(">");}
      }
    }
    F0("\n");

    /* no block in bin or overflow list, try coalesce if desirable */
    if (!(COALESCED & status) &&
         ((totalFree > (size + 4096) &&
           totalFree > userHeap/FRACTION_OF_HEAP_NEEDED_FREE)
         || FORCECOALESCE & status)) {
#ifdef STATS
      MakeEventRec(statsP->nextEvent, COALESCE, size);
#endif
      if (internal_coalesce() != OK) RELEASEANDRETURN(CORRUPT)
      status |= COALESCED;
      continue; /* try the allocation again */
    } else
      /* no block available in Storage, must go to OSStorage to get one */

#ifdef STATS
    if (COALESCED & status) {
      MakeEventRec(statsP->nextEvent-1, COALESCE_AND_EXTENSION, size);
    } else if (heapHigh > heapLow)
        MakeEventRec(statsP->nextEvent, EXTENSION, size);
#endif

    { BlockP blockCopy;
      size_t actual;
      /* now we have to get more heap */
      switch (GetMoreOSHeap(size, &blockCopy, &actual)) {
        case OK:
          block = blockCopy; actualSize = actual;
          if (InsertBlockInOverflowList(block) != OK) RELEASEANDRETURN(CORRUPT)
          goto get_from_overflow;
        case FAILED:
          block = blockCopy; actualSize = actual;
          if (!enoughMemoryForGC) {
            if (FORCECOALESCE & status) RELEASEANDRETURN(FAILED)
            else status |= FORCECOALESCE;
          }
          enoughMemoryForGC = FALSE;
          break;
        case CORRUPT:
          D0("**Heap CORRUPT getting more OS heap\n");
          return CORRUPT;
#ifdef DEBUG
        default: _alloc_die("internal error: bad switch selector", FAILED);
#endif
      }
    }
  }

got_block:
  if (fromHighMemory && (actualSize > size+MINBLOCKSIZE)) {
    /* split and put unwanted part of block into a bin or on overflow list*/
split_block:
    F0(", got block ")
    FD((unsigned)block, 16)
    F0(" to split, ")
    { BlockP tempBlock = block;
      totalFree -= OVERHEAD;
      /* large block taken from bottom of this block */
      /* medium and small blocks (and bitmaps) taken off top of this block */
      if ((size > LARGEBLOCK) || (!sys_heap_top)) ADDBYTESTO(tempBlock, size+OVERHEAD);
      else ADDBYTESTO(block, actualSize-size);
      block->size = size;
      /* set flags on block to Free */
      size = actualSize - (size + OVERHEAD);
      tempBlock->size = size | FREEBIT;

      if (!fromHighMemory) {
      /* The block has been cut from the start of the overflow block.
         This means that the large block that was in the overflow list
         has to be replaced with new one (tempBlock).
       */
        tempBlock->previous = block->previous;
        tempBlock->next = block->next;
        if (tempBlock->previous == NULL) bin[0] = tempBlock;
        else tempBlock->previous->next = tempBlock;
        if (tempBlock->next == NULL) bin[NBINS+1] = tempBlock;
        else tempBlock->next->previous = tempBlock;
      }
#ifdef BLOCKS_GUARDED
      tempBlock->guard = GUARDCONSTANT;
#endif

      if (size <= MAXBINSIZE) {
        /* work out the bin number */
        lookInBins = TRUE;
        index = size / BINRANGE;
        F0("remainder --> bin ") FD(index, 10)
        F0("\n")
        tempBlock->next = bin[index]; bin[index] = tempBlock;
      } else
        F0("remainder --> overflow list\n");
    }
  } else  /* no split, take the whole block */
    F0("no split, take the lot\n");

  size = SIZE(block);
#ifdef ANALYSIS
  LOW_OVERHEAD_F0("+") LOW_OVERHEAD_FD(size, 10) LOW_OVERHEAD_F0("\n")
#endif
  F0("  new allocated block at &") FD((unsigned) block, 16)
  F0(", size ") FD(size, 10) F0("\n")
  /* set flags to not Free, and gcbits */
  block->size = size;
#ifdef BLOCKS_GUARDED
  block->guard = GUARDCONSTANT;
#endif
  totalFree -= size;
  if (bin[NBINS+1] > lastFreeBlockOnHeap) lastFreeBlockOnHeap = bin[NBINS+1];
#ifdef STATS
  if (statsP != NULL) {
    statsP->stats.blocksAllocated++;
    statsP->stats.bytesAllocated += size;
    if (totalHeap-totalFree > statsP->stats.maxHeapRequirement)
      statsP->stats.maxHeapRequirement = totalHeap-totalFree;
  }
#endif
  ADDBYTESTO(block, OVERHEAD);
  RELEASEANDRETURN((int)block)
}

static int _primitive_dealloc(BlockP block)
{ int size;
  ACQUIREMUTEX;
  F0("!!primitive_dealloc: block ")
  FD((unsigned)block, 16);  F0("\n")

  if ((block <= heapLow) || (block >= heapHigh)) {
    if (block == NULL) RELEASEANDRETURN(OK)
    else RELEASEANDRETURN(FAILED)
  }
  ADDBYTESTO(block, -OVERHEAD);

#ifdef BLOCKS_GUARDED
  if (check.deallocates) {
    BlockP searchBlock = heapLow;
    for (; searchBlock != block; ) {
      if (searchBlock >= heapHigh) RELEASEANDRETURN(FAILED)
      if (INVALID(searchBlock)) RELEASEANDRETURN(CORRUPT)
      ADDBYTESTO(searchBlock, OVERHEAD + SIZE(searchBlock));
    }
  }

  if (INVALID(block)) RELEASEANDRETURN(CORRUPT)
#endif
  size = block->size;
  if (FREEBIT & size) RELEASEANDRETURN(FAILED)
  /* set flags to Free */
  size &= SIZEMASK;
#ifdef ANALYSIS
  LOW_OVERHEAD_F0("-") LOW_OVERHEAD_FD(size, 10) LOW_OVERHEAD_F0("\n")
#endif
  block->size = size | FREEBIT;
#ifdef STATS
  statsP->stats.blocksDeallocated++;
  statsP->stats.bytesDeallocated += size;
#endif
  totalFree += size;

  if (size <= MAXBINSIZE) { /* return to bin */
    lookInBins = TRUE; size /= BINRANGE;
    block->next = bin[size]; bin[size] = block;
  } else {
    /* put block on deallocate overflow list, for reuse after coalesce */
    if (block > lastFreeBlockOnHeap) lastFreeBlockOnHeap = block;
  }

  RELEASEANDRETURN(OK)
}

extern size_t _byte_size(void *p)
{ BlockP block = (BlockP)p;
  if (block != NULL) {
    /* decrement the pointer (block) by the number of overhead bytes */
    ADDBYTESTO(block, -OVERHEAD);
    if (!INVALID(block)) return (SIZE(block));
  }
  return 0;
}

extern void *malloc(size_t size)
{ void *ptr;
  if (_kernel_processor_mode() & 0xF) /* not USR26 or USR32 */
      return _kernel_RMAalloc(size);

  ENTRYTOALLOC(ptr);
  ptr = (void *)_primitive_alloc(BYTESTOWORDS(size));
  if ((unsigned)ptr >= MINHEAPERROR) {
#ifdef STATS
    ShowStats();
#endif
    if ((int)ptr == CORRUPT)
      _alloc_die(_kernel_getmessage("malloc failed", "C12"), CORRUPT);
    else return NULL;
  }
  return ptr;
}

extern void free(void *p)
{ int rc;
  if (p >= RMABase && p < RMAEnd) {
    _kernel_RMAfree(p); return;
  }
  rc = _primitive_dealloc((BlockP)p);
  /* following line may not be correct ANSI - but for the moment we
   * have problems if we don't detect invalid free's.
   */
  if (rc != OK) {
#ifdef STATS
    ShowStats();
#endif
    _alloc_die(_kernel_getmessage("free failed", "C15"), rc);
  }
}

extern void *_sys_alloc(size_t n)
{ void *a = malloc(n);
  if (a == NULL)
    _alloc_die(_kernel_getmessage("No store left for I/O buffer or the like", "C17"), FAILED);
  return a;
}

/*
 * End of veneer functions
 *
 * Garbage collection interface.
 */

extern int __coalesce(void)
{ int rc;
  ACQUIREMUTEX;
  rc = internal_coalesce();
  RELEASEMUTEX;
  if (rc != OK) {
#ifdef STATS
    ShowStats();
#endif
    _alloc_die(_kernel_getmessage("heap coalesce failed", "C18"), rc);
  }
  return rc;
}

#ifdef STATS
static void init_stats(void)
{
  /* grab stats record from heap */
  statsP = (StatsPtr) _primitive_alloc(BYTESTOWORDS(sizeof(StatsRec)));
  statsP->stats.coalesces = 0; statsP->stats.heapExtensions = 0;
  statsP->stats.heapHigh = heapHigh; statsP->stats.heapLow = heapLow;
  statsP->stats.userHeap = userHeap; statsP->stats.maxHeapRequirement = 0;
  statsP->stats.blocksAllocated = 0; statsP->stats.bytesAllocated = 0;
  statsP->stats.blocksDeallocated = 0; statsP->stats.bytesDeallocated = 0;
  statsP->events[0].allocates = 0; statsP->events[0].deallocates = 0;
  statsP->events[0].bytesAllocated = 0; statsP->events[0].bytesDeallocated = 0;
  statsP->nextEvent = 1;
}
#endif

extern void _terminate_user_alloc(void)
{
  heapLow = sys_heap_top;
}

extern void _init_user_alloc(void)
{
  sys_heap_top = heapLow;
  heapLow = bin[0];
  totalHeap = PTRDIFF(heapHigh, heapLow);
  userHeap = totalHeap;
}

#ifndef __APCS_32
static int arm2_swp(int newValue, int *location)
{
  int irqs_off = _kernel_irqs_disabled();
  if (!irqs_off) _swix(OS_IntOff, 0);
  int oldValue = *location;
  *location = newValue;
  if (!irqs_off) _swix(OS_IntOn, 0);
  return oldValue;
}
#endif

extern void _init_alloc(void)
{ int j;
  _kernel_swi_regs r;
  INITMUTEX;
  lastFreeBlockOnHeap = NULL;
  mapForExistingHeap = NULL;
  check.deallocates = FALSE;
  check.allocates = FALSE;
  enoughMemoryForGC = TRUE;
  /* to get rid of warnings */
  mapForExistingHeap = NULL;
  mapForNewHeap = NULL;
  endOfExistingHeap = NULL;
  startOfNewHeap = NULL;
  lookInBins = FALSE;
  totalFree = 0;
  endOfLastExtension = NULL;
  /* set allocate bins and overflow lists to empty */
  for (j=0; j <= NBINS+1; ++j) { bin[j] = NULL; }
  totalHeap = 0; userHeap = 0;
  /* get base and end of RMA */
  r.r[0] = 0x81;
  if (!_kernel_swi(OS_ReadDynamicArea,&r,&r)) {
    RMABase = (void *) r.r[0];
    RMAEnd = (void *) (r.r[0] + r.r[2]);
  } else {
    RMABase = (void *) 0x01800000;
    RMAEnd = (void *) 0x01C00000;
  }
#ifdef STATS
  statsP = NULL;
  init_stats();
#endif
  sys_heap_top = heapHigh = 0; heapLow = (BlockP) 0x7fffffff;
  _kernel_register_allocs(&malloc, &free);
}

void *realloc(void *p, size_t size)
{ int rc;
  size_t oldsize;
  void *newb = NULL;
  if (_kernel_processor_mode() & 0xF) /* not USR26 or USR32? */
      return _kernel_RMAextend(p, size);

  F0("!!realloc\n");
  size = BYTESTOWORDS(size)*BYTESPERWORD;
  if (p == NULL) return malloc(size);
  if (BADUSERBLOCK(p))
    _alloc_die(_kernel_getmessage("realloc failed, (bad user block)", "C13"), FAILED);

  oldsize = _byte_size(p);
  if (oldsize < size) {
    newb = malloc(size);
    if (newb == NULL) return NULL;
    memcpy(newb, p, oldsize);   /* copies 0 words for bad p! */
  }

  if ((oldsize < size) || (size == 0) ||
      (oldsize > size+MINBLOCKSIZE+BYTESPERWORD)) {
    if ((oldsize > size+MINBLOCKSIZE+BYTESPERWORD) && (size != 0)) {
      BlockP b = ADDBYTES(p, -OVERHEAD);
      b->size = size+BYTESPERWORD | (b->size&(!SIZEMASK));
      newb = p;
      ADDBYTESTO(b, size+BYTESPERWORD+OVERHEAD);
#ifdef BLOCKS_GUARDED
      b->guard = GUARDCONSTANT;
#endif
      b->size = (oldsize-OVERHEAD-BYTESPERWORD-size);
      p = ADDBYTES(b, OVERHEAD);
    }
    rc = _primitive_dealloc((BlockP) p);
    if (rc != OK) {
#ifdef STATS
      ShowStats();
#endif
      _alloc_die(_kernel_getmessage("deallocate of old block in realloc failed", "C14"), rc);
    }
    return newb;
  } else
    return p;
}

static void bad_size(size_t size)
{
  IGNORE(size);
  _alloc_die(_kernel_getmessage("Over-large or -ve size request", "C11"), FAILED);
}

extern void *calloc(size_t count, size_t size)
{ void *r;
/*
 * This miserable code computes a full 64-bit product for count & size
 * just so that it can verify that the said product really is in range
 * for handing to malloc.
 */
  unsigned h = (count>>16)*(size>>16);
  unsigned m1 = (count>>16)*(size&0xffff);
  unsigned m2 = (count&0xffff)*(size>>16);
  unsigned l = (count&0xffff)*(size&0xffff);
  h += (m1>>16) + (m2>>16);
  m1 = (m1&0xffff) + (m2&0xffff) + (l>>16);
  l = (l&0xffff) | (m1<<16);
  h += m1>>16;
  if (h) l = (unsigned)(-1);
  if (l >= MAXBYTES) bad_size(l);
  r = malloc(l);
#ifdef GC
  /* if garbage collecting, the block will already have been zeroed */
  if ((r != NULL) && (!garbageCollecting)) memset(r, 0, l);
#else
  if (r != NULL) memset(r, 0, l);
#endif
  return r;
}
@


4.9
log
@  Changes for Customer W; also suitable for building on an Iyonix.
Detail:
  * No longer uses aasm to build h.swis.
  * (Only) compatible with new C compilers: assumes const static data is
    placed in separate read-only areas, and suitable command-line switch
    is used to ensure that library static data is never placed in a zero-
    init area.
  * Stack extension code now thread-safe.
  * Heap thread-safety code (in clib) now uses SWP instruction test results
    from stack extension code (in kernel) - as a side effect, the exported
    symbol _swp_available now exists, if you need to use it elsewhere.
  * Slightly closer to having top-bit-set heap addresses working.
Admin:
  Tested in a Tungsten build, and with Customer W's test suite.

Version 5.48. Tagged as 'RISC_OSLib-5_48'
@
text
@d253 2
d256 1
a257 1
#endif
a260 1
#ifndef __APCS_32
a261 1
#endif
a266 2
#define INITMUTEX heapMutex = 1

d273 11
@


4.8
log
@Added ARM2 support to the threadsafe dynamic memory manager.
Retagged as 'RISC_OSLib-5_47'
@
text
@d251 1
a251 1
static jmp_buf *swpNotAvailable = NULL; /* boolean also used to pass jmp_buf to sig handler */
d253 1
d255 1
d260 1
a260 1
  if (swpNotAvailable) return arm2_swp(newValue, location);
a266 1
#ifdef __APCS_32
a267 15
#else
#define INITMUTEX \
  do { \
    heapMutex = 1; \
    jmp_buf jb; \
    swpNotAvailable = &jb; \
    void (*old_sig_handler)(int) = signal(SIGILL, temp_sig_handler); \
    if (setjmp(jb) == 0) { \
      int temp; \
      __asm { SWP temp, 1, [&heapMutex] } \
      swpNotAvailable = NULL; \
    } \
    signal(SIGILL, old_sig_handler); \
  } while (0)
#endif
d1066 1
a1066 1
  if ((int)ptr < OK) {
a1150 5
static void temp_sig_handler(int sig)
{
  longjmp(*swpNotAvailable, 1);
}

@


4.7
log
@  Changes required for Customer W.
Detail:
  * Implemented the thread-safety mutex macros for the user mode heap.
  * Created ansilibm, a version of ansilib suitable for building modules.
  * Fixed an overestimation of the size of the workspace passed to
    SharedCLibrary_LibInitModule[APCS_32] by the stubs.
  * Fixed bug in SharedCLibrary_LibInitModule[APCS_32] regarding handling of
    non-reentrant modules - if module code was at a higher address than the
    module data, then all the intervening data would be pointlessly copied
    back over itself with no allowance for volatility.
  * Removed the bsearch source file, since it was only required for APCS-A
    which the library has not supported since the year 2000, and which the
    compiler is now no longer able to build.
Admin:
  ansilibm builds working modules, but thread safety has not been
  extensively tested.

Version 5.47. Tagged as 'RISC_OSLib-5_47'
@
text
@d107 2
d248 2
d251 15
d267 16
d285 1
a285 3
    int oldValue; \
    __asm { SWP oldValue, 0, [&heapMutex] } \
    while (oldValue == 0) { \
a286 2
      __asm { SWP oldValue, 0, [&heapMutex] } \
    } \
d288 1
d1163 17
@


4.6
log
@ROM build fixed for 64-bit stuff.
PCI added to swis.h
alloc.c updated to handle bigger slots (new code merged from ARM libraries)
Various 32-bit fixes for backtracing, and general trap handling.
Polite "Application is not 32-bit compatible" message.
Headers <stdint.h> and <inttypes.h> fixed to work in non-C99 mode.
txt changed to do new-style Delete behaviour

Version 5.44. Tagged as 'RISC_OSLib-5_44'
@
text
@d246 12
a257 3
#define INITMUTEX
#define ACQUIREMUTEX
#define RELEASEMUTEX {}
@


4.5
log
@* Added two new library chunks, 4 and 5, which contain extensions to the kernel
  and C library respectively. These have no static data associated with them,
  just being extensions of the stub tables. The reason for this is to minimise
  wasted space in programs that don't use the C99 facilities; o.stubs is now
  a library split into 3 pieces - basic kernel and CLib, extra kernel and extra
  CLib; only the bits a program needs get included.

* Previous extensions to the C library stubs revoked - they now stop at _swix;
  all the new C99 functions now live in chunk 4. Anyone using those new
  functions should relink with new stubs and ensure this C library version.

* printf/scanf now support 64-bit types through "ll" and "j" length modifiers.

* Run-time support for VLAs (__rt_allocauto and __rt_freeauto) added. No
  attempt is currently made to clear up on longjmp or to cope with someone
  changing the kernel allocator while a VLA is active. These would be a
  future enhancement.

* Added complete 64-bit run-time support (48 functions) to kernel library;
  these functions are compatible with the ones used by the ARM ADS. Many of
  the simpler functions will not normally be used by the compiler, as it
  will generate inline code. There is scope for improvement by switching
  in MULL and CLZ-using forms of multiply and divide when possible.

* llabs and lldiv added to C library.

* Header files corrected in a few areas, and changed to match the C compiler.
  <stdint.h> and <stdbool.h> now require the compiler to be in C99 mode
  (as detected using __STDC_VERSION__).


Version 5.41. Tagged as 'RISC_OSLib-5_41'
@
text
@d104 5
a108 1
#ifdef CAMEL
d110 1
a110 8
#ifndef __stddef__h
#include "stddef.h"
#endif
#include "mcsuppt.h"            /* for memset(...), memcpy */
#include "m2core.h"             /* for locks, etc.         */
#include "m2raise.h"            /* for AEM-2 Exceptions    */
#include "OSLLIO.h"             /* used only by _alloc_die */
#include "exit.h"               /* used only by _alloc_die */
a111 10
#else
#include "h.hostsys"
#include "h.alloc"
#include "h.kernel"             /* for _alloc_chunk   */
#include <stddef.h>
/* #include "reinit.h"  */
#include <string.h>             /* for memset(...), memcpy(...) */
/* #include "brazstd.h.m2core"  / * for locks, etc.    */
#include "h.swis"               /* used only by free */
#endif
a113 3
#ifdef CAMEL
#include "h.printf"
#else
a115 1
#endif
a163 21
#ifndef __m2core__h
#define BITSIZE(bytes) ((bytes)<<3)
#endif
#define BITSPERWORD  BITSIZE(sizeof(int))
#define BITSPERBYTE  (BITSPERWORD/BYTESPERWORD)
/*
 * The following constants are all in address units
 */
/* MAXBYTES should be something outrageously big */
#define MAXBYTES     0x01C00000
#define OVERHEAD     (FIRSTUSERWORD * BYTESPERWORD)
#define HOLEOVERHEAD OVERHEAD
#define MINBLOCKSIZE (OVERHEAD + BYTESPERWORD)
#define HOLEBITS     (DATA | HEAPHOLEBIT)

/* the following constants are tunable */
/* multiple of required block size needing to be free before coalesce done */
#define BINRANGE     (BYTESPERWORD * 1) /* see assumptions */
#define NBINS        16
#define MAXBINSIZE   (BINRANGE*(NBINS)-1)
#define LARGEBLOCK   512
a173 2
typedef void *VoidStar;

d178 2
a179 2
static int RMABase;     /* base address of RMA */
static int RMAEnd;      /* end address of RMA */
d200 3
a202 2
static int checkDeallocates;
static int checkAllocates;
a206 1
static int garbageCollecting;
a207 2
static GCProc garbageCollect;
static BlockP gcLimit;  /* upper limit of the user heap */
a245 32
/*
 * Code macros.
 */
#define SIZE(block) ((size_t)((block)->size & SIZEMASK))
#define BITSTOWORDS(bits) ((bits+(BITSPERWORD-1))/BITSPERWORD)
#define BYTESTOWORDS(bytes) ((bytes+(BYTESPERWORD-1))/BYTESPERWORD)
#define ADDBYTES(bp, bytes) (BlockP)((char *)bp + (bytes))
#define ADDBYTESTO(bp, bytes) bp = (BlockP)((char *)bp + (bytes))
#define PTRDIFF(hi, lo) ((char *)hi - (char *)lo)
#define FREE(block) (FREEBIT & ((BlockP)block)->size)
#define HEAPHOLE(block) (HEAPHOLEBIT & block->size)
#ifdef BLOCKS_GUARDED
#define INVALID(block) (((BlockP)block)->guard != GUARDCONSTANT)
#else
#define INVALID(block) (0)
#endif
#define BADUSERBLOCK(block) (INVALID(ADDBYTES(block,-OVERHEAD)) \
                            || FREE(ADDBYTES(block,-OVERHEAD)))

#if defined(MULTITHREADED) && defined(CAMEL)
static Mutex storage = 0;
#define INITMUTEX LLThreads_InitMutex(&storage)
#define ACQUIREMUTEX int sl = OSSYSTEM_PreventStackExtension();\
                      LLThreads_Acquire(&storage);
#define RELEASEMUTEX {if (sl) OSSYSTEM_AllowStackExtension();\
                      LLThreads_Release(&storage);}
#else
#if defined(MULTITHREADED)
#define INITMUTEX
#define ACQUIREMUTEX _interrupts_off = 1;
#define RELEASEMUTEX {_raise_stacked_interrupts();}
#else
a248 2
#endif
#endif
d272 1
a272 1
static void _alloc_die(char *message, int rc)
d274 2
a275 1
  char *cs, *ct;
a280 9
#ifdef CAMEL
  _m2raise(_EALLOC, (int)message);
  OSLLIO_NewLine();
  OSLLIO_PutS("alloc: ");
  OSLLIO_PutS(message);
  if (rc = CORRUPT) OSLLIO_PutS(", (heap corrupt)");
  OSLLIO_NewLine();
  exit(1);               /* bad exit */
#else
a289 1
#endif
d292 1
a292 1
static void bad_size(size_t size)
d297 1
a297 1

a301 1
    case GARBAGE_COLLECT:               D0("Garbage Collect :"); break;
a304 3
    case GC_AND_EXTENSION:              D0("GC-Extend       :"); break;
    case COALESCE_AND_GC_AND_EXTENSION: D0("Coalesce-GC-Ext :"); break;
    case COALESCE_AND_GC:               D0("Coalesce-GC     :"); break;
d308 1
a308 1
static void MakeEventRec(int thisEvent, Events type, size_t size)
a319 2
  statsP->events[thisEvent].bytesGCd = statsP->stats.totalGCBytes;
  statsP->events[thisEvent].blocksGCd = statsP->stats.totalGCBlocks;
a329 2
  D1("bytesGCd %u, ", statsP->events[thisEvent].bytesGCd);
  D1("blocksGCd %u\n", statsP->events[thisEvent].blocksGCd);
d340 2
a341 3
extern void _NextHeapElement(BlockP *nextBase, unsigned int *guard,
                             size_t *size, int *free, int *heapHole,
                             int *bitmap, unsigned int *firstWord)
d353 2
a354 2
      *bitmap = ((DATA & junkBlock->size) == DATA);
      *heapHole = !*bitmap;
a377 2
   info->bytesGCd -= statsP->events[previous].bytesGCd;
   info->blocksGCd -= statsP->events[previous].blocksGCd;
a451 2
  D2("total garbage collected = %d in %d blocks\n",
      statsP->stat.totalGCBytes, statsP->stat.totalGCBlocks);
a463 3
    if (statsP->eventInfo.bytesGCd > 0)
      D2("   garbage collected %d in %d blocks since last event\n",
          statsP->eventInfo.bytesGCd, statsP->eventInfo.blocksGCd);
a468 32
#ifdef GC
static void SetBlockFree(BlockP block)
{
  D1("!!SetBlockFree &%X\n", (unsigned) block);
  block->size |= FREEBIT;
#ifdef STATS
  statsP->stats.totalGCBlocks++;
  statsP->stats.totalGCBytes += SIZE(block);
#endif
}

static void init_bitmaps(void)
{
  memset(mapForExistingHeap, ~0, SIZE(ADDBYTES(mapForExistingHeap,-OVERHEAD)));
}

extern int __register_gc_proc(GCProc proc)
{
  ACQUIREMUTEX;
  F0("!!__register_gc_proc\n");
  if (enoughMemoryForGC) {
    garbageCollecting = TRUE; garbageCollect = proc;
    if (userHeap > 0) init_bitmaps();
    RELEASEMUTEX;
    return OK;
  } else {
    RELEASEMUTEX;
    return FAILED;
  }
}
#endif

d472 1
a472 1
  checkDeallocates = on;
d477 1
a477 1
  checkAllocates = on;
d519 1
a519 1
      /* set flags to GCAble, Free, and not PureData */
d621 1
a621 2
static int GetMoreOSHeap(size_t minSize, BlockP *base_ptr,
                         size_t *size_ptr)
a623 3
#ifdef GC
  BlockP tempBlock;
#else
a626 1
#endif
a627 5
#ifdef GC
  char *oldMap;
  size_t mapSizeForExistingHeap;
  size_t totalMapSize;
#endif
a645 9
#ifdef GC
  if (enoughMemoryForGC || garbageCollecting) {
    minSize = (minSize + OVERHEAD + BITSPERWORD-1) / BITSPERWORD * BITSPERWORD;
    minSize = (((totalHeap + BITSPERWORD-1) / BITSPERWORD * BITSPERWORD
                + (BITSPERWORD * minSize + minSize)) + BITSPERWORD-1)
                                                              / BITSPERWORD;
  }
#endif

d648 1
a648 9
#ifdef CAMEL
  if (endOfLastExtension == NULL) size = 0;
  else size = HOLEOVERHEAD * BITSPERBYTE;
  gotWhatWasWanted = OSStorage_HeapAllocate(minSize*BITSPERBYTE,
                                            (VoidStar *)&base, &size);

  size /= BITSPERBYTE;
#else
  size = _kernel_alloc(BYTESTOWORDS(minSize),(VoidStar *)&base) * BYTESPERWORD;
a656 1
#endif
a662 83
#ifdef GC
  /* GC BITMAP */
  oldMap = mapForExistingHeap;
  if (gotWhatWasWanted && enoughMemoryForGC) {
    /* take out garbage collection bitmap */
    F0("  make GC bitmap");
    totalMapSize = size;
    mapSizeForExistingHeap =
                   (totalHeap + BITSPERWORD-1) / BITSPERWORD + OVERHEAD;
    /* reduce size by amount needed for map for existing heap */
    size -= mapSizeForExistingHeap;
    if (base >= endOfLastExtension) {
      /* extension not in a heap hole */
      /* take out 1/33 of remainder for map for new bit of heap */
      size = (size - (size+BITSPERWORD) / (BITSPERWORD+1))
                  / BYTESPERWORD * BYTESPERWORD - OVERHEAD;
    }
    totalMapSize -= size;
    tempBlock = ADDBYTES(base, size);
#ifdef BLOCKS_GUARDED
    tempBlock->guard = GUARDCONSTANT;
#endif
    /* set flags to NOT GCAble, and PureData */
    tempBlock->size = (totalMapSize-OVERHEAD) | HOLEBITS;
    mapForExistingHeap = (char *) ADDBYTES(tempBlock, OVERHEAD);
    if ((endOfLastExtension != NULL) && (base != endOfLastExtension)) {
      /* not contiguous */
      if (base > endOfLastExtension) {
        /* extension not in a heap hole */
        mapForNewHeap = (char *) ADDBYTES(tempBlock, mapSizeForExistingHeap);
        gcLimit = tempBlock;
        startOfNewHeap = base;
      } else gcLimit = heapHigh;
      endOfExistingHeap = endOfLastExtension;
    } else {
      endOfExistingHeap = tempBlock;
      gcLimit = tempBlock;
    }
  } else {
    F0("  not enough memory for GC bitmaps");
    totalMapSize = 0;
    mapForExistingHeap = NULL; tempBlock = NULL;
  }

  /* INCORPORATE THE OLD GC BITMAP BACK INTO HEAP, AND THE FREE LARGE BLOCK
     IMMEDIATLY BEFORE THE BITMAP (IF THERE IS ONE). */
  bitmap = base;
  if (gotWhatWasWanted || (!garbageCollecting && enoughMemoryForGC)) {
    if (oldMap != NULL) {
      /* yes, we have a bitmap to incorporate back into the heap */
      F0(", give back old one");
      bitmap = ADDBYTES(oldMap, -OVERHEAD);
      bitmap->size &= SIZEMASK;
      if (base == endOfLastExtension) {
        /* extension contiguous with bitmap, so merge them. */
        /* is there a large free block just before bitmap to merge as well */
        if (lastFreeBlockOnHeap != NULL &&
            ADDBYTES(lastFreeBlockOnHeap,
                          SIZE(lastFreeBlockOnHeap)+OVERHEAD) == bitmap) {
          /* yes, so do the merge */
          lastFreeBlockOnHeap->size = SIZE(lastFreeBlockOnHeap) + OVERHEAD;
          totalFree -= lastFreeBlockOnHeap->size;
          lastFreeBlockOnHeap->size += bitmap->size;

          bitmap = lastFreeBlockOnHeap;
          if (lastFreeBlockOnHeap == bin[NBINS+1]) {
            /* remove block from end of overflow list */
            if (lastFreeBlockOnHeap->previous == NULL) bin[0] = NULL;
            else lastFreeBlockOnHeap->previous->next = NULL;
            bin[NBINS+1] = lastFreeBlockOnHeap->previous;
          } /* else it is not in any list ie waiting for coalesce */
        }
        size += bitmap->size + OVERHEAD;
      } else {
        /* extension not contiguous with bitmap, add bitmap to overflow list */
        totalFree += bitmap->size;
        bitmap->size |= FREEBIT;
        if (InsertBlockInOverflowList(bitmap) != OK) return CORRUPT;
        bitmap = base;
      }
    }
  }
#else
a682 1
#endif
d695 1
a695 1
                 (PTRDIFF(base, endOfLastExtension) - HOLEOVERHEAD) | HOLEBITS;
a696 3
#ifdef GC
      endOfLastExtension = ADDBYTES(bitmap, size+totalMapSize);
#else
a697 1
#endif
d714 1
a714 1
        holeStart->size = PTRDIFF(base, holeStart) - HOLEOVERHEAD | HOLEBITS;
a718 8
#ifdef GC
      if (ADDBYTES(base, size+totalMapSize+HOLEOVERHEAD) == hole)
        if (gotWhatWasWanted && enoughMemoryForGC)
          tempBlock->size += HOLEOVERHEAD | HOLEBITS;
        else size += HOLEOVERHEAD;
      else { /* create a new hole at the end of the extension */
        tempBlock = ADDBYTES(base ,size+totalMapSize);
#else
a721 1
#endif
d725 1
a725 1
        tempBlock->size = (PTRDIFF(hole, tempBlock) - HOLEOVERHEAD) | HOLEBITS;
d737 1
a737 1
    /* set flags to GCAble, Free, and not PureData */
a751 3
#ifdef GC
  userHeap = totalHeap - totalMapSize;
#else
a752 1
#endif
d783 2
a784 3
#define COALESCED     (1u<<31)
#define DONEGC        (1u<<30)
#define FORCECOALESCE (1u<<29)
d786 1
a786 1
static int primitive_alloc(int gcBits, size_t size/*words*/)
d791 1
a791 1
  int status = 0;
d795 1
a795 1
  if (checkAllocates && check_heap() != OK) RELEASEANDRETURN(CORRUPT)
a864 16
#ifdef GC
    } else if (garbageCollecting && !(DONEGC & status)) {
        if (garbageCollect(heapLow, gcLimit, mapForExistingHeap, mapForNewHeap,
                     endOfExistingHeap, startOfNewHeap, SetBlockFree) != OK)
          RELEASEANDRETURN(CORRUPT)
#ifdef STATS
        if (COALESCED & status)
          MakeEventRec(statsP->nextEvent-1, COALESCE_AND_GC, size);
        else MakeEventRec(statsP->nextEvent, GARBAGE_COLLECT, size);
        statsP->stats.garbageCollects++;
#endif
        status |= DONEGC;
        if (internal_coalesce() != OK) RELEASEANDRETURN(CORRUPT)
        if (totalFree >= (userHeap / FRACTION_OF_HEAP_NEEDED_FREE))
          continue; /* try the allocation again */
#endif /* GC */
d869 1
a869 9
    if (COALESCED & status || DONEGC & status) {
#ifdef GC
      if (COALESCED & status) {
        if (DONEGC & status)
          MakeEventRec(statsP->nextEvent-1,COALESCE_AND_GC_AND_EXTENSION,size);
        else
          MakeEventRec(statsP->nextEvent-1, COALESCE_AND_EXTENSION, size);
      } else MakeEventRec(statsP->nextEvent-1, GC_AND_EXTENSION, size);
#else
a870 1
#endif
a881 3
#ifdef GC
          if (garbageCollecting) init_bitmaps();
#endif
a884 3
#ifdef GC
          if (garbageCollecting || !enoughMemoryForGC) {
#else
a885 1
#endif
d915 1
a915 1
      /* set flags on block to GCAble, Free, and not PureData */
d955 1
a955 1
  block->size = size | (gcBits & DATA);
a969 3
#ifdef GC
  if (garbageCollecting) /* zero intialise the block */ memset(block, 0, size);
#endif
d973 1
a973 1
static int primitive_dealloc(BlockP block)
d986 1
a986 1
  if (checkDeallocates) {
d999 1
a999 1
  /* set flags to GCAble, Free, and not PureData */
d1022 1
a1022 4
/*
 * Put the veneer functions here for now: don't really need all these.
 */
extern size_t _byte_size(VoidStar p)
d1032 2
a1033 2
extern VoidStar malloc(size_t size)
{ VoidStar ptr;
d1038 1
a1038 1
  ptr = (VoidStar) primitive_alloc(NOTGCABLEBIT, BYTESTOWORDS(size));
d1050 1
a1050 1
extern VoidStar realloc(VoidStar p, size_t size)
d1052 1
a1052 73
  size_t old;
  VoidStar new = NULL;
  if (_kernel_processor_mode() & 0xF) /* not USR26 or USR32? */
      return _kernel_RMAextend(p, size);

  F0("!!realloc\n");
  size = BYTESTOWORDS(size)*BYTESPERWORD;
  if (p == NULL) return malloc(size);
  if (BADUSERBLOCK(p))
    _alloc_die(_kernel_getmessage("realloc failed, (bad user block)", "C13"), FAILED);

  old = _byte_size(p);
  if (old < size) {
    new = malloc(size);
    if (new == NULL) return NULL;
    memcpy(new, p, old);        /* copies 0 words for bad p! */
  }
  if ((old < size) || (size == 0) || (old > size+MINBLOCKSIZE+BYTESPERWORD)) {
    if ((old > size+MINBLOCKSIZE+BYTESPERWORD) && (size != 0)) {
      BlockP b = ADDBYTES(p, -OVERHEAD);
      b->size = size+BYTESPERWORD | (b->size&(!SIZEMASK));
      new = p;
      ADDBYTESTO(b, size+BYTESPERWORD+OVERHEAD);
#ifdef BLOCKS_GUARDED
      b->guard = GUARDCONSTANT;
#endif
      b->size = (old-OVERHEAD-BYTESPERWORD-size) | DATA;
      p = ADDBYTES(b, OVERHEAD);
    }
    rc = primitive_dealloc((BlockP) p);
    if (rc != OK) {
#ifdef STATS
      ShowStats();
#endif
      _alloc_die(_kernel_getmessage("deallocate of old block in realloc failed", "C14"), rc);
    }
    return new;
  } else
    return p;
}

extern VoidStar calloc(size_t count, size_t size)
{ VoidStar r;
/*
 * This miserable code computes a full 64-bit product for count & size
 * just so that it can verify that the said product really is in range
 * for handing to malloc.
 */
  unsigned h = (count>>16)*(size>>16);
  unsigned m1 = (count>>16)*(size&0xffff);
  unsigned m2 = (count&0xffff)*(size>>16);
  unsigned l = (count&0xffff)*(size&0xffff);
  h += (m1>>16) + (m2>>16);
  m1 = (m1&0xffff) + (m2&0xffff) + (l>>16);
  l = (l&0xffff) | (m1<<16);
  h += m1>>16;
  if (h) l = (unsigned)(-1);
  if (l >= MAXBYTES) bad_size(l);
  r = malloc(l);
#ifdef GC
  /* if garbage collecting, the block will already have been zeroed */
  if ((r != NULL) && (!garbageCollecting)) memset(r, 0, l);
#else
  if (r != NULL) memset(r, 0, l);
#endif
  return r;
}

extern void free(VoidStar p)
{ int rc;
  /* free(0) now allowed!!! ECN - 21 09 93 */
  if (!p) return;
  if ((int)p >= RMABase && (int)p < RMAEnd) {
d1055 1
a1055 1
  rc = primitive_dealloc((BlockP)p);
d1067 2
a1068 34
#ifdef CAMEL
static void _allocate(VoidStar *a, size_t bitlen)
/* Default storage allocator */
{ int local;
  if (bitlen > 0) {
    IGNORE(local); ENTRYTOALLOC(local);
    *a = (VoidStar) primitive_alloc(NOTGCABLEBIT, BITSTOWORDS(bitlen));
    if ((int) *a <= 0) {
#ifdef STATS
      ShowStats();
#endif
      _alloc_die(_kernel_getmessage("allocate failed", "C64"), (int)*a);
    }
  } else *a = NULL;
}
#endif

extern void _deallocate(VoidStar *a, size_t bitlen)
/* Default storage deallocator */
{ VoidStar p = *a;
  int  rc;
  IGNORE(bitlen);
  *a = NULL;
  rc = primitive_dealloc((BlockP) p);
  if (rc != OK) {
#ifdef STATS
    ShowStats();
#endif
    _alloc_die(_kernel_getmessage("deallocate failed", "C16"), rc);
  }
}

extern VoidStar _sys_alloc(size_t n)
{ VoidStar a = malloc(n);
a1073 50
#ifdef GC
extern VoidStar _gc_malloc(int gcbits, size_t size)
{ VoidStar ptr;
  ptr = (VoidStar) primitive_alloc(gcbits, BYTESTOWORDS(size));
  if ((int)ptr == FAILED || (int)ptr == CORRUPT) {
#ifdef STATS
    ShowStats();
#endif
    if ((int)ptr == CORRUPT) _alloc_die("gc_malloc failed", CORRUPT);
    else return NULL;
  }
  return ptr;
}

extern void _gcallocate(VoidStar *a, size_t bitlen, int gcbits)
/* The M2 ALLOCATE function */
{ int local;
  if (bitlen > 0) {
    IGNORE(local); ENTRYTOALLOC(local);
    *a = (VoidStar) primitive_alloc(gcbits, BITSTOWORDS(bitlen));
    if ((int)*a == FAILED || (int)*a == CORRUPT) {
#ifdef STATS
      ShowStats();
#endif
      _alloc_die("gcallocate failed", (int) *a);
    }
  } else *a = NULL;
}

extern void _set_gcbits(VoidStar *a, int gcbits)
{ BlockP p = (BlockP)*a;
  /*
   * Must acquire the storage lock:- else we cannot change the word (which
   * contains the size remember) atomically.
   */
  ACQUIREMUTEX;
  if (BADUSERBLOCK(p)) {
    RELEASEMUTEX;
#ifdef STATS
      ShowStats();
#endif
    _alloc_die("_set_gcbits failed (bad user block)", FAILED);
  }
  ADDBYTESTO(p, -OVERHEAD);
  /*    clear bits  then    set bits */
  p->size = (p->size & ~DATA) | (gcbits & DATA);
  RELEASEMUTEX;
}
#endif /* GC */

d1098 1
a1098 3
  statsP = (StatsPtr) primitive_alloc(NOTGCABLEBIT,
                                      BYTESTOWORDS(sizeof(StatsRec)));
  statsP->stats.totalGCBlocks = 0; statsP->stats.totalGCBytes = 0;
a1099 1
  statsP->stats.garbageCollects = 0;
a1105 1
  statsP->events[0].bytesGCd = 0; statsP->events[0].blocksGCd = 0;
a1119 3
#ifdef GC
  userHeap = totalHeap - totalMapSize;
#else
a1120 51
#endif
}

static int _allocated_by_me(BlockP block)
{
  BlockP searchBlock = heapLow;
  if (block < heapLow) return 0;
  ADDBYTESTO(block, -OVERHEAD);
  while (searchBlock < heapHigh) {
    if (INVALID(block)) return 0;
    if (block == searchBlock) return 1;
    ADDBYTESTO(searchBlock, OVERHEAD + SIZE(searchBlock));
  }
  return 0;
}

extern int _alloc_reinit(void)
{
  _kernel_stack_chunk *prev = _kernel_current_stack_chunk();
  _kernel_stack_chunk *chunk = prev->sc_next;
  BlockP block = heapLow;
  ACQUIREMUTEX;
F0("reinitialisation... ")

  if ((prev->sc_mark != 0xf60690ff) || (prev->sc_prev != NULL))
    RELEASEANDRETURN(0);
F0("follow chunks... ");
  while (chunk != NULL) {
F0("next chunk");
    if (_allocated_by_me((BlockP) chunk)) {
      prev->sc_next = chunk->sc_next;
F0(" mine\n");
      if (chunk->sc_next != NULL) chunk->sc_next->sc_prev = prev;
    } else {
F0(" not mine\n");
      prev = chunk;
    }
    chunk = chunk->sc_next;
  }

  while (block < heapHigh) {
    if (INVALID(block)) return 0;
    if (HEAPHOLE(block)); /* skip */
    else {
F0("free ") FD((unsigned) block, 16) F0("\n")
      block->size = SIZE(block) | FREEBIT;
    }
    ADDBYTESTO(block, OVERHEAD + SIZE(block));
  }
  (void) internal_coalesce();
  RELEASEANDRETURN(1);
d1129 2
a1130 3
  garbageCollecting = FALSE;
  checkDeallocates = FALSE;
  checkAllocates = FALSE;
a1132 2
  garbageCollect = (GCProc) NULL;
  gcLimit = NULL;
d1140 2
a1141 4
  /* set allocate bins to empty */
  for (j=0; j <= NBINS; ++j) { bin[j] = NULL; }
  /* set overflow lists to empty */
  bin[0] = NULL; bin[NBINS+1] = NULL;
d1146 2
a1147 2
    RMABase = r.r[0];
    RMAEnd = r.r[0] + r.r[2];
d1149 2
a1150 2
    RMABase = 0x01800000;
    RMAEnd = 0x01C00000;
d1157 75
a1231 2
#ifdef CAMEL
  OSSYSTEM_SetAllocProcs(_allocate, _deallocate);
d1233 1
a1233 1
  _kernel_register_allocs(&malloc, &free);
d1235 1
@


4.4
log
@32-bit work merged from kbracey_32bit branch.

Version 5.06. Tagged as 'RISC_OSLib-5_06'
@
text
@d348 1
a348 3
static void _alloc_die(message, rc)
char *message;
int rc;
d377 1
a377 2
static void bad_size(size)
size_t size;
d384 1
a384 2
void print_event(event)
int event;
d397 1
a397 4
static void MakeEventRec(thisEvent, type, size)
int thisEvent;
Events type;
size_t size;
d427 1
a427 2
extern void _GetStorageInfo(info)
StorageInfoP info;
d433 3
a435 9
extern void _NextHeapElement(
                      nextBase, guard, size, free, heapHole, bitmap, firstWord)
BlockP *nextBase;
unsigned int *guard;
size_t *size;
int *free;
int *heapHole;
int *bitmap;
unsigned int *firstWord;
d459 1
a459 3
extern int _GetEventData(event, info)
int event;
EventInfoP info;
d571 1
a571 2
static void SetBlockFree(block)
BlockP block;
d586 1
a586 2
extern int __register_gc_proc(proc)
GCProc proc;
d603 1
a603 2
extern void __heap_checking_on_all_deallocates(on)
int on;
d608 1
a608 2
extern void __heap_checking_on_all_allocates(on)
int on;
d712 1
a712 2
static int InsertBlockInOverflowList(block)
BlockP block;
d754 2
a755 4
static int GetMoreOSHeap(minSize, base_ptr, size_ptr)
size_t minSize;
BlockP *base_ptr;
size_t *size_ptr;
d1045 5
a1049 7
#define COALESCED     (1<<31)
#define DONEGC        (1<<30)
#define FORCECOALESCE (1<<29)

static int primitive_alloc(gcBits, size/*words*/)
int gcBits;
size_t size;
d1271 1
a1271 2
static int primitive_dealloc(block)
BlockP block;
d1323 1
a1323 2
extern size_t _byte_size(p)
VoidStar p;
d1333 1
a1333 2
extern VoidStar malloc(size)
size_t size;
d1351 1
a1351 3
extern VoidStar realloc(p, size)
VoidStar p;
size_t size;
d1394 1
a1394 3
extern VoidStar calloc(count, size)
size_t count;
size_t size;
d1421 1
a1421 2
extern void free(p)
VoidStar p;
d1441 1
a1441 3
static void _allocate(a, bitlen)
VoidStar *a;
size_t bitlen;
d1457 1
a1457 3
extern void _deallocate(a, bitlen)
VoidStar *a;
size_t bitlen;
d1472 1
a1472 2
extern VoidStar _sys_alloc(n)
size_t n;
d1480 1
a1480 3
extern VoidStar _gc_malloc(gcbits, size)
int gcbits;
size_t size;
d1493 1
a1493 4
extern void _gcallocate(a, bitlen, gcbits)
VoidStar *a;
size_t bitlen;
int gcbits;
d1508 1
a1508 3
extern void _set_gcbits(a, gcbits)
VoidStar *a;
int gcbits;
d1599 1
a1599 1
extern int _alloc_reinit()
@


4.3
log
@Merged Spinner branch changes onto trunk (new RCA messages files
and Makefile changes). Convert to srccommit while we're at it.
Removed a few $Revision's to prevent future spurious merge clashes.

Version 4.87. Tagged as 'RISC_OSLib-4_87'
@
text
@d316 1
a316 1
#define INITMUTEX 
d395 1
a395 1
    case GC_AND_EXTENSION:              D0("GC-Extend       :"); break;      
d490 1
a490 1
  
d553 1
a553 1
  D3("%d coalesces, %d heap extensions, %d garbage collects\n", 
d828 1
a828 1
  
d1169 1
a1169 1
    } else 
d1271 1
a1271 1
#endif  
d1302 1
a1302 1
  
d1327 1
a1327 1
#endif  
d1363 1
a1363 1
  if (_kernel_processor_mode() != 0)
d1385 1
a1385 1
  if (_kernel_processor_mode() != 0)
d1603 1
a1603 1
  statsP->stats.heapHigh = heapHigh; statsP->stats.heapLow = heapLow; 
d1651 1
a1651 1
  
@


4.3.2.1
log
@More 32-bit compatibility added.  All the C code should now be OK.
Fixed some typos in kernel.s.k_body too.

Version 4.97, 4.12.2.6. Tagged as 'RISC_OSLib-4_97-4_12_2_6'
@
text
@d316 1
a316 1
#define INITMUTEX
d395 1
a395 1
    case GC_AND_EXTENSION:              D0("GC-Extend       :"); break;
d490 1
a490 1

d553 1
a553 1
  D3("%d coalesces, %d heap extensions, %d garbage collects\n",
d828 1
a828 1

d1169 1
a1169 1
    } else
d1271 1
a1271 1
#endif
d1302 1
a1302 1

d1327 1
a1327 1
#endif
d1363 1
a1363 1
  if (_kernel_processor_mode() & 0xF) /* not USR26 or USR32 */
d1385 1
a1385 1
  if (_kernel_processor_mode() & 0xF) /* not USR26 or USR32? */
d1603 1
a1603 1
  statsP->stats.heapHigh = heapHigh; statsP->stats.heapLow = heapLow;
d1651 1
a1651 1

@


4.2
log
@Module merged
@
text
@a16 3
  $Revision: 4.1.7.1 $  LDS 31-Jul-87 $ BJK 23-Oct-1987
  $Revision: 4.1.7.1 $  LH 22-Dec-1987
  $Revision: 4.1.7.1 $  LH 03-Feb-1988
@


4.1
log
@Initial revision
@
text
@d17 3
a19 3
  $Revision: 1.6 $  LDS 31-Jul-87 $ BJK 23-Oct-1987
  $Revision: 1.6 $  LH 22-Dec-1987
  $Revision: 1.6 $  LH 03-Feb-1988
@


4.1.7.1
log
@NCOS 1.06 Imported from Zip drive
@
text
@@


4.1.5.1
log
@Import from SrcFiler
@
text
@@


4.1.3.1
log
@Import from cleaned 370 CD
@
text
@@


4.1.1.1
log
@Import from cleaned 360 CD
@
text
@@
