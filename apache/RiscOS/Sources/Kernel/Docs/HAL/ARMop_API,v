head	1.4;
access;
symbols
	Kernel-6_14:1.4
	Kernel-6_01-3:1.4
	Kernel-6_13:1.4
	Kernel-6_12:1.4
	Kernel-6_11:1.4
	Kernel-6_10:1.4
	Kernel-6_09:1.4
	Kernel-6_08-4_129_2_10:1.4
	Kernel-6_08-4_129_2_9:1.4
	Kernel-6_08:1.4
	Kernel-6_07:1.4
	Kernel-6_06:1.4
	Kernel-6_05-4_129_2_8:1.4
	Kernel-6_05:1.4
	Kernel-6_04:1.4
	Kernel-6_03:1.4
	Kernel-6_01-2:1.4
	Kernel-6_01-4_146_2_1:1.4
	Kernel-6_02:1.4
	Kernel-6_01-1:1.4
	Kernel-6_01:1.4
	Kernel-6_00:1.4
	Kernel-5_99:1.4
	Kernel-5_98:1.4
	Kernel-5_97-4_129_2_7:1.4
	Kernel-5_97:1.4
	Kernel-5_96:1.4
	Kernel-5_95:1.4
	Kernel-5_94:1.4
	Kernel-5_93:1.4
	Kernel-5_92:1.4
	Kernel-5_91:1.4
	Kernel-5_90:1.4
	Kernel-5_89-4_129_2_6:1.4
	Kernel-5_89:1.4
	Kernel-5_88-4_129_2_5:1.4
	Kernel-5_88-4_129_2_4:1.4
	Kernel-5_88:1.4
	Kernel-5_87:1.4
	Kernel-5_86-4_129_2_3:1.4
	Kernel-5_86-4_129_2_2:1.4
	Kernel-5_86-4_129_2_1:1.4
	Kernel-5_86:1.4
	SMP:1.4.0.2
	SMP_bp:1.4
	Kernel-5_85:1.4
	Kernel-5_54-1:1.2
	Kernel-5_84:1.4
	Kernel-5_83:1.4
	Kernel-5_82:1.4
	Kernel-5_81:1.4
	Kernel-5_80:1.4
	Kernel-5_79:1.4
	Kernel-5_78:1.4
	Kernel-5_77:1.4
	Kernel-5_76:1.4
	Kernel-5_75:1.4
	Kernel-5_74:1.4
	Kernel-5_73:1.4
	Kernel-5_72:1.4
	Kernel-5_71:1.4
	Kernel-5_70:1.4
	Kernel-5_69:1.3
	Kernel-5_68:1.3
	Kernel-5_67:1.2
	Kernel-5_66:1.2
	Kernel-5_65:1.2
	Kernel-5_64:1.2
	Kernel-5_63:1.2
	Kernel-5_62:1.2
	Kernel-5_61:1.2
	Kernel-5_60:1.2
	Kernel-5_59:1.2
	Kernel-5_58:1.2
	Kernel-5_57:1.2
	Kernel-5_56:1.2
	Kernel-5_55:1.2
	Kernel-5_54:1.2
	Kernel-5_53:1.2
	Kernel-5_52:1.2
	Kernel-5_51:1.2
	Kernel-5_50:1.2
	Kernel-5_49:1.2
	HAL_merge:1.1.2.6
	Kernel-5_48:1.2
	Kernel-5_35-4_79_2_327:1.1.2.6
	Kernel-5_35-4_79_2_326:1.1.2.6
	Kernel-5_35-4_79_2_325:1.1.2.6
	Kernel-5_35-4_79_2_324:1.1.2.6
	Kernel-5_35-4_79_2_323:1.1.2.6
	Kernel-5_35-4_79_2_322:1.1.2.6
	Kernel-5_35-4_79_2_321:1.1.2.6
	Kernel-5_35-4_79_2_320:1.1.2.6
	Kernel-5_35-4_79_2_319:1.1.2.6
	Kernel-5_35-4_79_2_318:1.1.2.6
	Kernel-5_35-4_79_2_317:1.1.2.6
	Kernel-5_35-4_79_2_316:1.1.2.6
	Kernel-5_35-4_79_2_315:1.1.2.6
	Kernel-5_35-4_79_2_314:1.1.2.6
	Kernel-5_35-4_79_2_313:1.1.2.6
	Kernel-5_35-4_79_2_312:1.1.2.6
	Kernel-5_35-4_79_2_311:1.1.2.6
	Kernel-5_35-4_79_2_310:1.1.2.6
	Kernel-5_35-4_79_2_309:1.1.2.6
	Kernel-5_35-4_79_2_308:1.1.2.6
	Kernel-5_35-4_79_2_307:1.1.2.6
	Kernel-5_35-4_79_2_306:1.1.2.6
	Kernel-5_35-4_79_2_305:1.1.2.5
	Kernel-5_35-4_79_2_304:1.1.2.5
	Kernel-5_35-4_79_2_303:1.1.2.5
	Kernel-5_35-4_79_2_302:1.1.2.5
	Kernel-5_35-4_79_2_301:1.1.2.5
	Kernel-5_35-4_79_2_300:1.1.2.5
	Kernel-5_35-4_79_2_299:1.1.2.5
	Kernel-5_35-4_79_2_298:1.1.2.5
	Kernel-5_35-4_79_2_297:1.1.2.5
	Kernel-5_35-4_79_2_296:1.1.2.5
	Kernel-5_35-4_79_2_295:1.1.2.5
	Kernel-5_35-4_79_2_294:1.1.2.5
	Kernel-5_35-4_79_2_293:1.1.2.5
	Kernel-5_35-4_79_2_292:1.1.2.5
	Kernel-5_35-4_79_2_291:1.1.2.5
	Kernel-5_35-4_79_2_290:1.1.2.5
	Kernel-5_35-4_79_2_289:1.1.2.5
	Kernel-5_35-4_79_2_288:1.1.2.5
	Kernel-5_35-4_79_2_287:1.1.2.5
	Kernel-5_35-4_79_2_286:1.1.2.5
	Kernel-5_35-4_79_2_285:1.1.2.5
	Kernel-5_35-4_79_2_284:1.1.2.5
	Kernel-5_35-4_79_2_283:1.1.2.5
	Kernel-5_35-4_79_2_282:1.1.2.5
	Kernel-5_35-4_79_2_281:1.1.2.5
	Kernel-5_35-4_79_2_280:1.1.2.5
	Kernel-5_35-4_79_2_279:1.1.2.4
	Kernel-5_35-4_79_2_278:1.1.2.3
	Kernel-5_35-4_79_2_277:1.1.2.3
	Kernel-5_35-4_79_2_276:1.1.2.3
	Kernel-5_35-4_79_2_275:1.1.2.3
	Kernel-5_35-4_79_2_274:1.1.2.3
	Kernel-5_35-4_79_2_273:1.1.2.3
	Kernel-5_35-4_79_2_272:1.1.2.2
	Kernel-5_35-4_79_2_271:1.1.2.2
	Kernel-5_35-4_79_2_270:1.1.2.2
	Kernel-5_35-4_79_2_269:1.1.2.2
	Kernel-5_35-4_79_2_268:1.1.2.2
	Kernel-5_35-4_79_2_267:1.1.2.2
	Kernel-5_35-4_79_2_266:1.1.2.2
	Kernel-5_35-4_79_2_265:1.1.2.2
	Kernel-5_35-4_79_2_264:1.1.2.2
	Kernel-5_35-4_79_2_263:1.1.2.2
	Kernel-5_35-4_79_2_262:1.1.2.2
	Kernel-5_35-4_79_2_261:1.1.2.2
	Kernel-5_35-4_79_2_260:1.1.2.2
	Kernel-5_35-4_79_2_259:1.1.2.2
	Kernel-5_35-4_79_2_258:1.1.2.2
	Kernel-5_35-4_79_2_257:1.1.2.2
	Kernel-5_35-4_79_2_256:1.1.2.2
	Kernel-5_35-4_79_2_255:1.1.2.2
	Kernel-5_35-4_79_2_254:1.1.2.2
	Kernel-5_35-4_79_2_253:1.1.2.2
	Kernel-5_35-4_79_2_252:1.1.2.2
	Kernel-5_35-4_79_2_251:1.1.2.2
	Kernel-5_35-4_79_2_250:1.1.2.2
	Kernel-5_35-4_79_2_249:1.1.2.2
	Kernel-5_35-4_79_2_248:1.1.2.2
	Kernel-5_35-4_79_2_247:1.1.2.2
	Kernel-5_35-4_79_2_246:1.1.2.2
	Kernel-5_35-4_79_2_245:1.1.2.2
	Kernel-5_35-4_79_2_244:1.1.2.2
	Kernel-5_35-4_79_2_243:1.1.2.2
	Kernel-5_35-4_79_2_241:1.1.2.2
	Kernel-5_35-4_79_2_240:1.1.2.2
	Kernel-5_35-4_79_2_239:1.1.2.2
	Kernel-5_35-4_79_2_238:1.1.2.2
	Kernel-5_35-4_79_2_237:1.1.2.2
	Kernel-5_35-4_79_2_236:1.1.2.2
	Kernel-5_35-4_79_2_235:1.1.2.2
	Kernel-5_35-4_79_2_234:1.1.2.2
	Kernel-5_35-4_79_2_233:1.1.2.2
	Kernel-5_35-4_79_2_232:1.1.2.2
	Kernel-5_35-4_79_2_231:1.1.2.2
	Kernel-5_35-4_79_2_230:1.1.2.2
	Kernel-5_35-4_79_2_229:1.1.2.2
	Kernel-5_35-4_79_2_228:1.1.2.2
	Kernel-5_35-4_79_2_227:1.1.2.2
	Kernel-5_35-4_79_2_226:1.1.2.2
	Kernel-5_35-4_79_2_225:1.1.2.2
	Kernel-5_35-4_79_2_224:1.1.2.2
	Kernel-5_35-4_79_2_223:1.1.2.2
	Kernel-5_35-4_79_2_222:1.1.2.2
	Kernel-5_35-4_79_2_221:1.1.2.2
	Kernel-5_35-4_79_2_220:1.1.2.2
	Kernel-5_35-4_79_2_219:1.1.2.2
	Kernel-5_35-4_79_2_218:1.1.2.2
	Kernel-5_35-4_79_2_217:1.1.2.2
	Kernel-5_35-4_79_2_216:1.1.2.2
	Kernel-5_35-4_79_2_215:1.1.2.2
	Kernel-5_35-4_79_2_214:1.1.2.2
	Kernel-5_35-4_79_2_213:1.1.2.2
	Kernel-5_35-4_79_2_212:1.1.2.2
	Kernel-5_35-4_79_2_211:1.1.2.2
	Kernel-5_35-4_79_2_210:1.1.2.2
	Kernel-5_35-4_79_2_209:1.1.2.2
	Kernel-5_35-4_79_2_208:1.1.2.2
	Kernel-5_35-4_79_2_207:1.1.2.2
	Kernel-5_35-4_79_2_206:1.1.2.2
	Kernel-5_35-4_79_2_205:1.1.2.2
	Kernel-5_35-4_79_2_204:1.1.2.2
	Kernel-5_35-4_79_2_203:1.1.2.2
	Kernel-5_35-4_79_2_202:1.1.2.2
	Kernel-5_35-4_79_2_201:1.1.2.2
	Kernel-5_35-4_79_2_200:1.1.2.2
	Kernel-5_35-4_79_2_199:1.1.2.2
	Kernel-5_35-4_79_2_198:1.1.2.2
	Kernel-5_35-4_79_2_197:1.1.2.2
	Kernel-5_35-4_79_2_196:1.1.2.2
	Kernel-5_35-4_79_2_195:1.1.2.2
	Kernel-5_35-4_79_2_194:1.1.2.2
	Kernel-5_35-4_79_2_193:1.1.2.2
	Kernel-5_35-4_79_2_192:1.1.2.2
	Kernel-5_35-4_79_2_191:1.1.2.2
	Kernel-5_35-4_79_2_190:1.1.2.2
	Kernel-5_35-4_79_2_189:1.1.2.2
	Kernel-5_35-4_79_2_188:1.1.2.2
	Kernel-5_35-4_79_2_187:1.1.2.2
	Kernel-5_35-4_79_2_186:1.1.2.2
	Kernel-5_35-4_79_2_185:1.1.2.2
	Kernel-5_35-4_79_2_184:1.1.2.2
	Kernel-5_35-4_79_2_183:1.1.2.2
	Kernel-5_35-4_79_2_182:1.1.2.2
	Kernel-5_35-4_79_2_181:1.1.2.2
	Kernel-5_35-4_79_2_180:1.1.2.2
	Kernel-5_35-4_79_2_179:1.1.2.2
	Kernel-5_35-4_79_2_178:1.1.2.2
	Kernel-5_35-4_79_2_177:1.1.2.2
	Kernel-5_35-4_79_2_176:1.1.2.2
	Kernel-5_35-4_79_2_175:1.1.2.2
	Kernel-5_35-4_79_2_174:1.1.2.2
	Kernel-5_35-4_79_2_173:1.1.2.2
	Kernel-5_35-4_79_2_172:1.1.2.2
	Kernel-5_35-4_79_2_171:1.1.2.2
	Kernel-5_35-4_79_2_170:1.1.2.2
	Kernel-5_35-4_79_2_169:1.1.2.2
	Kernel-5_35-4_79_2_168:1.1.2.2
	Kernel-5_35-4_79_2_167:1.1.2.2
	Kernel-5_35-4_79_2_166:1.1.2.2
	Kernel-5_35-4_79_2_165:1.1.2.2
	RPi_merge:1.1.2.2
	Kernel-5_35-4_79_2_147_2_23:1.1.2.2
	Kernel-5_35-4_79_2_147_2_22:1.1.2.2
	Kernel-5_35-4_79_2_147_2_21:1.1.2.2
	Kernel-5_35-4_79_2_147_2_20:1.1.2.2
	Kernel-5_35-4_79_2_147_2_19:1.1.2.2
	Kernel-5_35-4_79_2_147_2_18:1.1.2.2
	Kernel-5_35-4_79_2_164:1.1.2.2
	Kernel-5_35-4_79_2_163:1.1.2.2
	Kernel-5_35-4_79_2_147_2_17:1.1.2.2
	Kernel-5_35-4_79_2_147_2_16:1.1.2.2
	Kernel-5_35-4_79_2_147_2_15:1.1.2.2
	Kernel-5_35-4_79_2_162:1.1.2.2
	Kernel-5_35-4_79_2_161:1.1.2.2
	Kernel-5_35-4_79_2_147_2_14:1.1.2.2
	Kernel-5_35-4_79_2_147_2_13:1.1.2.2
	Kernel-5_35-4_79_2_160:1.1.2.2
	Kernel-5_35-4_79_2_159:1.1.2.2
	Kernel-5_35-4_79_2_158:1.1.2.2
	Kernel-5_35-4_79_2_157:1.1.2.2
	Kernel-5_35-4_79_2_156:1.1.2.2
	Kernel-5_35-4_79_2_147_2_12:1.1.2.2
	Kernel-5_35-4_79_2_147_2_11:1.1.2.2
	Kernel-5_35-4_79_2_155:1.1.2.2
	Kernel-5_35-4_79_2_147_2_10:1.1.2.2
	Kernel-5_35-4_79_2_154:1.1.2.2
	Kernel-5_35-4_79_2_153:1.1.2.2
	Kernel-5_35-4_79_2_147_2_9:1.1.2.2
	Kernel-5_35-4_79_2_152:1.1.2.2
	Kernel-5_35-4_79_2_151:1.1.2.2
	Kernel-5_35-4_79_2_147_2_8:1.1.2.2
	Kernel-5_35-4_79_2_147_2_7:1.1.2.2
	Kernel-5_35-4_79_2_150:1.1.2.2
	Kernel-5_35-4_79_2_147_2_6:1.1.2.2
	Kernel-5_35-4_79_2_147_2_5:1.1.2.2
	Kernel-5_35-4_79_2_149:1.1.2.2
	Kernel-5_35-4_79_2_147_2_4:1.1.2.2
	Kernel-5_35-4_79_2_147_2_3:1.1.2.2
	Kernel-5_35-4_79_2_148:1.1.2.2
	Kernel-5_35-4_79_2_147_2_2:1.1.2.2
	Kernel-5_35-4_79_2_147_2_1:1.1.2.2
	RPi:1.1.2.2.0.6
	RPi_bp:1.1.2.2
	Kernel-5_35-4_79_2_98_2_52_2_1:1.1.2.2
	alees_Kernel_dev:1.1.2.2.0.4
	alees_Kernel_dev_bp:1.1.2.2
	Kernel-5_35-4_79_2_147:1.1.2.2
	Kernel-5_35-4_79_2_146:1.1.2.2
	Kernel-5_35-4_79_2_145:1.1.2.2
	Kernel-5_35-4_79_2_144:1.1.2.2
	Kernel-5_35-4_79_2_143:1.1.2.2
	Kernel-5_35-4_79_2_142:1.1.2.2
	Kernel-5_35-4_79_2_141:1.1.2.2
	Kernel-5_35-4_79_2_140:1.1.2.2
	Kernel-5_35-4_79_2_139:1.1.2.2
	Kernel-5_35-4_79_2_138:1.1.2.2
	Kernel-5_35-4_79_2_137:1.1.2.2
	Kernel-5_35-4_79_2_136:1.1.2.2
	Kernel-5_35-4_79_2_135:1.1.2.2
	Kernel-5_35-4_79_2_134:1.1.2.2
	Kernel-5_35-4_79_2_133:1.1.2.2
	Kernel-5_35-4_79_2_132:1.1.2.2
	Kernel-5_35-4_79_2_131:1.1.2.2
	Kernel-5_35-4_79_2_130:1.1.2.2
	Kernel-5_35-4_79_2_129:1.1.2.2
	Kernel-5_35-4_79_2_128:1.1.2.2
	Kernel-5_35-4_79_2_127:1.1.2.2
	Kernel-5_35-4_79_2_126:1.1.2.2
	Kernel-5_35-4_79_2_125:1.1.2.2
	Kernel-5_35-4_79_2_124:1.1.2.2
	Kernel-5_35-4_79_2_123:1.1.2.2
	Cortex_merge:1.1.2.2
	Kernel-5_35-4_79_2_122:1.1.2.2
	Kernel-5_35-4_79_2_98_2_54:1.1.2.2
	Kernel-5_35-4_79_2_98_2_53:1.1.2.2
	Kernel-5_35-4_79_2_98_2_52:1.1.2.2
	Kernel-5_35-4_79_2_98_2_51:1.1.2.2
	Kernel-5_35-4_79_2_98_2_50:1.1.2.2
	Kernel-5_35-4_79_2_98_2_49:1.1.2.2
	Kernel-5_35-4_79_2_98_2_48:1.1.2.2
	Kernel-5_35-4_79_2_121:1.1.2.2
	Kernel-5_35-4_79_2_98_2_47:1.1.2.2
	Kernel-5_35-4_79_2_120:1.1.2.2
	Kernel-5_35-4_79_2_98_2_46:1.1.2.2
	Kernel-5_35-4_79_2_119:1.1.2.2
	Kernel-5_35-4_79_2_98_2_45:1.1.2.2
	Kernel-5_35-4_79_2_98_2_44:1.1.2.2
	Kernel-5_35-4_79_2_118:1.1.2.2
	Kernel-5_35-4_79_2_98_2_43:1.1.2.2
	Kernel-5_35-4_79_2_117:1.1.2.2
	Kernel-5_35-4_79_2_116:1.1.2.2
	Kernel-5_35-4_79_2_98_2_42:1.1.2.2
	Kernel-5_35-4_79_2_115:1.1.2.2
	Kernel-5_35-4_79_2_98_2_41:1.1.2.2
	Kernel-5_35-4_79_2_98_2_40:1.1.2.2
	Kernel-5_35-4_79_2_114:1.1.2.2
	Kernel-5_35-4_79_2_98_2_39:1.1.2.2
	Kernel-5_35-4_79_2_98_2_38:1.1.2.2
	Kernel-5_35-4_79_2_113:1.1.2.2
	Kernel-5_35-4_79_2_112:1.1.2.2
	Kernel-5_35-4_79_2_98_2_37:1.1.2.2
	Kernel-5_35-4_79_2_98_2_36:1.1.2.2
	Kernel-5_35-4_79_2_98_2_35:1.1.2.2
	Kernel-5_35-4_79_2_98_2_34:1.1.2.2
	Kernel-5_35-4_79_2_98_2_33:1.1.2.2
	Kernel-5_35-4_79_2_98_2_32:1.1.2.2
	Kernel-5_35-4_79_2_98_2_31:1.1.2.2
	Kernel-5_35-4_79_2_98_2_30:1.1.2.2
	Kernel-5_35-4_79_2_98_2_29:1.1.2.2
	Kernel-5_35-4_79_2_98_2_28:1.1.2.2
	Kernel-5_35-4_79_2_98_2_27:1.1.2.2
	Kernel-5_35-4_79_2_98_2_26:1.1.2.2
	Kernel-5_35-4_79_2_111:1.1.2.2
	Kernel-5_35-4_79_2_98_2_25:1.1.2.2
	Kernel-5_35-4_79_2_98_2_24:1.1.2.2
	Kernel-5_35-4_79_2_98_2_23:1.1.2.2
	Kernel-5_35-4_79_2_110:1.1.2.2
	Kernel-5_35-4_79_2_98_2_22:1.1.2.2
	Kernel-5_35-4_79_2_109:1.1.2.2
	Kernel-5_35-4_79_2_98_2_21:1.1.2.2
	Kernel-5_35-4_79_2_98_2_20:1.1.2.2
	Kernel-5_35-4_79_2_108:1.1.2.2
	Kernel-5_35-4_79_2_107:1.1.2.2
	Kernel-5_35-4_79_2_98_2_19:1.1.2.2
	Kernel-5_35-4_79_2_98_2_18:1.1.2.2
	Kernel-5_35-4_79_2_98_2_17:1.1.2.2
	Kernel-5_35-4_79_2_98_2_16:1.1.2.2
	Kernel-5_35-4_79_2_98_2_15:1.1.2.2
	Kernel-5_35-4_79_2_106:1.1.2.2
	Kernel-5_35-4_79_2_105:1.1.2.2
	Kernel-5_35-4_79_2_104:1.1.2.2
	Kernel-5_35-4_79_2_98_2_14:1.1.2.2
	Kernel-5_35-4_79_2_98_2_13:1.1.2.2
	Kernel-5_35-4_79_2_98_2_12:1.1.2.2
	Kernel-5_35-4_79_2_98_2_11:1.1.2.2
	Kernel-5_35-4_79_2_98_2_10:1.1.2.2
	Kernel-5_35-4_79_2_98_2_9:1.1.2.2
	Kernel-5_35-4_79_2_103:1.1.2.2
	Kernel-5_35-4_79_2_102:1.1.2.2
	Kernel-5_35-4_79_2_98_2_8:1.1.2.2
	Kernel-5_35-4_79_2_98_2_7:1.1.2.2
	Kernel-5_35-4_79_2_98_2_6:1.1.2.2
	Kernel-5_35-4_79_2_98_2_5:1.1.2.2
	Kernel-5_35-4_79_2_98_2_4:1.1.2.2
	Kernel-5_35-4_79_2_101:1.1.2.2
	Kernel-5_35-4_79_2_100:1.1.2.2
	Kernel-5_35-4_79_2_99:1.1.2.2
	Kernel-5_35-4_79_2_98_2_3:1.1.2.2
	Kernel-5_35-4_79_2_98_2_2:1.1.2.2
	Kernel-5_35-4_79_2_98_2_1:1.1.2.2
	Cortex:1.1.2.2.0.2
	Cortex_bp:1.1.2.2
	Kernel-5_35-4_79_2_98:1.1.2.2
	Kernel-5_35-4_79_2_97:1.1.2.2
	Kernel-5_35-4_79_2_96:1.1.2.2
	Kernel-5_35-4_79_2_95:1.1.2.2
	Kernel-5_35-4_79_2_94:1.1.2.2
	Kernel-5_35-4_79_2_93:1.1.2.2
	Kernel-5_35-4_79_2_92:1.1.2.2
	Kernel-5_35-4_79_2_91:1.1.2.2
	Kernel-5_35-4_79_2_90:1.1.2.2
	Kernel-5_35-4_79_2_89:1.1.2.2
	Kernel-5_35-4_79_2_88:1.1.2.2
	Kernel-5_35-4_79_2_87:1.1.2.2
	Kernel-5_35-4_79_2_86:1.1.2.2
	Kernel-5_35-4_79_2_85:1.1.2.2
	Kernel-5_35-4_79_2_84:1.1.2.2
	Kernel-5_35-4_79_2_83:1.1.2.2
	Kernel-5_35-4_79_2_82:1.1.2.2
	Kernel-5_35-4_79_2_81:1.1.2.2
	Kernel-5_35-4_79_2_80:1.1.2.2
	Kernel-5_35-4_79_2_79:1.1.2.2
	Kernel-5_35-4_79_2_78:1.1.2.2
	Kernel-5_35-4_79_2_77:1.1.2.2
	RO_5_07:1.1.2.2
	Kernel-5_35-4_79_2_76:1.1.2.2
	Kernel-5_35-4_79_2_75:1.1.2.2
	Kernel-5_35-4_79_2_74:1.1.2.2
	Kernel-5_35-4_79_2_73:1.1.2.2
	Kernel-5_35-4_79_2_72:1.1.2.2
	Kernel-5_35-4_79_2_71:1.1.2.2
	Kernel-5_35-4_79_2_70:1.1.2.2
	Kernel-5_35-4_79_2_69:1.1.2.2
	Kernel-5_35-4_79_2_68:1.1.2.2
	Kernel-5_35-4_79_2_67:1.1.2.2
	Kernel-5_35-4_79_2_66:1.1.2.2
	Kernel-5_35-4_79_2_65:1.1.2.2
	Kernel-5_35-4_79_2_64:1.1.2.2
	Kernel-5_35-4_79_2_63:1.1.2.2
	Kernel-5_35-4_79_2_62:1.1.2.2
	Kernel-5_35-4_79_2_61:1.1.2.2
	Kernel-5_35-4_79_2_59:1.1.2.2
	Kernel-5_35-4_79_2_58:1.1.2.2
	Kernel-5_35-4_79_2_57:1.1.2.2
	Kernel-5_35-4_79_2_56:1.1.2.2
	Kernel-5_35-4_79_2_55:1.1.2.2
	Kernel-5_35-4_79_2_54:1.1.2.2
	Kernel-5_35-4_79_2_53:1.1.2.2
	Kernel-5_35-4_79_2_52:1.1.2.2
	Kernel-5_35-4_79_2_51:1.1.2.2
	Kernel-5_35-4_79_2_50:1.1.2.2
	Kernel-5_35-4_79_2_49:1.1.2.2
	Kernel-5_35-4_79_2_48:1.1.2.2
	Kernel-5_35-4_79_2_47:1.1.2.2
	Kernel-5_35-4_79_2_46:1.1.2.2
	Kernel-5_35-4_79_2_45:1.1.2.2
	Kernel-5_35-4_79_2_44:1.1.2.2
	Kernel-5_35-4_79_2_25_2_2:1.1.2.2
	Kernel-5_35-4_79_2_43:1.1.2.2
	Kernel-5_35-4_79_2_42:1.1.2.2
	Kernel-5_35-4_79_2_41:1.1.2.2
	Kernel-5_35-4_79_2_40:1.1.2.2
	Kernel-5_35-4_79_2_39:1.1.2.2
	Kernel-5_35-4_79_2_38:1.1.2.2
	Kernel-5_35-4_79_2_37:1.1.2.2
	Kernel-5_35-4_79_2_36:1.1.2.2
	Kernel-5_35-4_79_2_35:1.1.2.2
	Kernel-5_35-4_79_2_34:1.1.2.2
	Kernel-5_35-4_79_2_33:1.1.2.2
	Kernel-5_35-4_79_2_32:1.1.2.2
	Kernel-5_35-4_79_2_25_2_1:1.1.2.2
	Kernel-5_35-4_79_2_31:1.1.2.2
	Kernel-5_35-4_79_2_30:1.1.2.2
	Kernel-5_35-4_79_2_29:1.1.2.2
	Kernel-5_35-4_79_2_28:1.1.2.2
	Kernel-5_35-4_79_2_27:1.1.2.2
	Kernel-5_35-4_79_2_26:1.1.2.2
	Kernel-5_35-4_79_2_25:1.1.2.2
	Kernel-5_35-4_79_2_24:1.1.2.2
	Kernel-5_35-4_79_2_23:1.1.2.2
	Kernel-5_35-4_79_2_22:1.1.2.2
	Kernel-5_35-4_79_2_21:1.1.2.2
	Kernel-5_35-4_79_2_20:1.1.2.2
	Kernel-5_35-4_79_2_19:1.1.2.2
	Kernel-5_35-4_79_2_18:1.1.2.2
	Kernel-5_35-4_79_2_17:1.1.2.1
	Kernel-5_35-4_79_2_16:1.1.2.1
	Kernel-5_35-4_79_2_15:1.1.2.1
	HAL:1.1.0.2;
locks; strict;
comment	@# @;


1.4
date	2016.12.13.19.03.32;	author jlee;	state Exp;
branches;
next	1.3;
commitid	dvbJa4TQHit18Rxz;

1.3
date	2016.12.13.17.30.58;	author jlee;	state Exp;
branches;
next	1.2;
commitid	xwV3EbxXsXlhCQxz;

1.2
date	2016.06.30.20.07.36;	author jlee;	state Exp;
branches;
next	1.1;
commitid	IWoXxARWeuLDOwcz;

1.1
date	2001.01.12.13.52.12;	author mstephen;	state dead;
branches
	1.1.2.1;
next	;

1.1.2.1
date	2001.01.12.13.52.12;	author mstephen;	state Exp;
branches;
next	1.1.2.2;

1.1.2.2
date	2001.03.01.09.49.20;	author mstephen;	state Exp;
branches;
next	1.1.2.3;

1.1.2.3
date	2015.08.05.21.51.24;	author jlee;	state Exp;
branches;
next	1.1.2.4;
commitid	SpZpzVH47zb408wy;

1.1.2.4
date	2015.08.14.22.02.25;	author jlee;	state Exp;
branches;
next	1.1.2.5;
commitid	6gyfvmM0cNZULhxy;

1.1.2.5
date	2015.08.15.22.28.22;	author jlee;	state Exp;
branches;
next	1.1.2.6;
commitid	byHQvkwfp6VPSpxy;

1.1.2.6
date	2016.03.10.22.57.34;	author jlee;	state Exp;
branches;
next	;
commitid	DAXUqMY2ucjim9Yy;


desc
@@


1.4
log
@Make MMU_Changing ARMops perform the sub-operations in a sensible order
Detail:
  For a while we've known that the correct way of doing cache maintenance on ARMv6+ (e.g. when converting a page from cacheable to non-cacheable) is as follows:
  1. Write new page table entry
  2. Flush old entry from TLB
  3. Clean cache + drain write buffer
  The MMU_Changing ARMops (e.g. MMU_ChangingEntry) implement the last two items, but in the wrong order. This has caused the operations to fall out of favour and cease to be used, even in pre-ARMv6 code paths where the effects of improper cache/TLB management perhaps weren't as readily visible.
  This change re-specifies the relevant ARMops so that they perform their sub-operations in the correct order to make them useful on modern ARMs, updates the implementations, and updates the kernel to make use of the ops whereever relevant.
  File changes:
  - Docs/HAL/ARMop_API - Re-specify all the MMU_Changing ARMops to state that they are for use just after a page table entry has been changed (as opposed to before - e.g. 5.00 kernel behaviour). Re-specify the cacheable ones to state that the TLB invalidatation comes first.
  - s/ARM600, s/ChangeDyn, s/HAL, s/MemInfo, s/VMSAv6, s/AMBControl/memmap - Replace MMU_ChangingUncached + Cache_CleanInvalidate pairs with equivalent MMU_Changing op
  - s/ARMops - Update ARMop implementations to do everything in the correct order
  - s/MemMap2 - Update ARMop usage, and get rid of some lingering sledgehammer logic from ShuffleDoublyMappedRegionForGrow
Admin:
  Tested on pretty much everything currently supported


Version 5.70. Tagged as 'Kernel-5_70'
@
text
@12345678901234567890123456789012345678901234567890123456789012345678901234567890

mjs   12 Jan 2001   Early Draft
mjs   14 Feb 2001   XScale survey revised, ARMop reentrancy defined


RISC OS Kernel ARM core support
===============================

This document is concerned with the design of open ended support for
multiple ARM cores within the RISC OS kernel, as part of the work loosely
termed hardware abstraction. Note that the ARM core support is part of the
OS kernel, and so is not part of the hardware abstraction layer (HAL)
itself.

Background
----------

ARM core support (including caches and MMU) has historically been coded in a
tailored way for one or two specific variants. Since version 3.7 this has
meant just two variants; ARM 6/7 and StrongARM SA110. A more generic
approach is required for the next generation. This aims both to support
several cores in a more structured way, and to cover minor variants (eg.
cache size) with the same support code. The natural approach is to set up
run-time vectors to a set of ARM support routines.

Note that it is currently assumed that the ARM MMU architecture will not
change radically in future ARM cores. Hence, the kernel memory management
algorithms remain largely unchanged. This is believed to be a reasonable
assumption, since the last major memory management change was with Risc PC
and ARM 610 (when the on-chip MMU was introduced).

Note that all ARM support code must be 32-bit clean, as part of the 32-bit
clean kernel.

Survey of ARM core requirements
-------------------------------

At present, five broad ARM core types can be considered to be of interest;
ARM7 (and ARM6), ARM9, ARM10, StrongARM (SA1) and  XScale. These divide
primarily in terms of cache types, and cache and TLB maintenance
requirements. They also span a range of defined ARM architecture variants,
which introduced variants for system operations (primarily coprocessor 15
instructions).

The current ARM architecture is version 5. This (and version 4) has some
open ended definitions to allow code to determine cache size and types from
CP15 registers. Hence, the design of the support code can hope to be at
least tolerant of near future variations that are introduced.

ARM7
----

ARM7 cores may be architecture 3 or 4. They differ in required coprocessor
15 operations for the same cache and TLB control. ARM6 cores are much the
same as architecture 3 ARM7. The general character of all these cores is of
unified write-through caches that can only be invalidated on a global basis.
The TLBs are also unified, and can be invalidated per entry or globally.

ARM9
----

ARM9 cores are architecture 4. We ignore ARM9 variants without an MMU. The
kernel can read cache size and features. The ARM 920 or 922 have harvard
caches, with writeback and writethrough capable data caches (on a page or
section granularity). Data and instruction caches can be invalidated by
individual lines or globally. The data cache can be cleaned by virtual
address or cache segment/index, allowing for efficient cache maintenance.
Data and instruction TLBs can be invalidated by entry or globally.

ARM10
-----

ARM 10 is architecture 5. Few details available at present. Likely to be
similar to ARM9 in terms of cache features and available operations. 

StrongARM
---------

StrongARM is architecture 4. StrongARMs have harvard caches, the data cache
being writeback only (no writethrough option). The data cache can only be
globally cleaned in an indirect manner, by reading from otherwise unused
address space. This is inefficient because it requires external (to the
core) reads on the bus. In particular, the minimum cost of a clean, for a
nearly clean cache, is high. The data cache supports clean and invalidate by
individual virtual lines, so this is reasonably efficient for small ranges
of address. The data TLB can be invalidated by entry or globally.

The instruction cache can only be invalidated globally. This is inefficient
for cases such as IMBs over a small range (dynamic code). The instruction
TLB can only be invalidated globally.

Some StrongARM variants have a mini data cache. This is selected over the
main cache on a section or page by using the cachable/bufferable bits set to
C=1,B=0 in the MMU (this is not standard ARM architecture). The mini data
cache is writeback and must be cleaned in the same manner as the main data
cache.

XScale
------

XScale is architecture 5. It implements harvard caches, the data cache being
writeback or writethrough (on a page or section granularity). Data and
instruction caches can be invalidated by individual lines or globally. The
data cache can be fully cleaned by allocating lines from otherwise unused
address space. Unlike StrongARM, no external reads are needed for the clean
operation, so that cache maintenance is efficient.

XScale has a mini data cache. This is only available by using extension bits
in the MMU. This extension is not documented in the current manual for
architecture 5, but will presumably be properly recognised by ARM. It should
be a reasonably straightforward extension for RISC OS. The mini data cache
can only be cleaned by inefficient indirect reads as on StrongARM.

For XScale, the whole mini data cache can be configured as writethrough. The
most likely use for RISC OS is to map screen memory as mini cacheable, so
writethrough caching will be selected to prevent problems with delayed
screen update (and hence intricate screen/cache management code as in Ursula
for StrongARM). With writethrough configured, most operations can ignore the
mini cache, because invalidation by virtual address will invalidate mini or
main cache entries as appropriate. 

Unfortunately, for global cache invalidatation, things are very awkward.
RISC OS cannot use the global cache invalidate operation (which globally
invalidates both data caches), unless it is very careful to 100% clean the
main cache with all interrupts (IRQs and FIQs) disabled. This is to avoid
fatal loss of uncleaned lines from the writeback main cache. Disabling
interrupts for the duration of a main cache clean is an unacceptable
latency. Therefore, reluctantly, RISC OS must do the equivalent of cleaning
the mini cache (slow physical reads) in order to globally invalidate it as a
side effect.

The instruction and data TLBs can each be invalidated by entry or globally.


Kernel ARM operations
---------------------

This section lists the definitions and API of the set of ARM operations
(ARMops) required by the kernel for each major ARM type that is to be
supported. Some operations may be very simple on some ARMs. Others may need
support from the kernel environment - for example, readable parameters that
have been determined at boot, or address space available for cache clean
operations.

The general rules for register usage and preservation in calling these
ARMops iare:

  - any parameters are passed in r0,r1 etc. as required
  - r0 may be used as a scratch register
  - the routines see a valid stack via sp, at least 16 words are available
  - lr is the return link as required
  - on exit, all registers except r0 and lr must be preserved

Note that where register values are given as logical addresses, these are
RISC OS logical addresses. The equivalent ARM terminology is virtual address
(VA), or modified virtual address (MVA) for architectures with the fast
context switch extension.

Note also that where cache invalidation is required, it is implicit that any
associated operations for a particular ARM should be performed also. The
most obvious example is for an ARM with branch prediction, where it may be
necessary to invalidate a branch cache anywhere where instruction cache
invalidation is to be performed.

Any operation that is a null operation on the given ARM should be
implemented as a single return instruction:

  MOV pc, lr


ARMop reentrancy
----------------

In general, the operations will be called from SVC mode with interrupts
enabled. However, some use of some operations from interrupt mode is
expected. Notably, it is desirable for the IMB operations to be
available from interrupt mode. Therefore, it is intended that all
implementations of all ARMops be reentrant. Most will be so with no
difficulty. For ARMs with writeback data caches, the cleaning algorithm
may have to be constructed carefully to handle reentrancy (and to avoid
turning off interrupts for the duration of a clean).


Cache ARMops
------------

-- Cache_CleanInvalidateAll

The cache or caches are to be globally invalidated, with cleaning of any
writeback data being properly performed. 

   entry: -
   exit:  -

Note that any write buffer draining should also be performed by this
operation, so that memory is fully updated with respect to any writeaback
data.

The OS only expects the invalidation to be with respect to instructions/data
that are not involved in any currently active interrupts. In other words, it
is expected and desirable that interrupts remain enabled during any extended
clean operation, in order to avoid impact on interrupt latency.

-- Cache_CleanInvalidateRange

The cache or caches are to be invalidated for (at least) the given range, with
cleaning of any writeback data being properly performed. 

   entry: r0 = logical address of start of range
          r1 = logical address of end of range (exclusive)
          Note that r0 and r1 are aligned on cache line boundaries
   exit:  -

Note that any write buffer draining should also be performed by this
operation, so that memory is fully updated with respect to any writeaback
data.

The OS only expects the invalidation to be with respect to instructions/data
that are not involved in any currently active interrupts. In other words, it
is expected and desirable that interrupts remain enabled during any extended
clean operation, in order to avoid impact on interrupt latency.

-- Cache_CleanAll

The unified cache or data cache are to be globally cleaned (any writeback data
updated to memory). Invalidation is not required.

   entry: -
   exit:  -

Note that any write buffer draining should also be performed by this
operation, so that memory is fully updated with respect to any writeaback
data.

The OS only expects the cleaning to be with respect to data that are not
involved in any currently active interrupts. In other words, it is expected
and desirable that interrupts remain enabled during any extended clean
operation, in order to avoid impact on interrupt latency.

-- Cache_CleanRange

The cache or caches are to be cleaned for (at least) the given range.
Invalidation is not required.

   entry: r0 = logical address of start of range
          r1 = logical address of end of range (exclusive)
          Note that r0 and r1 are aligned on cache line boundaries
   exit:  -

Note that any write buffer draining should also be performed by this
operation, so that memory is fully updated with respect to any writeaback
data.

The OS only expects the invalidation to be with respect to instructions/data
that are not involved in any currently active interrupts. In other words, it
is expected and desirable that interrupts remain enabled during any extended
clean operation, in order to avoid impact on interrupt latency.

-- Cache_InvalidateAll

The cache or caches are to be globally invalidated. Cleaning of any writeback
data is not to be performed.

   entry: -
   exit:  -

This call is only required for special restart use, since it implies that
any writeback data are either irrelevant or not valid. It should be a very
simple operation on all ARMs.

-- Cache_InvalidateRange

The cache or caches are to be invalidated for the given range. Cleaning of any
writeback data is not to be performed.

   entry: r0 = logical address of start of range
          r1 = logical address of end of range (exclusive)
          Note that r0 and r1 are aligned on cache line boundaries
   exit:  -

This call is intended for use in situations where another bus master (e.g. DMA)
has written to an area of cacheable memory, and stale data is to be cleared
from the ARM's cache so that software can see the new values.

It is important that only the indicated region is invalidated - neighbouring
cache lines may contain valid data that has not yet been written back. Because
software should not have been writing to the DMA buffer while the DMA was in
progress, it is permissible for this operation to both clean and invalidate.
E.g. if a write-back cache is in use, it would be incorrect to promote a large
invalidate to a global invalidate, and so an implementation could instead
perform a global clean+invalidate.

The OS only expects the invalidation to be with respect to instructions/data
that are not involved in any currently active interrupts. In other words, it
is expected and desirable that interrupts remain enabled during any extended
clean operation, in order to avoid impact on interrupt latency.

-- Cache_RangeThreshold

Return a threshold value for an address range, above which it is advisable
to globally clean and/or invalidate caches, for performance reasons. For a
range less than or equal to the threshold, a ranged cache operation is
recommended.

   entry: -
   exit:  r0 = threshold value (bytes)

This call returns a value that the kernel may use to select between strategies
in some cache operations. This threshold may also be of use to some of the
ARM operations themselves (although they should typically be able to read
the parameter more directly).

The exact value is unlikely to be critical, but a sensible value may depend
on both the ARM and external factors such as memory bus speed.

-- Cache_Examine

Return information about a given cache level

   entry: r1 = cache level (0-based)
   exit:  r0 = Flags
               bits 0-2: cache type:
                  000 -> none
                  001 -> instruction
                  010 -> data
                  011 -> split
                  100 -> unified
                  1xx -> reserved
               Other bits: reserved
          r1 = D line length
          r2 = D size
          r3 = I line length
          r4 = I size
          r0-r4 = zero if cache level not present

For unified caches, r1-r2 will match r3-r4. This call mainly exists for the
benefit of OS_PlatformFeatures 33.

-- ICache_InvalidateAll

The instruction cache is to be globally invalidated.

   entry: -
   exit:  -

This operation should only act on instruction caches - not data or unified
caches. If only data or unified caches are present then the operation can be
implemented as a NOP.

-- ICache_InvalidateRange

The instruction cache is to be invalidated for the given range.

   entry: r0 = logical address of start of range
          r1 = logical address of end of range (exclusive)
          Note that r0 and r1 are aligned on cache line boundaries
   exit:  -

This operation should only act on instruction caches - not data or unified
caches. If only data or unified caches are present then the operation can be
implemented as a NOP.


Memory barrier ARMops
=====================

-- DSB_ReadWrite (previously, WriteBuffer_Drain)

This call is roughly equivalent to the ARMv7 "DSB SY" instruction:

 * Writebuffers are drained
 * Full read/write barrier - no data load/store will cross the instruction
 * Instructions following the barrier will only begin execution once the
   barrier is passed - but any prefetched instructions are not flushed

   entry: -
   exit:  -


-- DSB_Write

This call is roughly equivalent to the ARMv7 "DSB ST" instruction:

 * Writebuffers are drained
 * Write barrier - reads may cross the instruction
 * Instructions following the barrier will only begin execution once the
   barrier is passed - but any prefetched instructions are not flushed

   entry: -
   exit:  -


-- DSB_Read

There is no direct equivalent to this in ARMv7 (barriers are either W or RW).
However it's useful to define a read barrier, as (e.g.) on Cortex-A9 a RW
barrier would require draining the write buffer of the external PL310 cache,
while a R barrier can simply be an ordinary DSB instruction.

 * Read barrier - writes may cross the instruction
 * Instructions following the barrier will only begin execution once the
   barrier is passed - but any prefetched instructions are not flushed

   entry: -
   exit:  -


-- DMB_ReadWrite

This call is roughly equivalent to the ARMv7 "DMB SY" instruction:

 * Ensures in-order operation of data load/store instructions
 * Does not stall instruction execution
 * Does not guarantee that any preceeding memory operations complete in a
   timely manner (or at all)

   entry: -
   exit:  -

Although this call doesn't guarantee that any memory operation completes, it's
usually all that's required when interacting with hardware devices which use
memory-mapped IO. E.g. fill a buffer with data, issue a DMB, then write to a
hardware register to start some external DMA. The writes to the buffer will
have been guaranteed to complete by the time the write to the hardware register
completes.


-- DMB_Write

This call is roughly equivalent to the ARMv7 "DMB ST" instruction:

 * Ensures in-order operation of data store instructions
 * Does not stall instruction execution
 * Does not guarantee that any preceeding memory operations complete in a
   timely manner (or at all)

   entry: -
   exit:  -

Although this call doesn't guarantee that any memory operation completes, it's
usually all that's required when interacting with hardware devices which use
memory-mapped IO. E.g. fill a buffer with data, issue a DMB, then write to a
hardware register to start some external DMA. The writes to the buffer will
have been guaranteed to complete by the time the write to the hardware register
completes.


-- DMB_Read

There is no direct equivalent to this in ARMv7 (barriers are either W or RW).
However it's useful to define a read barrier, as (e.g.) on Cortex-A9 a RW
barrier would require draining the write buffer of the external PL310 cache,
while a R barrier can simply be an ordinary DMB instruction.

 * Ensures in-order operation of data load instructions
 * Does not stall instruction execution
 * Does not guarantee that any preceeding memory operations complete in a
   timely manner (or at all)

   entry: -
   exit:  -

Although this call doesn't guarantee that any memory operation completes, it's
usually all that's required when interacting with hardware devices which use
memory-mapped IO. E.g. after reading a hardware register to detect that a DMA
write to RAM has completed, issue a read barrier to ensure that any reads from
the data buffer see the final data.


TLB ARMops
----------

-- TLB_InvalidateAll

The TLB or TLBs are to be globally invalidated.

   entry: -
   exit:  -


-- TLB_InvalidateEntry

The TLB or TLBs are to be invalidated for the entry at the given logical
address.

   entry: r0 = logical address of entry to invalidate (page aligned)
   exit:  -

The address will always be page aligned (4k).


IMB ARMops
----------

-- IMB_Full

A global instruction memory barrier (IMB) is to be performed.

   entry: -
   exit:  -

An IMB is an operation that should be performed after new instructions have
been stored and before they are executed. It guarantees correct operation
for code modification (eg. something as simple as loading code to be
executed).

On some ARMs, this operation may be null. On ARMs with harvard architecture
this typically consists of:

  1) clean data cache
  2) drain write buffer
  3) invalidate instruction cache

There may be other considerations such as invalidating branch prediction
caches.


-- IMB_Range

An instruction memory barrier (IMB) is to be performed over a logical
address range.

   entry: r0 = logical address of start of range
          r1 = logical address of end of range (exclusive)
          Note that r0 and r1 are aligned on cache line boundaries
   exit: -

An IMB is an operation that should be performed after new instructions have
been stored and before they are executed. It guarantees correct operation
for code modification (eg. something as simple as loading code to be
executed).

On some ARMs, this operation may be null. On ARMs with harvard architecture
this typically consists of:

  1) clean data cache over the range
  2) drain write buffer
  3) invalidate instruction cache over the range

There may be other considerations such as invalidating branch prediction
caches.

Note that the range may be very large. The implementation of this call is
typically expected to use a threshold (related to Cache_RangeThreshold) to
decide when to perform IMB_Full instead, being faster for large ranges.


-- IMB_List

A variant of IMB_Range that accepts a list of address ranges.

   entry: r0 = pointer to word-aligned list of (start, end) address pairs
          r1 = pointer to end of list (past last valid entry)
          r2 = total amount of memory to be synchronised

If you have several areas to synchronise then using this call may result in
significant performance gains, both from reducing the function call overhead
and from optimisations in the algorithm itself (e.g. only flushing instruction
cache once for StrongARM).

As with IMB_Range, start & end addresses are inclusive-exclusive and must be
cache line aligned. The list must contain at least one entry, and must not
contain zero-length entries.


MMU mapping ARMops
------------------

-- MMU_Changing

The global MMU mapping has just changed.

   entry: -
   exit:  -

The operation must typically perform the following:

  1) globally invalidate TLB or TLBs
  2) globally clean and invalidate all caches
  3) drain write buffer

Note that it should not be necessary to disable IRQs. The OS ensures that
remappings do not affect currently active interrupts.

This operation should typically be used when a large number of cacheable pages
have had their attributes changed in a way which will affect cache behaviour.

-- MMU_ChangingEntry

The MMU mapping has just changed for a single page entry (4k).

   entry: r0 = logical address of entry (page aligned)
   exit:  -

The operation must typically perform the following:

  1) invalidate TLB or TLBs for the entry
  2) clean and invalidate all caches over the 4k range of the page
  3) drain write buffer

Note that it should not be necessary to disable IRQs. The OS ensures that
remappings do not affect currently active interrupts.

This operation should typically be used when a cacheable page has had its
attributes changed in a way which will affect cache behaviour.

-- MMU_ChangingUncached

The MMU mapping has just changed in a way that globally affects uncacheable
space.

   entry: -
   exit:  -

The operation must typically globally invalidate the TLB or TLBs. The OS
guarantees that cacheable space is not affected, so cache operations are not
required. However, there may still be considerations such as fill buffers
that operate in uncacheable space on some ARMs.

-- MMU_ChangingUncachedEntry

The MMU mapping has just changed for a single uncacheable page entry (4k).

   entry: r0 = logical address of entry (page aligned)
   exit:  -

The operation must typically invalidate the TLB or TLBs for the entry. The
OS guarantees that cacheable space is not affected, so cache operations are
not required. However, there may still be considerations such as fill
buffers that operate in uncacheable space on some ARMs.


-- MMU_ChangingEntries

The MMU mapping has just changed for a contiguous range of page entries
(multiple of 4k).

   entry: r0 = logical address of first page entry (page aligned)
          r1 = number of page entries ( >= 1)
   exit:  -

The operation must typically perform the following:

  1) invalidate TLB or TLBs over the range of the entries
  2) clean and invalidate all caches over the range of the pages
  3) drain write buffer

Note that it should not be necessary to disable IRQs. The OS ensures that
remappings do not affect currently active interrupts.

Note that the number of entries may be large. The operation is typically
expected to use a reasonable threshold, above which it performs a global
operation instead for speed reasons.

This operation should typically be used when cacheable pages have had their
attributes changed in a way which will affect cache behaviour.

-- MMU_ChangingUncachedEntries

The MMU mapping has just changed for a contiguous range of uncacheable page
entries (multiple of 4k).

   entry: r0 = logical address of first page entry (page aligned)
          r1 = number of page entries ( >= 1)
   exit:  -

The operation must typically invalidate the TLB or TLBs over the range of
the entries. The OS guarantees that cacheable space is not affected, so
cache operations are not required. However, there may still be
considerations such as fill buffers that operate in uncacheable space on
some ARMs.

Note that the number of entries may be large. The operation is typically
expected to use a reasonable threshold, above which it performs a global
operation instead for speed reasons.
@


1.3
log
@Add new ARMops. Add macros which map the ARMv7/v8 cache/TLB maintenance mnemonics (as featured in recent ARM ARMs) to MCR ops.
Detail:
  - Docs/HAL/ARMop_API - Document the new ARMops. These ops are intended to help with future work (DMA without OS_Memory 0 "make temp uncacheable", and minimising cache maintenance when unmapping pages) and aren't in use just yet.
  - hdr/Copro15ops - Add new macros for ARMv7+ which map the mnemonics seen in recent ARM ARMs to the corresponding MCR ops. This should make things easier when cross-referencing docs and reduce the risk of typos.
  - hdr/KernelWS - Shuffle kernel workspace a bit to make room for the new ARMops
  - hdr/OSMisc - Expose new ARMops via OS_MMUControl 2
  - s/ARMops - Implement the new ARMops. Change the ARMv7+ ARMops to use the new mnemonic macros. Also get rid of myDSB / myISB usage from ARMv7+ code paths; use DSB/ISB/etc. directly to ensure correct behaviour
  - s/HAL - Mnemonic + ISB/DSB updates. Change software RAM clear to do 16 bytes at a time for kernel workspace instead of 32 to allow the kernel workspace tweaks to work.
Admin:
  Binary diff shows that mnemonics map to the original MCR ops correctly
  Note: Raspberry Pi builds will now emit lots of warnings due to increased DSB/ISB instruction use. However it should be safe to ignore these as they should only be present in v7+ code paths.
  Note: New ARMops haven't been tested yet, will be disabled (or at least hidden from user code) in a future checkin


Version 5.68. Tagged as 'Kernel-5_68'
@
text
@d572 1
a572 1
The global MMU mapping is about to be changed.
d579 3
a581 3
  1) globally clean and invalidate all caches
  2) drain write buffer
  3) globally invalidate TLB or TLBs
d586 3
d591 1
a591 1
The MMU mapping is about to be changed for a single page entry (4k).
d598 3
a600 3
  1) clean and invalidate all caches over the 4k range of the page
  2) drain write buffer
  3) invalidate TLB or TLBs for the entry
d605 3
d610 2
a611 2
The MMU mapping is about to be changed in a way that globally affects
uncacheable space.
d623 1
a623 2
The MMU mapping is about to be changed for a single uncacheable page entry
(4k).
d636 2
a637 2
The MMU mapping is about to be changed for a contiguous range of page
entries (multiple of 4k).
d645 3
a647 3
  1) clean and invalidate all caches over the range of the pages
  2) drain write buffer
  3) invalidate TLB or TLBs over the range of the entries
d656 3
d661 2
a662 2
The MMU mapping is about to be changed for a contiguous range of uncacheable
page entries (multiple of 4k).
@


1.2
log
@Merge HAL branch to trunk
Detail:
  This change merges the past 15+ years of HAL branch development back to the trunk.
  This is effectively the end for non-HAL builds of the kernel, as no attempt has been made to maintain it during this merge, and all non-HAL & non-32bit code will soon be removed anyway.
  Rather than list everything that's been added to the HAL branch, it's easier to describe the change in terms of the things that the HAL branch was lacking:
  * Trunk version of Docs/32bit contained updated comments for the SVC stack structure during ErrorV
  * Trunk version of s/HeapMan contained a tweak to try and reduce the number of small free blocks that are created
  * Trunk version of s/Kernel contained a change to only copy 248 bytes of the error string to the error buffer (down from 252 bytes), to take into account the extra 4 bytes needed by the PSR. However this goes against the decision that's been made in the HAL branch that the error buffer should be enlarged to 260 bytes instead (ref: https://www.riscosopen.org/tracker/tickets/201), so the HAL build will retain its current behaviour.
  * Trunk version of s/MsgCode had RMNot32bit error in the list of error messages to count when countmsgusage {TRUE}
  * Trunk version of s/PMF/i2cutils contained support for OS_Memory 5, "read/write value of NVRamWriteSize". Currently the HAL branch doesn't have a use for this (in particular, the correct NVRamWriteSize should be specified by the HAL, so there should be no need for software to change it at runtime), and so this code will remain switched out in the HAL build.
Admin:
  Tested on Raspberry Pi


Version 5.48. Tagged as 'Kernel-5_48'
@
text
@d241 19
d272 27
d340 24
@


1.1
log
@file ARMop_API was initially added on branch HAL.
@
text
@d1 598
@


1.1.2.1
log
@kernel now attempts to substitute video mode numbers in face of
h/w with limited bits-per-pixel support (not tested yet)
HAL_API document added - early draft only, of interest to those
writing or modifying HALs for new h/w
ARMop_API document added - early draft only, of interest only
to those modifying kernel to support new ARM cores
*** polite comments on HAL_API welcome ***

Version 5.35, 4.79.2.15. Tagged as 'Kernel-5_35-4_79_2_15'
@
text
@a0 440
12345678901234567890123456789012345678901234567890123456789012345678901234567890

mjs   12 Jan 2001   Early Draft


RISC OS Kernel ARM core support
===============================

This document is concerned with the design of open ended support for
multiple ARM cores within the RISC OS kernel, as part of the work loosely
termed hardware abstraction. Note that the ARM core support is part of the
OS kernel, and so is not part of the hardware abstraction layer (HAL)
itself.

Background
----------

ARM core support (including caches and MMU) has historically been coded in a
tailored way for one or two specific variants. Since version 3.7 this has
meant just two variants; ARM 6/7 and StrongARM SA110. A more generic
approach is required for the next generation. This aims both to support
several cores in a more structured way, and to cover minor variants (eg.
cache size) with the same support code. The natural approach is to set up
run-time vectors to a set of ARM support routines.

Note that it is currently assumed that the ARM MMU architecture will not
change radically in future ARM cores. Hence, the kernel memory management
algorithms remain largely unchanged. This is believed to be a reasonable
assumption, since the last major memory management change was with Risc PC
and ARM 610 (when the on-chip MMU was introduced).

Note that all ARM support code must be 32-bit clean, as part of the 32-bit
clean kernel.

Survey of ARM core requirements
-------------------------------

At present, five broad ARM core types can be considered to be of interest;
ARM7 (and ARM6), ARM9, ARM10, StrongARM (SA1) and  XScale. These divide
primarily in terms of cache types, and cache and TLB maintenance
requirements. They also span a range of defined ARM architecture variants,
which introduced variants for system operations (primarily coprocessor 15
instructions).

The current ARM architecture is version 5. This (and version 4) has some
open ended definitions to allow code to determine cache size and types from
CP15 registers. Hence, the design of the support code can hope to be at
least tolerant of near future variations that are introduced.

ARM7
----

ARM7 cores may be architecture 3 or 4. They differ in required coprocessor
15 operations for the same cache and TLB control. ARM6 cores are much the
same as architecture 3 ARM7. The general character of all these cores is of
unified write-through caches that can only be invalidated on a global basis.
The TLBs are also unified, and can be invalidated per entry or globally.

ARM9
----

ARM9 cores are architecture 4. We ignore ARM9 variants without an MMU. The
kernel can read cache size and features. The ARM 920 or 922 have harvard
caches, with writeback and writethrough capable data caches (on a page or
section granularity). Data and instruction caches can be invalidated by
individual lines or globally. The data cache can be cleaned by virtual
address or cache segment/index, allowing for efficient cache maintenance.
Data and instruction TLBs can be invalidated by entry or globally.

ARM10
-----

ARM 10 is architecture 5. Few details available at present. Likely to be
similar to ARM9 in terms of cache features and available operations. 

StrongARM
---------

StrongARM is architecture 4. StrongARMs have harvard caches, the data cache
being writeback only (no writethrough option). The data cache can only be
globally cleaned in an indirect manner, by reading from otherwise unused
address space. This is inefficient because it requires external (to the
core) reads on the bus. In particular, the minimum cost of a clean, for a
nearly clean cache, is high. The data cache supports clean and invalidate by
individual virtual lines, so this is reasonably efficient for small ranges
of address. The data TLB can be invalidated by entry or globally.

The instruction cache can only be invalidated globally. This is inefficient
for cases such as IMBs over a small range (dynamic code). The instruction
TLB can only be invalidated globally.

Some StrongARM variants have a mini data cache. This is selected over the
main cache on a section or page by using the cachable/bufferable bits set to
C=1,B=0 in the MMU (this is not standard ARM architecture). The mini data
cache is writeback and must be cleaned in the same manner as the main data
cache.

XScale
------

XScale is architecture 5. It implements harvard caches, the data cache being
writeback or writethrough (on a page or section granularity). Data and
instruction caches can be invalidated by individual lines or globally. The
data cache can be fully cleaned by allocating lines from otherwise unused
address space. Unlike StrongARM, no external reads are needed for the clean
operation, so that cache maintenance is efficient.

XScale has a mini data cache. This is only available by using extension bits
in the MMU. This extension is not documented in the current manual for
architecture 5, but will presumably be properly recognised by ARM. It should
be a reasonably straightforward extension for RISC OS. The mini data cache
can only be cleaned by inefficient indirect reads as on StrongARM. However,
for XScale, the whole mini data cache can be configured as writethrough to
obviate this problem. The most likely use for RISC OS is to map screen
memory as mini cacheable, when writethrough caching will also be highly
desirable to prevent delayed screen update.

The instruction and data TLBs can each be invalidated by entry or globally.


Kernel ARM operations
---------------------

This section lists the definitions and API of the set of ARM operations
required by the kernel for each major ARM type that is to be supported. Some
operations may be very simple on some ARMs. Others may need support from the
kernel environment - for example, readable parameters that have been
determined at boot, or address space available for cache clean operations.

The general rules for register usage and preservation in calling these
operations is:

  - any parameters are passed in r0,r1 etc. as required
  - r0 may be used as a scratch register
  - the routines see a valid stack via sp, at least 16 words are available
  - lr is the return link as required
  - on exit, all registers except r0 and lr must be preserved

Note that where register values are given as logical addresses, these are
RISC OS logical addresses. The equivalent ARM terminology is virtual address
(VA), or modified virtual address (MVA) for architectures with the fast
context switch extension.

Note also that where cache invalidation is required, it is implicit that any
associated operations for a particular ARM should be performed also. The
most obvious example is for an ARM with branch prediction, where it may be
necessary to invalidate a branch cache anywhere where instruction cache
invalidation is to be performed.

Any operation that is a null operation on the given ARM should be
implemented as a single return instruction:

  MOV pc, lr


-- Cache_CleanInvalidateAll

The cache or caches are to be globally invalidated, with cleaning of any
writeback data being properly performed. 

   entry: -
   exit:  -

   IRQs are enabled
   call is not reentrant

Note that any write buffer draining should also be performed by this
operation, so that memory is fully updated with respect to any writeaback
data.

The OS only expects the invalidation to be with respect to instructions/data
that are not involved in any currently active interrupts. In other words, it
is expected and desirable that interrupts remain enabled during any extended
clean operation, in order to avoid impact on interrupt latency.

-- Cache_CleanAll

The unified cache or data cache are to be globally cleaned (any writeback data
updated to memory). Invalidation is not required.

   entry: -
   exit:  -

   IRQs are enabled
   call is not reentrant

Note that any write buffer draining should also be performed by this
operation, so that memory is fully updated with respect to any writeaback
data.

The OS only expects the cleaning to be with respect to data that are not
involved in any currently active interrupts. In other words, it is expected
and desirable that interrupts remain enabled during any extended clean
operation, in order to avoid impact on interrupt latency.

-- Cache_InvalidateAll

The cache or caches are to be globally invalidated. Cleaning of any writeback
data is not to be performed.

   entry: -
   exit:  -

   IRQs are enabled
   call is not reentrant

This call is only required for special restart use, since it implies that
any writeback data are either irrelevant or not valid. It should be a very
simple operation on all ARMs.

-- Cache_RangeThreshold

Return a threshold value for an address range, above which it is advisable
to globally clean and/or invalidate caches, for performance reasons. For a
range less than or equal to the threshold, a ranged cache operation is
recommended.

   entry: -
   exit:  r0 = threshold value (bytes)

   IRQs are enabled
   call is not reentrant

This call returns a value that the kernel may use to select between strategies
in some cache operations. This threshold may also be of use to some of the
ARM operations themselves (although they should typically be able to read
the parameter more directly).

The exact value is unlikely to be critical, but a sensible value may depend
on both the ARM and external factors such as memory bus speed.


-- TLB_InvalidateAll

The TLB or TLBs are to be globally invalidated.

   entry: -
   exit:  -

   IRQs are enabled
   call is not reentrant

-- TLB_InvalidateEntry

The TLB or TLBs are to be invalidated for the entry at the given logical
address.

   entry: r0 = logical address of entry to invalidate (page aligned)
   exit:  -

   IRQs are enabled
   call is not reentrant

The address will always be page aligned (4k).

-- WriteBuffer_Drain

Any writebuffers are to be drained so that any pending writes are guaranteed
completed to memory.

   entry: -
   exit:  -

   IRQs are enabled
   call is not reentrant

-- IMB_Full

A global instruction memory barrier (IMB) is to be performed.

   entry: -
   exit:  -

   IRQs are enabled
   call is not reentrant

An IMB is an operation that should be performed after new instructions have
been stored and before they are executed. It guarantees correct operation
for code modification (eg. something as simple as loading code to be
executed).

On some ARMs, this operation may be null. On ARMs with harvard architecture
this typically consists of:

  1) clean data cache
  2) drain write buffer
  3) invalidate instruction cache

There may be other considerations such as invalidating branch prediction
caches.

-- IMB_Range

An instruction memory barrier (IMB) is to be performed over a logical
address range.

   entry: r0 = logical address of start of range
          r1 = logical address of end of range (exclusive)
          Note that r0 and r1 are aligned on cache line boundaries
   exit: -

   IRQs are enabled
   call is not reentrant

An IMB is an operation that should be performed after new instructions have
been stored and before they are executed. It guarantees correct operation
for code modification (eg. something as simple as loading code to be
executed).

On some ARMs, this operation may be null. On ARMs with harvard architecture
this typically consists of:

  1) clean data cache over the range
  2) drain write buffer
  3) invalidate instruction cache over the range

There may be other considerations such as invalidating branch prediction
caches.

Note that the range may be very large. The implementation of this call is
typically expected to use a threshold (related to Cache_RangeThreshold) to
decide when to perform IMB_Full instead, being faster for large ranges.

-- MMU_Changing

The global MMU mapping is about to be changed.

   entry: -
   exit:  -

   IRQs are enabled
   call is not reentrant

The operation must typically perform the following:

  1) globally clean and invalidate all caches
  2) drain write buffer
  3) globally invalidate TLB or TLBs

Note that it should not be necessary to disable IRQs. The OS ensures that
remappings do not affect currently active interrupts.

-- MMU_ChangingEntry

The MMU mapping is about to be changed for a single page entry (4k).

   entry: r0 = logical address of entry (page aligned)
   exit:  -

   IRQs are enabled
   call is not reentrant

The operation must typically perform the following:

  1) clean and invalidate all caches over the 4k range of the page
  2) drain write buffer
  3) invalidate TLB or TLBs for the entry

Note that it should not be necessary to disable IRQs. The OS ensures that
remappings do not affect currently active interrupts.

-- MMU_ChangingUncached

The MMU mapping is about to be changed in a way that globally affects
uncacheable space.

   entry: -
   exit:  -

   IRQs are enabled
   call is not reentrant

The operation must typically globally invalidate the TLB or TLBs. The OS
guarantees that cacheable space is not affected, so cache operations are not
required. However, there may still be considerations such as fill buffers
that operate in uncacheable space on some ARMs.

-- MMU_ChangingUncachedEntry

The MMU mapping is about to be changed for a single uncacheable page entry
(4k).

   entry: r0 = logical address of entry (page aligned)
   exit:  -

   IRQs are enabled
   call is not reentrant

The operation must typically invalidate the TLB or TLBs for the entry. The
OS guarantees that cacheable space is not affected, so cache operations are
not required. However, there may still be considerations such as fill
buffers that operate in uncacheable space on some ARMs.


-- MMU_ChangingEntries

The MMU mapping is about to be changed for a contiguous range of page
entries (multiple of 4k).

   entry: r0 = logical address of first page entry (page aligned)
          r1 = number of page entries ( >= 1)
   exit:  -

   IRQs are enabled
   call is not reentrant

The operation must typically perform the following:

  1) clean and invalidate all caches over the range of the pages
  2) drain write buffer
  3) invalidate TLB or TLBs over the range of the entries

Note that it should not be necessary to disable IRQs. The OS ensures that
remappings do not affect currently active interrupts.

Note that the number of entries may be large. The operation is typically
expected to use a reasonable threshold, above which it performs a global
operation instead for speed reasons.

-- MMU_ChangingUncachedEntries

The MMU mapping is about to be changed for a contiguous range of uncacheable
page entries (multiple of 4k).

   entry: r0 = logical address of first page entry (page aligned)
          r1 = number of page entries ( >= 1)
   exit:  -

   IRQs are enabled
   call is not reentrant

The operation must typically invalidate the TLB or TLBs over the range of
the entries. The OS guarantees that cacheable space is not affected, so
cache operations are not required. However, there may still be
considerations such as fill buffers that operate in uncacheable space on
some ARMs.

Note that the number of entries may be large. The operation is typically
expected to use a reasonable threshold, above which it performs a global
operation instead for speed reasons.
@


1.1.2.2
log
@minor changes towards Chrontel support for Customer L demo
untested

Version 5.35, 4.79.2.18. Tagged as 'Kernel-5_35-4_79_2_18'
@
text
@a3 1
mjs   14 Feb 2001   XScale survey revised, ARMop reentrancy defined
d112 5
a116 19
can only be cleaned by inefficient indirect reads as on StrongARM.

For XScale, the whole mini data cache can be configured as writethrough. The
most likely use for RISC OS is to map screen memory as mini cacheable, so
writethrough caching will be selected to prevent problems with delayed
screen update (and hence intricate screen/cache management code as in Ursula
for StrongARM). With writethrough configured, most operations can ignore the
mini cache, because invalidation by virtual address will invalidate mini or
main cache entries as appropriate. 

Unfortunately, for global cache invalidatation, things are very awkward.
RISC OS cannot use the global cache invalidate operation (which globally
invalidates both data caches), unless it is very careful to 100% clean the
main cache with all interrupts (IRQs and FIQs) disabled. This is to avoid
fatal loss of uncleaned lines from the writeback main cache. Disabling
interrupts for the duration of a main cache clean is an unacceptable
latency. Therefore, reluctantly, RISC OS must do the equivalent of cleaning
the mini cache (slow physical reads) in order to globally invalidate it as a
side effect.
d125 4
a128 5
(ARMops) required by the kernel for each major ARM type that is to be
supported. Some operations may be very simple on some ARMs. Others may need
support from the kernel environment - for example, readable parameters that
have been determined at boot, or address space available for cache clean
operations.
d131 1
a131 1
ARMops iare:
a155 16
ARMop reentrancy
----------------

In general, the operations will be called from SVC mode with interrupts
enabled. However, some use of some operations from interrupt mode is
expected. Notably, it is desirable for the IMB operations to be
available from interrupt mode. Therefore, it is intended that all
implementations of all ARMops be reentrant. Most will be so with no
difficulty. For ARMs with writeback data caches, the cleaning algorithm
may have to be constructed carefully to handle reentrancy (and to avoid
turning off interrupts for the duration of a clean).


Cache ARMops
------------

d164 3
d184 3
d204 3
d221 3
a232 12
-- WriteBuffer_Drain

Any writebuffers are to be drained so that any pending writes are guaranteed
completed to memory.

   entry: -
   exit:  -


TLB ARMops
----------

d240 2
d251 3
d256 7
d264 2
a265 2
IMB ARMops
----------
d274 3
a291 1

d302 3
a323 4

MMU mapping ARMops
------------------

d331 3
d350 3
d370 3
d386 3
d404 3
d428 3
@


1.1.2.3
log
@Improve support for VMSAv6 cache policies & memory types. Expose raw ARMops via OS_MMUControl & cache information via OS_PlatformFeatures.
Detail:
  Docs/HAL/ARMop_API - Document two new ARMops: Cache_Examine and IMB_List
  hdr/KernelWS - Shuffle workspace round a bit to allow space for the two new ARMops. IOSystemType now deleted (has been deprecated and fixed at 0 for some time)
  s/ARM600 - Cosmetic changes to BangCam to make it clearer what's going on. Add OS_MMUControl 2 (get ARMop) implementation.
  s/ARMops - Switch out different ARMop implementations and XCB tables depending on MMU model - helps reduce assembler warnings and make it clearer what code paths are and aren't possible. Add implementations of the two new ARMops. Simplify ARM_Analyse_Fancy by removing some tests which we know will have certain results. Use CCSIDR constants in ARMv7 ARMops instead of magic numbers. Update XCB table comments, and add a new table for VMSAv6
  s/ChangeDyn - Define constant for the new NCB 'idempotent' cache policy (VMSAv6 normal, non-cacheable memory)
  s/HAL - Use CCSIDR constants instead of magic numbers. Extend RISCOS_MapInIO to allow the TEX bits to be specified.
  s/Kernel - OS_PlatformFeatures 33 (read cache information) implementation (actually, just calls through to an ARMop)
  s/MemInfo - Modify VMSAv6 OS_Memory 0 cache/uncache implementation to use the XCB table instead of modifying L2_C directly. This allows the cacheability to be changed without affecting the memory type - important for e.g. unaligned accesses to work correctly. Implement cache policy support for OS_Memory 13.
  s/Middle - Remove IOSystemType from OS_ReadSysInfo 6.
  s/VMSAv6 - Make sure BangCam uses the XCB table for working out the attributes of temp-uncacheable pages instead of manipulating L2_C directly. Add OS_MMUControl 2 implementation.
  s/AMBControl/memmap - Update VMSAv6 page table pokeing to use XCB table
  s/PMF/osinit - Remove IOSystemType reference, and switch out some pre-HAL code that was trying to use IOSystemType.
Admin:
  Tested on Iyonix, ARM11, Cortex-A7, -A8, -A9, -A15
  Note that contrary to the comments in the source the default NCB policy currently maps to VMSAv6 Device memory type (as per previous kernel versions). This is just a temporary measure, and it will be switched over to Normal, non-cacheable once appropriate memory barriers have been added to the affected IO code.


Version 5.35, 4.79.2.273. Tagged as 'Kernel-5_35-4_79_2_273'
@
text
@a251 23
-- Cache_Examine

Return information about a given cache level

   entry: r1 = cache level (0-based)
   exit:  r0 = Flags
               bits 0-2: cache type:
                  000 -> none
                  001 -> instruction
                  010 -> data
                  011 -> split
                  100 -> unified
                  1xx -> reserved
               Other bits: reserved
          r1 = D line length
          r2 = D size
          r3 = I line length
          r4 = I size
          r0-r4 = zero if cache level not present

For unified caches, r1-r2 will match r3-r4. This call mainly exists for the
benefit of OS_PlatformFeatures 33.

a339 18
-- IMB_List

A variant of IMB_Range that accepts a list of address ranges.

   entry: r0 = pointer to word-aligned list of (start, end) address pairs
          r1 = pointer to end of list (past last valid entry)
          r2 = total amount of memory to be synchronised

If you have several areas to synchronise then using this call may result in
significant performance gains, both from reducing the function call overhead
and from optimisations in the algorithm itself (e.g. only flushing instruction
cache once for StrongARM).

As with IMB_Range, start & end addresses are inclusive-exclusive and must be
cache line aligned. The list must contain at least one entry, and must not
contain zero-length entries.


@


1.1.2.4
log
@Replace WriteBuffer_Drain ARMop with a suite of memory barrier ARMops
Detail:
  - Docs/HAL/ARMop_API - Updated with documentation for the new ARMops.
  - s/ARMops - Set up pointers for the new memory barrier ARMops. Add full implementations for ARMv6 & ARMv7; older architectures should be able to get by with a mix of null ops & write buffer drain ops. Update ARMopPtrTable to validate structure against the list in hdr/OSMisc
  - hdr/KernelWS - Reserve workspace for new ARMops. Free up a bit of space by limiting ourselves to 2 cache levels with ARMv7. Remove some unused definitions.
  - hdr/OSMisc - New header defining OS_PlatformFeatures & OS_MMUControl reason codes, OS_PlatformFeatures 0 flags, and OS_MMUControl 2 ARMop indices
  - Makefile - Add export rules for OSMisc header
  - hdr/ARMops, s/ARM600, s/VMSAv6 - Remove CPUFlag_* and MMUCReason_* definitions. Update OS_MMUControl write buffer drain to use DSB_ReadWrite ARMop (which is what most existing write buffer drain implementations have been renamed to).
  - s/GetAll - Get Hdr:OSMisc
  - s/Kernel - Use OS_PlatformFeatures reason code symbols
  - s/vdu/vdudecl - Remove unused definition
Admin:
  Tested on ARM11, Cortex-A8, Cortex-A9


Version 5.35, 4.79.2.279. Tagged as 'Kernel-5_35-4_79_2_279'
@
text
@d276 1
a276 2
Memory barrier ARMops
=====================
d278 2
a279 30
-- DSB_ReadWrite (previously, WriteBuffer_Drain)

This call is roughly equivalent to the ARMv7 "DSB SY" instruction:

 * Writebuffers are drained
 * Full read/write barrier - no data load/store will cross the instruction
 * Instructions following the barrier will only begin execution once the barrier is passed - but any prefetched instructions are not flushed

   entry: -
   exit:  -


-- DSB_Write

This call is roughly equivalent to the ARMv7 "DSB ST" instruction:

 * Writebuffers are drained
 * Write barrier - reads may cross the instruction
 * Instructions following the barrier will only begin execution once the barrier is passed - but any prefetched instructions are not flushed

   entry: -
   exit:  -


-- DSB_Read

There is no direct equivalent to this in ARMv7 (barriers are either W or RW). However it's useful to define a read barrier, as (e.g.) on Cortex-A9 a RW barrier would require draining the write buffer of the external PL310 cache, while a R barrier can simply be an ordinary DSB instruction.

 * Read barrier - writes may cross the instruction
 * Instructions following the barrier will only begin execution once the barrier is passed - but any prefetched instructions are not flushed
a284 41
-- DMB_ReadWrite

This call is roughly equivalent to the ARMv7 "DMB SY" instruction:

 * Ensures in-order operation of data load/store instructions
 * Does not stall instruction execution
 * Does not guarantee that any preceeding memory operations complete in a timely manner (or at all)

   entry: -
   exit:  -

Although this call doesn't guarantee that any memory operation completes, it's usually all that's required when interacting with hardware devices which use memory-mapped IO. E.g. fill a buffer with data, issue a DMB, then write to a hardware register to start some external DMA. The writes to the buffer will have been guaranteed to complete by the time the write to the hardware register completes.

-- DMB_Write

This call is roughly equivalent to the ARMv7 "DMB ST" instruction:

 * Ensures in-order operation of data store instructions
 * Does not stall instruction execution
 * Does not guarantee that any preceeding memory operations complete in a timely manner (or at all)

   entry: -
   exit:  -

Although this call doesn't guarantee that any memory operation completes, it's usually all that's required when interacting with hardware devices which use memory-mapped IO. E.g. fill a buffer with data, issue a DMB, then write to a hardware register to start some external DMA. The writes to the buffer will have been guaranteed to complete by the time the write to the hardware register completes.


-- DMB_Read

There is no direct equivalent to this in ARMv7 (barriers are either W or RW). However it's useful to define a read barrier, as (e.g.) on Cortex-A9 a RW barrier would require draining the write buffer of the external PL310 cache, while a R barrier can simply be an ordinary DMB instruction.

 * Ensures in-order operation of data load instructions
 * Does not stall instruction execution
 * Does not guarantee that any preceeding memory operations complete in a timely manner (or at all)

   entry: -
   exit:  -

Although this call doesn't guarantee that any memory operation completes, it's usually all that's required when interacting with hardware devices which use memory-mapped IO. E.g. after reading a hardware register to detect that a DMA write to RAM has completed, issue a read barrier to ensure that any reads from the data buffer see the final data.


@


1.1.2.5
log
@Switch default NCB cache policy to MergingIdempotent (i.e. Normal, non-cacheable)
Detail:
  s/ARMops - Change VMSAv6 cache policy mapping so that default NCB policy maps to Normal, non-cacheable memory rather than Device memory. This allows unaligned accesses to work, and also delivers some major performance improvements for some activities (write performance seems about 2x better than Device)
  Docs/HAL/ARMop_API - Wrap the new barrier ARMop description text to match the rest of the file
Admin:
  Tested on ARM11, Cortex-A7, -A8, -A9
  Note - relaxed memory consistency rules for Normal vs. Device mean that now more than ever, developers authoring device drivers for ARMv6+ need to be careful to use memory barriers in all the right places (preferably the new ARMop barriers exposed by OS_MMUControl 2 to ensure compatibility with all machines)


Version 5.35, 4.79.2.280. Tagged as 'Kernel-5_35-4_79_2_280'
@
text
@d285 1
a285 2
 * Instructions following the barrier will only begin execution once the
   barrier is passed - but any prefetched instructions are not flushed
d297 1
a297 2
 * Instructions following the barrier will only begin execution once the
   barrier is passed - but any prefetched instructions are not flushed
d305 1
a305 4
There is no direct equivalent to this in ARMv7 (barriers are either W or RW).
However it's useful to define a read barrier, as (e.g.) on Cortex-A9 a RW
barrier would require draining the write buffer of the external PL310 cache,
while a R barrier can simply be an ordinary DSB instruction.
d308 1
a308 2
 * Instructions following the barrier will only begin execution once the
   barrier is passed - but any prefetched instructions are not flushed
d320 1
a320 2
 * Does not guarantee that any preceeding memory operations complete in a
   timely manner (or at all)
d325 1
a325 7
Although this call doesn't guarantee that any memory operation completes, it's
usually all that's required when interacting with hardware devices which use
memory-mapped IO. E.g. fill a buffer with data, issue a DMB, then write to a
hardware register to start some external DMA. The writes to the buffer will
have been guaranteed to complete by the time the write to the hardware register
completes.

d333 1
a333 2
 * Does not guarantee that any preceeding memory operations complete in a
   timely manner (or at all)
d338 1
a338 6
Although this call doesn't guarantee that any memory operation completes, it's
usually all that's required when interacting with hardware devices which use
memory-mapped IO. E.g. fill a buffer with data, issue a DMB, then write to a
hardware register to start some external DMA. The writes to the buffer will
have been guaranteed to complete by the time the write to the hardware register
completes.
d343 1
a343 4
There is no direct equivalent to this in ARMv7 (barriers are either W or RW).
However it's useful to define a read barrier, as (e.g.) on Cortex-A9 a RW
barrier would require draining the write buffer of the external PL310 cache,
while a R barrier can simply be an ordinary DMB instruction.
d347 1
a347 2
 * Does not guarantee that any preceeding memory operations complete in a
   timely manner (or at all)
d352 1
a352 5
Although this call doesn't guarantee that any memory operation completes, it's
usually all that's required when interacting with hardware devices which use
memory-mapped IO. E.g. after reading a hardware register to detect that a DMA
write to RAM has completed, issue a read barrier to ensure that any reads from
the data buffer see the final data.
@


1.1.2.6
log
@Cache maintenance fixes
Detail:
  This set of changes tackles two main issues:
  * Before mapping out a cacheable page or making it uncacheable, the OS performs a cache clean+invalidate op. However this leaves a small window where data may be fetched back into the cache, either accidentally (dodgy interrupt handler) or via agressive prefetch (as allowed for by the architecture). This rogue data can then result in coherency issues once the pages are mapped out or made uncacheable a short time later.
    The fix for this is to make the page uncacheable before performing the cache maintenance (although this isn't ideal, as prior to ARMv7 it's implementation defined whether address-based cache maintenance ops affect uncacheable pages or not - and on ARM11 it seems that they don't, so for that CPU we currently force a full cache clean instead)
  * Modern ARMs generally ignore unexpected cache hits, so there's an interrupt hole in the current OS_Memory 0 "make temporarily uncacheable" implementation where the cache is being flushed after the page has been made uncacheable (consider the case of a page that's being used by an interrupt handler, but the page is being made uncacheable so it can also be used by DMA). As well as affecting ARMv7+ devices this was found to affect XScale (and ARM11, although untested for this issue, would have presumably suffered from the "can't clean uncacheable pages" limitation)
    The fix for this is to disable IRQs around the uncache sequence - however FIQs are currently not being dealt with, so there's still a potential issue there.
  File changes:
  - Docs/HAL/ARMop_API, hdr/KernelWS, hdr/OSMisc - Add new Cache_CleanInvalidateRange ARMop
  - s/ARM600, s/VMSAv6 - BangCam updated to make the page uncacheable prior to flushing the cache. Add GetTempUncache macro to help with calculating the page flags required for making pages uncacheable. Fix abort in OS_MMUControl on Raspberry Pi - MCR-based ISB was resetting ZeroPage pointer to 0
  - s/ARMops - Cache_CleanInvalidateRange implementations. PL310 MMU_ChangingEntry/MMU_ChangingEntries refactored to rely on Cache_CleanInvalidateRange_PL310, which should be a more optimal implementation of the cache cleaning code that was previously in MMU_ChangingEntry_PL310.
  - s/ChangeDyn - Rename FastCDA_UpFront to FastCDA_Bulk, since the cache maintenance is no longer performed upfront. CheckCacheabilityR0ByMinusR2 now becomes RemoveCacheabilityR0ByMinusR2. PMP LogOp implementation refactored quite a bit to perform cache/TLB maintenance after making page table changes instead of before. One flaw with this new implementation is that mapping out large areas of cacheable pages will result in multiple full cache cleans while the old implementation would have (generally) only performed one - a two-pass approach over the page list would be needed to solve this.
  - s/GetAll - Change file ordering so GetTempUncache macro is available earlier
  - s/HAL - ROM decompression changed to do full MMU_Changing instead of MMU_ChangingEntries, to make sure earlier cached data is truly gone from the cache. ClearPhysRAM changed to make page uncacheable before flushing cache.
  - s/MemInfo - OS_Memory 0 interrupt hole fix
  - s/AMBControl/memmap - AMB_movepagesout_L2PT now split into cacheable+non-cacheable variants. Sparse map out operation now does two passes through the page list so that they can all be made uncacheable prior to the cache flush + map out.
Admin:
  Tested on StrongARM, XScale, ARM11, Cortex-A7, Cortex-A9, Cortex-A15, Cortex-A53
  Appears to fix the major issues plaguing SATA on IGEPv5


Version 5.35, 4.79.2.306. Tagged as 'Kernel-5_35-4_79_2_306'
@
text
@a204 19
-- Cache_CleanInvalidateRange

The cache or caches are to be invalidated for (at least) the given range, with
cleaning of any writeback data being properly performed. 

   entry: r0 = logical address of start of range
          r1 = logical address of end of range (exclusive)
          Note that r0 and r1 are aligned on cache line boundaries
   exit:  -

Note that any write buffer draining should also be performed by this
operation, so that memory is fully updated with respect to any writeaback
data.

The OS only expects the invalidation to be with respect to instructions/data
that are not involved in any currently active interrupts. In other words, it
is expected and desirable that interrupts remain enabled during any extended
clean operation, in order to avoid impact on interrupt latency.

@


