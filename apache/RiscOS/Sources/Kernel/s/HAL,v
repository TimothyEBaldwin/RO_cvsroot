head	4.10;
access;
symbols
	Kernel-6_15:4.10
	Kernel-6_14:4.10
	Kernel-6_01-3:4.9
	Kernel-6_13:4.10
	Kernel-6_12:4.10
	Kernel-6_11:4.9
	Kernel-6_10:4.9
	Kernel-6_09:4.9
	Kernel-6_08-4_129_2_10:4.8.2.1
	Kernel-6_08-4_129_2_9:4.8.2.1
	Kernel-6_08:4.9
	Kernel-6_07:4.9
	Kernel-6_06:4.9
	Kernel-6_05-4_129_2_8:4.8.2.1
	Kernel-6_05:4.9
	Kernel-6_04:4.9
	Kernel-6_03:4.9
	Kernel-6_01-2:4.9
	Kernel-6_01-4_146_2_1:4.9
	Kernel-6_02:4.9
	Kernel-6_01-1:4.9
	Kernel-6_01:4.9
	Kernel-6_00:4.9
	Kernel-5_99:4.9
	Kernel-5_98:4.9
	Kernel-5_97-4_129_2_7:4.8.2.1
	Kernel-5_97:4.9
	Kernel-5_96:4.9
	Kernel-5_95:4.9
	Kernel-5_94:4.9
	Kernel-5_93:4.9
	Kernel-5_92:4.9
	Kernel-5_91:4.9
	Kernel-5_90:4.9
	Kernel-5_89-4_129_2_6:4.8.2.1
	Kernel-5_89:4.9
	Kernel-5_88-4_129_2_5:4.8
	Kernel-5_88-4_129_2_4:4.8
	Kernel-5_88:4.8
	Kernel-5_87:4.8
	Kernel-5_86-4_129_2_3:4.8
	Kernel-5_86-4_129_2_2:4.8
	Kernel-5_86-4_129_2_1:4.8
	Kernel-5_86:4.8
	SMP:4.8.0.2
	SMP_bp:4.8
	Kernel-5_85:4.8
	Kernel-5_54-1:4.3
	Kernel-5_84:4.8
	Kernel-5_83:4.8
	Kernel-5_82:4.8
	Kernel-5_81:4.8
	Kernel-5_80:4.8
	Kernel-5_79:4.8
	Kernel-5_78:4.8
	Kernel-5_77:4.8
	Kernel-5_76:4.8
	Kernel-5_75:4.8
	Kernel-5_74:4.8
	Kernel-5_73:4.8
	Kernel-5_72:4.8
	Kernel-5_71:4.7
	Kernel-5_70:4.6
	Kernel-5_69:4.5
	Kernel-5_68:4.5
	Kernel-5_67:4.4
	Kernel-5_66:4.4
	Kernel-5_65:4.4
	Kernel-5_64:4.4
	Kernel-5_63:4.4
	Kernel-5_62:4.4
	Kernel-5_61:4.4
	Kernel-5_60:4.4
	Kernel-5_59:4.4
	Kernel-5_58:4.4
	Kernel-5_57:4.4
	Kernel-5_56:4.4
	Kernel-5_55:4.4
	Kernel-5_54:4.3
	Kernel-5_53:4.3
	Kernel-5_52:4.3
	Kernel-5_51:4.3
	Kernel-5_50:4.2
	Kernel-5_49:4.2
	HAL_merge:1.1.2.51
	Kernel-5_48:4.1
	Kernel-5_35-4_79_2_327:1.1.2.51
	Kernel-5_35-4_79_2_326:1.1.2.50
	Kernel-5_35-4_79_2_325:1.1.2.50
	Kernel-5_35-4_79_2_324:1.1.2.50
	Kernel-5_35-4_79_2_323:1.1.2.50
	Kernel-5_35-4_79_2_322:1.1.2.50
	Kernel-5_35-4_79_2_321:1.1.2.50
	Kernel-5_35-4_79_2_320:1.1.2.50
	Kernel-5_35-4_79_2_319:1.1.2.49
	Kernel-5_35-4_79_2_318:1.1.2.49
	Kernel-5_35-4_79_2_317:1.1.2.49
	Kernel-5_35-4_79_2_316:1.1.2.49
	Kernel-5_35-4_79_2_315:1.1.2.49
	Kernel-5_35-4_79_2_314:1.1.2.49
	Kernel-5_35-4_79_2_313:1.1.2.49
	Kernel-5_35-4_79_2_312:1.1.2.49
	Kernel-5_35-4_79_2_311:1.1.2.49
	Kernel-5_35-4_79_2_310:1.1.2.49
	Kernel-5_35-4_79_2_309:1.1.2.49
	Kernel-5_35-4_79_2_308:1.1.2.49
	Kernel-5_35-4_79_2_307:1.1.2.49
	Kernel-5_35-4_79_2_306:1.1.2.49
	Kernel-5_35-4_79_2_305:1.1.2.48
	Kernel-5_35-4_79_2_304:1.1.2.47
	Kernel-5_35-4_79_2_303:1.1.2.46
	Kernel-5_35-4_79_2_302:1.1.2.46
	Kernel-5_35-4_79_2_301:1.1.2.46
	Kernel-5_35-4_79_2_300:1.1.2.46
	Kernel-5_35-4_79_2_299:1.1.2.46
	Kernel-5_35-4_79_2_298:1.1.2.46
	Kernel-5_35-4_79_2_297:1.1.2.46
	Kernel-5_35-4_79_2_296:1.1.2.46
	Kernel-5_35-4_79_2_295:1.1.2.46
	Kernel-5_35-4_79_2_294:1.1.2.46
	Kernel-5_35-4_79_2_293:1.1.2.46
	Kernel-5_35-4_79_2_292:1.1.2.46
	Kernel-5_35-4_79_2_291:1.1.2.46
	Kernel-5_35-4_79_2_290:1.1.2.46
	Kernel-5_35-4_79_2_289:1.1.2.46
	Kernel-5_35-4_79_2_288:1.1.2.46
	Kernel-5_35-4_79_2_287:1.1.2.46
	Kernel-5_35-4_79_2_286:1.1.2.46
	Kernel-5_35-4_79_2_285:1.1.2.46
	Kernel-5_35-4_79_2_284:1.1.2.46
	Kernel-5_35-4_79_2_283:1.1.2.45
	Kernel-5_35-4_79_2_282:1.1.2.45
	Kernel-5_35-4_79_2_281:1.1.2.45
	Kernel-5_35-4_79_2_280:1.1.2.45
	Kernel-5_35-4_79_2_279:1.1.2.45
	Kernel-5_35-4_79_2_278:1.1.2.45
	Kernel-5_35-4_79_2_277:1.1.2.45
	Kernel-5_35-4_79_2_276:1.1.2.45
	Kernel-5_35-4_79_2_275:1.1.2.45
	Kernel-5_35-4_79_2_274:1.1.2.45
	Kernel-5_35-4_79_2_273:1.1.2.44
	Kernel-5_35-4_79_2_272:1.1.2.43
	Kernel-5_35-4_79_2_271:1.1.2.43
	Kernel-5_35-4_79_2_270:1.1.2.43
	Kernel-5_35-4_79_2_269:1.1.2.43
	Kernel-5_35-4_79_2_268:1.1.2.43
	Kernel-5_35-4_79_2_267:1.1.2.43
	Kernel-5_35-4_79_2_266:1.1.2.43
	Kernel-5_35-4_79_2_265:1.1.2.43
	Kernel-5_35-4_79_2_264:1.1.2.42
	Kernel-5_35-4_79_2_263:1.1.2.42
	Kernel-5_35-4_79_2_262:1.1.2.42
	Kernel-5_35-4_79_2_261:1.1.2.42
	Kernel-5_35-4_79_2_260:1.1.2.42
	Kernel-5_35-4_79_2_259:1.1.2.42
	Kernel-5_35-4_79_2_258:1.1.2.42
	Kernel-5_35-4_79_2_257:1.1.2.42
	Kernel-5_35-4_79_2_256:1.1.2.41
	Kernel-5_35-4_79_2_255:1.1.2.41
	Kernel-5_35-4_79_2_254:1.1.2.41
	Kernel-5_35-4_79_2_253:1.1.2.41
	Kernel-5_35-4_79_2_252:1.1.2.41
	Kernel-5_35-4_79_2_251:1.1.2.41
	Kernel-5_35-4_79_2_250:1.1.2.41
	Kernel-5_35-4_79_2_249:1.1.2.41
	Kernel-5_35-4_79_2_248:1.1.2.41
	Kernel-5_35-4_79_2_247:1.1.2.41
	Kernel-5_35-4_79_2_246:1.1.2.41
	Kernel-5_35-4_79_2_245:1.1.2.41
	Kernel-5_35-4_79_2_244:1.1.2.41
	Kernel-5_35-4_79_2_243:1.1.2.40
	Kernel-5_35-4_79_2_242:1.1.2.40
	Kernel-5_35-4_79_2_241:1.1.2.40
	Kernel-5_35-4_79_2_240:1.1.2.40
	Kernel-5_35-4_79_2_239:1.1.2.40
	Kernel-5_35-4_79_2_238:1.1.2.40
	Kernel-5_35-4_79_2_237:1.1.2.40
	Kernel-5_35-4_79_2_236:1.1.2.40
	Kernel-5_35-4_79_2_235:1.1.2.40
	Kernel-5_35-4_79_2_234:1.1.2.40
	Kernel-5_35-4_79_2_233:1.1.2.40
	Kernel-5_35-4_79_2_232:1.1.2.40
	Kernel-5_35-4_79_2_231:1.1.2.40
	Kernel-5_35-4_79_2_230:1.1.2.39
	Kernel-5_35-4_79_2_229:1.1.2.39
	Kernel-5_35-4_79_2_228:1.1.2.39
	Kernel-5_35-4_79_2_227:1.1.2.39
	Kernel-5_35-4_79_2_226:1.1.2.39
	Kernel-5_35-4_79_2_225:1.1.2.39
	Kernel-5_35-4_79_2_224:1.1.2.39
	Kernel-5_35-4_79_2_223:1.1.2.38
	Kernel-5_35-4_79_2_222:1.1.2.38
	Kernel-5_35-4_79_2_221:1.1.2.37
	Kernel-5_35-4_79_2_220:1.1.2.36
	Kernel-5_35-4_79_2_219:1.1.2.36
	Kernel-5_35-4_79_2_218:1.1.2.36
	Kernel-5_35-4_79_2_217:1.1.2.36
	Kernel-5_35-4_79_2_216:1.1.2.36
	Kernel-5_35-4_79_2_215:1.1.2.36
	Kernel-5_35-4_79_2_214:1.1.2.35
	Kernel-5_35-4_79_2_213:1.1.2.35
	Kernel-5_35-4_79_2_212:1.1.2.35
	Kernel-5_35-4_79_2_211:1.1.2.34
	Kernel-5_35-4_79_2_210:1.1.2.34
	Kernel-5_35-4_79_2_209:1.1.2.34
	Kernel-5_35-4_79_2_208:1.1.2.34
	Kernel-5_35-4_79_2_207:1.1.2.34
	Kernel-5_35-4_79_2_206:1.1.2.34
	Kernel-5_35-4_79_2_205:1.1.2.34
	Kernel-5_35-4_79_2_204:1.1.2.34
	Kernel-5_35-4_79_2_203:1.1.2.34
	Kernel-5_35-4_79_2_202:1.1.2.34
	Kernel-5_35-4_79_2_201:1.1.2.34
	Kernel-5_35-4_79_2_200:1.1.2.34
	Kernel-5_35-4_79_2_199:1.1.2.34
	Kernel-5_35-4_79_2_198:1.1.2.34
	Kernel-5_35-4_79_2_197:1.1.2.34
	Kernel-5_35-4_79_2_196:1.1.2.34
	Kernel-5_35-4_79_2_195:1.1.2.33
	Kernel-5_35-4_79_2_194:1.1.2.33
	Kernel-5_35-4_79_2_193:1.1.2.33
	Kernel-5_35-4_79_2_192:1.1.2.33
	Kernel-5_35-4_79_2_191:1.1.2.33
	Kernel-5_35-4_79_2_190:1.1.2.33
	Kernel-5_35-4_79_2_189:1.1.2.33
	Kernel-5_35-4_79_2_188:1.1.2.32
	Kernel-5_35-4_79_2_187:1.1.2.32
	Kernel-5_35-4_79_2_186:1.1.2.32
	Kernel-5_35-4_79_2_185:1.1.2.31
	Kernel-5_35-4_79_2_184:1.1.2.31
	Kernel-5_35-4_79_2_183:1.1.2.30
	Kernel-5_35-4_79_2_182:1.1.2.30
	Kernel-5_35-4_79_2_181:1.1.2.29
	Kernel-5_35-4_79_2_180:1.1.2.29
	Kernel-5_35-4_79_2_179:1.1.2.29
	Kernel-5_35-4_79_2_178:1.1.2.29
	Kernel-5_35-4_79_2_177:1.1.2.29
	Kernel-5_35-4_79_2_176:1.1.2.29
	Kernel-5_35-4_79_2_175:1.1.2.29
	Kernel-5_35-4_79_2_174:1.1.2.29
	Kernel-5_35-4_79_2_173:1.1.2.29
	Kernel-5_35-4_79_2_172:1.1.2.29
	Kernel-5_35-4_79_2_171:1.1.2.29
	Kernel-5_35-4_79_2_170:1.1.2.29
	Kernel-5_35-4_79_2_169:1.1.2.29
	Kernel-5_35-4_79_2_168:1.1.2.29
	Kernel-5_35-4_79_2_167:1.1.2.28
	Kernel-5_35-4_79_2_166:1.1.2.28
	Kernel-5_35-4_79_2_165:1.1.2.28
	RPi_merge:1.1.2.23.2.6
	Kernel-5_35-4_79_2_147_2_23:1.1.2.23.2.6
	Kernel-5_35-4_79_2_147_2_22:1.1.2.23.2.5
	Kernel-5_35-4_79_2_147_2_21:1.1.2.23.2.5
	Kernel-5_35-4_79_2_147_2_20:1.1.2.23.2.5
	Kernel-5_35-4_79_2_147_2_19:1.1.2.23.2.4
	Kernel-5_35-4_79_2_147_2_18:1.1.2.23.2.4
	Kernel-5_35-4_79_2_164:1.1.2.27
	Kernel-5_35-4_79_2_163:1.1.2.27
	Kernel-5_35-4_79_2_147_2_17:1.1.2.23.2.3
	Kernel-5_35-4_79_2_147_2_16:1.1.2.23.2.2
	Kernel-5_35-4_79_2_147_2_15:1.1.2.23.2.2
	Kernel-5_35-4_79_2_162:1.1.2.26
	Kernel-5_35-4_79_2_161:1.1.2.26
	Kernel-5_35-4_79_2_147_2_14:1.1.2.23.2.2
	Kernel-5_35-4_79_2_147_2_13:1.1.2.23.2.2
	Kernel-5_35-4_79_2_160:1.1.2.26
	Kernel-5_35-4_79_2_159:1.1.2.25
	Kernel-5_35-4_79_2_158:1.1.2.25
	Kernel-5_35-4_79_2_157:1.1.2.25
	Kernel-5_35-4_79_2_156:1.1.2.25
	Kernel-5_35-4_79_2_147_2_12:1.1.2.23.2.2
	Kernel-5_35-4_79_2_147_2_11:1.1.2.23.2.2
	Kernel-5_35-4_79_2_155:1.1.2.25
	Kernel-5_35-4_79_2_147_2_10:1.1.2.23.2.2
	Kernel-5_35-4_79_2_154:1.1.2.25
	Kernel-5_35-4_79_2_153:1.1.2.24
	Kernel-5_35-4_79_2_147_2_9:1.1.2.23.2.2
	Kernel-5_35-4_79_2_152:1.1.2.24
	Kernel-5_35-4_79_2_151:1.1.2.24
	Kernel-5_35-4_79_2_147_2_8:1.1.2.23.2.2
	Kernel-5_35-4_79_2_147_2_7:1.1.2.23.2.2
	Kernel-5_35-4_79_2_150:1.1.2.24
	Kernel-5_35-4_79_2_147_2_6:1.1.2.23.2.2
	Kernel-5_35-4_79_2_147_2_5:1.1.2.23.2.2
	Kernel-5_35-4_79_2_149:1.1.2.23
	Kernel-5_35-4_79_2_147_2_4:1.1.2.23.2.2
	Kernel-5_35-4_79_2_147_2_3:1.1.2.23.2.2
	Kernel-5_35-4_79_2_148:1.1.2.23
	Kernel-5_35-4_79_2_147_2_2:1.1.2.23.2.1
	Kernel-5_35-4_79_2_147_2_1:1.1.2.23.2.1
	RPi:1.1.2.23.0.2
	RPi_bp:1.1.2.23
	Kernel-5_35-4_79_2_98_2_52_2_1:1.1.2.21.2.14.2.1
	alees_Kernel_dev:1.1.2.21.2.14.0.2
	alees_Kernel_dev_bp:1.1.2.21.2.14
	Kernel-5_35-4_79_2_147:1.1.2.23
	Kernel-5_35-4_79_2_146:1.1.2.23
	Kernel-5_35-4_79_2_145:1.1.2.23
	Kernel-5_35-4_79_2_144:1.1.2.23
	Kernel-5_35-4_79_2_143:1.1.2.23
	Kernel-5_35-4_79_2_142:1.1.2.23
	Kernel-5_35-4_79_2_141:1.1.2.23
	Kernel-5_35-4_79_2_140:1.1.2.23
	Kernel-5_35-4_79_2_139:1.1.2.23
	Kernel-5_35-4_79_2_138:1.1.2.23
	Kernel-5_35-4_79_2_137:1.1.2.22
	Kernel-5_35-4_79_2_136:1.1.2.22
	Kernel-5_35-4_79_2_135:1.1.2.22
	Kernel-5_35-4_79_2_134:1.1.2.22
	Kernel-5_35-4_79_2_133:1.1.2.22
	Kernel-5_35-4_79_2_132:1.1.2.22
	Kernel-5_35-4_79_2_131:1.1.2.22
	Kernel-5_35-4_79_2_130:1.1.2.22
	Kernel-5_35-4_79_2_129:1.1.2.22
	Kernel-5_35-4_79_2_128:1.1.2.22
	Kernel-5_35-4_79_2_127:1.1.2.22
	Kernel-5_35-4_79_2_126:1.1.2.22
	Kernel-5_35-4_79_2_125:1.1.2.22
	Kernel-5_35-4_79_2_124:1.1.2.22
	Kernel-5_35-4_79_2_123:1.1.2.22
	Cortex_merge:1.1.2.21.2.15
	Kernel-5_35-4_79_2_122:1.1.2.21
	Kernel-5_35-4_79_2_98_2_54:1.1.2.21.2.15
	Kernel-5_35-4_79_2_98_2_53:1.1.2.21.2.15
	Kernel-5_35-4_79_2_98_2_52:1.1.2.21.2.14
	Kernel-5_35-4_79_2_98_2_51:1.1.2.21.2.14
	Kernel-5_35-4_79_2_98_2_50:1.1.2.21.2.13
	Kernel-5_35-4_79_2_98_2_49:1.1.2.21.2.13
	Kernel-5_35-4_79_2_98_2_48:1.1.2.21.2.12
	Kernel-5_35-4_79_2_121:1.1.2.21
	Kernel-5_35-4_79_2_98_2_47:1.1.2.21.2.11
	Kernel-5_35-4_79_2_120:1.1.2.21
	Kernel-5_35-4_79_2_98_2_46:1.1.2.21.2.11
	Kernel-5_35-4_79_2_119:1.1.2.21
	Kernel-5_35-4_79_2_98_2_45:1.1.2.21.2.11
	Kernel-5_35-4_79_2_98_2_44:1.1.2.21.2.11
	Kernel-5_35-4_79_2_118:1.1.2.21
	Kernel-5_35-4_79_2_98_2_43:1.1.2.21.2.11
	Kernel-5_35-4_79_2_117:1.1.2.21
	Kernel-5_35-4_79_2_116:1.1.2.21
	Kernel-5_35-4_79_2_98_2_42:1.1.2.21.2.11
	Kernel-5_35-4_79_2_115:1.1.2.21
	Kernel-5_35-4_79_2_98_2_41:1.1.2.21.2.11
	Kernel-5_35-4_79_2_98_2_40:1.1.2.21.2.11
	Kernel-5_35-4_79_2_114:1.1.2.21
	Kernel-5_35-4_79_2_98_2_39:1.1.2.21.2.11
	Kernel-5_35-4_79_2_98_2_38:1.1.2.21.2.10
	Kernel-5_35-4_79_2_113:1.1.2.21
	Kernel-5_35-4_79_2_112:1.1.2.21
	Kernel-5_35-4_79_2_98_2_37:1.1.2.21.2.9
	Kernel-5_35-4_79_2_98_2_36:1.1.2.21.2.8
	Kernel-5_35-4_79_2_98_2_35:1.1.2.21.2.8
	Kernel-5_35-4_79_2_98_2_34:1.1.2.21.2.8
	Kernel-5_35-4_79_2_98_2_33:1.1.2.21.2.8
	Kernel-5_35-4_79_2_98_2_32:1.1.2.21.2.7
	Kernel-5_35-4_79_2_98_2_31:1.1.2.21.2.6
	Kernel-5_35-4_79_2_98_2_30:1.1.2.21.2.6
	Kernel-5_35-4_79_2_98_2_29:1.1.2.21.2.6
	Kernel-5_35-4_79_2_98_2_28:1.1.2.21.2.6
	Kernel-5_35-4_79_2_98_2_27:1.1.2.21.2.6
	Kernel-5_35-4_79_2_98_2_26:1.1.2.21.2.5
	Kernel-5_35-4_79_2_111:1.1.2.21
	Kernel-5_35-4_79_2_98_2_25:1.1.2.21.2.5
	Kernel-5_35-4_79_2_98_2_24:1.1.2.21.2.5
	Kernel-5_35-4_79_2_98_2_23:1.1.2.21.2.5
	Kernel-5_35-4_79_2_110:1.1.2.21
	Kernel-5_35-4_79_2_98_2_22:1.1.2.21.2.5
	Kernel-5_35-4_79_2_109:1.1.2.21
	Kernel-5_35-4_79_2_98_2_21:1.1.2.21.2.5
	Kernel-5_35-4_79_2_98_2_20:1.1.2.21.2.5
	Kernel-5_35-4_79_2_108:1.1.2.21
	Kernel-5_35-4_79_2_107:1.1.2.21
	Kernel-5_35-4_79_2_98_2_19:1.1.2.21.2.5
	Kernel-5_35-4_79_2_98_2_18:1.1.2.21.2.5
	Kernel-5_35-4_79_2_98_2_17:1.1.2.21.2.5
	Kernel-5_35-4_79_2_98_2_16:1.1.2.21.2.5
	Kernel-5_35-4_79_2_98_2_15:1.1.2.21.2.5
	Kernel-5_35-4_79_2_106:1.1.2.21
	Kernel-5_35-4_79_2_105:1.1.2.21
	Kernel-5_35-4_79_2_104:1.1.2.21
	Kernel-5_35-4_79_2_98_2_14:1.1.2.21.2.4
	Kernel-5_35-4_79_2_98_2_13:1.1.2.21.2.4
	Kernel-5_35-4_79_2_98_2_12:1.1.2.21.2.4
	Kernel-5_35-4_79_2_98_2_11:1.1.2.21.2.4
	Kernel-5_35-4_79_2_98_2_10:1.1.2.21.2.4
	Kernel-5_35-4_79_2_98_2_9:1.1.2.21.2.4
	Kernel-5_35-4_79_2_103:1.1.2.21
	Kernel-5_35-4_79_2_102:1.1.2.21
	Kernel-5_35-4_79_2_98_2_8:1.1.2.21.2.4
	Kernel-5_35-4_79_2_98_2_7:1.1.2.21.2.4
	Kernel-5_35-4_79_2_98_2_6:1.1.2.21.2.4
	Kernel-5_35-4_79_2_98_2_5:1.1.2.21.2.4
	Kernel-5_35-4_79_2_98_2_4:1.1.2.21.2.3
	Kernel-5_35-4_79_2_101:1.1.2.21
	Kernel-5_35-4_79_2_100:1.1.2.21
	Kernel-5_35-4_79_2_99:1.1.2.21
	Kernel-5_35-4_79_2_98_2_3:1.1.2.21.2.2
	Kernel-5_35-4_79_2_98_2_2:1.1.2.21.2.1
	Kernel-5_35-4_79_2_98_2_1:1.1.2.21
	Cortex:1.1.2.21.0.2
	Cortex_bp:1.1.2.21
	Kernel-5_35-4_79_2_98:1.1.2.21
	Kernel-5_35-4_79_2_97:1.1.2.21
	Kernel-5_35-4_79_2_96:1.1.2.21
	Kernel-5_35-4_79_2_95:1.1.2.21
	Kernel-5_35-4_79_2_94:1.1.2.21
	Kernel-5_35-4_79_2_93:1.1.2.21
	Kernel-5_35-4_79_2_92:1.1.2.21
	Kernel-5_35-4_79_2_91:1.1.2.21
	Kernel-5_35-4_79_2_90:1.1.2.21
	Kernel-5_35-4_79_2_89:1.1.2.21
	Kernel-5_35-4_79_2_88:1.1.2.21
	Kernel-5_35-4_79_2_87:1.1.2.21
	Kernel-5_35-4_79_2_86:1.1.2.21
	Kernel-5_35-4_79_2_85:1.1.2.21
	Kernel-5_35-4_79_2_84:1.1.2.21
	Kernel-5_35-4_79_2_83:1.1.2.21
	Kernel-5_35-4_79_2_82:1.1.2.21
	Kernel-5_35-4_79_2_81:1.1.2.21
	Kernel-5_35-4_79_2_80:1.1.2.21
	Kernel-5_35-4_79_2_79:1.1.2.21
	Kernel-5_35-4_79_2_78:1.1.2.21
	Kernel-5_35-4_79_2_77:1.1.2.21
	RO_5_07:1.1.2.21
	Kernel-5_35-4_79_2_76:1.1.2.21
	Kernel-5_35-4_79_2_75:1.1.2.21
	Kernel-5_35-4_79_2_74:1.1.2.21
	Kernel-5_35-4_79_2_73:1.1.2.21
	Kernel-5_35-4_79_2_72:1.1.2.21
	Kernel-5_35-4_79_2_71:1.1.2.21
	Kernel-5_35-4_79_2_70:1.1.2.21
	Kernel-5_35-4_79_2_69:1.1.2.21
	Kernel-5_35-4_79_2_68:1.1.2.21
	Kernel-5_35-4_79_2_67:1.1.2.21
	Kernel-5_35-4_79_2_66:1.1.2.20
	Kernel-5_35-4_79_2_65:1.1.2.19
	Kernel-5_35-4_79_2_64:1.1.2.19
	Kernel-5_35-4_79_2_63:1.1.2.19
	Kernel-5_35-4_79_2_62:1.1.2.19
	Kernel-5_35-4_79_2_61:1.1.2.19
	Kernel-5_35-4_79_2_59:1.1.2.19
	Kernel-5_35-4_79_2_58:1.1.2.18
	Kernel-5_35-4_79_2_57:1.1.2.18
	Kernel-5_35-4_79_2_56:1.1.2.18
	Kernel-5_35-4_79_2_55:1.1.2.18
	Kernel-5_35-4_79_2_54:1.1.2.17
	Kernel-5_35-4_79_2_53:1.1.2.17
	Kernel-5_35-4_79_2_52:1.1.2.17
	Kernel-5_35-4_79_2_51:1.1.2.16
	Kernel-5_35-4_79_2_50:1.1.2.15
	Kernel-5_35-4_79_2_49:1.1.2.15
	Kernel-5_35-4_79_2_48:1.1.2.14
	Kernel-5_35-4_79_2_47:1.1.2.13
	Kernel-5_35-4_79_2_46:1.1.2.13
	Kernel-5_35-4_79_2_45:1.1.2.13
	Kernel-5_35-4_79_2_44:1.1.2.13
	Kernel-5_35-4_79_2_25_2_2:1.1.2.9
	Kernel-5_35-4_79_2_43:1.1.2.13
	Kernel-5_35-4_79_2_42:1.1.2.13
	Kernel-5_35-4_79_2_41:1.1.2.13
	Kernel-5_35-4_79_2_40:1.1.2.12
	Kernel-5_35-4_79_2_39:1.1.2.12
	Kernel-5_35-4_79_2_38:1.1.2.12
	Kernel-5_35-4_79_2_37:1.1.2.12
	Kernel-5_35-4_79_2_36:1.1.2.11
	Kernel-5_35-4_79_2_35:1.1.2.11
	Kernel-5_35-4_79_2_34:1.1.2.11
	Kernel-5_35-4_79_2_33:1.1.2.11
	Kernel-5_35-4_79_2_32:1.1.2.10
	Kernel-5_35-4_79_2_25_2_1:1.1.2.9
	Kernel-5_35-4_79_2_31:1.1.2.10
	Kernel-5_35-4_79_2_30:1.1.2.10
	Kernel-5_35-4_79_2_29:1.1.2.10
	Kernel-5_35-4_79_2_28:1.1.2.10
	Kernel-5_35-4_79_2_27:1.1.2.9
	Kernel-5_35-4_79_2_26:1.1.2.9
	Kernel-5_35-4_79_2_25:1.1.2.9
	Kernel-5_35-4_79_2_24:1.1.2.9
	Kernel-5_35-4_79_2_23:1.1.2.9
	Kernel-5_35-4_79_2_22:1.1.2.9
	Kernel-5_35-4_79_2_21:1.1.2.9
	Kernel-5_35-4_79_2_20:1.1.2.9
	Kernel-5_35-4_79_2_19:1.1.2.9
	Kernel-5_35-4_79_2_18:1.1.2.9
	Kernel-5_35-4_79_2_17:1.1.2.9
	Kernel-5_35-4_79_2_16:1.1.2.8
	Kernel-5_35-4_79_2_15:1.1.2.8
	Kernel-5_35-4_79_2_14:1.1.2.8
	Kernel-5_35-4_79_2_13:1.1.2.8
	Kernel-5_35-4_79_2_12:1.1.2.7
	Kernel-5_35-4_79_2_11:1.1.2.7
	Kernel-5_35-4_79_2_10:1.1.2.6
	Kernel-5_35-4_79_2_9:1.1.2.5
	Kernel-5_35-4_79_2_8:1.1.2.4
	Kernel-5_35-4_79_2_7:1.1.2.3
	Kernel-5_35-4_79_2_6:1.1.2.3
	Kernel-5_35-4_79_2_5:1.1.2.3
	Kernel-5_35-4_79_2_4:1.1.2.2
	Kernel-5_35-4_79_2_3:1.1.2.2
	Kernel-5_35-4_79_2_2:1.1.2.2
	Kernel-5_35-4_79_2_1:1.1.2.1
	HAL:1.1.0.2;
locks; strict;
comment	@# @;


4.10
date	2018.08.04.10.30.46;	author jlee;	state Exp;
branches;
next	4.9;
commitid	6Hkm8k8YLxZn2NMA;

4.9
date	2017.09.09.10.35.42;	author rool;	state Exp;
branches;
next	4.8;
commitid	SuZJGVb4fAmIDv6A;

4.8
date	2016.12.13.19.46.53;	author jlee;	state Exp;
branches
	4.8.2.1;
next	4.7;
commitid	Wof9jMT2CrjUmRxz;

4.7
date	2016.12.13.19.41.12;	author jlee;	state Exp;
branches;
next	4.6;
commitid	XeVhUEC50BLVkRxz;

4.6
date	2016.12.13.19.03.34;	author jlee;	state Exp;
branches;
next	4.5;
commitid	dvbJa4TQHit18Rxz;

4.5
date	2016.12.13.17.31.03;	author jlee;	state Exp;
branches;
next	4.4;
commitid	xwV3EbxXsXlhCQxz;

4.4
date	2016.08.02.22.10.43;	author jlee;	state Exp;
branches;
next	4.3;
commitid	CnQYuUGzojQfrMgz;

4.3
date	2016.06.30.20.59.46;	author jlee;	state Exp;
branches;
next	4.2;
commitid	skOEjp3ipLHx6xcz;

4.2
date	2016.06.30.20.28.55;	author jlee;	state Exp;
branches;
next	4.1;
commitid	lMnWzoE9eJz3Wwcz;

4.1
date	2016.06.30.20.08.08;	author jlee;	state Exp;
branches;
next	1.1;
commitid	IWoXxARWeuLDOwcz;

1.1
date	2000.09.15.12.38.01;	author kbracey;	state dead;
branches
	1.1.2.1;
next	;

4.8.2.1
date	2017.09.10.11.27.21;	author jlee;	state Exp;
branches;
next	;
commitid	EGooxXrB27MqTD6A;

1.1.2.1
date	2000.09.15.12.38.01;	author kbracey;	state Exp;
branches;
next	1.1.2.2;

1.1.2.2
date	2000.10.02.08.52.19;	author kbracey;	state Exp;
branches;
next	1.1.2.3;

1.1.2.3
date	2000.10.05.13.54.49;	author kbracey;	state Exp;
branches;
next	1.1.2.4;

1.1.2.4
date	2000.10.05.16.46.36;	author dellis;	state Exp;
branches;
next	1.1.2.5;

1.1.2.5
date	2000.10.09.15.59.15;	author kbracey;	state Exp;
branches;
next	1.1.2.6;

1.1.2.6
date	2000.10.16.11.55.38;	author kbracey;	state Exp;
branches;
next	1.1.2.7;

1.1.2.7
date	2000.10.20.14.58.21;	author kbracey;	state Exp;
branches;
next	1.1.2.8;

1.1.2.8
date	2000.11.10.15.51.34;	author kbracey;	state Exp;
branches;
next	1.1.2.9;

1.1.2.9
date	2001.02.13.09.36.05;	author kbracey;	state Exp;
branches;
next	1.1.2.10;

1.1.2.10
date	2001.05.01.14.10.59;	author mstephen;	state Exp;
branches;
next	1.1.2.11;

1.1.2.11
date	2001.06.11.11.33.31;	author kbracey;	state Exp;
branches;
next	1.1.2.12;

1.1.2.12
date	2001.06.15.09.39.57;	author mstephen;	state Exp;
branches;
next	1.1.2.13;

1.1.2.13
date	2001.06.26.09.37.11;	author mstephen;	state Exp;
branches;
next	1.1.2.14;

1.1.2.14
date	2002.10.07.17.29.41;	author kbracey;	state Exp;
branches;
next	1.1.2.15;

1.1.2.15
date	2002.10.16.17.23.13;	author bavison;	state Exp;
branches;
next	1.1.2.16;

1.1.2.16
date	2002.11.30.00.31.08;	author bavison;	state Exp;
branches;
next	1.1.2.17;

1.1.2.17
date	2002.12.13.17.38.11;	author bavison;	state Exp;
branches;
next	1.1.2.18;

1.1.2.18
date	2003.01.27.15.25.33;	author kbracey;	state Exp;
branches;
next	1.1.2.19;

1.1.2.19
date	2003.03.31.09.44.10;	author kbracey;	state Exp;
branches;
next	1.1.2.20;

1.1.2.20
date	2004.05.06.16.02.01;	author kbracey;	state Exp;
branches;
next	1.1.2.21;

1.1.2.21
date	2004.05.07.12.36.34;	author kbracey;	state Exp;
branches
	1.1.2.21.2.1;
next	1.1.2.22;

1.1.2.22
date	2011.11.26.21.11.15;	author jlee;	state Exp;
branches;
next	1.1.2.23;
commitid	cI3W0zbtALQG6TIv;

1.1.2.23
date	2012.02.25.16.19.37;	author jlee;	state Exp;
branches
	1.1.2.23.2.1;
next	1.1.2.24;
commitid	ad3WnPntkzrizyUv;

1.1.2.24
date	2012.05.21.19.31.39;	author rsprowson;	state Exp;
branches;
next	1.1.2.25;
commitid	oEtPURiKNEPMRC5w;

1.1.2.25
date	2012.06.21.07.59.24;	author rsprowson;	state Exp;
branches;
next	1.1.2.26;
commitid	IX8PhIHHo1Sy2y9w;

1.1.2.26
date	2012.07.06.00.52.19;	author bavison;	state Exp;
branches;
next	1.1.2.27;
commitid	7T20ZtwZOZX9crbw;

1.1.2.27
date	2012.08.14.23.45.35;	author jlee;	state Exp;
branches;
next	1.1.2.28;
commitid	idAOw3o0ecfzxzgw;

1.1.2.28
date	2012.09.18.22.01.15;	author jlee;	state Exp;
branches;
next	1.1.2.29;
commitid	eFa3Y1QY0MjZP3lw;

1.1.2.29
date	2012.09.30.16.29.22;	author jlee;	state Exp;
branches;
next	1.1.2.30;
commitid	sRDLisRa5FxeCzmw;

1.1.2.30
date	2013.01.22.23.19.30;	author jlee;	state Exp;
branches;
next	1.1.2.31;
commitid	ySNxVh2X08tIdgBw;

1.1.2.31
date	2013.03.24.09.58.56;	author rsprowson;	state Exp;
branches;
next	1.1.2.32;
commitid	t65zAOQAgqJqP1Jw;

1.1.2.32
date	2013.03.28.21.36.24;	author jlee;	state Exp;
branches;
next	1.1.2.33;
commitid	UN0GP6eB0LlNyBJw;

1.1.2.33
date	2013.05.27.09.55.14;	author rsprowson;	state Exp;
branches;
next	1.1.2.34;
commitid	9CS36AVUbRvJKfRw;

1.1.2.34
date	2013.09.28.07.03.56;	author rsprowson;	state Exp;
branches;
next	1.1.2.35;
commitid	AHbJg79uTkiQOa7x;

1.1.2.35
date	2014.03.25.23.08.36;	author bavison;	state Exp;
branches;
next	1.1.2.36;
commitid	h2bydHMFxtw2r8ux;

1.1.2.36
date	2014.04.04.22.19.19;	author jballance;	state Exp;
branches;
next	1.1.2.37;
commitid	bAG8kI0jJo5cQpvx;

1.1.2.37
date	2014.04.19.14.45.41;	author jlee;	state Exp;
branches;
next	1.1.2.38;
commitid	41mDnYJhlOjHQixx;

1.1.2.38
date	2014.04.20.17.00.21;	author jlee;	state Exp;
branches;
next	1.1.2.39;
commitid	6eesW4yWEAvSyrxx;

1.1.2.39
date	2014.04.30.07.58.03;	author rsprowson;	state Exp;
branches;
next	1.1.2.40;
commitid	dsGMRzlpD9bVeGyx;

1.1.2.40
date	2014.07.23.00.03.42;	author jlee;	state Exp;
branches;
next	1.1.2.41;
commitid	kKkrGTuKshFNUqJx;

1.1.2.41
date	2014.10.27.23.02.35;	author rsprowson;	state Exp;
branches;
next	1.1.2.42;
commitid	KW8Dvj3soUAvsTVx;

1.1.2.42
date	2015.02.07.02.08.00;	author jlee;	state Exp;
branches;
next	1.1.2.43;
commitid	Vfg1zqR3J94Qc19y;

1.1.2.43
date	2015.06.14.17.22.57;	author jlee;	state Exp;
branches;
next	1.1.2.44;
commitid	YEFeXUsjjD9Cbqpy;

1.1.2.44
date	2015.08.05.21.51.31;	author jlee;	state Exp;
branches;
next	1.1.2.45;
commitid	SpZpzVH47zb408wy;

1.1.2.45
date	2015.08.06.20.11.25;	author jlee;	state Exp;
branches;
next	1.1.2.46;
commitid	EHeTssj64ggMpfwy;

1.1.2.46
date	2015.08.31.19.28.37;	author jlee;	state Exp;
branches;
next	1.1.2.47;
commitid	Ni3KL17bG70fnszy;

1.1.2.47
date	2016.02.29.09.34.29;	author bavison;	state Exp;
branches;
next	1.1.2.48;
commitid	MTrfU2BUG24IeNWy;

1.1.2.48
date	2016.02.29.21.05.39;	author jlee;	state Exp;
branches;
next	1.1.2.49;
commitid	9aBO9sfSEp6R3RWy;

1.1.2.49
date	2016.03.10.22.57.41;	author jlee;	state Exp;
branches;
next	1.1.2.50;
commitid	DAXUqMY2ucjim9Yy;

1.1.2.50
date	2016.05.20.19.48.10;	author jlee;	state Exp;
branches;
next	1.1.2.51;
commitid	zKFcnjcl1yRR1g7z;

1.1.2.51
date	2016.06.15.19.24.52;	author jlee;	state Exp;
branches;
next	;
commitid	QoMRsy6vZNP34Baz;

1.1.2.21.2.1
date	2009.02.21.17.41.25;	author jlee;	state Exp;
branches;
next	1.1.2.21.2.2;

1.1.2.21.2.2
date	2009.03.06.23.23.43;	author jlee;	state Exp;
branches;
next	1.1.2.21.2.3;

1.1.2.21.2.3
date	2009.04.23.23.39.48;	author jlee;	state Exp;
branches;
next	1.1.2.21.2.4;

1.1.2.21.2.4
date	2009.05.10.18.49.16;	author jlee;	state Exp;
branches;
next	1.1.2.21.2.5;

1.1.2.21.2.5
date	2009.11.06.23.17.45;	author jlee;	state Exp;
branches;
next	1.1.2.21.2.6;

1.1.2.21.2.6
date	2010.06.23.22.34.27;	author jlee;	state Exp;
branches;
next	1.1.2.21.2.7;

1.1.2.21.2.7
date	2010.10.04.22.22.14;	author jlee;	state Exp;
branches;
next	1.1.2.21.2.8;

1.1.2.21.2.8
date	2011.02.19.22.19.34;	author jlee;	state Exp;
branches;
next	1.1.2.21.2.9;

1.1.2.21.2.9
date	2011.05.22.19.45.03;	author jlee;	state Exp;
branches;
next	1.1.2.21.2.10;
commitid	FcIrTQvrz0zQFIkv;

1.1.2.21.2.10
date	2011.06.04.15.54.32;	author jlee;	state Exp;
branches;
next	1.1.2.21.2.11;
commitid	xmzeXYEfZlUPYmmv;

1.1.2.21.2.11
date	2011.06.08.23.09.41;	author jlee;	state Exp;
branches;
next	1.1.2.21.2.12;
commitid	n5CIwG9YV7k9gVmv;

1.1.2.21.2.12
date	2011.08.08.23.28.26;	author jlee;	state Exp;
branches;
next	1.1.2.21.2.13;
commitid	D7rzILnwRRSXoLuv;

1.1.2.21.2.13
date	2011.08.22.22.26.17;	author jlee;	state Exp;
branches;
next	1.1.2.21.2.14;
commitid	ekk1ti4Ejl7OBywv;

1.1.2.21.2.14
date	2011.09.12.20.31.39;	author jlee;	state Exp;
branches
	1.1.2.21.2.14.2.1;
next	1.1.2.21.2.15;
commitid	aYOriTZbxBByifzv;

1.1.2.21.2.15
date	2011.09.24.19.55.54;	author jlee;	state Exp;
branches;
next	;
commitid	kEjQnYmCIZvfIMAv;

1.1.2.21.2.14.2.1
date	2012.05.10.03.09.52;	author bavison;	state Exp;
branches;
next	;
commitid	HTVG5Da5tRqSM74w;

1.1.2.23.2.1
date	2012.05.10.03.28.04;	author bavison;	state Exp;
branches;
next	1.1.2.23.2.2;
commitid	kuJoT3AcfB16T74w;

1.1.2.23.2.2
date	2012.05.18.10.22.09;	author bavison;	state Exp;
branches;
next	1.1.2.23.2.3;
commitid	mphs1jjnueKfVb5w;

1.1.2.23.2.3
date	2012.08.14.23.42.17;	author jlee;	state Exp;
branches;
next	1.1.2.23.2.4;
commitid	TbwQU6RrwF9qwzgw;

1.1.2.23.2.4
date	2012.09.02.19.56.52;	author jlee;	state Exp;
branches;
next	1.1.2.23.2.5;
commitid	q3Bny0G3EqBdFZiw;

1.1.2.23.2.5
date	2012.09.07.23.05.51;	author jlee;	state Exp;
branches;
next	1.1.2.23.2.6;
commitid	Ik0jMROsIiH5yEjw;

1.1.2.23.2.6
date	2012.09.18.15.50.01;	author jlee;	state Exp;
branches;
next	;
commitid	jeuxYpI6CQUxM1lw;


desc
@@


4.10
log
@Fix OS_Hardware 3 to be re-entrant
Detail:
  s/HAL - OS_Hardware 3 (remove HAL device) will now re-scan the device list for the device following the Service_Hardware call, so that the device list won't become corrupt if the service call triggers addition/removal of devices.
Admin:
  Tested on iMX6
  *HDMIOff now correctly removes the HDMI audio device and SoundDMA's software mixer device (SoundDMA removes mixer in response to the HDMI audio device vanishing, but re-entrancy bug meant that the HDMI device was left on the list)
  Note that this only covers re-entrancy via Service_Hardware. OS_Hardware 2/3/4/5 are not re-entrant from other locations (e.g. IRQ handlers or memory allocation service calls).


Version 6.12. Tagged as 'Kernel-6_12'
@
text
@; Copyright 2000 Pace Micro Technology plc
;
; Licensed under the Apache License, Version 2.0 (the "License");
; you may not use this file except in compliance with the License.
; You may obtain a copy of the License at
;
;     http://www.apache.org/licenses/LICENSE-2.0
;
; Unless required by applicable law or agreed to in writing, software
; distributed under the License is distributed on an "AS IS" BASIS,
; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
; See the License for the specific language governing permissions and
; limitations under the License.
;
        GBLL    MinorL2PThack
MinorL2PThack SETL {TRUE}

; Fixed page allocation is as follows

                        ^       0
DRAMOffset_FirstFixed   #       0
DRAMOffset_ScratchSpace #       16*1024
DRAMOffset_PageZero     #       16*1024
DRAMOffset_L1PT         #       16*1024         ; L1PT must be 16K-aligned
DRAMOffset_LastFixed    #       0

;        IMPORT  Init_ARMarch
;        IMPORT  ARM_Analyse

 [ MEMM_Type = "VMSAv6"
mmuc_init_new
        ; MMUC initialisation flags for ARMv6/ARMv7
        ; This tries to leave the reserved/unpredictable bits untouched, while initialising everything else to what we want
                ; ARMv7MP (probably) wants SW. ARMv6 wants U+XP (which should both be fixed at 1 on ARMv7)
        DCD     MMUC_SW+MMUC_U+MMUC_XP
                ; M+C+W+Z+I+L2 clear to keep MMU/caches off.
                ; A to keep alignment exceptions off (for now at least)
                ; B+EE clear for little-endian
                ; S+R+RR clear to match mmuc_init_old
                ; V+VE clear to keep processor vectors at &0
                ; FI clear to disable fast FIQs (interruptible LDM/STM)
                ; TRE+AFE clear for our VMSAv6 implementation
                ; TE clear for processor vectors to run in ARM mode
        DCD     MMUC_M+MMUC_A+MMUC_C+MMUC_W+MMUC_B+MMUC_S+MMUC_R+MMUC_Z+MMUC_I+MMUC_V+MMUC_RR+MMUC_FI+MMUC_VE+MMUC_EE+MMUC_L2+MMUC_TRE+MMUC_AFE+MMUC_TE
mmuc_init_old
        ; MMUC initialisation flags for ARMv5 and below, as per ARM600 MMU code
                ; Late abort (ARM6 only), 32-bit Data and Program space. No Write buffer (ARM920T
                ; spec says W bit should be set, but I reckon they're bluffing).
                ;
                ; The F bit's tricky. (1 => CPCLK=FCLK, 0=>CPCLK=FCLK/2). The only chip using it was the
                ; ARM700, it never really reached the customer, and it's always been programmed with
                ; CPCLK=FCLK. Therefore we'll keep it that way, and ignore the layering violation.
        DCD     MMUC_F+MMUC_L+MMUC_D+MMUC_P
                ; All of these bits should be off already, but just in case...
        DCD     MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M+MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S
 ]

; void RISCOS_InitARM(unsigned int flags)
;
RISCOS_InitARM
        MOV     a4, lr
        ; Check if we're architecture 3. If so, don't read the control register.
        BL      Init_ARMarch
        MOVEQ   a3, #0
        ARM_read_control a3, NE
 [ MEMM_Type = "VMSAv6"
        CMP     a1, #ARMv6
        CMPNE   a1, #ARMvF
        ADREQ   a2, mmuc_init_new
        ADRNE   a2, mmuc_init_old
        LDMIA   a2, {a2, lr}
        ORR     a3, a3, a2
        BIC     a3, a3, lr     
 |
        ; Late abort (ARM6 only), 32-bit Data and Program space. No Write buffer (ARM920T
        ; spec says W bit should be set, but I reckon they're bluffing).
        ;
        ; The F bit's tricky. (1 => CPCLK=FCLK, 0=>CPCLK=FCLK/2). The only chip using it was the
        ; ARM700, it never really reached the customer, and it's always been programmed with
        ; CPCLK=FCLK. Therefore we'll keep it that way, and ignore the layering violation.
        ORR     a3, a3, #MMUC_F+MMUC_L+MMUC_D+MMUC_P
        ; All of these bits should be off already, but just in case...
        BIC     a3, a3, #MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M
        BIC     a3, a3, #MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S
 ]

        ; Off we go.
        ARM_write_control a3
        MOV     a2, #0
 [ MEMM_Type = "VMSAv6"
        myISB   ,a2,,y ; Ensure the update went through
 ]

        ; In case it wasn't a hard reset
 [ MEMM_Type = "VMSAv6"
        CMP     a1, #ARMvF
        ; Assume that all ARMvF ARMs have multi-level caches and thus no single MCR op for invalidating all the caches
        ADREQ   lr, %FT01
        BEQ     HAL_InvalidateCache_ARMvF
        MCRNE   ARM_config_cp,0,a2,ARMv4_cache_reg,C7           ; invalidate I+D caches
01
 ]
        CMP     a1, #ARMv3
        BNE     %FT01
        MCREQ   ARM_config_cp,0,a2,ARMv3_TLBflush_reg,C0        ; flush TLBs
        B       %FT02
01      MCRNE   ARM_config_cp,0,a2,ARMv4_TLB_reg,C7             ; flush TLBs
02
 [ MEMM_Type = "VMSAv6"
        myDSB   ,a2,,y
        myISB   ,a2,,y
 ]

        ; We assume that ARMs with an I cache can have it enabled while the MMU is off.
        [ :LNOT:CacheOff
        ORRNE   a3, a3, #MMUC_I
        ARM_write_control a3, NE                                ; whoosh
 [ MEMM_Type = "VMSAv6"
        myISB   ,a2,,y ; Ensure the update went through
 ]
        ]

        ; Check if we are in a 26-bit mode.
        MRS     a2, CPSR
        ; Keep a soft copy of the CR in a banked register (R13_und)
        MSR     CPSR_c, #F32_bit+I32_bit+UND32_mode
        MOV     sp, a3
        ; Switch into SVC32 mode (we may have been in SVC26 before).
        MSR     CPSR_c, #F32_bit+I32_bit+SVC32_mode

        ; If we were in a 26-bit mode, the lr value given to us would have had PSR flags in.
        TST     a2, #2_11100
        MOVNE   pc, a4
        BICEQ   pc, a4, #ARM_CC_Mask


; void *RISCOS_AddRAM(unsigned int flags, void *start, void *end, uintptr_t sigbits, void *ref)
;   Entry:
;     flags   bit 0: video memory (currently only one block permitted)
;             bit 1: video memory is not suitable for general use
;             bit 2: memory can't be used for DMA (sound, video, or other)
;             bits 8-11: speed indicator (arbitrary, higher => faster)
;             other bits reserved (SBZ)
;     start   = start address of RAM (inclusive) (no alignment requirements)
;     end     = end address of RAM (exclusive) (no alignment requirements, but must be >= start)
;     sigbits = significant address bit mask (1 => this bit of addr decoded, 0 => this bit ignored)
;     ref     = reference handle (NULL for first call)

; A table is built up at the head of the first block of memory.
; The table consists of (addr, len, flags) pairs, terminated by a count of those pairs; ref points to that
; counter.
; Twelve bits of flags are stored at the bottom of the length word.

        ROUT
RISCOS_AddRAM
        Push    "v1,v2,v3,v4,lr"
        LDR     v4, [sp, #5*4]          ; Get ref

        ; Round to pages. If we were extra sneaky we could not do this and chuck out incomplete
        ; pages after concatanation, but it would be a weird HAL that gave us pages split across
        ; calls.
        ;
        ADD     a2, a2, #4096           ; round start address up
        SUB     a2, a2, #1
        MOV     a2, a2, LSR #12
        MOV     a2, a2, LSL #12
        MOV     a3, a3, LSR #12         ; round end address down
        MOV     a3, a3, LSL #12

        CMP     a3, a2
        BLS     %FT90                   ; check we aren't now null

        CMP     v4, #0
        BEQ     %FT20

        ; We are not dealing with the first block since v4 != 0.  Make an attempt to merge this block
        ; with the previous block.
        LDMDB   v4, {v1, v2}            ; Get details of the previous block
        MOV     v3, v2, LSL #20         ; Isolate flags
        BIC     v2, v2, v3, LSR #20     ; And strip from length
        ADD     v2, v1, v2              ; Get the end address
        EOR     v2, v2, a2              ; Compare with the current block start address...
        TST     v2, a4                  ; ... but only check the decoded bits.
        EOR     v2, v2, a2              ; Restore the previous block end address.
        TEQEQ   v3, a1, LSL #20         ; And are the page flags the same?
        BNE     %FT10                   ; We can't merge it after the previous block

        ; v1 = previous start
        ; v2 = previous end
        ; The block is just after the previous block.  That means the start address is unchanged, but
        ; the length is increased.
        SUB     v2, v2, v1              ; Calculate the previous block length.
        SUB     a3, a3, a2              ; Find the length of the new block.
        ; a3 = length of block
        ADD     v2, v2, a3              ; Add it to the previous length.
        ORR     v2, v2, v3, LSR #20     ; And put the flags back in.
        STR     v2, [v4, #-4]           ; Update the block size in memory.
        MOV     a1,v4
        Pull    "v1,v2,v3,v4,pc"

        ; The block is not just after the previous block, but it may be just before.  This may be the
        ; case if we are softloaded.
10      SUB     v1, v1, #1              ; Compare the address before the previous block start ...
        SUB     a3, a3, #1              ; ... with the address of the last byte in this block ...
        EOR     v1, v1, a3
        TST     v1, a4                  ; ... but check only the decoded bits.
        ADD     a3, a3, #1              ; Restore the end address.
        TEQEQ   v3, a1, LSL #20         ; And are the page flags the same?
        BNE     %FT20                   ; Skip if we cannot merge the block.

        ; The block is just before the previous block.  The start address and length both change.
        LDR     v1, [v4, #-8]           ; Get the previous block start again.

        SUB     a3, a3, a2              ; Calculate the current block size.
        SUB     v1, v1, a3              ; Subtract from the previous block start address.
        SUB     v2, v2, v1              ; Calculate the new length=end-start
        ORR     v2, v2, v3, LSR #20     ; And put the flags back in.
        STMDB   v4, {v1, v2}            ; Update the block info in memory.
        MOV     a1,v4
        Pull    "v1,v2,v3,v4,pc"

        ; We now have a region which does not merge with a previous region.  We move it up to the
        ; highest address we can in the hope that this block will merge with the next block.
20      SUB     a3, a3, a2              ; Calculate the block size
        MOV     a1, a1, LSL #20
        ORR     a3, a3, a1, LSR #20     ; Put the flags at the bottom
        MVN     v1, a4                  ; Get the non-decoded address lines.
        ORR     a2, v1, a2              ; Set the non-decoded address bit in the start address.

30      CMP     v4, #0                  ; If the workspace has not been allocated...
        MOVEQ   v4, a2                  ; ... use this block.
        MOVEQ   v1, #0                  ; Initialise the counter.

        ; The block/fragment to be added is between a2 and a2+a3.
        LDRNE   v1, [v4]                ; Get the old counter if there was one.
        STMIA   v4!, {a2, a3}           ; Store address and size.
        ADD     v1, v1, #1              ; Increment the counter.
        STR     v1, [v4]                ; Store the counter.

90      MOV     a1,v4
        Pull    "v1,v2,v3,v4,pc"        ; We've done with this block now.



; Subtractv1v2fromRAMtable
;
; On entry: v1 = base of memory area
;           v2 = size of memory area
;           a4 = RAM table handle (ie pointer to terminator word containing number of entries)
;
; On exit:  a1-a3 preserved
;           a4 and RAM table updated
;           other registers corrupted
Subtractv1v2fromRAMtable
        ADD     v2, v1, v2              ; v2 = end address
        MOV     v1, v1, LSR #12
        MOV     v1, v1, LSL #12         ; round base down
        ADD     v2, v2, #4096
        SUB     v2, v2, #1
        MOV     v2, v2, LSR #12
        MOV     v2, v2, LSL #12         ; round end up

        LDR     v5, [a4]
        SUB     v8, a4, v5, LSL #3
10      TEQ     v8, a4
        MOVEQ   pc, lr
        LDMIA   v8!, {v3, v4}
        MOV     v6, v4, LSR #12
        ADD     v6, v3, v6, LSL #12     ; v6 = end of RAM block
        CMP     v2, v3                  ; if our end <= RAM block start
        CMPHI   v6, v1                  ; or RAM block end <= our start
        BLS     %BT10                   ; then no intersection

        MOV     v4, v4, LSL #20         ; extract flags

        CMP     v1, v3
        BHI     not_bottom

        ; our area is at the bottom
        CMP     v2, v6
        BHS     remove_block

        SUB     v6, v6, v2              ; v6 = new size
        ORR     v6, v6, v4, LSR #20     ; + flags
        STMDB   v8, {v2, v6}            ; store new base (= our end) and size
        B       %BT10

        ; we've completely covered a block. Remove it.
remove_block
        MOV     v7, v8
20      TEQ     v7, a4                  ; shuffle down subsequent blocks in table
        LDMNEIA v7, {v3, v4}
        STMNEDB v7, {v3, v4}
        ADDNE   v7, v7, #8
        BNE     %BT20
        SUB     v5, v5, #1
        SUB     a4, a4, #8
        STR     v5, [a4]
        SUB     v8, v8, #8
        B       %BT10

        ; our area is not at the bottom.
not_bottom
        CMP     v2, v6
        BLO     split_block

        ; our area is at the top
        SUB     v6, v1, v3              ; v6 = new size
        ORR     v6, v6, v4, LSR #20     ; + flags
        STMDB   v8, {v3, v6}            ; store original base and new size
        B       %BT10

split_block
        MOV     v7, a4
30      TEQ     v7, v8                  ; shuffle up subsequent blocks in table
        LDMDB   v7, {v3, v4}
        STMNEIA v7, {v3, v4}
        SUBNE   v7, v7, #8
        BNE     %BT30
        ADD     v5, v5, #1
        ADD     a4, a4, #8
        STR     v5, [a4]

        MOV     v4, v4, LSL #20         ; (re)extract flags

        SUB     v7, v1, v3              ; v7 = size of first half
        SUB     v6, v6, v2              ; v6 = size of second half
        ORR     v7, v7, v4, LSR #20
        ORR     v6, v6, v4, LSR #20     ; + flags
        STMDB   v8, {v3, v7}
        STMIA   v8!, {v2, v6}
        B       %BT10


;void RISCOS_Start(unsigned int flags, int *riscos_header, int *hal_header, void *ref)
;

; We don't return, so no need to obey ATPCS, except for parameter passing.
; register usage:   v4 = location of VRAM
;                   v6 = amount of VRAM

        ROUT
RISCOS_Start
        TEQ     a4, #0
01      BEQ     %BT01                           ; Stop here if no RAM

        ; subtract the HAL and OS from the list of RAM areas
        MOV     v1, a2
        LDR     v2, [a2, #OSHdr_ImageSize]
        BL      Subtractv1v2fromRAMtable
        LDR     v1, [a3, #HALDesc_Start]
        ADD     v1, a3, v1
        LDR     v2, [a3, #HALDesc_Size]
        BL      Subtractv1v2fromRAMtable

        LDR     v5, [a4]                        ; v5 = the number of RAM blocks
        SUB     v8, a4, v5, LSL #3              ; Jump back to the start of the list.

        ; Search for some VRAM
05      LDMIA   v8!, {v1, v2}                   ; Get a block from the list. (v1,v2)=(addr,size+flags)
        TST     v2, #OSAddRAM_IsVRAM            ; Is it VRAM?
        BNE     %FT20                           ; If so, deal with it below
        TEQ     v8, a4                          ; Carry on until end of list or we find some.
        BNE     %BT05

        ; Extract some pseudo-VRAM from first DMA-capable RAM block
        SUB     v8, a4, v5, LSL #3              ; Rewind again.
06      LDMIA   v8!, {v1, v2}
        TEQ     v8, a4                          ; End of list?
        TSTNE   v2, #OSAddRAM_NoDMA             ; DMA capable?
        BNE     %BT06
        MOV     v2, v2, LSR #12                 ; Remove flags
        MOV     v2, v2, LSL #12
        ; Is this the only DMA-capable block?
        MOV     v4, v8
        MOV     v6, #OSAddRAM_NoDMA
07      TEQ     v4, a4
        BEQ     %FT08
        LDR     v6, [v4, #4]
        ADD     v4, v4, #8
        TST     v6, #OSAddRAM_NoDMA
        BNE     %BT07
08
        ; v6 has NoDMA set if v8 was the only block
        TST     v6, #OSAddRAM_NoDMA
        MOV     v4, v1                          ; Allocate block as video memory
        MOV     v6, v2
        BEQ     %FT09
        SUBS    v6, v6, #16*1024*1024           ; Leave 16M if it was the only DMA-capable block
        MOVLS   v6, v2, LSR #1                  ; If that overflowed, take half the bank.
09
        CMP     v6, #32*1024*1024
        MOVHS   v6, #32*1024*1024               ; Limit allocation to 32M (arbitrary)

        ADD     v1, v1, v6                      ; Adjust the RAM block base...
        SUBS    v2, v2, v6                      ; ... and the size
        BEQ     %FT22                           ; pack array tighter if this block is all gone
        STR     v1, [v8, #-8]                   ; update base
        LDR     v1, [v8, #-4]
        MOV     v1, v1, LSL #20
        ORR     v1, v1, v2, LSR #12
        MOV     v1, v1, ROR #20                 ; merge flags back into size
        STR     v1, [v8, #-4]                   ; update size
        B       %FT30

        ; Note real VRAM parameters
20      MOV     v6, v2                          ; Remember the size and address
        MOV     v4, v1                          ; of the VRAM
22      TEQ     v8, a4                          ; if not at the end of the array
        LDMNEIA v8, {v1, v2}                    ; pack the array tighter
        STMNEDB v8, {v1, v2}
        ADDNE   v8, v8, #8
        BNE     %BT22
25      SUB     v5, v5, #1                      ; decrease the counter
        STR     v5, [a4, #-8]!                  ; and move the end marker down

30      SUB     v8, a4, v5, LSL #3              ; Rewind to start of list

        ; Scan forwards to find the fastest block of non-DMAable memory which is at least DRAMOffset_LastFixed size
        LDMIA   v8!, {v1, v2}
31
        TEQ     v8, a4
        BEQ     %FT32
        LDMIA   v8!, {v7, ip}
        CMP     ip, #DRAMOffset_LastFixed
        ANDHS   sp, ip, #&F*OSAddRAM_Speed+OSAddRAM_NoDMA
        ANDHS   lr, v2, #&F*OSAddRAM_Speed+OSAddRAM_NoDMA
        ASSERT  OSAddRAM_Speed = 1:SHL:8
        ASSERT  OSAddRAM_NoDMA < OSAddRAM_Speed
        MOVHS   sp, sp, ROR #8                  ; Give NoDMA flag priority over speed when sorting
        CMPHS   sp, lr, ROR #8
        MOVHI   v1, v7
        MOVHI   v2, ip
        B       %BT31
32        
        ; Fill in the Kernel's permanent memory table, sorting by speed and DMA ability
        ; Non-DMAable RAM is preferred over DMAable, as the kernel requires very little DMAable RAM, and we don't want to permanently claim DMAable RAM if we're not actually using it for DMA (in case machine only has a tiny amount available)
        ADD     ip, v1, #DRAMOffset_PageZero

        ADD     sp, v1, #DRAMOffset_ScratchSpace + ScratchSpaceSize

        Push    "a1,a2,a3"                      ; Remember our arguments

        SUB     v8, a4, v5, LSL #3              ; Rewind to start of list
        CMP     v5, #DRAMPhysTableSize          ; Don't overflow our table
        ADDHI   a4, v8, #DRAMPhysTableSize*8 - 8
        
        ; First put the VRAM information in to free up some regs
        ADD     v7, ip, #VideoPhysAddr
        STMIA   v7!, {v4, v6}

        ; Now fill in the rest
        ASSERT  DRAMPhysAddrA = VideoPhysAddr+8
        STMIA   v7!, {v1, v2}                   ; workspace block must be first
33
        TEQ     v8, a4
        BEQ     %FT39
        LDMIA   v8!, {v1, v2}
        CMP     v2, #4096                       ; skip zero-length sections
        BLO     %BT33
        ; Perform insertion sort
        ; a1-a3, v3-v6, ip, lr free
        ADD     a1, ip, #DRAMPhysAddrA
        LDMIA   a1!, {a2, a3}
        TEQ     v1, a2
        BEQ     %BT33                           ; don't duplicate the initial block
        AND     v3, v2, #&F*OSAddRAM_Speed+OSAddRAM_NoDMA
        ASSERT  OSAddRAM_Speed = 1:SHL:8
        ASSERT  OSAddRAM_NoDMA < OSAddRAM_Speed
        MOV     v3, v3, ROR #8                  ; Give NoDMA flag priority over speed when sorting
34
        AND     v4, a3, #&F*OSAddRAM_Speed+OSAddRAM_NoDMA
        CMP     v3, v4, ROR #8
        BHI     %FT35
        TEQ     a1, v7
        LDMNEIA a1!, {a2, a3}
        BNE     %BT34
        ADD     a1, a1, #8
35
        ADD     v7, v7, #8
        ; Insert at a1-8, overwriting {a2, a3}
36
        STMDB   a1, {v1, v2}                   ; store new entry
        TEQ     a1, v7
        MOVNE   v1, a2                         ; if not at end, shuffle
        MOVNE   v2, a3                         ; overwritten entry down one,
        LDMNEIA a1!, {a2, a3}                  ; load next to be overwritten,
        BNE     %BT36                          ; and loop
        B       %BT33

39
        ; Now we have to work out the total RAM size
        MOV     a2, #0
        ADD     v6, ip, #PhysRamTable
        MOV     a3, v6
40
        LDMIA   v6!, {v1, v2}                   ; get address, size
        ADD     a2, a2, v2, LSR #12             ; add on size
        TEQ     v6, v7
        BNE     %BT40
        MOV     a2, a2, LSL #12

        ; Work out how much DMAable RAM the HAL/kernel needs
        LDR     a1, [sp, #8]
        LDR     a1, [a1, #HALDesc_Flags]
        TST     a1, #HALFlag_NCNBWorkspace              ; do they want uncacheable workspace?
        LDRNE   a1, =SoundDMABuffers-CursorChunkAddress + ?SoundDMABuffers + 32*1024 + DRAMOffset_LastFixed
        LDREQ   a1, =SoundDMABuffers-CursorChunkAddress + ?SoundDMABuffers + DRAMOffset_LastFixed
        ; Scan PhysRamTable for a DMAable block of at least this size, extract it, and stash it in InitDMABlock
        ; Once the initial memory claiming is done we can re-insert it
        ADD     a4, a3, #DRAMPhysAddrA-VideoPhysAddr    ; don't claim VRAM
        
        ; First block needs special treatment as we've already claimed some of it
        LDMIA   a4!, {v1, v2}
        TST     v2, #OSAddRAM_NoDMA
        BNE     %FT41
        CMP     v2, a1
        BLO     %FT41
        ; Oh crumbs, the first block is a match for our DMA block
        ; Claim it as normal, but set InitDMAEnd to v1+DRAMOffset_LastFixed so
        ; that the already used bit won't get used for DMA
        ; We also need to be careful later on when picking the initial v2 value
        ADD     lr, v1, #DRAMOffset_LastFixed
        STR     lr, [ip, #InitDMAEnd]
        B       %FT43
41
        ; Go on to check the rest of PhysRamTable
        SUB     a1, a1, #DRAMOffset_LastFixed
42
        LDMIA   a4!, {v1, v2}
        TST     v2, #OSAddRAM_NoDMA
        BNE     %BT42
        CMP     v2, a1
        BLO     %BT42
        ; Make a note of this block
        STR     v1, [ip, #InitDMAEnd]
43
        STR     v1, [ip, #InitDMABlock]
        STR     v2, [ip, #InitDMABlock+4]
        SUB     lr, a4, a3
        STR     lr, [ip, #InitDMAOffset]
        ; Now shrink/remove this memory from PhysRamTable
        SUB     v2, v2, a1
        ADD     v1, v1, a1
        CMP     v2, #4096               ; Block all gone?
        STMHSDB a4, {v1, v2}            ; no, just shrink it
        BHS     %FT55
45
        CMP     a4, v7
        LDMNEIA a4, {v1, v2}
        STMNEDB a4, {v1, v2}
        ADDNE   a4, a4, #8
        BNE     %BT45
        SUB     v7, v7, #8

; a2 = Total memory size (bytes)
; a3 = PhysRamTable
; v7 = After last used entry in PhysRamTable

; now store zeros to fill out table

55
        ADD     v2, a3, #PhysRamTableEnd-PhysRamTable
        MOV     v3, #0
        MOV     v4, #0
57
        CMP     v7, v2
        STMLOIA v7!, {v3, v4}
        BLO     %BT57

; Time to set up the L1PT. Just zero it out for now.

        LDR     a4, =DRAMOffset_L1PT+16*1024-(PhysRamTable+DRAMOffset_PageZero) ; offset from a3 to L1PT end
        ADD     a3, a3, a4
        MOV     a4, #16*1024
        MOV     v2, #0
        MOV     v3, #0
        MOV     v4, #0
        MOV     v5, #0
        MOV     v6, #0
        MOV     v7, #0
        MOV     v8, #0
        MOV     ip, #0
60
        STMDB   a3!, {v2-v8,ip}                         ; start at end and work back
        SUBS    a4, a4, #8*4
        BNE     %BT60

        ADD     v1, a3, #DRAMOffset_PageZero - DRAMOffset_L1PT
        ADD     v2, a3, #DRAMOffset_LastFixed - DRAMOffset_L1PT
        STR     a2, [v1, #RAMLIMIT]                     ; remember the RAM size
        MOV     lr, a2, LSR #12
        SUB     lr, lr, #1
        STR     lr, [v1, #MaxCamEntry]
        MOV     lr, a2, LSR #12-CAM_EntrySizeLog2+12
        CMP     a2, lr, LSL #12-CAM_EntrySizeLog2+12
        ADDNE   lr, lr, #1
        MOV     lr, lr, LSL #12
        STR     lr, [v1, #SoftCamMapSize]
        STR     a3, [v1, #InitUsedStart]                ; store start of L1PT

        ADD     v1, v1, #DRAMPhysAddrA
        MOV     v3, a3

        ; Detect if the DMA claiming adjusted the first block
        ; If so, we'll need to reset v2 to the start of the block at v1
        LDR     a1, [v1]
        ADD     lr, a1, #DRAMOffset_LastFixed
        TEQ     lr, v2
        MOVNE   v2, a1

; For the next batch of allocation routines, v1-v3 are treated as globals.
; v1 -> current entry in PhysRamTable
; v2 -> next address to allocate in v1 (may point at end of v1)
; v3 -> L1PT (or 0 if MMU on - not yet)

; Set up some temporary PCBTrans and PPLTrans pointers, and the initial page flags used by the page tables
        ADD     a1, v3, #DRAMOffset_PageZero - DRAMOffset_L1PT
        BL      Init_PCBTrans

; Allocate the L2PT backing store for the logical L2PT space, to
; prevent recursion.
        LDR     a1, =L2PT
        MOV     a2, #&00400000
        BL      AllocateL2PT

; Allocate workspace for the HAL

        ADD     a4, v3, #DRAMOffset_PageZero - DRAMOffset_L1PT
        LDR     a3, [sp, #8]                            ; recover pushed HAL header
        LDR     a1, =HALWorkspace
        LDR     a2, =AreaFlags_HALWorkspace
        LDR     lr, [a3, #HALDesc_Workspace]            ; their workspace
        LDR     ip, [a3, #HALDesc_NumEntries]           ; plus 1 word per entry
        CMP     ip, #KnownHALEntries
        MOVLO   ip, #KnownHALEntries
        ADD     lr, lr, ip, LSL #2
        MOV     a3, lr, LSR #12                         ; round workspace up to whole
        MOV     a3, a3, LSL #12                         ; number of pages
        CMP     a3, lr
        ADDNE   a3, a3, #&1000
        STR     a3, [a4, #HAL_WsSize]                   ; Make a note of allocated space
        ADD     ip, a1, ip, LSL #2                      ; Their workspace starts
        STR     ip, [a4, #HAL_Workspace]                ; after our table of entries
        BL      Init_MapInRAM

        LDR     a3, [sp, #8]                            ; recover pushed HAL header
        LDR     lr, [a3, #HALDesc_Flags]
        TST     lr, #HALFlag_NCNBWorkspace              ; do they want uncacheable
        LDRNE   a1, =HALWorkspaceNCNB                   ; workspace?
        LDRNE   a2, =AreaFlags_HALWorkspaceNCNB
        LDRNE   a3, =32*1024
        BLNE    Init_MapInRAM_DMA

; Bootstrap time. We want to get the MMU on ASAP. We also don't want to have to
; clear up too much mess later. So what we'll do is map in the three fixed areas
; (L1PT, scratch space and page zero), the CAM, ourselves, and the HAL,
; then turn on the MMU. The CAM will be filled in once the MMU is on, by
; reverse-engineering the page tables?

        ; Map in page zero
        ADD     a1, v3, #DRAMOffset_PageZero - DRAMOffset_L1PT
        LDR     a2, =ZeroPage
        LDR     a3, =AreaFlags_ZeroPage
        MOV     a4, #16*1024
        BL      Init_MapIn

        ; Map in scratch space
        ADD     a1, v3, #DRAMOffset_ScratchSpace - DRAMOffset_L1PT
        MOV     a2, #ScratchSpace
        LDR     a3, =AreaFlags_ScratchSpace
        MOV     a4, #16*1024
        BL      Init_MapIn

        ; Map in L1PT
        MOV     a1, v3
        LDR     a2, =L1PT
        ADD     a3, v3, #DRAMOffset_PageZero - DRAMOffset_L1PT
        LDR     a3, [a3, #PageTable_PageFlags]
        ORR     a3, a3, #PageFlags_Unavailable
        MOV     a4, #16*1024
        BL      Init_MapIn

        ; Map in L1PT again in PhysicalAccess (see below)
        MOV     a1, v3, LSR #20
        MOV     a1, a1, LSL #20                 ; megabyte containing L1PT
        LDR     a2, =PhysicalAccess
        ADD     a3, v3, #DRAMOffset_PageZero - DRAMOffset_L1PT
        LDR     a3, [a3, #PageTable_PageFlags]
        ORR     a3, a3, #PageFlags_Unavailable
        MOV     a4, #1024*1024
        BL      Init_MapIn


        ; Examine HAL and RISC OS locations
        LDMFD   sp, {v4,v5,v6}                  ; v4 = flags, v5 = RO desc, v6 = HAL desc
        LDR     lr, [v6, #HALDesc_Size]
        LDR     v7, [v6, #HALDesc_Start]
        ADD     v6, v6, v7                      ; (v6,v8)=(start,end) of HAL
        ADD     v8, v6, lr

        LDR     v7, [v5, #OSHdr_ImageSize]
        ADD     v7, v5, v7                      ; (v5,v7)=(start,end) of RISC OS

        TEQ     v8, v5                          ; check contiguity (as in a ROM image)
        BNE     %FT70

        ; HAL and RISC OS are contiguous. Yum.
        MOV     a1, v6
        LDR     a2, =RISCOS_Header
        SUB     a2, a2, lr

        SUB     ip, a2, a1                      ; change physical addresses passed in
        LDMIB   sp, {a3, a4}                    ; into logical addresses
        ADD     a3, a3, ip
        ADD     a4, a4, ip
        STMIB   sp, {a3, a4}
        LDR     a3, [v5, #OSHdr_DecompressHdr]  ; check if ROM is compressed, and if so, make writeable
        CMP     a3, #0                          
        MOVNE   a3, #OSAP_None
        MOVEQ   a3, #OSAP_ROM
        SUB     a4, v7, v6
        BL      Init_MapIn
        MOV     a3, v6
        B       %FT75

70
        ; HAL is separate. (We should cope with larger images)
        LDR     a2, =ROM
        MOV     a1, v6
        SUB     ip, a2, a1                      ; change physical address passed in
        LDR     a3, [sp, #8]                    ; into logical address
        ADD     a3, a3, ip
        STR     a3, [sp, #8]
        SUB     a4, v8, v6
        MOV     a3, #OSAP_ROM
        BL      Init_MapIn

        ; And now map in RISC OS
        LDR     a2, =RISCOS_Header              ; Hmm - what if position independent?
        MOV     a1, v5
        SUB     ip, a2, a1                      ; change physical address passed in
        LDR     a3, [sp, #4]                    ; into logical address
        ADD     a3, a3, ip
        STR     a3, [sp, #4]
        SUB     a4, v7, v5
        LDR     a3, [v5, #OSHdr_DecompressHdr]
        CMP     a3, #0
        MOVNE   a3, #0
        MOVEQ   a3, #OSAP_ROM
        BL      Init_MapIn
        MOV     a3, v5
75
        ; We've now allocated all the pages we're going to before the MMU comes on.
        ; Note the end address (for RAM clear)
        ADD     a1, v3, #DRAMOffset_PageZero - DRAMOffset_L1PT
        STR     v1, [a1, #InitUsedBlock]
        STR     v2, [a1, #InitUsedEnd]
        STR     a3, [a1, #ROMPhysAddr]

        ; Note the HAL flags passed in.
        LDR     a2, [sp, #0]
        STR     a2, [a1, #HAL_StartFlags]

        ; Set up a reset IRQ handler (for IIC CMOS access)
        MSR     CPSR_c, #IRQ32_mode + I32_bit + F32_bit
        LDR     sp_irq, =ScratchSpace + 1024    ; 1K is plenty since Reset_IRQ_Handler now runs in SVC mode
        MSR     CPSR_c, #SVC32_mode + I32_bit + F32_bit
        LDR     a2, =Reset_IRQ_Handler
        STR     a2, [a1, #InitIRQHandler]

        ; Fill in some initial processor vectors. These will be used during ARM
        ; analysis, once the MMU is on. We do it here before the data cache is
        ; activated to save any IMB issues.
        ADRL    a2, InitProcVecs
        ADD     a3, a2, #InitProcVecsEnd - InitProcVecs
76      LDR     a4, [a2], #4
        CMP     a2, a3
        STR     a4, [a1], #4
        BLO     %BT76

MMU_activation_zone
; The time has come to activate the MMU. Steady now... Due to unpredictability of MMU
; activation, need to ensure that mapped and unmapped addresses are equivalent. To
; do this, we temporarily make the section containing virtual address MMUon_instr map
; to the same physical address. In case the code crosses a section boundary, do the
; next section as well.
;
        MOV     a1, #4_0000000000000001                         ; domain 0 client only
        ARM_MMU_domain a1

        ADR     a1, MMU_activation_zone
        MOV     a1, a1, LSR #20                 ; a1 = megabyte number (stays there till end)
        ADD     lr, v3, a1, LSL #2              ; lr -> L1PT entry
        LDMIA   lr, {a2, a3}                    ; remember old mappings
 [ MEMM_Type = "VMSAv6"
        LDR     ip, =(AP_ROM * L1_APMult) + L1_Section
 |
  [ ARM6support
        LDR     ip, =(AP_None * L1_APMult) + L1_U + L1_Section
  |
        LDR     ip, =(AP_ROM * L1_APMult) + L1_U + L1_Section
  ]
 ]
        ORR     a4, ip, a1, LSL #20             ; not cacheable, as we don't want
        ADD     v4, a4, #1024*1024              ; to fill the cache with rubbish
        STMIA   lr, {a4, v4}

        MOV     a4, a1
        Push    "a2,lr"
        MOV     a1, v3
        ADD     a2, v3, #DRAMOffset_PageZero-DRAMOffset_L1PT
        BL      SetTTBR
        Pull    "a2,lr"
        BL      Init_ARMarch                    ; corrupts a1 and ip
        MOV     ip, a1                          ; Remember architecture for later
        MOV     a1, a4

        MSREQ   CPSR_c, #F32_bit+I32_bit+UND32_mode ; Recover the soft copy of the CR
        MOVEQ   v5, sp
        ARM_read_control v5, NE
  [ CacheOff
        ORR     v5, v5, #MMUC_M                 ; MMU on
        ORR     v5, v5, #MMUC_R                 ; ROM mode enable
  |
        ORR     v5, v5, #MMUC_W+MMUC_C+MMUC_M   ; Write buffer, data cache, MMU on
        ORR     v5, v5, #MMUC_R+MMUC_Z          ; ROM mode enable, branch predict enable
  ]
  [ MEMM_Type = "VMSAv6"
        ORR     v5, v5, #MMUC_XP ; Extended pages enabled (v6)
        BIC     v5, v5, #MMUC_TRE+MMUC_AFE ; TEX remap, Access Flag disabled
        BIC     v5, v5, #MMUC_EE+MMUC_TE+MMUC_VE ; Exceptions = nonvectored LE ARM
      [ SupportARMv6 :LAND: NoARMv7
        ; Deal with a couple of ARM11 errata
        ARM_read_ID lr
        LDR     a4, =&FFF0
        AND     lr, lr, a4
        LDR     a4, =&B760
        TEQ     lr, a4
        BNE     %FT01
        ORR     v5, v5, #MMUC_FI ; Erratum 716151: Disable hit-under-miss (enable fast interrupt mode) to prevent D-cache corruption from D-cache cleaning (the other workaround, ensuring a DSB exists inbetween the clean op and the next store access to that cache line, feels a bit heavy-handed since we'd probably have to disable IRQs to make it fully safe)
        ; Update the aux control register
        MRC     p15, 0, lr, c1, c0, 1
        ; Bit 28: Erratum 714068: Set PHD bit to prevent deadlock from PLI or I-cache invalidate by MVA
        ; Bit 31: Erratum 716151: Set FIO bit to override some of the behaviour implied by FI bit
        ORR     lr, lr, #(1:SHL:28)+(1:SHL:31)
        MCR     p15, 0, lr, c1, c0, 1
        myISB   ,lr
01
      ]
  ]
  [ NoUnaligned
        ORR     v5, v5, #MMUC_A ; Alignment exceptions on
  ]
  [ HiProcVecs
        ORR     v5, v5, #MMUC_V ; High processor vectors enabled
  ]

MMUon_instr
; Note, no RAM access until we've reached MMUon_nol1ptoverlap and the flat
; logical-physical mapping of the ROM has been removed (we can't guarantee that
; the RAM mapping hasn't been clobbered, and SP is currently bogus).
        ARM_write_control v5
  [ MEMM_Type = "VMSAv6"
        MOV     lr, #0
        myISB   ,lr,,y ; Just in case
  ]
        MOVEQ   sp, v5
        MSREQ   CPSR_c, #F32_bit+I32_bit+SVC32_mode

  [ MEMM_Type = "VMSAv6"
        CMP     ip, #ARMvF
        BEQ     %FT01
        MCRNE   ARM_config_cp,0,lr,ARMv4_cache_reg,C7           ; junk MMU-off contents of I-cache (works on ARMv3)
        B       %FT02
01      MCREQ   p15, 0, lr, c7, c5, 0           ; invalidate instruction cache
        MCREQ   p15, 0, lr, c8, c7, 0           ; invalidate TLBs
        MCREQ   p15, 0, lr, c7, c5, 6           ; invalidate branch predictor
        myISB   ,lr,,y ; Ensure below branch works
        BLEQ    HAL_InvalidateCache_ARMvF       ; invalidate data cache (and instruction+TLBs again!)
02
  |
        MOV     lr, #0                                          ; junk MMU-off contents of I-cache
        MCR     ARM_config_cp,0,lr,ARMv4_cache_reg,C7           ; (works on ARMv3)
  ]

; MMU now on. Need to jump to logical copy of ourselves. Complication arises if our
; physical address overlaps our logical address - in that case we need to map
; in another disjoint copy of ourselves and branch to that first, then restore the
; original two sections.
        ADRL    a4, RISCOS_Header
        LDR     ip, =RISCOS_Header
        SUB     ip, ip, a4
        ADR     a4, MMU_activation_zone
        MOV     a4, a4, LSR #20
        MOV     a4, a4, LSL #20                 ; a4 = base of scrambled region
        ADD     v4, a4, #2*1024*1024            ; v4 = top of scrambled region
        SUB     v4, v4, #1                      ;      (inclusive, in case wrapped to 0)
        ADR     v5, MMUon_resume
        ADD     v5, v5, ip                      ; v5 = virtual address of MMUon_resume
        CMP     v5, a4
        BLO     MMUon_nooverlap
        CMP     v5, v4
        BHI     MMUon_nooverlap

        ASSERT  ROM > 3*1024*1024
; Oh dear. We know the ROM lives high up, so we'll mangle 00100000-002FFFFF.
; But as we're overlapping the ROM, we know we're not overlapping the page tables.
        LDR     lr, =L1PT                       ; accessing the L1PT virtually now
 [ MEMM_Type = "VMSAv6"
        LDR     ip, =(AP_ROM * L1_APMult) + L1_Section
 |
  [ ARM6support
        LDR     ip, =(AP_None * L1_APMult) + L1_U + L1_Section
  |
        LDR     ip, =(AP_ROM * L1_APMult) + L1_U + L1_Section
  ]
 ]
        ORR     v6, a4, ip
        ADD     ip, v6, #1024*1024
        LDMIB   lr, {v7, v8}                    ; sections 1 and 2
        STMIB   lr, {v6, ip}
        RSB     ip, a4, #&00100000
        ADD     pc, pc, ip
        NOP
MMUon_overlapresume                             ; now executing from 00100000
        ADD     ip, lr, a4, LSR #18
        STMIA   ip, {a2, a3}                    ; restore original set of mappings
        BL      Init_PageTablesChanged

        MOV     a2, v7                          ; arrange for code below
        MOV     a3, v8                          ; to restore section 1+2 instead
        MOV     a1, #1

MMUon_nooverlap
        ADRL    lr, RISCOS_Header
        LDR     ip, =RISCOS_Header
        SUB     ip, ip, lr
        ADD     pc, pc, ip
        NOP
MMUon_resume
; What if the logical address of the page tables is at the physical address of the code?
; Then we have to access it via PhysicalAccess instead.
        LDR     lr, =L1PT
        CMP     lr, a4
        BLO     MMUon_nol1ptoverlap
        CMP     lr, v4
        BHI     MMUon_nol1ptoverlap
; PhysicalAccess points to the megabyte containing the L1PT. Find the L1PT within it.
        LDR     lr, =PhysicalAccess
        MOV     v6, v3, LSL #12
        ORR     lr, lr, v6, LSR #12
MMUon_nol1ptoverlap
        ADD     lr, lr, a1, LSL #2
        STMIA   lr, {a2, a3}
        BL      Init_PageTablesChanged

; The MMU is now on. Wahey. Let's get allocating.

        LDR     sp, =ScratchSpace + ScratchSpaceSize - 4*3 ; 3 items already on stack :)

        LDR     a1, =ZeroPage

        ADD     lr, v3, #DRAMOffset_PageZero-DRAMOffset_L1PT   ; lr = PhysAddr of zero page
        SUB     v1, v1, lr
        ADD     v1, v1, a1                              ; turn v1 from PhysAddr to LogAddr

        LDR     a2, [a1, #InitUsedBlock]                ; turn this from Phys to Log too
        SUB     a2, a2, lr
        ADD     a2, a2, a1
        STR     a2, [a1, #InitUsedBlock]


; Store the logical address of the HAL descriptor
        LDR     a2, [sp, #8]
        STR     a2, [a1, #HAL_Descriptor]

        MOV     v3, #0                                  ; "MMU is on" signal

        BL      ARM_Analyse

        ChangedProcVecs a1

        MOV     a1, #L1_Fault
        BL      RISCOS_ReleasePhysicalAddress

        LDR     a1, =HALWorkspace
        LDR     a2, =ZeroPage
        LDR     a3, [a2, #HAL_WsSize]
      [ ZeroPage <> 0
        MOV     a2, #0
      ]
        BL      memset

        LDR     a2, =ZeroPage
        LDR     a1, =IOLimit
        STR     a1, [a2, #IOAllocLimit]
        LDR     a1, =IO
        STR     a1, [a2, #IOAllocPtr]

        BL      SetUpHALEntryTable

; Initialise the HAL. Due to its memory claiming we need to get our v1 and v2 values
; into workspace and out again around it.

        LDR     a1, =ZeroPage
        STR     v1, [a1, #InitUsedBlock]
        STR     v2, [a1, #InitUsedEnd]

        LDR     a1, =RISCOS_Header
        LDR     a2, =HALWorkspaceNCNB
        AddressHAL
        CallHAL HAL_Init

        DebugTX "HAL initialised"

        MOV     a1, #64 ; Old limit prior to OMAP3 port
        CallHAL HAL_IRQMax
        CMP     a1, #MaxInterrupts
        MOVHI   a1, #MaxInterrupts ; Avoid catastrophic failure if someone forgot to increase MaxInterrupts
        LDR     a2, =ZeroPage
        STR     a1, [a2, #IRQMax]        

        LDR     v1, [a2, #InitUsedBlock]
        LDR     v2, [a2, #InitUsedEnd]

; Start timer zero, at 100 ticks per second
        MOV     a1, #0
        CallHAL HAL_TimerGranularity

        MOV     a2, a1
        MOV     a1, #100
        BL      __rt_udiv

        MOV     a2, a1
        MOV     a1, #0
        CallHAL HAL_TimerSetPeriod

        DebugTX "IICInit"

        BL      IICInit

; Remember some stuff that's about to get zapped
        LDR     ip, =ZeroPage
        LDR     v4, [ip, #ROMPhysAddr]
        LDR     v5, [ip, #RAMLIMIT]
        LDR     v7, [ip, #MaxCamEntry]
        LDR     v8, [ip, #IRQMax]

        LDR     a1, [ip, #HAL_StartFlags]
        TST     a1, #OSStartFlag_RAMCleared
        BLEQ    ClearWkspRAM            ; Only clear the memory if the HAL didn't
         
; Put it back
        LDR     ip, =ZeroPage
        STR     v4, [ip, #ROMPhysAddr]
        STR     v5, [ip, #RAMLIMIT]
        STR     v7, [ip, #MaxCamEntry]
        STR     v8, [ip, #IRQMax]

; Calculate CPU feature flags
        BL      ReadCPUFeatures

        MOV     v8, ip

        DebugTX "HAL_CleanerSpace"

; Set up the data cache cleaner space if necessary (eg. for StrongARM core)
        MOV     a1, #-1
        CallHAL HAL_CleanerSpace
        CMP     a1, #-1          ;-1 means none needed (HAL only knows this if for specific ARM core eg. system-on-chip)
        BEQ     %FT20
        LDR     a2, =DCacheCleanAddress
        LDR     a3, =AreaFlags_DCacheClean
        ASSERT  DCacheCleanSize = 4*&10000                ; 64k of physical space used 4 times (allows large page mapping)
        MOV     a4, #&10000
        MOV     ip, #4
        SUB     sp, sp, #5*4       ;room for a1-a4,ip
10
        STMIA   sp, {a1-a4, ip}
        BL      Init_MapIn
        LDMIA   sp, {a1-a4, ip}
        SUBS    ip, ip, #1
        ADD     a2, a2, #&10000
        BNE     %BT10
        ADD     sp, sp, #5*4

20
; Decompress the ROM
        LDR     a1, =RISCOS_Header
        LDR     a2, [a1, #OSHdr_DecompressHdr]
        CMP     a2, #0
        BEQ     %FT30
        ADD     ip, a1, a2
        ASSERT  OSDecompHdr_WSSize = 0
        ASSERT  OSDecompHdr_Code = 4
        LDMIA   ip, {a3-a4}
        ADRL    a2, SyncCodeAreas
        CMP     a3, #0 ; Any workspace required?
        ADD     a4, a4, ip
   [ DebugHALTX
        BNE     %FT25
        DebugTX "Decompressing ROM, no workspace required"
      [ NoARMv5
        MOV     lr, pc
        MOV     pc, a4
      |
        BLX     a4
      ]
        DebugTX "Decompression complete"
        B       %FT27
25
   |
        ADREQ   lr, %FT27
        MOVEQ   pc, a4
   ]
        Push    "a1-a4,v1-v2,v5-v7"
; Allocate workspace for decompression code
; Workspace is located at a 4MB-aligned log addr, and is a multiple of 1MB in
; size. This greatly simplifies the code required to free the workspace, since
; we can guarantee it will have been section-mapped, and won't hit any
; partially-allocated L2PT blocks (where 4 L1PT entries point to subsections of
; the same L2PT page)
; This means all we need to do to free the workspace is zap the L1PT entries
; and rollback v1 & v2
; Note: This is effectively a MB-aligned version of Init_MapInRAM
ROMDecompWSAddr * 4<<20
        DebugTX "Allocating decompression workspace"
        LDR     v5, =(1<<20)-1
        ADD     v7, a3, v5
        BIC     v7, v7, v5 ; MB-aligned size
        STR     v7, [sp, #8] ; Overwrite stacked WS size
        MOV     v6, #ROMDecompWSAddr ; Current log addr
26
        ADD     v2, v2, v5
        BIC     v2, v2, v5 ; MB-aligned physram
        LDMIA   v1, {a2, a3}
        SUB     a2, v2, a2 ; Amount of bank used
        SUB     a2, a3, a2 ; Amount of bank remaining
        MOVS    a2, a2, ASR #20 ; Round down to nearest MB
        LDRLE   v2, [v1, #8]! ; Move to next bank if 0MB left
        BLE     %BT26
        CMP     a2, v7, LSR #20
        MOVHS   a4, v7
        MOVLO   a4, a2, LSL #20 ; a4 = amount to take
        MOV     a1, v2 ; set up parameters for MapIn call
        MOV     a2, v6
        MOV     a3, #OSAP_None
        SUB     v7, v7, a4 ; Decrease amount to allocate
        ADD     v2, v2, a4 ; Increase physram ptr
        ADD     v6, v6, a4 ; Increase logram ptr
        BL      Init_MapIn
        CMP     v7, #0
        BNE     %BT26
        Pull    "a1-a2,v1-v2" ; Pull OS header, IMB func ptr, workspace size, decompression code
        MOV     a3, #ROMDecompWSAddr
        DebugTX "Decompressing ROM"
      [ NoARMv5
        MOV     lr, pc
        MOV     pc, v2
      |
        BLX     v2
      ]
        DebugTX "Decompression complete"
; Before we free the workspace, make sure we zero it
        MOV     a1, #ROMDecompWSAddr
        MOV     a2, #0
        MOV     a3, v1
        BL      memset
; Flush the workspace from the cache so we can unmap it
; Really we should make the pages uncacheable first, but for simplicity we do a
; full cache+TLB clean+invalidate later on when changing the ROM permissions
        MOV     a1, #ROMDecompWSAddr
        ADD     a2, a1, v1
        ARMop   Cache_CleanInvalidateRange
; Zero each L1PT entry
        LDR     a1, =L1PT+(ROMDecompWSAddr>>18)
        MOV     a2, #0
        MOV     a3, v1, LSR #18
        BL      memset
; Pop our registers and we're done
        Pull    "v1-v2,v5-v7"
        DebugTX "ROM decompression workspace freed"
27
; Now that the ROM is decompressed we need to change the ROM page mapping to
; read-only. The easiest way to do this is to make another call to Init_MapIn.
; But before we can do that we need to work out if the HAL+OS are contiguous in
; physical space. To do this we can just check if the L1PT entry for the OS is a
; section mapping.
        LDR     a1, =L1PT+(ROM>>18)
        LDR     a1, [a1]
        ASSERT  L1_Section = 2
        ASSERT  L1_Page = 1
        TST     a1, #2
; Section mapped, get address from L1PT
        MOVNE   a1, a1, LSR #20
        MOVNE   a1, a1, LSL #20
        MOVNE   a2, #ROM
        MOVNE   a4, #OSROM_ImageSize*1024
        BNE     %FT29
; Page/large page mapped, get address from L2PT
        LDR     a2, =RISCOS_Header
        LDR     a1, =L2PT
        LDR     a1, [a1, a2, LSR #10]
        LDR     a4, [a2, #OSHdr_ImageSize]
        MOV     a1, a1, LSR #12
        MOV     a1, a1, LSL #12
29
        MOV     a3, #OSAP_ROM
        BL      Init_MapIn
; Flush & invalidate cache/TLB to ensure everything respects the new page access
; Putting a flush here also means the decompression code doesn't have to worry
; about IMB'ing the decompressed ROM
        ARMop   MMU_Changing ; Perform full clean+invalidate to ensure any lingering cache lines for the decompression workspace are gone
        DebugTX "ROM access changed to read-only"
30
; Allocate the CAM
        LDR     a3, [v8, #SoftCamMapSize]
        LDR     a2, =AreaFlags_CAM
        LDR     a1, =CAM
        BL      Init_MapInRAM

; Allocate the supervisor stack
        LDR     a1, =SVCStackAddress
        LDR     a2, =AreaFlags_SVCStack
        LDR     a3, =SVCStackSize
        BL      Init_MapInRAM

; Allocate the interrupt stack
        LDR     a1, =IRQStackAddress
        LDR     a2, =AreaFlags_IRQStack
        LDR     a3, =IRQStackSize
        BL      Init_MapInRAM

; Allocate the abort stack
        LDR     a1, =ABTStackAddress
        LDR     a2, =AreaFlags_ABTStack
        LDR     a3, =ABTStackSize
        BL      Init_MapInRAM

; Allocate the undefined stack
        LDR     a1, =UNDStackAddress
        LDR     a2, =AreaFlags_UNDStack
        LDR     a3, =UNDStackSize
        BL      Init_MapInRAM

; Allocate the system heap (just 32K for now - will grow as needed)
        LDR     a1, =SysHeapAddress
        LDR     a2, =AreaFlags_SysHeap
        LDR     a3, =32*1024
        BL      Init_MapInRAM_Clear

; Allocate the cursor/system/sound block - first the cached bit
        LDR     a1, =CursorChunkAddress
        LDR     a2, =AreaFlags_CursorChunkCacheable
        LDR     a3, =SoundDMABuffers - CursorChunkAddress
        BL      Init_MapInRAM_DMA
; then the uncached bit
        LDR     a1, =SoundDMABuffers
        LDR     a2, =AreaFlags_CursorChunk
        LDR     a3, =?SoundDMABuffers
        BL      Init_MapInRAM_DMA

        LDR     a1, =KbuffsBaseAddress
        LDR     a2, =AreaFlags_Kbuffs
        LDR     a3, =(KbuffsSize + &FFF) :AND: &FFFFF000  ;(round to 4k)
        BL      Init_MapInRAM_Clear

 [ HiProcVecs
        ; Map in DebuggerSpace
        LDR     a1, =DebuggerSpace
        LDR     a2, =AreaFlags_DebuggerSpace
        LDR     a3, =(DebuggerSpace_Size + &FFF) :AND: &FFFFF000
        BL      Init_MapInRAM_Clear
 ]

 [ MinorL2PThack
; Allocate backing L2PT for application space
; Note that ranges must be 4M aligned, as AllocateL2PT only does individual
; (1M) sections, rather than 4 at a time, corresponding to a L2PT page. The
; following space is available for dynamic areas, and ChangeDyn.s will get
; upset if it sees only some out of a set of 4 section entries pointing to the
; L2PT page.
        MOV     a1, #0
        MOV     a2, #AplWorkMaxSize             ; Not quite right, but the whole thing's wrong anyway
        ASSERT  AplWorkMaxSize :MOD: (4*1024*1024) = 0
        BL      AllocateL2PT
; And for the system heap. Sigh
        LDR     a1, =SysHeapAddress
        LDR     a2, =SysHeapMaxSize
        ASSERT  SysHeapAddress :MOD: (4*1024*1024) = 0 
        ASSERT  SysHeapMaxSize :MOD: (4*1024*1024) = 0
        BL      AllocateL2PT
 ]

        STR     v2, [v8, #InitUsedEnd]

        ; Put InitDMABlock back into PhysRamTable
        Push    "v1-v7"
        ASSERT  InitDMAOffset = InitDMABlock+8
        ADD     v1, v8, #InitDMABlock
        LDMIA   v1, {v1-v3}
        ADD     v3, v3, #PhysRamTable
        ADD     v3, v3, v8
        ; Work out whether the block was removed or merely shrunk
        LDMDB   v3, {v4-v5}
        ADD     v6, v1, v2
        ADD     v7, v4, v5
        STMDB   v3, {v1-v2}
        TEQ     v6, v7
        BEQ     %FT40                   ; End addresses match, it was shrunk
35
        LDMIA   v3, {v1-v2}             ; Shuffle following entries down
        STMIA   v3!, {v4-v5}
        MOV     v4, v1
        MOVS    v5, v2
        BNE     %BT35
40
        Pull    "v1-v7"

        MSR     CPSR_c, #F32_bit+I32_bit+IRQ32_mode
        LDR     sp, =IRQSTK
        MSR     CPSR_c, #F32_bit+I32_bit+ABT32_mode
        LDR     sp, =ABTSTK
        MSR     CPSR_c, #F32_bit+I32_bit+UND32_mode
        LDR     sp, =UNDSTK
        MSR     CPSR_c, #F32_bit+SVC2632
        LDR     sp, =SVCSTK

        LDR     ip, =CAM
        STR     ip, [v8, #CamEntriesPointer]

        BL      ConstructCAMfromPageTables

        MOV     a1, #4096
        STR     a1, [v8, #Page_Size]

        BL      CountPageTablePages

        B       Continue_after_HALInit

        LTORG

 [ MEMM_Type = "VMSAv6"
HAL_InvalidateCache_ARMvF
        ; Cache invalidation for ARMs with multiple cache levels, used before ARMop initialisation
        ; This function gets called before we have a stack set up, so we've got to preserve as many registers as possible
        ; The only register we can safely change is ip, but we can switch into FIQ mode with interrupts disabled and use the banked registers there
        MRS     ip, CPSR
        MSR     CPSR_c, #F32_bit+I32_bit+FIQ32_mode
        MOV     r9, #0
        MCR     p15, 0, r9, c7, c5, 0           ; invalidate instruction cache
        MCR     p15, 0, r9, c8, c7, 0           ; invalidate TLBs
        MCR     p15, 0, r9, c7, c5, 6           ; invalidate branch target predictor
        myDSB   ,r9,,y                          ; Wait for completion
        myISB   ,r9,,y
        ; Check whether we're ARMv7 (and thus multi-level cache) or ARMv6 (and thus single-level cache)
        MRC     p15, 0, r8, c0, c0, 1
        TST     r8, #&80000000 ; EQ=ARMv6, NE=ARMv7
        BEQ     %FT80

        ; This is basically the same algorithm as the MaintainDataCache_WB_CR7_Lx macro, but tweaked to use less registers and to read from CP15 directly
        MRC     p15, 1, r8, c0, c0, 1           ; Read CLIDR to r8
        TST     r8, #&07000000
        BEQ     %FT50
        MOV     r11, #0 ; Current cache level
10 ; Loop1
        ADD     r10, r11, r11, LSR #1 ; Work out 3 x cachelevel
        MOV     r9, r8, LSR r10 ; bottom 3 bits are the Cache type for this level
        AND     r9, r9, #7 ; get those 3 bits alone
        CMP     r9, #2
        BLT     %FT40 ; no cache or only instruction cache at this level
        MCR     p15, 2, r11, c0, c0, 0 ; write CSSELR from r11
        ISB
        MRC     p15, 1, r9, c0, c0, 0 ; read current CSSIDR to r9
        AND     r10, r9, #CCSIDR_LineSize_mask ; extract the line length field
        ADD     r10, r10, #4 ; add 4 for the line length offset (log2 16 bytes)
        LDR     r8, =CCSIDR_Associativity_mask:SHR:CCSIDR_Associativity_pos
        AND     r8, r8, r9, LSR #CCSIDR_Associativity_pos ; r8 is the max number on the way size (right aligned)
        CLZ     r13, r8 ; r13 is the bit position of the way size increment
        LDR     r12, =CCSIDR_NumSets_mask:SHR:CCSIDR_NumSets_pos
        AND     r12, r12, r9, LSR #CCSIDR_NumSets_pos ; r12 is the max number of the index size (right aligned)
20 ; Loop2
        MOV     r9, r12 ; r9 working copy of the max index size (right aligned)
30 ; Loop3
        ORR     r14, r11, r8, LSL r13 ; factor in the way number and cache number into r14
        ORR     r14, r14, r9, LSL r10 ; factor in the index number
        DCISW   r14 ; Invalidate
        SUBS    r9, r9, #1 ; decrement the index
        BGE     %BT30
        SUBS    r8, r8, #1 ; decrement the way number
        BGE     %BT20
        DSB                ; Cortex-A7 errata 814220: DSB required when changing cache levels when using set/way operations. This also counts as our end-of-maintenance DSB.
        MRC     p15, 1, r8, c0, c0, 1
40 ; Skip
        ADD     r11, r11, #2
        AND     r14, r8, #&07000000
        CMP     r14, r11, LSL #23
        BGT     %BT10        

50 ; Finished
        ; Wait for clean to complete
        MOV     r8, #0
        MCR     p15, 0, r8, c7, c5, 0           ; invalidate instruction cache
        MCR     p15, 0, r8, c8, c7, 0           ; invalidate TLBs
        MCR     p15, 0, r8, c7, c5, 6           ; invalidate branch target predictor
        myDSB   ,r8,,y                          ; Wait for completion
        myISB   ,r8,,y
        ; All caches clean; switch back to SVC, then recover the stored PSR from ip (although we can be fairly certain we started in SVC anyway)
        MSR     CPSR_c, #F32_bit+I32_bit+SVC32_mode
        MSR     CPSR_cxsf, ip
        MOV     pc, lr
80 ; ARMv6 case
        MCR     ARM_config_cp,0,r9,ARMv4_cache_reg,C7 ; ARMv3-ARMv6 I+D cache flush
        B       %BT50
 ] ; MEMM_Type = "VMSAv6"

CountPageTablePages ROUT
        Entry
        LDR     a1, =ZeroPage
        LDR     a2, =CAM
        LDR     a3, [a1, #MaxCamEntry]
      [ ZeroPage <> 0
        MOV     a1, #0
      ]
        ADD     a3, a3, #1
        ADD     a4, a2, a3, LSL #CAM_EntrySizeLog2
        ASSERT  (L2PT :AND: &3FFFFF) = 0
        LDR     lr, =L2PT :SHR: 22
10      LDR     ip, [a4, #CAM_LogAddr-CAM_EntrySize]!
        TEQ     lr, ip, LSR #22
        ADDEQ   a1, a1, #4096
        TEQ     a4, a2
        BNE     %BT10
        LDR     a2, =ZeroPage
        STR     a1, [a2, #L2PTUsed]
        EXIT

; int PhysAddrToPageNo(void *addr)
;
; Converts a physical address to the page number of the page containing it.
; Returns -1 if address is not in RAM.

PhysAddrToPageNo
        MOV     a4, #0
        LDR     ip, =ZeroPage + PhysRamTable
10      LDMIA   ip!, {a2, a3}                   ; get phys addr, size
        MOVS    a3, a3, LSR #12                 ; end of list? (size=0)
        BEQ     %FT90                           ;   then it ain't RAM
        SUB     a2, a1, a2                      ; a2 = amount into this bank
        CMP     a2, a3, LSL #12                 ; if more than size
        ADDHS   a4, a4, a3, LSL #12             ;   increase counter by size of bank
        BHS     %BT10                           ;   and move to next
        ADD     a4, a4, a2                      ; add offset to counter
        MOV     a1, a4, LSR #12                 ; convert counter to a page number
        MOV     pc, lr

90      MOV     a1, #-1
        MOV     pc, lr


; A routine to construct the soft CAM from the page tables. This is used
; after a soft reset, and also on a hard reset as it's an easy way of
; clearing up after the recursive page table allocaton.

        ROUT
ConstructCAMfromPageTables
        Push    "v1-v8, lr"
        LDR     a1, =ZeroPage
        LDR     a2, [a1, #MaxCamEntry]
        LDR     v1, =CAM                        ; v1 -> CAM (for whole routine)
        ADD     a2, a2, #1
        ADD     a2, v1, a2, LSL #CAM_EntrySizeLog2

        LDR     a3, =DuffEntry                  ; Clear the whole CAM, from
        MOV     a4, #AreaFlags_Duff             ; the top down.
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
        ASSERT  CAM_PMP=8
        ASSERT  CAM_PMPIndex=12
        ASSERT  CAM_EntrySize=16
        MOV     v2, #0
        MOV     v3, #-1
10      STMDB   a2!, {a3, a4, v2, v3}
        CMP     a2, v1
        BHI     %BT10

        MOV     v2, #0                          ; v2 = logical address
        LDR     v3, =L1PT                       ; v3 -> L1PT (not used much)
        LDR     v4, =L2PT                       ; v4 -> L2PT
30      LDR     a1, [v3, v2, LSR #18]           ; a1 = first level descriptor
        BL      DecodeL1Entry                   ; a1 = phys addr, a2 = page flags/type
        CMP     a2, #-2                         ; Only care about page table pointers
        BEQ     %FT40
        ADDS    v2, v2, #&00100000
        BCC     %BT30
        Pull    "v1-v8, pc"

40      LDR     a1, [v4, v2, LSR #10]           ; a1 = second level descriptor
        BL      DecodeL2Entry                   ; a1 = phys addr, a2 = flags (-1 if fault), a3 = page size (bytes)
        CMP     a2, #-1                         ; move to next page if fault
        BEQ     %FT80
        SUBS    a3, a3, #4096                   ; large pages get bits 12-15 from the virtual address
        ANDNE   lr, v2, a3
        ORR     v6, a2, #PageFlags_Unavailable
        ORRNE   a1, a1, lr
        BL      PhysAddrToPageNo                ; -1 if unknown page
        ADDS    a1, v1, a1, LSL #CAM_EntrySizeLog2 ; a1 -> CAM entry
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
        STMCCIA a1, {v2, v6}                    ; store logical address, PPL
 
80      ADD     v2, v2, #&00001000
        TST     v2, #&000FF000
        BNE     %BT40
        TEQ     v2, #0                          ; yuck (could use C from ADDS but TST corrupts C
        BNE     %BT30                           ; because of big constant)
        
        Pull    "v1-v8, pc"



; Allocate a physical page from DRAM
;
; On entry:
;    v1 -> current entry in PhysRamTable
;    v2 -> end of last used physical page
; On exit:
;    a1 -> next free page
;    v1, v2 updated
;
; No out of memory check...

Init_ClaimPhysicalPage
        MOV     a1, v2
        LDMIA   v1, {a2, a3}
        MOV     a3, a3, LSR #12
        ADD     a2, a2, a3, LSL #12             ; ip = end of this bank
        CMP     v2, a2                          ; advance v2 to next bank if
        LDRHS   a1, [v1, #8]!                   ; this bank is fully used
        ADD     v2, a1, #4096
        MOV     pc, lr

; Allocate and map in some RAM.
;
; On entry:
;    a1 = logical address
;    a2 = access permissions (see Init_MapIn)
;    a3 = length (4K multiple)
;    v1 -> current entry in PhysRamTable
;    v2 = next physical address
;    v3 -> L1PT
;
; On exit:
;    a1 -> physical address of start of RAM (deduce the rest from PhysRamTable)
;
; No out of memory check...
Init_MapInRAM ROUT
        Push    "v4-v8,lr"
        MOV     v8, #-1
        MOV     v5, a3                          ; v5 = amount of memory required
        MOV     v6, a1                          ; v6 = logical address
        MOV     v7, a2                          ; v7 = access permissions
10      LDMIA   v1, {v4, ip}                    ; v4 = addr of bank, ip = len+flags
        MOV     ip, ip, LSR #12
        SUB     v4, v2, v4                      ; v4 = amount of bank used
        RSBS    v4, v4, ip, LSL #12             ; v4 = amount of bank left
        LDREQ   v2, [v1, #8]!                   ; move to next bank if 0 left
        BEQ     %BT10

        CMP     v8, #-1                         ; is this the first bank?
        MOVEQ   v8, v2                          ; remember it

        CMP     v4, v5                          ; sufficient in this bank?
        MOVHS   a4, v5
        MOVLO   a4, v4                          ; a4 = amount to take

        MOV     a1, v2                          ; set up parameters for MapIn call
        MOV     a2, v6                          ; then move globals (in case MapIn
        MOV     a3, v7                          ; needs to allocate for L2PT)
        ADD     v2, v2, a4                      ; advance physaddr
        SUB     v5, v5, a4                      ; decrease wanted
        ADD     v6, v6, a4                      ; advance address pointer
        BL      Init_MapIn                      ; map in the RAM
        TEQ     v5, #0                          ; more memory still required?
        BNE     %BT10

        MOV     a1, v8
        Pull    "v4-v8,pc"

Init_MapInRAM_Clear ROUT                        ; same as Init_MapInRAM but also
        Push    "a1,a3,v5,lr"                   ; clears the mapped in result
        BL      Init_MapInRAM
        MOV     v5, a1
        Pull    "a1,a3"
        MOV     a2, #0
        BL      memset        
        MOV     a1, v5
        Pull    "v5,pc"

; Allocate and map a physically contigous chunk of some DMAable RAM.
;
; On entry:
;    a1 = logical address
;    a2 = access permissions (see Init_MapIn)
;    a3 = length (4K multiple)
;    v1 -> current entry in PhysRamTable
;    v2 = next physical address
;    v3 -> L1PT
;
; On exit:
;    a1 -> physical address of start of RAM (deduce the rest from PhysRamTable)
;
; Use this routine with caution - correct total amount of required DMA memory
; must have been calculated beforehand and stashed in InitDMABlock
Init_MapInRAM_DMA ROUT
        Push    "a1,a3,v4-v5,ip,lr"
        TEQ     v3, #0                          ; MMU on?
        LDREQ   v4, =ZeroPage                   ; get workspace directly
        ADDNE   v4, v3, #DRAMOffset_PageZero-DRAMOffset_L1PT ; deduce from L1PT
        LDR     v5, [v4, #InitDMAEnd]
        ADD     lr, v5, a3                      ; claim the RAM
        STR     lr, [v4, #InitDMAEnd]

        MOV     a4, a3
        MOV     a3, a2
        MOV     a2, a1
        MOV     a1, v5
        BL      Init_MapIn                      ; map it in
        ; DMA regions won't get cleared by ClearWkspRam, so do it manually
        ; Could potentially skip this if the HAL says RAM is already clear, but
        ; for now do it anyway (especially since startup flags haven't been set
        ; when we're first called)
        Pull    "a1,a3"
        TEQ     v3, #0
        MOVNE   a1, v5
        MOV     a2, #0
        BL      memset        
        MOV     a1, v5
        Pull    "v4-v5,ip,pc"

; Map a range of physical addresses to a range of logical addresses.
;
; On entry:
;    a1 = physical address
;    a2 = logical address
;    a3 = DA flags
;    a4 = area size (4K multiple)
;    v1 -> current entry in PhysRamTable
;    v2 = last used physical address
;    v3 -> L1PT (or 0 if MMU on)

Init_MapIn ROUT
        Entry   "v4-v7"
        MOV     v4, a1                          ; v4 = physaddr
        MOV     v5, a2                          ; v5 = logaddr
        MOV     v6, a3                          ; v6 = page flags
        MOV     v7, a4                          ; v7 = area size
        ; Set up a2-a4 for the Get*PTE functions
        TEQ     v3, #0
        LDREQ   a3, =ZeroPage
        ADDNE   a3, v3, #DRAMOffset_PageZero-DRAMOffset_L1PT
        MOV     a2, v6
        LDR     a4, [a3, #MMU_PCBTrans]
        LDR     a3, [a3, #MMU_PPLTrans]

        ORR     lr, v4, v5                      ; OR together, physaddr, logaddr
        ORR     lr, lr, v7                      ; and size.
        MOVS    ip, lr, LSL #12                 ; If all bottom 20 bits 0
        BEQ     %FT50                           ; it's section mapped

        MOV     a1, #0                          ; We don't want the address in the result

        MOVS    ip, lr, LSL #16                 ; If bottom 16 bits not all 0
        ADR     lr, %FT10
        BNE     Get4KPTE                        ; then small pages (4K)

        BL      Get64KPTE                       ; else large pages (64K)
10
        MOV     v6, a1                          ; v6 = access permissions

20      MOV     a1, v4
        MOV     a2, v5
        MOV     a3, v6
        BL      Init_MapInPage                  ; Loop through mapping in each
        ADD     v4, v4, #4096                   ; page in turn
        ADD     v5, v5, #4096
        SUBS    v7, v7, #4096
        BNE     %BT20
        EXIT

50
        BL      Get1MPTE
        MOVS    ip, v3                          ; is MMU on?
        LDREQ   ip, =L1PT                       ; then use virtual address
        ADD     a2, ip, v5, LSR #18             ; a2 -> L1PT entry
70      STR     a1, [a2], #4                    ; And store in L1PT
        ADD     a1, a1, #1024*1024              ; Advance one megabyte
        SUBS    v7, v7, #1024*1024              ; and loop
        BNE     %BT70
        EXIT

; Map a logical page to a physical page, allocating L2PT as necessary.
;
; On entry:
;    a1 = physical address
;    a2 = logical address
;    a3 = access permissions + C + B bits + size (all non-address bits, of appropriate type)
;    v1 -> current entry in PhysRamTable
;    v2 = last used physical address
;    v3 -> L1PT (or 0 if MMU on)
; On exit:
;    a1 = logical address
;    a2-a4, ip corrupt
;    v1, v2 updated
;

Init_MapInPage  ROUT
        Entry   "v4-v6"
        MOV     v4, a1                          ; v4 = physical address
        MOV     v5, a2                          ; v5 = logical address
        MOV     v6, a3                          ; v6 = access permissions
        MOV     a1, v5
        MOV     a2, #4096
        BL      AllocateL2PT
        TEQ     v3, #0                          ; if MMU on, access L2PT virtually...
        LDREQ   a1, =L2PT                       ; a1 -> L2PT virtual address
        MOVEQ   ip, v5                          ; index using whole address
        BEQ     %FT40
        MOV     ip, v5, LSR #20
        LDR     a1, [v3, ip, LSL #2]            ; a1 = level one descriptor
        MOV     a1, a1, LSR #10
        MOV     a1, a1, LSL #10                 ; a1 -> L2PT tables for this section
        AND     ip, v5, #&000FF000              ; extract L2 table index bits
40      AND     lr, v6, #3
        TEQ     lr, #L2_LargePage               ; strip out surplus address bits from
        BICEQ   v4, v4, #&0000F000              ; large page descriptors
        ORR     lr, v4, v6                      ; lr = value for L2PT entry
        STR     lr, [a1, ip, LSR #10]           ; update L2PT entry
        MOV     a1, v5
        EXIT



; On entry:
;    a1 = virtual address L2PT required for
;    a2 = number of bytes of virtual space
;    v1 -> current entry in PhysRamTable
;    v2 = last used physical address
;    v3 -> L1PT (or 0 if MMU on)
; On exit
;    a1-a4,ip corrupt
;    v1, v2 updated
AllocateL2PT ROUT
        Entry   "v4-v8"
        MOV     v8, a1, LSR #20                 ; round base address down to 1M
        ADD     lr, a1, a2
        MOV     v7, lr, LSR #20
        TEQ     lr, v7, LSL #20
        ADDNE   v7, v7, #1                      ; round end address up to 1M

        MOVS    v6, v3
        LDREQ   v6, =L1PT                       ; v6->L1PT (whole routine)

05      LDR     v5, [v6, v8, LSL #2]            ; L1PT contains 1 word per M
        TEQ     v5, #0                          ; if non-zero, the L2PT has
                                                ; already been allocated
        BNE     %FT40

        BIC     lr, v8, #3                      ; round down to 4M - each page
        ADD     lr, v6, lr, LSL #2              ; of L2PT maps to 4 sections
        LDMIA   lr, {a3,a4,v5,ip}               ; check if any are page mapped
        ASSERT  L1_Fault = 2_00 :LAND: L1_Page = 2_01 :LAND: L1_Section = 2_10
        TST     a3, #1
        TSTEQ   a4, #1
        TSTEQ   v5, #1
        TSTEQ   ip, #1
        BEQ     %FT20                           ; nothing page mapped - claim a page

        TST     a4, #1                          ; at least one of the sections is page mapped
        SUBNE   a3, a4, #1*1024                 ; find out where it's pointing to and
        TST     v5, #1                          ; derive the corresponding address for our
        SUBNE   a3, v5, #2*1024                 ; section
        TST     ip, #1
        SUBNE   a3, ip, #3*1024

        AND     lr, v8, #3
        ORR     a3, a3, lr, LSL #10
        STR     a3, [v6, v8, LSL #2]            ; fill in the L1PT entry
        B       %FT40                           ; no more to do

20      BL      Init_ClaimPhysicalPage          ; Claim a page to put L2PT in
        MOV     v4, a1

      [ MEMM_Type = "VMSAv6"
        ORR     a3, a1, #L1_Page
      |
        ORR     a3, a1, #L1_Page + L1_U         ; Set the U bit for ARM6 (assume L2 pages will generally be cacheable)
      ]
        AND     lr, v8, #3
        ORR     a3, a3, lr, LSL #10
        STR     a3, [v6, v8, LSL #2]            ; fill in the L1PT

; Need to zero the L2PT. Must do it before calling in MapInPage, as that may well
; want to put something in the thing we are clearing. If the MMU is off, no problem,
; but if the MMU is on, then the L2PT isn't accessible until we've called MapInPage.
; Solution is to use the AccessPhysicalAddress call.

        TEQ     v3, #0                          ; MMU on?
        MOVNE   a1, v4                          ; if not, just access v4
        MOVEQ   a1, #L1_B                       ; if so, map in v4
        MOVEQ   a2, v4
        SUBEQ   sp, sp, #4
        MOVEQ   a3, sp
        BLEQ    RISCOS_AccessPhysicalAddress

        MOV     a2, #0
        MOV     a3, #4*1024
        BL      memset

        TEQ     v3, #0
        LDREQ   a1, [sp], #4
        BLEQ    RISCOS_ReleasePhysicalAddress

        ; Get the correct page table entry flags for Init_MapInPage
        TEQ     v3, #0
        LDREQ   a3, =ZeroPage
        ADDNE   a3, v3, #DRAMOffset_PageZero-DRAMOffset_L1PT
        LDR     a2, [a3, #PageTable_PageFlags]
        LDR     a4, [a3, #MMU_PCBTrans]
        LDR     a3, [a3, #MMU_PPLTrans]        
        MOV     a1, #0
        BL      Get4KPTE

        MOV     a3, a1
        MOV     a1, v4                          ; Map in the L2PT page itself
        LDR     a2, =L2PT                       ; (can't recurse, because L2PT
        ADD     a2, a2, v8, LSL #10             ; backing for L2PT is preallocated)
        BIC     a2, a2, #&C00
        BL      Init_MapInPage


40      ADD     v8, v8, #1                      ; go back until all
        CMP     v8, v7                          ; pages allocated
        BLO     %BT05

        EXIT


; void *RISCOS_AccessPhysicalAddress(unsigned int flags, void *addr, void **oldp)
RISCOS_AccessPhysicalAddress ROUT
        ; Only flag user can ask for is bufferable
        ; Convert to appropriate DA flags
        ; (n.b. since this is an internal routine we should really change it to pass in DA flags directly)
        TST     a1, #L1_B
        LDR     a1, =OSAP_None + DynAreaFlags_NotCacheable ; SVC RW, USR none
        ORREQ   a1, a1, #DynAreaFlags_NotBufferable
RISCOS_AccessPhysicalAddressUnchecked                   ; well OK then, I trust you know what you're doing
        LDR     ip, =L1PT + (PhysicalAccess:SHR:18)     ; ip -> L1PT entry
        MOV     a4, a2, LSR #20                         ; rounded to section
        MOV     a4, a4, LSL #20
        GetPTE  a1, 1M, a4, a1                          ; a1 = complete descriptor
 [ MEMM_Type = "VMSAv6"
        ORR     a1, a1, #L1_XN                          ; force non-executable to prevent speculative instruction fetches
 ]
        TEQ     a3, #0
        LDRNE   a4, [ip]                                ; read old value (if necessary)
        STR     a1, [ip]                                ; store new one
        STRNE   a4, [a3]                                ; put old one in [oldp]

        LDR     a1, =PhysicalAccess
        MOV     a3, a2, LSL #12                         ; take bottom 20 bits of address
        ORR     a3, a1, a3, LSR #12                     ; and make an offset within PhysicalAccess
        Push    "a3,lr"
        ARMop   MMU_ChangingUncached                    ; sufficient, cause not cacheable
        Pull    "a1,pc"

; void RISCOS_ReleasePhysicalAddress(void *old)
RISCOS_ReleasePhysicalAddress
        LDR     ip, =L1PT + (PhysicalAccess:SHR:18)     ; ip -> L1PT entry
        STR     a1, [ip]
        LDR     a1, =PhysicalAccess
        ARMop   MMU_ChangingUncached,,tailcall          ; sufficient, cause not cacheable


; void Init_PageTablesChanged(void)
;
; A TLB+cache invalidation that works on all known ARMs. Invalidate all I+D TLB is the _only_ TLB
; op that works on ARM720T, ARM920T and SA110. Ditto invalidate all I+D cache.
;
; DOES NOT CLEAN THE DATA CACHE. This is a helpful simplification, but requires that don't use
; this routine after we've started using normal RAM.
;
Init_PageTablesChanged
        MOV     a3, lr
        BL      Init_ARMarch
        MOV     ip, #0
        BNE     %FT01
        MCREQ   ARM_config_cp,0,ip,ARMv3_TLBflush_reg,C0
        B       %FT02
01      MCRNE   ARM_config_cp,0,ip,ARMv4_TLB_reg,C7
02
 [ MEMM_Type = "VMSAv6"
        CMP     a1, #ARMvF
        ADREQ   lr, %FT01
        BEQ     HAL_InvalidateCache_ARMvF
        MCRNE   ARM_config_cp,0,ip,ARMv4_cache_reg,C7           ; works on ARMv3
01
 |
        MCR     ARM_config_cp,0,ip,ARMv4_cache_reg,C7           ; works on ARMv3
 ]
        MOV     pc, a3




;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
;
;       ClearWkspRAM - Routine to clear "all" workspace
;
; We have to avoid anything between InitUsedStart and InitUsedEnd - i.e.
; the page tables, HAL workspace, etc.
;
; Note that zero page workspace isn't included in InitUsedStart/InitUsedEnd.
; Sensitive areas of it (e.g. PhysRamTable, IRQ vector) are skipped via the
; help of RamSkipTable
;
; The bulk of RAM is cleared during the keyboard scan (ClearFreePoolSection).

;
; out:  r4-r11, r13 preserved
;

ClearWkspRAM ROUT
        MSR     CPSR_c, #F32_bit+FIQ32_mode             ; get some extra registers
        MOV     r8, #0
        MOV     r9, #0
        MOV     r10, #0
        MOV     r11, #0
        MOV     r12, #0
        MOV     r13, #0
        MOV     r14, #0
        MSR     CPSR_c, #F32_bit+SVC32_mode

        LDR     r0,=ZeroPage+InitClearRamWs             ;we can preserve r4-r11,lr in one of the skipped regions
        STMIA   r0,{r4-r11,lr}
 
        DebugTX "ClearWkspRAM"

        ; Start off by clearing zero page + scratch space, as these:
        ; (a) are already mapped in and
        ; (b) may require the use of the skip table
        LDR     r0, =ZeroPage
        ADD     r1, r0, #16*1024
        ADR     r6, RamSkipTable
        MSR     CPSR_c, #F32_bit+FIQ32_mode             ; switch to our bank o'zeros
        LDR     r5, [r6], #4                            ; load first skip addr
10
        TEQ     r0, r1
        TEQNE   r0, r5
        STMNEIA r0!, {r8-r11}
        BNE     %BT10
        TEQ     r0, r1
        BEQ     %FT20
        LDR     r5, [r6], #4                            ; load skip amount
        ADD     r0, r0, r5                              ; and skip it
        LDR     r5, [r6], #4                            ; load next skip addr
        B       %BT10
20
        LDR     r0, =ScratchSpace
        ADD     r1, r0, #ScratchSpaceSize
30
        TEQ     r0, r1
        STMNEIA r0!, {r8-r11}
        STMNEIA r0!, {r8-r11}
        BNE     %BT30

        MSR     CPSR_c, #F32_bit+SVC32_mode

        LDR     r0, =ZeroPage+InitClearRamWs
        LDMIA   r0, {r4-r11,r14}                        ;restore

      [ {FALSE} ; NewReset sets this later
        LDR     r0, =ZeroPage+OsbyteVars + :INDEX: LastBREAK
        MOV     r1, #&80
        STRB    r1, [r0]                                ; flag the fact that RAM cleared
      ]

        MSR     CPSR_c, #F32_bit + UND32_mode           ; retrieve the MMU control register
        LDR     r0, =ZeroPage                           ; soft copy
        STR     sp, [r0, #MMUControlSoftCopy]
        MSR     CPSR_c, #F32_bit + SVC32_mode

        MOV     pc, lr

        LTORG

        MACRO
        MakeSkipTable $addr, $size
        ASSERT  ($addr :AND: 15) = 0
        ASSERT  ($size :AND: 15) = 0
        ASSERT  ($addr-ZeroPage) < 16*1024
        &       $addr, $size
        MEND

        MACRO
        EndSkipTables
        &       -1
        MEND

RamSkipTable
        MakeSkipTable   ZeroPage, InitWsEnd
        MakeSkipTable   ZeroPage+SkippedTables, SkippedTablesEnd-SkippedTables
        EndSkipTables

;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
;
;       ClearFreePoolSection - Routine to clear a section of the free pool
;
; During keyboard scanning we soak up slack time clearing the bulk of RAM
; by picking a section of the free pool, mapping it in, clearing & flushing.

;
; In:   r0 = CAM entry to continue from
; Out:  r0 = updated
;

ClearFreePoolSection ROUT
        Push    "r1-r3, lr"
        LDR     r1, =ZeroPage
        LDR     r1, [r1, #MaxCamEntry]
        LDR     r2, =ZeroPage+FreePoolDANode
        CMP     r0, r1
        BHI     %FT30

        LDR     r3, =CAM
        ADD     r1, r3, r1, LSL #CAM_EntrySizeLog2      ; top entry (inc)
        ADD     r3, r3, r0, LSL #CAM_EntrySizeLog2      ; starting entry
10
        LDR     r14, [r3, #CAM_PageFlags]
        TST     r14, #DynAreaFlags_PMP
        BEQ     %FT20

        LDR     r14, [r3, #CAM_PMP]
        TEQ     r14, r2
        BEQ     %FT40
20
        ADD     r3, r3, #CAM_EntrySize                  ; next
        CMP     r3, r1
        BLS     %BT10
30
        MOV     r0, #-1
        Pull    "r1-r3, pc"
40
        Push    "r0-r12"

        ; This is a PMP entry in the free pool
        LDR     r14, [r3, #CAM_PMPIndex]                ; list index
        LDR     r9, [r2, #DANode_PMP]                   ; PMP list base
        LDR     r3, [r9, r14, LSL #2]                   ; ppn
        BL      ppn_to_physical                         ; => r5 = PA

      [ MEMM_Type = "ARM600"
        ; Map in this section, cacheable + bufferable to ensure burst writes
        ; are performed (StrongARM will only perform burst writes to CB areas)
        MOV     a1, #OSAP_None
      |
        ; Map in this section with default NCB cache policy. Making it cacheable
        ; is liable to slow things down significantly on some platforms (e.g.
        ; PL310 L2 cache)
        LDR     a1, =OSAP_None + DynAreaFlags_NotCacheable
      ]
        MOV     a2, r5
        MOV     a3, #0
        BL      RISCOS_AccessPhysicalAddressUnchecked

        MOV     r4, #0                                  ; clear to this value
        MOV     r6, r4
        MOV     r7, r4
        MOV     r8, r4
        MOV     r12, r4
45
        MOV     r9, r4
        MOV     r10, r4
        MOV     r11, r4

        ; Fill that page
        ADD     r2, r0, #4096
50
        STMIA   r0!, {r4,r6-r12}
        STMIA   r0!, {r4,r6-r12}
        TEQ     r0, r2
        BNE     %BT50

        ; Step the CAM until there are no more pages in that section
        LDR     r1, [sp, #1*4]
        LDR     r2, [sp, #2*4]
        LDR     r11, [sp, #3*4]
        B       %FT65
60
        LDR     r14, [r11, #CAM_PageFlags]
        TST     r14, #DynAreaFlags_PMP
        BEQ     %FT65

        LDR     r14, [r11, #CAM_PMP]
        TEQ     r14, r2
        BEQ     %FT70
65
        ADD     r11, r11, #CAM_EntrySize                ; next
        CMP     r11, r1
        BLS     %BT60

        MOV     r14, #-1                                ; CAM top, no more
        B       %FT80
70
        MOV     r10, r5                                 ; previous PA

        ; Next PMP entry in the free pool
        LDR     r14, [r11, #CAM_PMPIndex]               ; list index
        LDR     r9, [r2, #DANode_PMP]                   ; PMP list base
        LDR     r3, [r9, r14, LSL #2]                   ; ppn
        BL      ppn_to_physical                         ; => r5 = PA

        MOV     r14, r10, LSR #20
        TEQ     r14, r5, LSR #20                        ; same MB as previous?
        LDRNE   r14, =CAM
        SUBNE   r14, r11, r14
        MOVNE   r14, r14, LSR #CAM_EntrySizeLog2        ; no, so compute continuation point
        LDREQ   r0, =PhysicalAccess
        MOVEQ   r14, r5, LSL #12
        ORREQ   r0, r0, r14, LSR #12
        STREQ   r11, [sp, #3*4]
        BEQ     %BT45                                   ; yes, so clear it
80
        STR     r14, [sp, #0*4]                         ; return value for continuation

 [ MEMM_Type = "ARM600" ; VMSAv6 maps as non-cacheable, so no flush required
        ; Make page uncacheable so the following is safe
        MOV     r4, r0

        MOV     r0, #L1_B
        MOV     r1, r10
        MOV     r2, #0
        BL      RISCOS_AccessPhysicalAddress

        MOV     r0, r4

        ; Clean & invalidate the cache before the 1MB window closes
      [ CacheCleanerHack
        ; StrongARM requires special clean code, because we haven't mapped in
        ; DCacheCleanAddress yet. Cheat and only perform a clean, not full
        ; clean + invalidate (should be safe as we've only been writing)
        ARM_read_ID r2
        AND     r2, r2, #&F000
        CMP     r2, #&A000
        BNE     %FT90
85
        SUB     r0, r0, #32                             ; rewind 1 cache line
        ARMA_clean_DCentry r0
        MOVS    r1, r0, LSL #12                         ; start of the MB?
        BNE     %BT85
        B       %FT91
90
      ]
        ARMop Cache_CleanInvalidateAll
 ]
91
        MOV     a1, #L1_Fault
        BL      RISCOS_ReleasePhysicalAddress           ; reset to default

        Pull    "r0-r12"

        Pull    "r1-r3, pc"

InitProcVecs
        BKPT    &C000                                   ; Reset
        BKPT    &C004                                   ; Undefined Instruction
        BKPT    &C008                                   ; SWI
        BKPT    &C00C                                   ; Prefetch Abort
        SUBS    pc, lr, #4                              ; ignore data aborts
        BKPT    &C014                                   ; Address Exception
        LDR     pc, InitProcVecs + InitIRQHandler       ; IRQ
        BKPT    &C01C                                   ; FIQ
InitProcVec_FIQ
        DCD     0
InitProcVecsEnd

;
; In:  a1 = flags  (L1_B,L1_C,L1_TEX)
;           bit 20 set if doubly mapped
;           bit 21 set if L1_AP specified (else default to AP_None)
;      a2 = physical address
;      a3 = size
; Out: a1 = assigned logical address, or 0 if failed (no room)
;
; Will detect and return I/O space already mapped appropriately, or map and return new space
; For simplicity and speed of search, works on a section (1Mb) granularity
;

        ASSERT  L1_B = 1:SHL:2
        ASSERT  L1_C = 1:SHL:3
 [ MEMM_Type = "VMSAv6"
        ASSERT  L1_AP = 2_100011 :SHL: 10
        ASSERT  L1_TEX = 2_111 :SHL: 12
 |
        ASSERT  L1_AP = 3:SHL:10
        ASSERT  L1_TEX = 2_1111 :SHL: 12
 ]
MapInFlag_DoublyMapped * 1:SHL:20
MapInFlag_APSpecified * 1:SHL:21

RISCOS_MapInIO ROUT
        TST     a1, #MapInFlag_APSpecified
        BICEQ   a1, a1, #L1_AP
        ; For VMSAv6, assume HAL knows what it's doing and requests correct settings for AP_ROM
        ORREQ   a1, a1, #L1_APMult * AP_None
        BIC     a1, a1, #3
 [ MEMM_Type = "VMSAv6"
        ORR     a1, a1, #L1_Section+L1_XN               ; force non-executable to prevent speculative instruction fetches
 |
        ORR     a1, a1, #L1_Section
 ]
RISCOS_MapInIO_PTE ; a1 bits 0-19 = L1 section entry flags, bits 20+ = our extra flags
        Entry   "v1-v5,v7"
        LDR     v7, =(1:SHL:20)-1
        AND     v4, a2, v7                              ; v4 = offset of original within section-aligned area
        ADD     a3, a2, a3                              ; a3 -> end (exclusive)
        BIC     a2, a2, v7                              ; round a2 down to a section boundary
        ADD     a3, a3, v7
        BIC     a3, a3, v7                              ; round a3 up to a section boundary

        ANDS    v5, a1, #MapInFlag_DoublyMapped
        SUBNE   v5, a3, a2                              ; v5 = offset of second mapping or 0

        LDR     ip, =ZeroPage
        LDR     a4, =L1PT
        AND     a1, a1, v7                              ; mask out our extra flags
        LDR     v2, =IO                                 ; logical end (exclusive) of currently mapped IO
        LDR     v1, [ip, #IOAllocPtr]                   ; logical start (inclusive)

        SUB     v1, v1, #&100000
10
        ADD     v1, v1, #&100000                        ; next mapped IO section
        CMP     v1, v2
        BHS     %FT32                                   ; no more currently mapped IO
        LDR     v3, [a4, v1, LSR #(20-2)]               ; L1PT entry (must be for mapped IO)
        MOV     lr, v3, LSR #20                         ; physical address bits
        CMP     lr, a2, LSR #20
        BNE     %BT10                                   ; no address match
        AND     lr, v3, v7
        TEQ     lr, a1
        BNE     %BT10                                   ; no flags match

        TEQ     v5, #0                                  ; doubly mapped?
        BEQ     %FT19

        ADD     lr, v1, v5                              ; address of second copy
        CMP     lr, v2
        BHS     %FT32
        LDR     v3, [a4, lr, LSR #(20-2)]
        MOV     lr, v3, LSR #20                         ; physical address bits
        CMP     lr, a2, LSR #20
        BNE     %BT10                                   ; no address match
        AND     lr, v3, v7
        TEQ     lr, a1
        BNE     %BT10                                   ; no flags match

19
;
; alright, found start of requested IO already mapped, and with required flags
;
        Push    "a2, v1"
20
        ADD     a2, a2, #&100000
        CMP     a2, a3
        Pull    "a2, v1", HS
        BHS     %FT40                                  ; its all there already!
        ADD     v1, v1, #&100000                       ; next mapped IO section
        CMP     v1, v2
        BHS     %FT30                                  ; no more currently mapped IO
        LDR     v3, [a4, v1, LSR #(20-2)]              ; L1PT entry
        MOV     lr, v3, LSR #20                        ; physical address bits
        CMP     lr, a2, LSR #20
        BNE     %FT29                                  ; address match failed
        AND     lr, v3, v7
        TEQ     lr, a1
        TEQEQ   v5, #0                                 ; doubly mapped?
        BEQ     %BT20                                  ; address and flags match so far
        ADD     lr, v1, v5                             ; where duplicate should be
        CMP     lr, v2
        BHS     %FT30                                  ; no more currently mapped IO
        LDR     v3, [a4, lr, LSR #(20-2)]
        MOV     lr, v3, LSR #20                        ; physical address bits
        CMP     lr, a2, LSR #20
        BNE     %FT29                                  ; address match failed
        AND     lr, v3, v7
        TEQ     lr, a1
        BEQ     %BT20
29
        Pull    "a2, v1"
        B       %BT10
30
        Pull    "a2, v1"
;
; request not currently mapped, only partially mapped, or mapped with wrong flags
;
32
        LDR     ip, =ZeroPage
        LDR     v2, [ip, #IOAllocPtr]
        ADD     v1, v2, a2
        SUB     v1, v1, a3                              ; attempt to allocate size of a3-a2
        SUB     v1, v1, v5                              ; double if necessary
        LDR     v3, [ip, #IOAllocLimit]                 ; can't extend down below limit
        CMP     v1, v3
        MOVLS   a1, #0
        BLS     %FT90
        STR     v1, [ip, #IOAllocPtr]
        ORR     a2, a2, a1                              ; first L1PT value
34
        STR     a2, [a4, v1, LSR #(20-2)]
        TEQ     v5, #0
        ADDNE   v2, v1, v5
        STRNE   a2, [a4, v2, LSR #(20-2)]
        ADD     a2, a2, #&100000
        ADD     v1, v1, #&100000                        ; next section
        CMP     a2, a3
        BLO     %BT34
        PageTableSync
        LDR     v1, [ip, #IOAllocPtr]
40
        ADD     a1, v1, v4                              ; logical address for request
90
        EXIT


; void RISCOS_AddDevice(unsigned int flags, struct device *d)
RISCOS_AddDevice
        ADDS    a1, a2, #0      ; also clears V
        B       HardwareDeviceAdd_Common

; uint32_t RISCOS_LogToPhys(const void *log)
RISCOS_LogToPhys ROUT
        Push    "r4,r5,r8,r9,lr"
        MOV     r4, a1
        LDR     r8, =L2PT
        BL      logical_to_physical
        MOVCC   a1, r5
        BCC     %FT10
        ; Try checking L1PT for any section mappings (logical_to_physical only
        ; deals with regular 4K page mappings)
        ; TODO - Add large page support
        LDR     r9, =L1PT
        MOV     r5, r4, LSR #20
        LDR     a1, [r9, r5, LSL #2]
        ASSERT  L1_Section = 2
        EOR     a1, a1, #2
        TST     a1, #3
        MOVNE   a1, #-1
        BNE     %FT10
        ; Apply offset from bits 0-19 of logical addr
      [ NoARMT2
        MOV     a1, a1, LSR #20
        ORR     a1, a1, r4, LSL #12
        MOV     a1, a1, ROR #12
      |
        BFI     a1, r4, #0, #20
      ]  
10
        Pull    "r4,r5,r8,r9,pc"

; int RISCOS_IICOpV(IICDesc *descs, uint32_t ndesc_and_bus)
RISCOS_IICOpV ROUT
        Push    "lr"
        BL      IIC_OpV
        MOVVC   a1, #IICStatus_Completed
        Pull    "pc", VC
        ; Map from RISC OS error numbers to abstract IICStatus return values
        LDR     a1, [a1]
        LDR     lr, =ErrorNumber_IIC_NoAcknowledge
        SUB     a1, a1, lr              ; 0/1/2 = NoAck/Error/Busy
        CMP     a1, #3
        MOVCS   a1, #3                  ; 3+ => unknown, either way it's an Error
        ADR     lr, %FT10
        LDRB    a1, [lr, a1]
        Pull    "pc"
10
        ASSERT    (ErrorNumber_IIC_Error - ErrorNumber_IIC_NoAcknowledge) = 1
        ASSERT    (ErrorNumber_IIC_Busy - ErrorNumber_IIC_NoAcknowledge) = 2
        DCB       IICStatus_NoACK, IICStatus_Error, IICStatus_Busy, IICStatus_Error
        ALIGN
        
SetUpHALEntryTable ROUT
        LDR     a1, =ZeroPage
        LDR     a2, [a1, #HAL_Descriptor]
        LDR     a3, [a1, #HAL_Workspace]
        LDR     a4, [a2, #HALDesc_Entries]
        LDR     ip, [a2, #HALDesc_NumEntries]
        ADD     a4, a2, a4                              ; a4 -> entry table
        MOV     a2, a4                                  ; a2 -> entry table (increments)
10      SUBS    ip, ip, #1                              ; decrement counter
        LDRCS   a1, [a2], #4
        BCC     %FT20
        TEQ     a1, #0
        ADREQ   a1, NullHALEntry
        ADDNE   a1, a4, a1                              ; convert offset to absolute
        STR     a1, [a3, #-4]!                          ; store backwards below HAL workspace
        B       %BT10
20      LDR     a1, =ZeroPage                           ; pad table with NullHALEntries
        LDR     a4, =HALWorkspace                       ; in case where HAL didn't supply enough
        ADR     a1, NullHALEntry
30      CMP     a3, a4
        STRHI   a1, [a3, #-4]!
        BHI     %BT30
        MOV     pc, lr


NullHALEntry
        MOV     pc, lr

; Can freely corrupt r10-r12 (v7,v8,ip).
HardwareSWI
        AND     ip, v5, #&FF

        CMP     ip, #OSHW_LookupRoutine
        ASSERT  OSHW_CallHAL < OSHW_LookupRoutine
        BLO     HardwareCallHAL
        BEQ     HardwareLookupRoutine

        CMP     ip, #OSHW_DeviceRemove
        ASSERT  OSHW_DeviceAdd < OSHW_DeviceRemove
        BLO     HardwareDeviceAdd
        BEQ     HardwareDeviceRemove

        CMP     ip, #OSHW_DeviceEnumerateChrono
        ASSERT  OSHW_DeviceEnumerate < OSHW_DeviceEnumerateChrono
        ASSERT  OSHW_DeviceEnumerateChrono < OSHW_MaxSubreason 
        BLO     HardwareDeviceEnumerate
        BEQ     HardwareDeviceEnumerateChrono
        BHI     HardwareBadReason

HardwareCallHAL
        Push    "v1-v4,sb,lr"
        ADD     v8, sb, #1                              ; v8 = entry no + 1
        LDR     ip, =ZeroPage
        LDR     v7, [ip, #HAL_Descriptor]
        AddressHAL ip                                   ; sb set up
        LDR     v7, [v7, #HALDesc_NumEntries]           ; v7 = number of entries
        CMP     v8, v7                                  ; entryno + 1 must be <= number of entries
        BHI     HardwareBadEntry2
        LDR     ip, [sb, -v8, LSL #2]
        ADR     v7, NullHALEntry
        TEQ     ip, v7
        BEQ     HardwareBadEntry2
      [ NoARMv5
        MOV     lr, pc
        MOV     pc, ip
      |
        BLX     ip
      ]
        ADD     sp, sp, #4*4
        Pull    "sb,lr"
        ExitSWIHandler

HardwareLookupRoutine
        ADD     v8, sb, #1                              ; v8 = entry no + 1
        LDR     ip, =ZeroPage
        LDR     v7, [ip, #HAL_Descriptor]
        AddressHAL ip
        LDR     v7, [v7, #HALDesc_NumEntries]
        CMP     v8, v7                                  ; entryno + 1 must be <= number of entries
        BHI     HardwareBadEntry
        LDR     a1, [sb, -v8, LSL #2]
        ADR     v7, NullHALEntry
        TEQ     a1, v7
        BEQ     HardwareBadEntry
        MOV     a2, sb
        ExitSWIHandler

HardwareDeviceAdd
        Push    "r1-r3,lr"
        BL      HardwareDeviceAdd_Common
        Pull    "r1-r3,lr"
        B       SLVK_TestV

HardwareDeviceRemove
        Push    "r1-r3,lr"
        BL      HardwareDeviceRemove_Common
        Pull    "r1-r3,lr"
        B       SLVK_TestV

HardwareDeviceAdd_Common
        Entry
        BL      HardwareDeviceRemove_Common             ; first try to remove any device already at the same address
        EXIT    VS
        LDR     lr, =ZeroPage
        LDR     r1, [lr, #DeviceCount]
        LDR     r2, [lr, #DeviceTable]
        TEQ     r2, #0
        BEQ     %FT80
        ADD     r1, r1, #1                              ; increment DeviceCount
        LDR     lr, [r2, #-4]                           ; word before heap block is length including length word
        TEQ     r1, lr, LSR #2                          ; block already full?
        BEQ     %FT81
        LDR     lr, =ZeroPage
10      STR     r1, [lr, #DeviceCount]
        ADD     lr, r2, r1, LSL #2
        SUB     lr, lr, #4
11      LDR     r1, [lr, #-4]!                          ; copy existing devices up, so new ones get enumerated first
        STR     r1, [lr, #4]
        CMP     lr, r2
        BHI     %BT11
        STR     r0, [r2]
        MOV     r2, r0
        MOV     r1, #Service_Hardware
        MOV     r0, #0
        BL      Issue_Service
        ADDS    r0, r2, #0                              ; exit with V clear
        EXIT

80      ; Claim a system heap block for the device table
        Push    "r0"
        MOV     r3, #16
        BL      ClaimSysHeapNode
        ADDVS   sp, sp, #4
        EXIT    VS
        Pull    "r0"
        LDR     lr, =ZeroPage
        MOV     r1, #1
        STR     r2, [lr, #DeviceTable]
        B       %BT10

81      ; Extend the system heap block
        Push    "r0"
        MOV     r0, #HeapReason_ExtendBlock
        MOV     r3, #16
        BL      DoSysHeapOpWithExtension
        ADDVS   sp, sp, #4
        EXIT    VS
        Pull    "r0"
        LDR     lr, =ZeroPage
        LDR     r1, [lr, #DeviceCount]
        STR     r2, [lr, #DeviceTable]
        ADD     r1, r1, #1
        B       %BT10

HardwareDeviceRemove_Common ROUT
        Entry   "r4"
        LDR     lr, =ZeroPage
        LDR     r3, [lr, #DeviceCount]
        LDR     r4, [lr, #DeviceTable]
        TEQ     r3, #0
        EXIT    EQ                                      ; no devices registered
10      LDR     r2, [r4], #4
        SUBS    r3, r3, #1
        TEQNE   r2, r0
        BNE     %BT10
        TEQ     r2, r0
        EXIT    NE                                      ; this device not registered
        MOV     r0, #1
        MOV     r1, #Service_Hardware
        BL      Issue_Service
        CMP     r1, #0                                  ; if service call claimed
        CMPEQ   r1, #1:SHL:31                           ; then set V (r0 already points to error block)
        EXIT    VS                                      ; and exit
        ; Search for device again - we may have been re-entered
        MOV     r0, r2
        LDR     lr, =ZeroPage
        LDR     r3, [lr, #DeviceCount]
        LDR     r4, [lr, #DeviceTable]
        TEQ     r3, #0
        EXIT    EQ                                      ; no devices registered
20      LDR     r2, [r4], #4
        SUBS    r3, r3, #1
        TEQNE   r2, r0
        BNE     %BT20
        TEQ     r2, r0
        EXIT    NE                                      ; this device not registered
        SUBS    r3, r3, #1
30      LDRCS   r2, [r4], #4                            ; copy down remaining devices
        STRCS   r2, [r4, #-8]
        SUBCSS  r3, r3, #1
        BCS     %BT30
        LDR     lr, =ZeroPage
        LDR     r3, [lr, #DeviceCount]
        SUB     r3, r3, #1
        STR     r3, [lr, #DeviceCount]
        EXIT

HardwareDeviceEnumerate
        Push    "r3-r4,lr"
        LDR     lr, =ZeroPage
        LDR     r2, [lr, #DeviceCount]
        LDR     r3, [lr, #DeviceTable]
        SUBS    r4, r2, r1
        MOVLS   r1, #-1
        BLS     %FT90                                   ; if r1 is out of range then exit
        ADD     r3, r3, r1, LSL #2
10      ADD     r1, r1, #1
        LDR     r2, [r3], #4
        LDR     lr, [r2, #HALDevice_Type]
        EOR     lr, lr, r0
        MOVS    lr, lr, LSL #16                         ; EQ if types match
        SUBNES  r4, r4, #1
        BNE     %BT10
        TEQ     lr, #0
        MOVNE   r1, #-1
        BNE     %FT90
        LDR     lr, [r2, #HALDevice_Version]
        MOV     lr, lr, LSR #16
        CMP     lr, r0, LSR #16                         ; newer than our client understands?
        BLS     %FT90
        SUBS    r4, r4, #1
        BHI     %BT10
        MOV     r1, #-1
90
        Pull    "r3-r4,lr"
        ExitSWIHandler

HardwareDeviceEnumerateChrono
        Push    "r3-r4,lr"
        LDR     lr, =ZeroPage
        LDR     r2, [lr, #DeviceCount]
        LDR     r3, [lr, #DeviceTable]
        SUBS    r4, r2, r1
        MOVLS   r1, #-1
        BLS     %FT90                                   ; if r1 is out of range then exit
        ADD     r3, r3, r4, LSL #2
10      ADD     r1, r1, #1
        LDR     r2, [r3, #-4]!
        LDR     lr, [r2, #HALDevice_Type]
        EOR     lr, lr, r0
        MOVS    lr, lr, LSL #16                         ; EQ if types match
        SUBNES  r4, r4, #1
        BNE     %BT10
        TEQ     lr, #0
        MOVNE   r1, #-1
        BNE     %FT90
        LDR     lr, [r2, #HALDevice_Version]
        MOV     lr, lr, LSR #16
        CMP     lr, r0, LSR #16                         ; newer than our client understands?
        BLS     %FT90
        SUBS    r4, r4, #1
        BHI     %BT10
        MOV     r1, #-1
90
        Pull    "r3-r4,lr"
        ExitSWIHandler

HardwareBadReason
        ADR     r0, ErrorBlock_HardwareBadReason
 [ International
        Push    "lr"
        BL      TranslateError
        Pull    "lr"
 ]
        B       SLVK_SetV

HardwareBadEntry2
        ADD     sp, sp, #4*4
        Pull    "sb,lr"
HardwareBadEntry
        ADR     r0, ErrorBlock_HardwareBadEntry
 [ International
        Push    "lr"
        BL      TranslateError
        Pull    "lr"
 ]
        B       SLVK_SetV

        MakeErrorBlock HardwareBadReason
        MakeErrorBlock HardwareBadEntry

 [ DebugTerminal
DebugTerminal_Rdch
        Push    "a2-a4,sb,ip"
        WritePSRc SVC_mode, r1
        MOV     sb, ip
20
        CallHAL HAL_DebugRX
        CMP     a1, #27
        BNE     %FT25
        LDR     a2, =ZeroPage + OsbyteVars + :INDEX: RS423mode
        LDRB    a2, [a2]
        TEQ     a2, #0                  ; is RS423 raw data,or keyb emulator?
        BNE     %FT25
        LDR     a2, =ZeroPage
        LDRB    a1, [a2, #ESC_Status]
        ORR     a1, a1, #&40
        STRB    a1, [a2, #ESC_Status]   ; mark escape flag
        MOV     a1, #27
        SEC                             ; tell caller to look carefully at R0
        Pull    "a2-a4,sb,ip,pc"
25
        CMP     a1, #-1
        Pull    "a2-a4,sb,ip,pc",NE     ; claim it
        LDR     R0, =ZeroPage
        LDRB    R14, [R0, #CallBack_Flag]
        TST     R14, #CBack_VectorReq
        BLNE    process_callback_chain
        B       %BT20


DebugTerminal_Wrch
        Push    "a1-a4,sb,ip,lr"
        MOV     sb, ip
        CallHAL HAL_DebugTX
        Pull    "a1-a4,sb,ip,pc"        ; don't claim it
 ]


Reset_IRQ_Handler
        SUB     lr, lr, #4
        Push    "a1-a4,v1-v2,sb,ip,lr"
        MRS     a1, SPSR
        MRS     a2, CPSR
        ORR     a3, a2, #SVC32_mode
        MSR     CPSR_c, a3
        Push    "a1-a2,lr"

        ; If it's not an IIC interrupt, mute it
        LDR     v2, =ZeroPage
        AddressHAL v2
        CallHAL HAL_IRQSource
        ADD     v1, v2, #IICBus_Base
        MOV     ip, #0
10
        LDR     a2, [v1, #IICBus_Type]
        TST     a2, #IICFlag_Background
        BEQ     %FT20
        LDR     a2, [v1, #IICBus_Device]
        CMP     a2, a1
        ADREQ   lr, Reset_IRQ_Exit
        BEQ     IICIRQ
20
        ADD     ip, ip, #1
        ADD     v1, v1, #IICBus_Size
        CMP     ip, #IICBus_Count
        BNE     %BT10

        CallHAL HAL_IRQDisable ; Stop the rogue device from killing us completely

Reset_IRQ_Exit
        MyCLREX a1, a2
        Pull    "a1-a2,lr"
        MSR     CPSR_c, a2
        MSR     SPSR_cxsf, a1
        Pull    "a1-a4,v1-v2,sb,ip,pc",,^

 [ DebugHALTX
DebugHALPrint
        Push    "a1-a4,v1,sb,ip"
        AddressHAL
        MOV     v1, lr
10      LDRB    a1, [v1], #1
        TEQ     a1, #0
        BEQ     %FT20
        CallHAL HAL_DebugTX
        B       %BT10
20      MOV     a1, #13
;        CallHAL HAL_DebugTX
        MOV     a1, #10
;        CallHAL HAL_DebugTX
        ADD     v1, v1, #3
        BIC     lr, v1, #3
        Pull    "a1-a4,v1,sb,ip"
        MOV     pc, lr
 ]


 [ DebugHALTX
DebugHALPrintReg ; Output number on top of stack to the serial port
        Push    "a1-a4,v1-v4,sb,ip,lr"   ; this is 11 regs
        LDR     v2, [sp,#11*4]           ; find TOS value on stack
        ADR     v3, hextab
        MOV     v4, #8
05
       AddressHAL
10      LDRB    a1, [v3, v2, LSR #28]
       CallHAL  HAL_DebugTX
        MOV     v2, v2, LSL #4
        SUBS    v4, v4, #1
        BNE     %BT10
        MOV     a1, #13
       CallHAL  HAL_DebugTX
        MOV     a1, #10
       CallHAL  HAL_DebugTX

        Pull    "a1-a4,v1-v4,sb,ip,lr"
        ADD     sp, sp, #4
        MOV     pc, lr

hextab  DCB "0123456789abcdef"


 ]
;
;
; [ DebugHALTX
;HALDebugHexTX
;       stmfd    r13!, {r0-r3,sb,ip,lr}
;       AddressHAL
;       b        jbdt1
;HALDebugHexTX2
;       stmfd    r13!, {r0-r3,sb,ip,lr}
;       AddressHAL
;       mov      r0,r0,lsl #16
;       b        jbdt2
;HALDebugHexTX4
;       stmfd    r13!, {r0-r3,sb,ip,lr}
;       AddressHAL
;       mov      r0,r0,ror #24          ; hi byte
;       bl       jbdtxh
;       mov      r0,r0,ror #24
;       bl       jbdtxh
;jbdt2
;       mov      r0,r0,ror #24
;       bl       jbdtxh
;       mov      r0,r0,ror #24
;jbdt1
;       bl       jbdtxh
;       mov      r0,#' '
;       CallHAL  HAL_DebugTX
;       ldmfd    r13!, {r0-r3,sb,ip,pc}
;
;jbdtxh stmfd    r13!,{a1,v1,lr}        ; print byte as hex. corrupts a2-a4, ip, assumes sb already AddressHAL'd
;       and      v1,a1,#&f              ; get low nibble
;       and      a1,a1,#&f0             ; get hi nibble
;       mov      a1,a1,lsr #4           ; shift to low nibble
;       cmp      a1,#&9                 ; 9?
;       addle    a1,a1,#&30
;       addgt    a1,a1,#&37             ; convert letter if needed
;       CallHAL  HAL_DebugTX
;       cmp      v1,#9
;       addle    a1,v1,#&30
;       addgt    a1,v1,#&37
;       CallHAL  HAL_DebugTX
;       ldmfd    r13!,{a1,v1,pc}
; ]
;
        END

@


4.9
log
@Change module initialisation to be a two pass scheme
Detail:
  To make it easier to support arbitrary complexity keyboard controllers (eg. USB via DWCDriver on the Pi) have the kernel do the early keyboard recovery key press detection instead of the HAL.
  During the first pass those modules used for reading the keyboard are started, ignoring the CMOS frugal bits.
  The keyboard is then scanned for 3s, during which time the RAM is cleared (unless the HAL indicated it has already been done).
  During the second pass the remaining modules are started respecting the CMOS frugal bits. Any which were already started in the first pass are inserted into the new chain, so the keyboard is reset once and only once.

  Boot times, with a 300cs key scan time in NewReset.
  Risc PC with 160MB RAM (128+32+0).
  Times from turning on power to initial "beep", using a stopwatch.
                RISC OS 3.70 RISC OS 5.22 This OS
  ARM610        12.5         10.4         10.3
  ARM710        11.8         10.2         9.7
  StrongARM 233 11.1         9.5          8.4

  In NewReset.s:
  Remove old KbdScan code (leave Reset_IRQ_Handler for IIC only)
  If HAL_KbdScanDependencies returns a null string then present KbdDone flag and skip to full init.
  A few vestiges of soft resets removed.
  Do RAM clear when waiting for INKEY (being careful not to trash the running modules...).
  Clearing just the freepool on a 2GB Titanium cleared 7EFD6 pages (99.2%).

  In ModHand.s:
  2nd pass need to sneaky renumber the nodes (so *ROMModules is in the right order, frugal bits line up) without resetting the chain

  In HAL.s:
  Change ClearPhysRAM to ClearWkspRAM, such that it only clears the kernel workspace rather than all RAM. The bulk of the RAM is cleared during the keyboard scan by new function ClearFreePoolSection.
  Add a variant of Init_MapInRAM which clears the mapped in RAM too (as these very early claims will not be in the free pool when the RAM is cleared later).
  Remove HAL keyboard scan setup & IRQ handler.
  Fix bug in HALDebugHexTX2, the input value needs pre-shifting by 16b before continuing.

  In GetAll.s, PMF/osbyte.s:
  Use Hdr:Countries and Hdr:OsBytes for constants.

  In PMF/key.s, PMF/osinit.s:
  Relocate the key post init from PostInit to KeyPostInit.
  Changed PostInit to not tail call KeyPostInit so they can be called independently.

  In hdr/KernelWs:
  Improve comments, add InitWsStart label to refer to.

  In hdr/HALEntries:
  Add HAL_KbdScanDependencies.
  Delete KbdFlag exports.
  Took the opportunity to reorder some of the higher numbered HAL entries and re-grouping, specifically (112,120) (84,106,108,117).
Admin:
  Tested on an ARM6/ARM7/SA Risc PC, BeagleBoard xM, Iyonix, Pandaboard ES, Wandboard Quad, IPEGv5, Titanium, Pi 2 and 3.
  Requires corresponding HAL change.
  Submission for USB bounty.

Version 5.89. Tagged as 'Kernel-5_89'
@
text
@d2546 1
a2546 1
HardwareDeviceRemove_Common
d2553 1
a2553 1
01      LDR     r2, [r4], #4
d2556 1
a2556 1
        BNE     %BT01
d2565 1
d2567 11
d2579 1
a2579 1
02      LDRCS   r2, [r4], #4                            ; copy down remaining devices
d2582 1
a2582 1
        BCS     %BT02
@


4.8
log
@Implement some ARM11 errata workarounds
Detail:
  s/ARMops, s/HAL - Add workarounds for some of the scary errata that were previously weren't dealing with (720013, 716151, 714068)
Admin:
  Tested on Raspberry Pi 1


Version 5.72. Tagged as 'Kernel-5_72'
@
text
@d765 1
a765 2
        ; Set up a reset IRQ handler (used during RAM clear for keyboard
        ; scan, and later for IIC CMOS access)
a1042 10
        LDR     a1, =ZeroPage+InitIRQWs
        MOV     a2, #1
        STRB    a2, [a1, #KbdScanActive]

        DebugTX "HAL_KbdScanSetup"

        CallHAL HAL_KbdScanSetup

        MSR     CPSR_c, #F32_bit+SVC32_mode             ; enable IRQs for scan

d1052 1
a1052 1
        BLEQ    ClearPhysRAM            ; Only clear the memory if the HAL didn't
d1061 1
a1061 1
; Calculate CPU feature flags (if moving this to before ClearPhysRAM, ensure the workspace also gets moved into the skipped region)
d1251 1
a1251 1
        BL      Init_MapInRAM
d1267 1
a1267 1
        BL      Init_MapInRAM
d1274 1
a1274 1
        BL      Init_MapInRAM
a1339 8
 [ {FALSE}
        MOV     a1, #InitIRQWs
        MOV     a2, #0
        MOV     a3, #0
        STMIA   a1!, {a2,a3}
        STMIA   a1!, {a2,a3}
 ]

d1591 10
d1630 1
a1630 1
        ; DMA regions won't get cleared by ClearPhysRam, so do it manually
d1919 1
a1919 4
;       ClearPhysRAM - Routine to clear "all" memory
;
; While this routine is running, keyboard IRQs may happen. For this reason
; it avoids the base of logical RAM (hardware IRQ vector and workspace).
d1921 1
a1921 1
; We also have to avoid anything between InitUsedStart and InitUsedEnd - i.e.
d1927 2
a1928 5

; We don't have to worry about trampling on the ROM image as it's already been
; excluded from PhysRamTable. We also don't have to worry about skipping the
; special DMA block, because at this point in time that won't be listed in
; PhysRamTable either.
d1934 1
a1934 1
ClearPhysRAM ROUT
a1944 1
;now let us do the clear
d1947 2
a1948 2

        DebugTX "ClearPhysRAM"
d1978 98
a2075 6
        ; Now walk PhysRamTable and clear everything else, except for the stuff
        ; between InitUsedStart and InitUsedEnd.
        ;
        ; To skip these areas properly, we convert their addresses to physical
        ; page numbers. This is because PhysRamTable isn't guaranteed to be in
        ; ascending address order.
d2077 12
a2088 18
        MSR     CPSR_c, #F32_bit+SVC32_mode
        LDR     r9, =ZeroPage
        LDR     r5, [r9, #InitUsedStart]
        SUB     r0, r5, #DRAMOffset_L1PT                ; Scratch space + zero page are already cleared, so add them to InitUsedStart..InitUsedEnd
        BL      PhysAddrToPageNo
        ; If the DMA region was taken from the first RAM block, we won't be able to look up the page number
        ; Instead, let's use the first DRAM page for InitUsedStart - as this will correspond to the first page that isn't hidden inside the DMA region
        CMP     r0, #-1
        LDREQ   r0, [r9, #DRAMPhysAddrA]
        BLEQ    PhysAddrToPageNo
        MOV     r5, r0
        LDR     r0, [r9, #InitUsedEnd]
        BL      PhysAddrToPageNo
        SUB     r6, r0, #1
        ADD     r9, r9, #PhysRamTable
        MOV     r4, #0                                  ; current page no
        LDMIA   r9!, {r10, r11}
        MOV     r11, r11, LSR #12                       ; get rid of flags
d2090 4
a2093 8
        ; Map in this area, cacheable + bufferable to ensure burst writes are
        ; performed. We're careful to not partially overwrite any pages which
        ; are being used, so this shouldn't cause any issues due to being
        ; cachable + potentially doubly mapped.
        MOV     r0, #0
        MOV     r1, r10
        MOV     r2, #0
        BL      RISCOS_AccessPhysicalAddressUnchecked
d2095 5
a2099 4
        ; Inner loop will process one page at a time to keep things simple
        MOV     r3, r11
        MSR     CPSR_c, #F32_bit+FIQ32_mode             ; switch to our bank o'zeros
        MOV     r2, #0
d2101 14
a2114 5
        CMP     r4, r5
        CMPHS   r6, r4
        ADD     r1, r0, #4096
        BHS     %FT80
        ; Clear this page
d2116 18
a2133 4
        STMIA   r0!, {r2,r8-r14}
        STMIA   r0!, {r2,r8-r14}
        TEQ     r0, r1
        BNE     %BT70
d2135 1
a2135 5
        MOV     r0, r1                                  ; increment log addr
        ADD     r4, r4, #1                              ; increment page no
        SUBS    r3, r3, #1                              ; decrement length
        MOVNES  r1, r0, LSL #12                         ; check for MB limit
        BNE     %BT60
d2137 3
a2139 1
        MSR     CPSR_c, #F32_bit+SVC32_mode
a2140 2
        ; Make page uncacheable so the following is safe
        Push    "r0-r3"
d2145 2
a2146 1
        Pull    "r0-r3"
d2166 1
a2167 8

        ADD     r10, r10, r11, LSL #12                  ; r10+(r11-r3) = next MB
        MOVS    r11, r3                                 ; next block needed? also resets r11 ready for next pass
        SUBNE   r10, r10, r3, LSL #12
        LDMEQIA r9!, {r10, r11}                         ; grab next block if necessary
        MOVEQS  r11, r11, LSR #12                       ; anything left to do?
        BNE     %BT50

d2171 1
a2171 37
        LDR     r0, =ZeroPage+InitClearRamWs
        LDMIA   r0, {r4-r11,r14}                        ;restore

CPR_skipped

        LDR     r0, =ZeroPage+OsbyteVars + :INDEX: LastBREAK

        MOV     r1, #&80
        STRB    r1, [r0]                                ; flag the fact that RAM cleared

        MSR     CPSR_c, #F32_bit + UND32_mode           ; retrieve the MMU control register
        LDR     r0, =ZeroPage                           ; soft copy
        STR     sp, [r0, #MMUControlSoftCopy]
        MSR     CPSR_c, #F32_bit + SVC32_mode

        MOV     pc, lr

        LTORG

        MACRO
        MakeSkipTable $addr, $size
        ASSERT  ($addr :AND: 15) = 0
        ASSERT  ($size :AND: 15) = 0
        ASSERT  ($addr-ZeroPage) < 16*1024
        &       $addr, $size
        MEND

        MACRO
        EndSkipTables
        &       -1
        MEND


RamSkipTable
        MakeSkipTable   ZeroPage, InitWsEnd
        MakeSkipTable   ZeroPage+SkippedTables, SkippedTablesEnd-SkippedTables
        EndSkipTables
d2173 1
d2707 2
a2708 1
        ; If it's not an IIC interrupt, pass it on to the keyboard scan code
d2727 3
a2729 8
        LDRB    a2, [v2, #InitIRQWs+KbdScanActive]
        TEQ     a2, #0
        CallHAL HAL_KbdScanInterrupt,NE
        ; Keyboard scan code will have return -1 if it handled the IRQ
        ; If it didn't handle it, or keyboard scanning is inactive, something
        ; bad has happened
        CMP     a1, #-1
        CallHAL HAL_IRQDisable,NE ; Stop the rogue device from killing us completely
d2794 1
@


4.8.2.1
log
@Merge latest changes from main branch

Version 5.89, 4.129.2.6. Tagged as 'Kernel-5_89-4_129_2_6'
@
text
@d765 2
a766 1
        ; Set up a reset IRQ handler (for IIC CMOS access)
d1044 10
d1063 1
a1063 1
        BLEQ    ClearWkspRAM            ; Only clear the memory if the HAL didn't
d1072 1
a1072 1
; Calculate CPU feature flags
d1262 1
a1262 1
        BL      Init_MapInRAM_Clear
d1278 1
a1278 1
        BL      Init_MapInRAM_Clear
d1285 1
a1285 1
        BL      Init_MapInRAM_Clear
d1351 8
a1609 10
Init_MapInRAM_Clear ROUT                        ; same as Init_MapInRAM but also
        Push    "a1,a3,v5,lr"                   ; clears the mapped in result
        BL      Init_MapInRAM
        MOV     v5, a1
        Pull    "a1,a3"
        MOV     a2, #0
        BL      memset        
        MOV     a1, v5
        Pull    "v5,pc"

d1639 1
a1639 1
        ; DMA regions won't get cleared by ClearWkspRam, so do it manually
d1928 4
a1931 1
;       ClearWkspRAM - Routine to clear "all" workspace
d1933 1
a1933 1
; We have to avoid anything between InitUsedStart and InitUsedEnd - i.e.
d1939 5
a1943 2
;
; The bulk of RAM is cleared during the keyboard scan (ClearFreePoolSection).
d1949 1
a1949 1
ClearWkspRAM ROUT
d1960 1
d1963 2
a1964 2
 
        DebugTX "ClearWkspRAM"
d1994 57
d2053 38
d2094 2
a2095 1
      [ {FALSE} ; NewReset sets this later
d2097 1
a2099 1
      ]
d2123 1
a2128 158
;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
;
;       ClearFreePoolSection - Routine to clear a section of the free pool
;
; During keyboard scanning we soak up slack time clearing the bulk of RAM
; by picking a section of the free pool, mapping it in, clearing & flushing.

;
; In:   r0 = CAM entry to continue from
; Out:  r0 = updated
;

ClearFreePoolSection ROUT
        Push    "r1-r3, lr"
        LDR     r1, =ZeroPage
        LDR     r1, [r1, #MaxCamEntry]
        LDR     r2, =ZeroPage+FreePoolDANode
        CMP     r0, r1
        BHI     %FT30

        LDR     r3, =CAM
        ADD     r1, r3, r1, LSL #CAM_EntrySizeLog2      ; top entry (inc)
        ADD     r3, r3, r0, LSL #CAM_EntrySizeLog2      ; starting entry
10
        LDR     r14, [r3, #CAM_PageFlags]
        TST     r14, #DynAreaFlags_PMP
        BEQ     %FT20

        LDR     r14, [r3, #CAM_PMP]
        TEQ     r14, r2
        BEQ     %FT40
20
        ADD     r3, r3, #CAM_EntrySize                  ; next
        CMP     r3, r1
        BLS     %BT10
30
        MOV     r0, #-1
        Pull    "r1-r3, pc"
40
        Push    "r0-r12"

        ; This is a PMP entry in the free pool
        LDR     r14, [r3, #CAM_PMPIndex]                ; list index
        LDR     r9, [r2, #DANode_PMP]                   ; PMP list base
        LDR     r3, [r9, r14, LSL #2]                   ; ppn
        BL      ppn_to_physical                         ; => r5 = PA

      [ MEMM_Type = "ARM600"
        ; Map in this section, cacheable + bufferable to ensure burst writes
        ; are performed (StrongARM will only perform burst writes to CB areas)
        MOV     a1, #OSAP_None
      |
        ; Map in this section with default NCB cache policy. Making it cacheable
        ; is liable to slow things down significantly on some platforms (e.g.
        ; PL310 L2 cache)
        LDR     a1, =OSAP_None + DynAreaFlags_NotCacheable
      ]
        MOV     a2, r5
        MOV     a3, #0
        BL      RISCOS_AccessPhysicalAddressUnchecked

        MOV     r4, #0                                  ; clear to this value
        MOV     r6, r4
        MOV     r7, r4
        MOV     r8, r4
        MOV     r12, r4
45
        MOV     r9, r4
        MOV     r10, r4
        MOV     r11, r4

        ; Fill that page
        ADD     r2, r0, #4096
50
        STMIA   r0!, {r4,r6-r12}
        STMIA   r0!, {r4,r6-r12}
        TEQ     r0, r2
        BNE     %BT50

        ; Step the CAM until there are no more pages in that section
        LDR     r1, [sp, #1*4]
        LDR     r2, [sp, #2*4]
        LDR     r11, [sp, #3*4]
        B       %FT65
60
        LDR     r14, [r11, #CAM_PageFlags]
        TST     r14, #DynAreaFlags_PMP
        BEQ     %FT65

        LDR     r14, [r11, #CAM_PMP]
        TEQ     r14, r2
        BEQ     %FT70
65
        ADD     r11, r11, #CAM_EntrySize                ; next
        CMP     r11, r1
        BLS     %BT60

        MOV     r14, #-1                                ; CAM top, no more
        B       %FT80
70
        MOV     r10, r5                                 ; previous PA

        ; Next PMP entry in the free pool
        LDR     r14, [r11, #CAM_PMPIndex]               ; list index
        LDR     r9, [r2, #DANode_PMP]                   ; PMP list base
        LDR     r3, [r9, r14, LSL #2]                   ; ppn
        BL      ppn_to_physical                         ; => r5 = PA

        MOV     r14, r10, LSR #20
        TEQ     r14, r5, LSR #20                        ; same MB as previous?
        LDRNE   r14, =CAM
        SUBNE   r14, r11, r14
        MOVNE   r14, r14, LSR #CAM_EntrySizeLog2        ; no, so compute continuation point
        LDREQ   r0, =PhysicalAccess
        MOVEQ   r14, r5, LSL #12
        ORREQ   r0, r0, r14, LSR #12
        STREQ   r11, [sp, #3*4]
        BEQ     %BT45                                   ; yes, so clear it
80
        STR     r14, [sp, #0*4]                         ; return value for continuation

 [ MEMM_Type = "ARM600" ; VMSAv6 maps as non-cacheable, so no flush required
        ; Make page uncacheable so the following is safe
        MOV     r4, r0

        MOV     r0, #L1_B
        MOV     r1, r10
        MOV     r2, #0
        BL      RISCOS_AccessPhysicalAddress

        MOV     r0, r4

        ; Clean & invalidate the cache before the 1MB window closes
      [ CacheCleanerHack
        ; StrongARM requires special clean code, because we haven't mapped in
        ; DCacheCleanAddress yet. Cheat and only perform a clean, not full
        ; clean + invalidate (should be safe as we've only been writing)
        ARM_read_ID r2
        AND     r2, r2, #&F000
        CMP     r2, #&A000
        BNE     %FT90
85
        SUB     r0, r0, #32                             ; rewind 1 cache line
        ARMA_clean_DCentry r0
        MOVS    r1, r0, LSL #12                         ; start of the MB?
        BNE     %BT85
        B       %FT91
90
      ]
        ARMop Cache_CleanInvalidateAll
 ]
91
        MOV     a1, #L1_Fault
        BL      RISCOS_ReleasePhysicalAddress           ; reset to default

        Pull    "r0-r12"

        Pull    "r1-r3, pc"
d2662 1
a2662 2

        ; If it's not an IIC interrupt, mute it
d2681 8
a2688 3

        CallHAL HAL_IRQDisable ; Stop the rogue device from killing us completely

a2752 1
;       mov      r0,r0,lsl #16
@


4.7
log
@Implement support for cacheable pagetables
Detail:
  Modern ARMs (ARMv6+) introduce the possibility for the page table walk hardware to make use of the data cache(s) when performing memory accesses. This can significantly reduce the cost of a TLB miss on the system, and since the accesses are cache-coherent with the CPU it allows us to make the page tables cacheable for CPU (program) accesses also, improving the performance of page table manipulation by the OS.
  Even on ARMs where the page table walk can't use the data cache, it's been measured that page table manipulation operations can still benefit from placing the page tables in write-through or bufferable memory.
  So with that in mind, this set of changes updates the OS to allow cacheable/bufferable page tables to be used by the OS + MMU, using a system-appropriate cache policy.
  File changes:
  - hdr/KernelWS - Allocate workspace for storing the page flags that are to be used by the page tables
  - hdr/OSMem - Re-specify CP_CB_AlternativeDCache as having a different behaviour on ARMv6+ (inner write-through, outer write-back)
  - hdr/Options - Add CacheablePageTables option to allow switching back to non-cacheable page tables if necessary. Add SyncPageTables var which will be set {TRUE} if either the OS or the architecture requires a DSB after writing to a faulting page table entry.
  - s/ARM600, s/VMSAv6 - Add new SetTTBR & GetPageFlagsForCacheablePageTables functions. Update VMSAv6 for wider XCBTable (now 2 bytes per element)
  - s/ARMops - Update pre-ARMv7 MMU_Changing ARMops to drain the write buffer on entry if cacheable pagetables are in use (ARMv7+ already has this behaviour due to architectural requirements). For VMSAv6 Normal memory, change the way that the OS encodes the cache policy in the page table entries so that it's more compatible with the encoding used in the TTBR.
  - s/ChangeDyn - Update page table page flag handling to use PageTable_PageFlags. Make use of new PageTableSync macro.
  - s/Exceptions, s/AMBControl/memmap - Make use of new PageTableSync macro.
  - s/HAL - Update MMU initialisation sequence to make use of PageTable_PageFlags + SetTTBR
  - s/Kernel - Add PageTableSync macro, to be used after any write to a faulting page table entry
  - s/MemInfo - Update OS_Memory 0 page flag conversion. Update OS_Memory 24 to use new symbol for page table access permissions.
  - s/MemMap2 - Use PageTableSync. Add routines to enable/disable cacheable pagetables
  - s/NewReset - Enable cacheable pagetables once we're fully clear of the MMU initialision sequence (doing earlier would be trickier due to potential double-mapping)
Admin:
  Tested on pretty much everything currently supported
  Delivers moderate performance benefits to page table ops on old systems (e.g. 10% faster), astronomical benefits on some new systems (up to 8x faster)
  Stats: https://www.riscosopen.org/forum/forums/3/topics/2728?page=2#posts-58015


Version 5.71. Tagged as 'Kernel-5_71'
@
text
@d834 18
@


4.6
log
@Make MMU_Changing ARMops perform the sub-operations in a sensible order
Detail:
  For a while we've known that the correct way of doing cache maintenance on ARMv6+ (e.g. when converting a page from cacheable to non-cacheable) is as follows:
  1. Write new page table entry
  2. Flush old entry from TLB
  3. Clean cache + drain write buffer
  The MMU_Changing ARMops (e.g. MMU_ChangingEntry) implement the last two items, but in the wrong order. This has caused the operations to fall out of favour and cease to be used, even in pre-ARMv6 code paths where the effects of improper cache/TLB management perhaps weren't as readily visible.
  This change re-specifies the relevant ARMops so that they perform their sub-operations in the correct order to make them useful on modern ARMs, updates the implementations, and updates the kernel to make use of the ops whereever relevant.
  File changes:
  - Docs/HAL/ARMop_API - Re-specify all the MMU_Changing ARMops to state that they are for use just after a page table entry has been changed (as opposed to before - e.g. 5.00 kernel behaviour). Re-specify the cacheable ones to state that the TLB invalidatation comes first.
  - s/ARM600, s/ChangeDyn, s/HAL, s/MemInfo, s/VMSAv6, s/AMBControl/memmap - Replace MMU_ChangingUncached + Cache_CleanInvalidate pairs with equivalent MMU_Changing op
  - s/ARMops - Update ARMop implementations to do everything in the correct order
  - s/MemMap2 - Update ARMop usage, and get rid of some lingering sledgehammer logic from ShuffleDoublyMappedRegionForGrow
Admin:
  Tested on pretty much everything currently supported


Version 5.70. Tagged as 'Kernel-5_70'
@
text
@d617 1
a617 1
; Set up some temporary PCBTrans and PPLTrans pointers
d678 3
a680 1
        LDR     a3, =AreaFlags_L1PT
d688 3
a690 1
        LDR     a3, =AreaFlags_L1PT
d790 2
a791 5
; Also note, no RAM access until we've finished the operation, as we don't know it's
; available, and we might lose it due to lack of cache cleaning.
;
        MOV     a1, #4_3333333333333333         ; All domain manager - in case MMU already on
        ARM_MMU_domain a1                       ; (who knows what domains/permissions they are using)
d811 5
d817 1
a817 2
        MOV     ip, a1 ; Remember architecture for later
        
d819 1
a830 31
        ; If we're using shareable pages, set the appropriate flag in the TTBR to let the CPU know the page tables themselves are shareable
        ADD     lr, v3, #DRAMOffset_PageZero - DRAMOffset_L1PT
        LDR     lr, [lr, #MMU_PPLTrans]
        LDRH    lr, [lr]
        TST     lr, #L2_S
        ORRNE   v3, v3, #2
  ]
        ARM_MMU_transbase v3                    ; Always useful to tell it where L1PT is...
  [ MEMM_Type = "VMSAv6"
        BIC     v3, v3, #2                      ; (clear flags, the address in v3 is needed later on)
        CMP     ip, #0
  ]

        MOV     lr, #0
        BNE     %FT01
        MCREQ   p15, 0, lr, c5, c0              ; MMU may already be on (but flat mapped)
        B       %FT02
01      MCRNE   p15, 0, lr, c8, c7              ; if HAL needed it (eg XScale with ECC)
02                                              ; so flush TLBs now
  [ MEMM_Type = "VMSAv6"
        MCR     p15, 0, lr, c2, c0, 2           ; TTBCR: Ensure only TTBR0 is used
        ; Check if security extensions are supported
        CMP     ip, #ARMvF
        BNE     %FT01
        MRC     p15, 0, lr, c0, c1, 1           ; ID_PFR1
        TST     lr, #15<<4
        MOV     lr, #0
        BEQ     %FT01
        MCR     p15, 0, lr, c12, c0, 0          ; VBAR: Ensure exception vector base is 0 (security extensions)
01
        myISB   ,lr,,y
a833 1
        CMP     ip, #0
d841 1
d843 3
a869 3
        MOV     ip, #4_0000000000000001                         ; domain 0 client only
        ARM_MMU_domain ip

d1820 1
a1823 1
        LDR     a2, =AreaFlags_L2PT
d2265 1
a2265 6
      [ MEMM_Type = "VMSAv6"
        ; DSB + ISB required to ensure effect of page table write is fully
        ; visible (after overwriting a faulting entry)
        myDSB   ,a1
        myISB   ,a1,,y
      ]
@


4.5
log
@Add new ARMops. Add macros which map the ARMv7/v8 cache/TLB maintenance mnemonics (as featured in recent ARM ARMs) to MCR ops.
Detail:
  - Docs/HAL/ARMop_API - Document the new ARMops. These ops are intended to help with future work (DMA without OS_Memory 0 "make temp uncacheable", and minimising cache maintenance when unmapping pages) and aren't in use just yet.
  - hdr/Copro15ops - Add new macros for ARMv7+ which map the mnemonics seen in recent ARM ARMs to the corresponding MCR ops. This should make things easier when cross-referencing docs and reduce the risk of typos.
  - hdr/KernelWS - Shuffle kernel workspace a bit to make room for the new ARMops
  - hdr/OSMisc - Expose new ARMops via OS_MMUControl 2
  - s/ARMops - Implement the new ARMops. Change the ARMv7+ ARMops to use the new mnemonic macros. Also get rid of myDSB / myISB usage from ARMv7+ code paths; use DSB/ISB/etc. directly to ensure correct behaviour
  - s/HAL - Mnemonic + ISB/DSB updates. Change software RAM clear to do 16 bytes at a time for kernel workspace instead of 32 to allow the kernel workspace tweaks to work.
Admin:
  Binary diff shows that mnemonics map to the original MCR ops correctly
  Note: Raspberry Pi builds will now emit lots of warnings due to increased DSB/ISB instruction use. However it should be safe to ignore these as they should only be present in v7+ code paths.
  Note: New ARMops haven't been tested yet, will be disabled (or at least hidden from user code) in a future checkin


Version 5.68. Tagged as 'Kernel-5_68'
@
text
@d1188 3
a1190 3
; Flush the workspace from the cache & TLB so we can unmap it
; Really we should make the pages uncacheable first, but for simplicity we just
; do a full cache clean+invalidate later on when changing the ROM permissions
d1192 2
a1193 2
        MOV     a2, v1, LSR #12
        ARMop   MMU_ChangingEntries
@


4.4
log
@Add support for shareable pages and additional access privileges
Detail:
  This set of changes:
  * Refactors page table entry encoding/decoding so that it's (mostly) performed via functions in the MMU files (s.ARM600, s.VMSAv6) rather than on an ad-hoc basis as was the case previously
  * Page table entry encoding/decoding performed during ROM init is also handled via the MMU functions, which resolves some cases where the wrong cache policy was in use on ARMv6+
  * Adds basic support for shareable pages - on non-uniprocessor systems all pages will be marked as shareable (however, we are currently lacking ARMops which broadcast cache maintenance operations to other cores, so safe sharing of cacheable regions isn't possible yet)
  * Adds support for the VMSA XN flag and the "privileged ROM" access permission. These are exposed via RISC OS access privileges 4 and above, taking advantage of the fact that 4 bits have always been reserved for AP values but only 4 values were defined
  * Adds OS_Memory 17 and 18 to convert RWX-style access flags to and from RISC OS access privelege numbers; this allows us to make arbitrary changes to the mappings of AP values 4+ between different OS/hardware versions, and allows software to more easily cope with cases where the most precise AP isn't available (e.g. no XN on <=ARMv5)
  * Extends OS_Memory 24 (CheckMemoryAccess) to return executability information
  * Adds exported OSMem header containing definitions for OS_Memory and OS_DynamicArea
  File changes:
  - Makefile - export C and assembler versions of hdr/OSMem
  - Resources/UK/Messages - Add more text for OS_Memory errors
  - hdr/KernelWS - Correct comment regarding DCacheCleanAddress. Allocate workspace for MMU_PPLTrans and MMU_PPLAccess.
  - hdr/OSMem - New file containing exported OS_Memory and OS_DynamicArea constants, and public page flags
  - hdr/Options - Reduce scope of ARM6support to only cover builds which require ARMv3 support
  - s/AMBControl/Workspace - Clarify AMBNode_PPL usage
  - s/AMBControl/growp, mapslot, mapsome, memmap - Use AreaFlags_ instead of AP_
  - s/AMBControl/main, memmap - Use GetPTE instead of generating page table entry manually
  - s/ARM600 - Remove old coments relating to lack of stack. Update BangCam to use GetPTE. Update PPL tables, removing PPLTransL1 (L1 entries are now derived from L2 table on demand) and adding a separate table for ARM6. Implement the ARM600 versions of the Get*PTE ('get page table entry') and Decode*Entry functions
  - s/ARMops - Add Init_PCBTrans function to allow relevant MMU_PPLTrans/MMU_PCBTrans pointers to be set up during the pre-MMU stage of ROM init. Update ARM_Analyse to set up the pointers that are used post MMU init.
  - s/ChangeDyn - Move a bunch of flags to hdr/OSMem. Rename the AP_ dynamic area flags to AreaFlags_ to avoid name clashes and confusion with the page table AP_ values exported by Hdr:MEMM.ARM600/Hdr:MEMM.VMSAv6. Also generate the relevant flags for OS_Memory 24 so that it can refer to the fixed areas by their name instead of hardcoding the permissions.
  - s/GetAll - GET Hdr:OSMem
  - s/HAL - Change initial page table setup to use DA/page flags and GetPTE instead of building page table entries manually. Simplify AllocateL2PT by removing the requirement for the user to supply the access perimssions that will be used for the area; instead for ARM6 we just assume that cacheable memory is the norm and set L1_U for any L1 entry we create here.
  - s/Kernel - Add GetPTE macro (for easier integration of Get*PTE functions) and GenPPLAccess macro (for easy generation of OS_Memory 24 flags)
  - s/MemInfo - Fixup OS_Memory 0 to not fail on seeing non-executable pages. Implement OS_Memory 17 & 18. Tidy up some error generation. Make OS_Memory 13 use GetPTE. Extend OS_Memory 24 to return (non-) executability information, to use the named CMA_ constants generated by s/ChangeDyn, and to use the Decode*Entry functions when it's necessary to decode page table entries.
  - s/NewReset - Use AreaFlags_ instead of AP_
  - s/VMSAv6 - Remove old comments relating to lack of stack. Update BangCam to use GetPTE. Update PPL tables, removing PPLTransL1 (L1 entries are now derived from L2 table on demand) and adding a separate table for shareable pages. Implement the VMSAv6 versions of the Get*PTE and Decode*Entry functions.
Admin:
  Tested on Raspberry Pi 1, Raspberry Pi 3, Iyonix, RPCEmu (ARM6 & ARM7), comparing before and after CAM and page table dumps to check for any unexpected differences


Version 5.55. Tagged as 'Kernel-5_55'
@
text
@d1400 1
a1400 1
        myISB   ,r9
d1414 1
a1414 1
        MCR     p15, 0, r14, c7, c6, 2 ; Invalidate
d1419 1
a1419 1
        myDSB   ,r8        ; Cortex-A7 errata 814220: DSB required when changing cache levels when using set/way operations. This also counts as our end-of-maintenance DSB.
a1984 1
        STMNEIA r0!, {r8-r11}
d2119 3
a2121 2
        ASSERT  ($addr :AND: 31) = 0
        ASSERT  ($size :AND: 31) = 0
@


4.3
log
@Delete lots of old switches
Detail:
  This change gets rid of the following switches from the source (picking appropriate code paths for a 32bit HAL build):
  * FixCallBacks
  * UseProcessTransfer
  * CanLiveOnROMCard
  * BleedinDaveBell
  * NewStyleEcfs
  * DoVdu23_0_12
  * LCDPowerCtrl
  * HostVdu
  * Print
  * EmulatorSupport
  * TubeInfo
  * AddTubeBashers
  * TubeChar, TubeString, TubeDumpNoStack, TubeNewlNoStack macros
  * FIQDebug
  * VCOstartfix
  * AssemblingArthur (n.b. still defined for safety with anything in Hdr: which uses it, but not used explicitly by the kernel)
  * MouseBufferFix
  * LCDInvert
  * LCDSupport
  * DoInitialiseMode
  * Interruptible32bitModes
  * MouseBufferManager
  * StrongARM (new CacheCleanerHack and InterruptDelay switches added to hdr/Options to cover some functionality that StrongARM previously covered)
  * SAcleanflushbroken
  * StrongARM_POST
  * IrqsInClaimRelease
  * CheckProtectionLink
  * GSWorkspaceInKernelBuffers
  * EarlierReentrancyInDAShrink
  * LongCommandLines
  * ECC
  * NoSPSRcorruption
  * RMTidyDoesNowt
  * RogerEXEY
  * StorkPowerSave
  * DebugForcedReset
  * AssembleKEYV
  * AssemblePointerV
  * ProcessorVectors
  * Keyboard_Type
  Assorted old files have also been deleted.
Admin:
  Identical binary to previous revision for IOMD & Raspberry Pi builds


Version 5.51. Tagged as 'Kernel-5_51'
@
text
@d617 4
a624 1
        LDR     a3, =(AP_None * L2X_APMult)
d632 1
a632 1
        LDR     a2, =(AP_Read * L2X_APMult) + L2_C + L2_B
d651 1
a651 1
        LDRNE   a2, =(AP_None * L2X_APMult)
d664 1
a664 1
        LDR     a3, =(AP_Read * L2X_APMult) + L2_C + L2_B
d671 1
a671 1
        LDR     a3, =(AP_Read * L2X_APMult) + L2_C + L2_B
d678 1
a678 1
        LDR     a3, =(AP_None * L2X_APMult)
d686 1
a686 1
        LDR     a3, =(AP_None * L2X_APMult)
d716 2
a717 2
        MOVNE   a3, #(AP_Full * L2X_APMult) + L2_C + L2_B
        MOVEQ   a3, #(AP_ROM * L2X_APMult) + L2_C + L2_B
d732 1
a732 1
        MOV     a3, #(AP_ROM * L2X_APMult) + L2_C + L2_B
d745 2
a746 2
        MOVNE   a3, #(AP_Full * L2X_APMult) + L2_C + L2_B
        MOVEQ   a3, #(AP_ROM * L2X_APMult) + L2_C + L2_B
d824 8
d833 4
d845 9
a853 5
        CMP     ip, #ARMv6
        BLT     %FT01
        MCRGE   p15, 0, lr, c2, c0, 2           ; Ensure only TTBR0 is used (v6)
        BLE     %FT01
        MCRGT   p15, 0, lr, c12, c0, 0          ; Ensure exception vector base is 0 (Cortex)
d856 3
a858 3
        ORRGE   v5, v5, #MMUC_XP ; Extended pages enabled (v6)
        BICGE   v5, v5, #MMUC_TRE+MMUC_AFE ; TEX remap, Access Flag disabled
        BICGE   v5, v5, #MMUC_EE+MMUC_TE+MMUC_VE ; Exceptions = nonvectored LE ARM
a1081 2
; Set v4 to XCB bits for default cacheable+bufferable
; Set v5 to XCB bits for default bufferable
a1082 3
        LDR     ip, [v8, #MMU_PCBTrans]
        LDRB    v4, [ip, #XCB_CB]
        LDRB    v5, [ip, #XCB_NC]
d1092 1
a1092 1
        ORR     a3, v4, #AP_None * L2X_APMult             ; ideally, svc read only, user none but hey ho
d1145 1
d1151 1
a1151 1
        MOV     v6, #4<<20 ; Current log addr
d1166 1
a1166 1
        ORR     a3, v4, #AP_None * L2X_APMult
d1174 1
a1174 1
        MOV     a3, #4<<20
d1184 1
a1184 1
        MOV     a1, #4<<20
d1191 1
a1191 1
        MOV     a1, #4<<20
d1195 1
a1195 1
        LDR     a1, =L1PT+(4<<2)
d1227 1
a1227 1
        MOV     a3, #(AP_ROM * L2X_APMult) + L2_C + L2_B
d1237 1
a1237 1
        ORR     a2, v4, #AP_None * L2X_APMult
d1243 1
a1243 1
        ORR     a2, v4, #AP_Read * L2X_APMult
d1249 1
a1249 1
        ORR     a2, v4, #AP_None * L2X_APMult
d1255 1
a1255 1
        ORR     a2, v4, #AP_None * L2X_APMult
d1261 1
a1261 1
        ORR     a2, v4, #AP_None * L2X_APMult
d1265 1
a1265 1
; Allocate the system heap
d1267 1
a1267 1
        ORR     a2, v4, #AP_Full * L2X_APMult
d1273 1
a1273 1
        ORR     a2, v4, #AP_Read * L2X_APMult           ; Should be AP_None?
d1278 1
a1278 1
        ORR     a2, v5, #AP_Read * L2X_APMult           ; Should be AP_None?
d1283 1
a1283 1
        ORR     a2, v4, #AP_Read * L2X_APMult
d1290 1
a1290 1
        ORR     a2, v4, #AP_Read * L2X_APMult
a1304 1
        ORR     a3, v4, #AP_Full * L2X_APMult
a1310 1
        ORR     a3, v4, #AP_Full * L2X_APMult
a1496 1
        LDR     ip, [a1, #MMU_PCBTrans]
d1502 1
a1502 1
        MOV     a4, #AP_Duff                    ; the top down.
d1517 4
a1520 6

30      LDR     v5, [v3, v2, LSR #18]           ; v5 = first level descriptor
        ASSERT  L1_Fault = 0
        AND     a1, v5, #2_11                   ; move to next section if not
        TEQ     a1, #L1_Page                    ; a page table (we ignore section maps
        BEQ     %FT40                           ; and we don't do fine tables)
d1525 3
a1527 3
40      LDR     v5, [v4, v2, LSR #10]           ; v5 = second level descriptor
        ASSERT  L2_Fault = 0
        ANDS    v6, v5, #2_11                   ; move to next page if fault
d1529 6
a1534 69

 [ MEMM_Type <> "VMSAv6"
        TEQ     v6, #L2_SmallPage               ; convert small pages to extended pages
        BNE     %FT50
        LDR     a2, =ZeroPage                   ; if we now know that CPU supports them
        LDR     a1, [a2, #ProcessorFlags]
        TST     a1, #CPUFlag_ExtendedPages
        BEQ     %FT50

        LDR     a1, [a2, #MMU_PCBTrans]         ; reprocess C and B bits as per XCB table
        MOV     lr, #0
        TST     v5, #L2_C                       ; (eg if C and B both set, replace with
        ORREQ   lr, lr, #DynAreaFlags_NotCacheable ; default cacheable+bufferable XCB)
        TST     v5, #L2_B
        ORREQ   lr, lr, #DynAreaFlags_NotBufferable
        LDRB    lr, [a1, lr, LSR #2]
        EOR     v5, v5, #L2_ExtPage:EOR:L2_SmallPage
        BIC     v5, v5, #2_111111000000         ; remove excess 3 AP fields
        BIC     v5, v5, #2_000000001100         ; remove old C+B
        BIC     a1, v5, #2_000000110011         ; remove all other bits for just address
        ORR     v5, v5, lr                      ; put in new XCB
        ARMop   MMU_ChangingEntry,,,a2
        STR     v5, [v4, v2, LSR #10]           ; update page table
 ]

50      MOV     a1, v5, LSR #12
        MOV     a1, a1, LSL #12                 ; a1 = address (flags stripped out),,,
        TEQ     v6, #L2_LargePage
        BICEQ   a1, a1, #&0000F000              ; large pages get bits 12-15
        ANDEQ   lr, v2, #&0000F000              ; from the virtual address
        ORREQ   a1, a1, lr

        Push    "ip"
        BL      PhysAddrToPageNo
        Pull    "ip"
        CMP     a1, #-1
        BEQ     %FT80

        ADD     a2, v1, a1, LSL #CAM_EntrySizeLog2 ; a2 -> CAM entry

 [ MEMM_Type = "VMSAv6"
        AND     a1, v5, #L2_AP                  ; a1 = access permission
        MOV     a1, a1, LSR #L2_APShift
        ; Map AP_ROM to 0
        CMP     a1, #AP_ROM
        MOVEQ   a1, #0
        ; Now ARM access goes 0 => all R/O, 1 => user none, 2 => user R/O, 3  => user R/W
        ; PPL access goes 0 => user R/W, 1 => user R/O, 2 => user none, (and let's say 3 all R/O)
        RSB     v6, a1, #3                      ; v6 = PPL access
        AND     a1, v5, #2_11                   ; a1 = page type
        CMP     a1, #L2_ExtPage
        ANDHS   a1, v5, #L2_TEX+L2_C+L2_B       ; Extended TEX and CB bits
        ANDLO   a1, v5, #L2_C+L2_B              ; Large CB bits only
        ANDLO   lr, v5, #L2L_TEX                ; Large TEX bits
        ORRLO   a1, a1, lr, LSR #L2L_TEXShift-L2_TEXShift ; Move Large TEX back to Extended TEX position
        MOV     lr, #3                          ; lr = PCB value (funny loop to do NCNB first)
60      LDRB    a3, [ip, lr]                    ; look in XCBTrans table
        TEQ     a3, a1                          ; found a match for our XCB?
        BEQ     %FT70
        TST     lr, #2_11
        SUBNE   lr, lr, #1                      ; loop goes 3,2,1,0,7,6,5,4,...,31,30,29,28
        ADDEQ   lr, lr, #7
        TEQ     lr, #35
        BNE     %BT60
70      AND     a1, lr, #2_00011
        ORR     v6, v6, a1, LSL #4              ; extract NCNB bits
        AND     a1, lr, #2_11100
        ORR     v6, v6, a1, LSL #10             ; extract P bits
        ORR     v6, v6, #PageFlags_Unavailable               ; ???? pages from scratch to cam only?
d1537 1
a1537 31
        STMIA   a2, {v2, v6}                    ; store logical address, PPL
 |
        AND     a1, v5, #&30                    ; a1 = access permission
        MOV     a1, a1, LSR #4
        ; ARM access goes 0 => all R/O, 1 => user none, 2 => user R/O, 3  => user R/W
        ; PPL access goes 0 => user R/W, 1 => user R/O, 2 => user none, (and let's say 3 all R/O)
        RSB     v6, a1, #3                      ; v6 = PPL access
        AND     a1, v5, #2_11                   ; a1 = page type
        CMP     a1, #L2_SmallPage
        ANDHI   a1, v5, #2_0000001111001100     ; Extended TEX and CB bits
        ANDLS   a1, v5, #2_0000000000001100     ; Small/Large CB bits only
        ANDLO   lr, v5, #2_1111000000000000     ; Large TEX bits
        ORRLO   a1, a1, lr, LSR #6              ; Move Large TEX back to Extended TEX position
        MOV     lr, #3                          ; lr = PCB value (funny loop to do NCNB first)
60      LDRB    a3, [ip, lr]                    ; look in XCBTrans table
        TEQ     a3, a1                          ; found a match for our XCB?
        BEQ     %FT70
        TST     lr, #2_11
        SUBNE   lr, lr, #1                      ; loop goes 3,2,1,0,7,6,5,4,...,31,30,29,28
        ADDEQ   lr, lr, #7
        TEQ     lr, #35
        BNE     %BT60
70      AND     a1, lr, #2_00011
        ORR     v6, v6, a1, LSL #4              ; extract NCNB bits
        AND     a1, lr, #2_11100
        ORR     v6, v6, a1, LSL #10             ; extract P bits
        ORR     v6, v6, #PageFlags_Unavailable               ; ???? pages from scratch to cam only?
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
        STMIA   a2, {v2, v6}                    ; store logical address, PPL
 ]
d1544 1
a1544 1

d1663 1
a1663 1
;    a3 = access permissions+C+B bits (bits 11-2 of an L2 extended small page)
d1670 15
a1684 3
        Push    "lr"
        ORR     lr, a1, a2                      ; OR together, physaddr, logaddr
        ORR     lr, lr, a4                      ; and size.
d1686 3
a1688 1
        BEQ     Init_MapIn_Sections             ; it's section mapped
d1691 2
a1692 2
        ORRNE   a3, a3, #L2_ExtPage             ; then extended small pages (4K)
        BNE     %FT10
d1694 1
a1694 15
 [ MEMM_Type = "VMSAv6"
        ORR     a3, a3, #L2_LargePage           ; else large pages (64K)
        AND     lr, a3, #L2_TEX                 ; extract TEX from ext page flags
        BIC     a3, a3, #L2_TEX                 ; small page TEX bits SBZ for large pages
        ORR     a3, a3, lr, LSL #L2L_TEXShift-L2_TEXShift ; replace TEX in large page position     
 |
        ORR     a3, a3, #L2_LargePage           ; else large pages (64K)
        AND     lr, a3, #L2_TEX                 ; extract TEX from ext page flags
        AND     ip, a3, #L2X_AP                 ; extract AP from ext page flags
        BIC     a3, a3, #L2_AP                  ; clear way for large page AP
        ORR     ip, ip, ip, LSL #2              ; duplicate up AP to 4 sub-pages
        ORR     ip, ip, ip, LSL #4
        ORR     a3, a3, lr, LSL #6              ; replace TEX in large page position
        ORR     a3, a3, ip                      ; replace quadrupled AP
 ]
d1696 1
a1696 5
        Push    "v4-v7"
        MOV     v4, a1                          ; v4 = physaddr
        MOV     v5, a2                          ; v5 = logaddr
        MOV     v6, a3                          ; v6 = access permissions
        MOV     v7, a4                          ; v7 = area size
d1706 1
a1706 1
        Pull    "v4-v7,pc"
d1708 2
a1709 2
 [ MEMM_Type = "VMSAv6"
Init_MapIn_Sections
d1712 1
a1712 6
        AND     lr, a3, #L2_TEX + L2_AP         ; extract TEX, AP, APX bits (input is extended small page)
        BIC     a3, a3, #L2_TEX + L2_AP         ; and clear them
        ORR     a3, a3, lr, LSL #6              ; put TEX and AP bits back in new position
        ORR     a3, a3, #L1_Section             ; Mark as section
        ORR     a1, a1, a3                      ; Merge with physical address
        ADD     a2, ip, a2, LSR #18             ; a2 -> L1PT entry
d1715 1
a1715 1
        SUBS    a4, a4, #1024*1024              ; and loop
d1717 1
a1717 28
        Pull    "pc"
 |
Init_MapIn_Sections
        MOVS    ip, v3                          ; is MMU on?
        LDREQ   ip, =L1PT                       ; then use virtual address
        AND     lr, a3, #4_033300               ; extract TEX and AP bits
        BIC     a3, a3, #4_033300               ; and clear them (now P, Domain and U bits)
        ORR     a3, a3, lr, LSL #6              ; put TEX and AP bits back in new position
  [ ARM6support
        ASSERT  AP_ROM = 0
        TST     a3, #4_300000                   ; If ROM permission
        ARM_6   lr,EQ                           ;   and ARM 6
        ORREQ   a3, a3, #AP_Read * L1_APMult    ;     then make it Read permission, non-updateable
        ORRNE   a3, a3, #L1_U
        ORRS    a3, a3, #L1_Section             ; Add section indicator to permission
  |
        ORRS    a3, a3, #L1_U+L1_Section        ; Add section + U indicators to permission
  ]
        ORRMI   a3, a3, #L1_P
        BICMI   a3, a3, #1:SHL:31
        ORR     a1, a1, a3                      ; Merge with physical address
        ADD     a2, ip, a2, LSR #18             ; a2 -> L1PT entry
70      STR     a1, [a2], #4                    ; And store in L1PT
        ADD     a1, a1, #1024*1024              ; Advance one megabyte
        SUBS    a4, a4, #1024*1024              ; and loop
        BNE     %BT70
        Pull    "pc"
 ]
a1732 2
; ROM permission is caught if on an ARM 6 and turned into Read
; Extended pages are caught if not available (or MMU off) and turned into Small
d1734 2
a1735 2
Init_MapInPage
        Push    "v4-v6, lr"
d1753 2
a1754 24
        BICEQ   v6, v6, #&0000F000              ; large page descriptors
 [ MEMM_Type <> "VMSAv6"
        TEQ     lr, #L2_ExtPage
        BNE     %FT50
        TEQ     v3, #0                          ; if we've been given an extended page
        BNE     %FT45                           ; must check that (a) the MMU is on
        LDR     lr, =ZeroPage
        LDR     lr, [lr, #ProcessorFlags]       ; and (b) the CPU supports them
        TST     lr, #CPUFlag_ExtendedPages
        BNE     %FT50
45      EOR     v6, v6, #L2_SmallPage :EOR: L2_ExtPage
        AND     lr, v6, #L2X_AP                 ; convert the extended page descriptor
        BIC     v6, v6, #L2_AP                  ; into a small page descriptor
        ORR     lr, lr, lr, LSL #2              ; (losing the TEX bits in the
        ORR     v6, v6, lr                      ; process, but they should have been 0)
        ORR     v6, v6, lr, LSL #4
 ]
50      ORR     lr, v4, v6                      ; lr = value for L2PT entry
  [ ARM6support
        ASSERT  AP_ROM = 0
        TST     lr, #4_333300                   ; if ROM permission
        ARM_6   a2,EQ                           ;   and ARM 6
        ORREQ   lr, lr, #AP_Read * L2_APMult    ;     then make it Read permission
  ]
d1757 1
a1757 1
        Pull    "v4-v6, pc"
a1763 1
;    a3 = access permissions that will be placed in L2PT (bits 11-2 of Extended L2 descriptor)
d1770 2
a1771 2
AllocateL2PT
        Push    "a3,v4-v8,lr"
a1783 12
 [ ARM6support
        BEQ     %FT10

        TST     v5, #L1_U                       ; if section is already updateable
        BNE     %FT40                           ; leave it.
        LDR     a3, [sp, #0]
        TST     a3, #4_333300                   ; if they want anything other than
        ORRNE   v5, v5, #L1_U                   ; ROM access, make the section
        STRNE   v5, [v6, v8, LSL #2]            ; updateable.
        B       %FT40
10
 |
a1784 1
 ]
a1804 10
  [ ARM6support
        ASSERT  AP_ROM = 0
        ARM_6   lr                              ; if ARM 6
        BNE     %FT15
        LDR     lr, [sp, #0]                    ; do they want ROM access?
        TST     lr, #4_000300
        BICEQ   a3, a3, #L1_U                   ; set the updateable bit accordingly
        ORRNE   a3, a3, #L1_U
15
  ]
d1811 1
a1811 1
 [ MEMM_Type = "VMSAv6"
d1813 3
a1815 12
 |
  [ ARM6support
        ASSERT  AP_ROM = 0
        ARM_6   lr                              ; if ARM 6
        LDREQ   lr, [sp, #0]                    ; do they want ROM access?
        TSTEQ   lr, #4_000300
        ORREQ   a3, a1, #L1_Page                ; set the updateable bit accordingly
        ORRNE   a3, a1, #L1_Page + L1_U
  |
        ORR     a3, a1, #L1_Page + L1_U
  ]
 ]
d1841 11
a1855 1
        LDR     a3, =(AP_None * L2X_APMult) + L2_ExtPage
d1863 1
a1863 1
        Pull    "a3,v4-v8,pc"
d1868 6
a1873 1
        AND     a1, a1, #L1_B                           ; user can ask for bufferable
d1876 3
d1880 1
a1880 3
        LDR     a4, =(AP_None * L1_APMult) + L1_Section + L1_XN
 |
        LDR     a4, =(AP_None * L1_APMult) + L1_U + L1_Section
a1881 3
        ORR     a1, a4, a1                              ; a1 = flags for 1st level descriptor
        MOV     a4, a2, LSR #20                         ; rounded to section
        ORR     a1, a1, a4, LSL #20                     ; a1 = complete descriptor
d2032 1
a2032 1
        MOV     r0, #L1_B+L1_C
d2175 1
a2175 14
        Entry   "v1-v5,v7"
        LDR     v7, =L1_B:OR:L1_C:OR:L1_AP:OR:L1_TEX    ; v7 = user-specifiable flags
        MOV     v5, a1                                  ; v5 = original flags
        MOV     v4, a2                                  ; v4 = original requested address
        ADD     a3, a2, a3                              ; a3 -> end (exclusive)
        MOV     a2, a2, LSR #20
        MOV     a2, a2, LSL #20                         ; round a2 down to a section boundary
        SUB     v4, v4, a2                              ; v4 = offset of original within section-aligned area
        MOV     lr, a3, LSR #20
        TEQ     a3, lr, LSL #20
        ADDNE   lr, lr, #1
        MOV     a3, lr, LSL #20                         ; round a3 up to a section boundary

        TST     v5, #MapInFlag_APSpecified
d2179 14
d2194 1
a2194 1
        ANDS    v5, v5, #MapInFlag_DoublyMapped
d2199 1
a2199 5
        AND     a1, a1, v7                              ; mask out unsupported attributes
 [ MEMM_Type = "VMSAv6"
        ORR     a1, a1, #L1_XN                          ; force non-executable to prevent speculative instruction fetches
        ORR     v7, v7, #L1_XN
 ]
d2280 1
a2280 2
        ORR     a2, a2, a1
        ORR     a2, a2, #L1_Section                     ; first L1PT value
@


4.2
log
@Delete pre-HAL and 26bit code
Detail:
  This change gets rid of the following switches from the source (picking appropriate code paths for a 32bit HAL build):
  * HAL
  * HAL26
  * HAL32
  * No26bitCode
  * No32bitCode
  * IncludeTestSrc
  * FixR9CorruptionInExtensionSWI
  Various old files have also been removed (POST code, Arc/STB keyboard drivers, etc.)
Admin:
  Identical binary to previous revision for IOMD & Raspberry Pi builds


Version 5.49. Tagged as 'Kernel-5_49'
@
text
@a660 3
  [ ECC
        LDR     a3, =(AP_Read * L2X_APMult) + L2_C + L2_B + 1:SHL:31
  |
a661 1
  ]
a667 3
  [ ECC
        LDR     a3, =(AP_Read * L2X_APMult) + L2_C + L2_B + 1:SHL:31
  |
a668 1
  ]
a674 3
 [ ECC
        LDR     a3, =(AP_None * L2X_APMult) + 1:SHL:31
 |
a675 1
 ]
a682 3
 [ ECC
        LDR     a3, =(AP_None * L2X_APMult) + 1:SHL:31
 |
a683 1
 ]
a1266 1
 [ LongCommandLines
a1270 1
 ]
a1688 3
 [ ECC
        ORR     a3, v7, #1:SHL:31
 |
a1689 1
 ]
a1724 3
 [ ECC
        ORR     a3, a2, #1:SHL:31
 |
a1725 1
 ]
a1746 2
;           (also set bit 31 to indicate that P bit in L1PT should
;            be set)
a1843 1
;           (also set bit 31 to indicate that P bit in L1PT should be set)
a1907 1
;           (also set bit 31 to indicate that P bit in L1PT should be set)
a1990 3
  [ ECC
        ORR     a3, a3, #L1_P
  ]
a2021 3
 [ ECC
        ORR     a3, a3, #1:SHL:31
 ]
a2035 8
 [ ECC
        Push    "a1-a3,lr"
        MOV     a1, a2
        BL      PhysAddrToPageNo
        CMP     a1, #-1
        Pull    "a1-a3,lr"
        ORRNE   a1, a1, #L1_P
 ]
a2129 5
      [ EmulatorSupport
        ARM_on_emulator r0
        BEQ     CPR_skipped
      ]

d2233 1
a2233 1
      [ StrongARM
@


4.1
log
@Merge HAL branch to trunk
Detail:
  This change merges the past 15+ years of HAL branch development back to the trunk.
  This is effectively the end for non-HAL builds of the kernel, as no attempt has been made to maintain it during this merge, and all non-HAL & non-32bit code will soon be removed anyway.
  Rather than list everything that's been added to the HAL branch, it's easier to describe the change in terms of the things that the HAL branch was lacking:
  * Trunk version of Docs/32bit contained updated comments for the SVC stack structure during ErrorV
  * Trunk version of s/HeapMan contained a tweak to try and reduce the number of small free blocks that are created
  * Trunk version of s/Kernel contained a change to only copy 248 bytes of the error string to the error buffer (down from 252 bytes), to take into account the extra 4 bytes needed by the PSR. However this goes against the decision that's been made in the HAL branch that the error buffer should be enlarged to 260 bytes instead (ref: https://www.riscosopen.org/tracker/tickets/201), so the HAL build will retain its current behaviour.
  * Trunk version of s/MsgCode had RMNot32bit error in the list of error messages to count when countmsgusage {TRUE}
  * Trunk version of s/PMF/i2cutils contained support for OS_Memory 5, "read/write value of NVRamWriteSize". Currently the HAL branch doesn't have a use for this (in particular, the correct NVRamWriteSize should be specified by the HAL, so there should be no need for software to change it at runtime), and so this code will remain switched out in the HAL build.
Admin:
  Tested on Raspberry Pi


Version 5.48. Tagged as 'Kernel-5_48'
@
text
@a1247 1
 [ HAL32
a1252 1
 ]
@


1.1
log
@file HAL was initially added on branch HAL.
@
text
@d1 3017
@


1.1.2.1
log
@* Converted to building with ObjAsm (but still a single object file using ORG).
* Added ARM_IMB and ARM_IMBRange SWIs as recommended by ARMv5.
* Some early prototype HAL bits popped in - a lot of source restructuring still
  to come.
* New debug target creates an AIF image with debug information, and translates
  this into an ASCII object file for the 16702B logic analyser.

Version 5.35, 4.79.2.1. Tagged as 'Kernel-5_35-4_79_2_1'
@
text
@a0 1541
; Copyright 2000 Pace Micro Technology plc
;
; Licensed under the Apache License, Version 2.0 (the "License");
; you may not use this file except in compliance with the License.
; You may obtain a copy of the License at
;
;     http://www.apache.org/licenses/LICENSE-2.0
;
; Unless required by applicable law or agreed to in writing, software
; distributed under the License is distributed on an "AS IS" BASIS,
; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
; See the License for the specific language governing permissions and
; limitations under the License.
;
        GBLL    CacheOff
CacheOff SETL {TRUE}

; Sets $reg and sb up ready for CallHAL. $zero, if supplied, points to zero page (saves an instruction)
        MACRO
        AddressHAL $reg, $zero
 [ "$zero" = ""
        MOV     $reg, #ZeroPage
        LDR     $reg, [$reg, #HAL_Entries]
 |
        LDR     $reg, [$zero, #HAL_Entries]
 ]
        LDR     sb, =HAL_Workspace
        MEND

; Calls the HAL. $rout is the routine, $hal is the register set up by AddressHAL.
        MACRO
        CallHAL $rout, $hal
        LDR     ip, [$hal, #$rout * 4]
        MOV     lr, pc
        ADD     pc, $hal, ip
        MEND

a1      RN      0
a2      RN      1
a3      RN      2
a4      RN      3
v1      RN      4
v2      RN      5
v3      RN      6
v4      RN      7
v5      RN      8
v6      RN      9
sb      RN      9
v7      RN      10
v8      RN      11

; Layouts of HAL and image headers

                        ^       0
HALDesc_Flags           #       4
HALDesc_Start           #       4
HALDesc_Size            #       4
HALDesc_EntryTable      #       4
HALDesc_Entries         #       4
HALDesc_WorkspaceSize   #       4

                        ^       0
ImgDesc_MagicWord       #       4
ImgDesc_Flags           #       4
ImgDesc_Address         #       4
ImgDesc_Size            #       4
ImgDesc_EntryTable      #       4
ImgDesc_Entries         #       4


; Fixed page allocation is as follows

                        ^       0
DRAMOffset_FirstFixed   #       0
DRAMOffset_ScratchSpace #       16*1024
DRAMOffset_PageZero     #       16*1024
DRAMOffset_L1PT         #       16*1024         ; L1PT must be 16K-aligned
DRAMOffset_CAM          #       0
DRAMOffset_LastFixed    #       0

ARMv3   *       0
ARMv4   *       1
ARMv4T  *       2
ARMv5   *       3
ARMv5T  *       4

ARM2    *       0
ARM3    *       1
ARM6    *       2
ARM7    *       3
ARM7T   *       4
ARM8    *       5
SA110   *       6

CPUFlag_BaseRestored            * 1:SHL:0
CPUFlag_StorePCplus8            * 1:SHL:1
CPUFlag_No26bitMode             * 1:SHL:8
CPUFlag_VectorReadException     * 1:SHL:9

; ARM keep changing their mind about ID field layout.
; Here's a summary, courtesy of the ARM ARM (v5):
;
; pre-ARM 7:   xxxx0xxx
; ARM 7:       xxxx7xxx where bit 23 indicates v4T/~v3
; post-ARM 7:  xxxanxxx where n<>0 or 7 and a = architecture (1=4,2=4T,3=5,4=5T)
;

; int Init_ARMarch(void)
; Returns architecture, as above in a1. Also EQ if ARMv3, NE if ARMv4 or later.
; Corrupts only ip, no RAM usage.
Init_ARMarch
        ARM_read_ID ip
        ANDS    a1, ip, #&0000F000
        MOVEQ   pc, lr                          ; ARM 3 or ARM 6
        TEQ     a1, #&00007000
        BNE     %FT20
        TST     ip, #&00800000                  ; ARM 7 - check for Thumb
        MOVNE   a1, #ARMv4T
        MOVEQ   a1, #ARMv3
        MOV     pc, lr
20      ANDS    a1, ip, #&000F0000              ; post-ARM 7
        MOV     a1, a1, LSR #16
        MOV     pc, lr


; void RISCOS_InitARM(unsigned int flags)
;
RISCOS_InitARM
        MOV     a4, lr
        ; Check if we're architecture 3. If so, don't read the control register.
        BL      Init_ARMarch
        MOVEQ   a1, #0
        ARM_read_control a1, NE
        ; We assume that ARMs with an I cache can have it enabled while the MMU is off.
        [ CacheOff
        BICNE   a1, a1, #MMUC_I
        |
        ORRNE   a1, a1, #MMUC_I
        ]
        ; Late abort (ARM6 only), 32-bit Data and Program space. No Write buffer (ARM920T
        ; spec says W bit should be set, but I reckon they're bluffing).
        ;
        ; The F bit's tricky. (1 => CPCLK=FCLK, 0=>CPCLK=FCLK/2). The only chip using it was the
        ; ARM700, it never really reached the customer, and it's always been programmed with
        ; CPCLK=FCLK. Therefore we'll keep it that way, and ignore the layering violation.
        ORR     a1, a1, #MMUC_F+MMUC_L+MMUC_D+MMUC_P
        ; All of these bits should be off already, but just in case...
        BIC     a1, a1, #MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M
        BIC     a1, a1, #MMUC_RR+MMUC_V+MMUC_Z+MMUC_R+MMUC_S+MMUC_R

        ; Off we go.
        ARM_write_control a1

        ; In case it wasn't a hard reset
        MOV     a2, #0
        MCR     ARM_config_cp,0,a2,ARMv4_cache_reg,C7           ; invalidate I+D caches
        MCREQ   ARM_config_cp,0,a2,ARMv3_TLBflush_reg,C0        ; flush TLBs
        MCRNE   ARM_config_cp,0,a2,ARMv4_TLB_reg,C7             ; flush TLBs

        ; Check if we are in a 26-bit mode.
        MRS     a2, CPSR
        ; Keep a soft copy of the CR in a banked register (R8_fiq)
        MSR     CPSR_c, #F32_bit+I32_bit+FIQ32_mode
        MOV     v5, a1
        ; Switch into SVC32 mode (we may have been in SVC26 before).
        MSR     CPSR_c, #F32_bit+I32_bit+SVC32_mode

        ; If we were in a 26-bit mode, the lr value given to us would have had PSR flags in.
        TST     a2, #2_11100
        MOVNE   pc, a4
        BICEQ   pc, a4, #ARM_CC_Mask


; void *RISCOS_AddRAM(unsigned int flags, void *start, void *end, uintptr_t sigbits, void *ref)
;   Entry:
;     flags   bit 0: video memory (currently only one block permitted)
;             bits 8-11: speed indicator (arbitrary, higher => faster)
;             other bits reserved (SBZ)
;     start   = start address of RAM (inclusive) (no alignment requirements)
;     end     = end address of RAM (exclusive) (no alignment requirements, but must be >= start)
;     sigbits = significant address bit mask (1 => this bit of addr decoded, 0 => this bit ignored)
;     ref     = reference handle (NULL for first call)

; A table is built up at the head of the first block of memory.
; The table consists of (addr, len, flags) pairs, terminated by a count of those pairs; ref points to that
; counter.
; Twelve bits of flags are stored at the bottom of the length word.

        ROUT
RISCOS_AddRAM
        Push    "v1,v2,v3,v4,lr"
        LDR     v4, [sp, #20]           ; Get ref

        ; Round to pages. If we were extra sneaky we could not do this and chuck out incomplete
        ; pages after concatanation, but it would be a weird HAL that gave us pages split across
        ; calls.
        ;
        ADD     a2, a2, #4096           ; round start address up
        SUB     a2, a2, #1
        MOV     a2, a2, LSR #12
        MOV     a2, a2, LSL #12
        MOV     a3, a3, LSR #12         ; round end address down
        MOV     a3, a3, LSL #12

        CMP     a3, a2
        BLS     %FT90                   ; check we aren't now null

        CMP     v4, #0
        BEQ     %FT20

        ; We are not dealing with the first block since v4 != 0.  Make an attempt to merge this block
        ; with the previous block.
        LDMDB   v4, {v1, v2}            ; Get details of the previous block
        MOV     v3, v2, LSL #20         ; Isolate flags
        BIC     v2, v2, v3, LSR #20     ; And strip from length
        ADD     v2, v1, v2              ; Get the end address
        EOR     v2, v2, a2              ; Compare with the current block start address...
        TST     v2, a4                  ; ... but only check the decoded bits.
        EOR     v2, v2, a2              ; Restore the previous block end address.
        TEQEQ   v3, a1, LSL #20         ; And are the page flags the same?
        BNE     %FT10                   ; We can't merge it after the previous block

        ; v1 = previous start
        ; v2 = previous end
        ; The block is just after the previous block.  That means the start address is unchanged, but
        ; the length is increased.
        SUB     v2, v2, v1              ; Calculate the previous block length.
        SUB     a3, a3, a2              ; Find the length of the new block.
        ; a3 = length of block
        ADD     v2, v2, a3              ; Add it to the previous length.
        ORR     v2, v2, v3, LSR #20     ; And put the flags back in.
        STR     v2, [v4, #-4]           ; Update the block size in memory.
        MOV     a1,v4
        Pull    "v1,v2,v3,v4,pc"

        ; The block is not just after the previous block, but it may be just before.  This may be the
        ; case if we are softloaded.
10      SUB     v1, v1, #1              ; Compare the address before the previous block start ...
        SUB     a3, a3, #1              ; ... with the address of the last byte in this block ...
        EOR     v1, v1, a3
        TST     v1, a4                  ; ... but check only the decoded bits.
        ADD     a3, a3, #1              ; Restore the end address.
        TEQEQ   v3, a1, LSL #20         ; And are the page flags the same?
        BNE     %FT20                   ; Skip if we cannot merge the block.

        ; The block is just before the previous block.  The start address and length both change.
        LDR     v1, [v4, #-8]           ; Get the previous block start again.

        SUB     a3, a3, a2              ; Calculate the current block size.
        SUB     v1, v1, a3              ; Subtract from the previous block start address.
        SUB     v2, v2, v1              ; Calculate the new length=end-start
        ORR     v2, v2, v3, LSR #20     ; And put the flags back in.
        STMDB   v4, {v1, v2}            ; Update the block info in memory.
        MOV     a1,v4
        Pull    "v1,v2,v3,v4,pc"

        ; We now have a region which does not merge with a previous region.  We move it up to the
        ; highest address we can in the hope that this block will merge with the next block.
20      SUB     a3, a3, a2              ; Calculate the block size
        MOV     a1, a1, LSL #20
        ORR     a3, a3, a1, LSR #20     ; Put the flags at the bottom
        MVN     v1, a4                  ; Get the non-decoded address lines.
        ORR     a2, v1, a2              ; Set the non-decoded address bit in the start address.

30      CMP     v4, #0                  ; If the workspace has not been allocated...
        MOVEQ   v4, a2                  ; ... use this block.
        MOVEQ   v1, #0                  ; Initialise the counter.

        ; The block/fragment to be added is between a2 and a2+a3.
        LDRNE   v1, [v4]                ; Get the old counter if there was one.
        STMIA   v4!, {a2, a3}           ; Store address and size.
        ADD     v1, v1, #1              ; Increment the counter.
        STR     v1, [v4]                ; Store the counter.

90      MOV     a1,v4
        Pull    "v1,v2,v3,v4,pc"        ; We've done with this block now.




;void RISCOS_Start(unsigned int flags, int *riscos_header, int *hal_header, void *ref)
;

; We don't return, so no need to obey ATPCS, except for parameter passing.
; register usage:   v4 = location of VRAM
;                   v6 = amount of VRAM

        ROUT
RISCOS_Start
        TEQ     a4, #0
01      BEQ     %BT01                           ; Stop here if no RAM

        LDR     v5, [a4]                        ; v5 = the number of RAM blocks
        SUB     v8, a4, v5, LSL #3              ; Jump back to the start of the list.

        ; Search for some VRAM
05      LDMIA   v8!, {v1, v2}                   ; Get a block from the list. (v1,v2)=(addr,size+flags)
        TST     v2, #1                          ; Is it VRAM?
        BNE     %FT20                           ; If so, deal with it below
        TEQ     v8, a4                          ; Carry on until end of list or we find some.
        BNE     %BT05

        ; Extract some pseudo-VRAM from first RAM block
        SUB     v8, a4, v5, LSL #3              ; Rewind again.
        LDMIA   v8!, {v1, v2}
        MOV     v2, v2, LSR #12                 ; Remove flags
        MOV     v2, v2, LSL #12
        MOV     v4, v1                          ; Allocate first block as video memory
        MOV     v6, v2
        TEQ     v8, a4                          ; Was this the only block? If so, leave 1M
        SUBEQS  v6, v6, #1024*1024
        MOVCC   v6, v2, LSR #1                  ; If that overflowed, take half the bank.
        ;CMP     v6, #8*1024*1024
        ;MOVHS   v6, #8*1024*1024               ; Limit allocation to 8M (arbitrary)

        ADD     v1, v1, v6                      ; Adjust the RAM block base...
        SUBS    v2, v2, v6                      ; ... and the size
        LDMEQIA v8!, {v1, v2}                   ; Fetch the next block if we claimed it all
        B       %FT30

        ; Note real VRAM parameters
20      MOV     v6, v2                          ; Remember the size and address
        MOV     v4, v1                          ; of the VRAM
22      TEQ     v8, a4                          ; if not at the end of the array
        LDMNEIA v8, {v1, v2}                    ; pack the array tighter
        STMNEDB v8, {v1, v2}
        ADDNE   v8, v8, #8
        BNE     %BT22
25      SUB     v5, v5, #1                      ; decrease the counter
        STR     v5, [a4, #-8]!                  ; and move the end marker down

        SUB     v8, a4, v5, LSL #3              ; Rewind to start of list
        LDMIA   v8!, {v1, v2}

        ; Fill in the Kernel's permanent memory table
30      ADD     ip, v1, #DRAMOffset_PageZero
        ADD     v7, ip, #DRAMPhysAddrA

        ADD     sp, v1, #DRAMOffset_ScratchSpace + ScratchSpaceSize

        Push    "a1,a2,a3"                      ; Remember our arguments

        CMP     v5, #DRAMPhysTableSize          ; Don't overflow our table
        ADDHI   a4, v8, #DRAMPhysTableSize*8 - 8

35      MOV     v2, v2, LSR #12
        MOVS    v2, v2, LSL #12                 ; strip out flags
        STMNEIA v7!, {v1, v2}                   ; if non-zero length, add it to real list
        TEQ     v8, a4
        LDMNEIA v8!, {v1, v2}
        BNE     %BT35

        ; Now go back and put the VRAM information in

        MOV     v6, v6, LSR #12
        MOV     v6, v6, LSL #12                 ; strip out flags
        ADD     a3, ip, #VideoPhysAddr
        STMIA   a3, {v4, v6}

        ; Now we have to work out the total RAM size
        MOV     a2, #0
        MOV     v6, a3
40
        LDMIA   v6!, {v1, v2}                   ; get address, size
        ADD     a2, a2, v2                      ; add on size
        TEQ     v6, v7
        BNE     %BT40


; a2 = Total memory size (bytes)
; a3 = PhysRamTable
; v7 = After last used entry in PhysRamTable

; now store zeros to fill out table

55
        ADD     v2, a3, #PhysRamTableEnd-PhysRamTable
        MOV     v3, #0
        MOV     v4, #0
57
        CMP     v7, v2
        STMLOIA v7!, {v3, v4}
        BLO     %BT57

; Time to set up the L1PT. Just zero it out for now.

        LDR     a3, [a3, #DRAMPhysAddrA-PhysRamTable]   ; get address of 1st RAM bank
        ADD     a3, a3, #DRAMOffset_L1PT+16*1024        ; make a3 -> L1PT end
        MOV     a4, #16*1024
        MOV     v2, #0
        MOV     v3, #0
        MOV     v4, #0
        MOV     v5, #0
        MOV     v6, #0
        MOV     v7, #0
        MOV     v8, #0
        MOV     ip, #0
60
        STMDB   a3!, {v2-v8,ip}                         ; start at end and work back
        SUBS    a4, a4, #8*4
        BNE     %BT60

        ADD     v1, a3, #DRAMOffset_PageZero - DRAMOffset_L1PT
        ADD     v2, a3, #DRAMOffset_CAM - DRAMOffset_L1PT
        STR     a2, [v1, #RAMLIMIT]                     ; remember the RAM size
        MOV     lr, a2, LSR #12
        SUB     lr, lr, #1
        STR     lr, [v1, #MaxCamEntry]
        MOV     lr, a2, LSR #12-3+12
        CMP     a2, lr, LSL #12-3+12
        ADDNE   lr, lr, #1
        MOV     lr, lr, LSL #12
        STR     lr, [v1, #SoftCamMapSize]
        STR     a3, [v1, #InitUsedStart]                ; store start of L1PT

        ADD     v1, v1, #DRAMPhysAddrA
        MOV     v3, a3

; For the next batch of allocation routines, v1-v3 are treated as globals.
; v1 -> current entry in PhysRamTable
; v2 -> next address to allocate in v1 (may point at end of v1)
; v3 -> L1PT (or 0 if MMU on - not yet)

; Allocate the CAM

        MOV     a3, lr
        LDR     a2, =(AP_None * L2_APMult) + L2_C + L2_B
        LDR     a1, =CAM
        BL      Init_MapInRAM

; Allocate workspace for the HAL

        LDR     lr, [sp, #2*4]                          ; recover pushed HAL header
        LDR     a2, =(AP_None * L2_APMult) + L2_C + L2_B
        LDR     lr, [lr, #HALDesc_WorkspaceSize]
        MOV     a3, lr, LSR #12                         ; round workspace up to whole
        MOV     a3, a3, LSL #12                         ; number of pages
        CMP     a3, lr
        ADDNE   a3, a3, #&1000
        STR     a3, [a1, #HAL_WsSize]
        LDR     a1, =HALWorkspace
        BL      Init_MapInRAM

        ASSERT  ZeroPage = 0
        LDR     a1, =HALWorkspace
        MOV     a2, #0
        LDR     a3, [a2, #HAL_WsSize]
        BL      memset

; Bootstrap time. We want to get the MMU on ASAP. We also don't want to have to
; clear up too much mess later. So what we'll do is map in the three fixed areas
; (L1PT, scratch space and page zero), the CAM, ourselves, and the HAL,
; then turn on the MMU. The CAM will be filled in once the MMU is on, by
; reverse-engineering the page tables?

        ; Map in page zero
        ADD     a1, v3, #DRAMOffset_PageZero - DRAMOffset_L1PT
        MOV     a2, #0
        LDR     a3, =(AP_Full * L2_APMult) + L2_C + L2_B
        MOV     a4, #16*1024
        BL      Init_MapIn

        ; Map in scratch space
        ADD     a1, v3, #DRAMOffset_ScratchSpace - DRAMOffset_L1PT
        MOV     a2, #ScratchSpace
        LDR     a3, =(AP_Full * L2_APMult) + L2_C + L2_B
        MOV     a4, #16*1024
        BL      Init_MapIn

        ; Map in L1PT
        MOV     a1, v3
        LDR     a2, =L1PT
        LDR     a3, =(AP_None * L2_APMult)
        MOV     a4, #16*1024
        BL      Init_MapIn

        ; Map in L1PT again in PhysicalAccess (see below)
        MOV     a1, v3, LSR #20
        MOV     a1, a1, LSL #20                 ; megabyte containing L1PT
        LDR     a2, =PhysicalAccess
        LDR     a3, =(AP_None * L2_APMult)
        MOV     a4, #1024*1024
        BL      Init_MapIn

        ; Examine HAL and RISC OS locations
        LDMFD   sp, {v4,v5,v6}                  ; v4 = flags, v5 = RO desc, v6 = HAL desc
        LDR     lr, [v6, #HALDesc_Size]
        LDR     v7, [v6, #HALDesc_Start]
        ADD     v6, v6, v7                      ; (v6,v8)=(start,end) of HAL
        ADD     v8, v6, lr

        LDR     v7, [v5, #ImgDesc_Size]
        ADD     v7, v5, v7                      ; (v5,v7)=(start,end) of RISC OS

        TEQ     v8, v5                          ; check contiguity (as in a ROM image)
        BNE     %FT70

        ; HAL and RISC OS are contiguous. Yum.
        MOV     a1, v6
        LDR     a2, =RISCOS_Header
        SUB     a2, a2, lr

        SUB     ip, a2, a1                      ; change physical addresses passed in
        LDMIB   sp, {a3, a4}                    ; into logical addresses
        ADD     a3, a3, ip
        ADD     a4, a4, ip
        STMIB   sp, {a3, a4}

        MOV     a3, #(AP_ROM * L2_APMult) + L2_C + L2_B
        SUB     a4, v7, v6
        BL      Init_MapIn
        B       %FT75

70
        ; HAL is separate. Map it in at the start of IO space
        LDR     a2, =IO
        MOV     a1, v6
        SUB     ip, a2, a1                      ; change physical address passed in
        LDR     a3, [sp, #8]                    ; into logical address
        ADD     a3, a3, ip
        STR     a3, [sp, #8]
        SUB     a4, v8, v6
        MOV     a3, #(AP_ROM * L2_APMult) + L2_C + L2_B
        BL      Init_MapIn

        ; And now map in RISC OS
        LDR     a2, =RISCOS_Header              ; Hmm - what if position independent?
        MOV     a1, v5
        SUB     ip, a2, a1                      ; change physical address passed in
        LDR     a3, [sp, #4]                    ; into logical address
        ADD     a3, a3, ip
        STR     a3, [sp, #4]
        SUB     a4, v7, v5
        MOV     a3, #(AP_ROM * L2_APMult) + L2_C + L2_B
        BL      Init_MapIn
75
        ; We've now allocated all the pages we're going to before the MMU comes on.
        ; Note the end address (for RAM clear)
        ADD     a1, v3, #DRAMOffset_PageZero - DRAMOffset_L1PT
        STR     v2, [a1, #InitUsedEnd]

        ; Fill in some initial processor vectors. These will be used during ARM
        ; analysis, once the MMU is on. We do it here before the data cache is
        ; activated to save any IMB issues.
        ADRL    a2, InitProcVecs
        ADD     a3, a2, #InitProcVecsEnd - InitProcVecs
76      LDR     a4, [a2], #4
        CMP     a2, a3
        STR     a4, [a1], #4
        BLO     %BT76


; The time has come to activate the MMU. Steady now... Due to unpredictability of MMU
; activation, need to ensure that mapped and unmapped addresses are equivalent. To
; do this, we temporarily make the section containing virtual address MMUon_instr map
; to the same physical address. In case the code crosses a section boundary, do the
; next section as well.
;
; Also note, no RAM access until we've finished the operation, as we don't know it's
; available, and we might lose it due to lack of cache cleaning.
;
        ARM_MMU_transbase v3                    ; Always useful to tell it where L1PT is...

        MOV     a1, #4_0000000000000001         ; Domain 0 client, no access to other domains
        ARM_MMU_domain a1

        ADR     a1, MMUon_instr
        MOV     a1, a1, LSR #20                 ; a1 = megabyte number (stays there till end)
        ADD     lr, v3, a1, LSL #2              ; lr -> L1PT entry
        LDMIA   lr, {a2, a3}                    ; remember old mappings
  [ ARM6support
        LDR     ip, =(AP_None * L1_APMult) + L1_U + L1_Section
  |
        LDR     ip, =(AP_ROM * L1_APMult) + L1_U + L1_Section
  ]
        ORR     a4, ip, a1, LSL #20             ; not cacheable, as we don't want
        ADD     v4, a4, #1024*1024              ; to fill the cache with rubbish
        STMIA   lr, {a4, v4}

        MSR     CPSR_c, #F32_bit+I32_bit+FIQ32_mode ; Recover the soft copy of the CR
  [ CacheOff
        ORR     v5, v5, #MMUC_M                 ; MMU on
        ORR     v5, v5, #MMUC_R                 ; ROM mode enable
  |
        ORR     v5, v5, #MMUC_W+MMUC_C+MMUC_M   ; Write buffer, data cache, MMU on
        ORR     v5, v5, #MMUC_R+MMUC_Z          ; ROM mode enable, branch predict enable
  ]
MMUon_instr
        ARM_write_control v5
        MSR     CPSR_c, #F32_bit+I32_bit+SVC32_mode

; MMU now on. Need to jump to logical copy of ourselves. Complication arises if our
; physical address overlaps our logical address - in that case we need to map
; in another disjoint copy of ourselves and branch to that first, then restore the
; original two sections.
        ADRL    a4, RISCOS_Header
        LDR     ip, =RISCOS_Header
        SUB     ip, ip, a4
        ADR     a4, MMUon_instr
        MOV     a4, a4, LSR #20
        MOV     a4, a4, LSL #20                 ; a4 = base of scrambled region
        ADD     v4, a4, #2*1024*1024            ; v4 = top of scrambled region
        SUB     v4, v4, #1                      ;      (inclusive, in case wrapped to 0)
        ADR     v5, MMUon_resume
        ADD     v5, v5, ip                      ; v5 = virtual address of MMUon_resume
        CMP     v5, a4
        BLO     MMUon_nooverlap
        CMP     v5, v4
        BHI     MMUon_nooverlap

        ASSERT  ROM > 3*1024*1024
; Oh dear. We know the ROM lives high up, so we'll mangle 00100000-002FFFFF.
; But as we're overlapping the ROM, we know we're not overlapping the page tables.
        LDR     lr, =L1PT                       ; accessing the L1PT virtually now
  [ ARM6support
        LDR     ip, =(AP_None * L1_APMult) + L1_U + L1_Section
  |
        LDR     ip, =(AP_ROM * L1_APMult) + L1_U + L1_Section
  ]
        ORR     v6, a4, ip
        ADD     ip, v6, #1024*1024
        LDMIB   lr, {v7, v8}                    ; sections 1 and 2
        STMIB   lr, {v6, ip}
        RSB     ip, a4, #&00100000
        ADD     pc, pc, ip
        NOP
MMUon_overlapresume                             ; now executing from 00100000
        ADD     ip, lr, a4, LSR #18
        STMIA   ip, {a2, a3}                    ; restore original set of mappings
        BL      Init_PageTablesChanged

        MOV     a2, v7                          ; arrange for code below
        MOV     a3, v8                          ; to restore section 1+2 instead
        MOV     a1, #1

MMUon_nooverlap
        ADRL    lr, RISCOS_Header
        LDR     ip, =RISCOS_Header
        SUB     ip, ip, lr
        ADD     pc, pc, ip
        NOP
MMUon_resume
; What if the logical address of the page tables is at the physical address of the code?
; Then we have to access it via PhysicalAccess instead.
        LDR     lr, =L1PT
        CMP     lr, a4
        BLO     MMUon_nol1ptoverlap
        CMP     lr, v4
        BHI     MMUon_nol1ptoverlap
; PhysicalAccess points to the megabyte containing the L1PT. Find the L1PT within it.
        LDR     lr, =PhysicalAccess
        MOV     v6, v3, LSL #12
        ORR     lr, lr, v6, LSR #12
MMUon_nol1ptoverlap
        ADD     lr, lr, a1, LSL #2
        STMIA   lr, {a2, a3}
        BL      Init_PageTablesChanged

; The MMU is now on. Wahey. Let's get allocating.

        LDR     sp, =ScratchSpace + ScratchSpaceSize - 4*3 ; 3 items already on stack :)

        ADD     lr, v3, #DRAMOffset_PageZero-DRAMOffset_L1PT   ; lr = PhysAddr of zero page
        SUB     v1, v1, lr
        ADD     v1, v1, a1                              ; turn v1 from LogAddr to PhysAddr

        MOV     v3, #0                                  ; "MMU is on" signal

        BL      ARM_Analyse

        MOV     a1, #L1_Fault
        BL      RISCOS_ReleasePhysicalAddress

        BL      ConstructCAMfromPageTables

        LDR     a1, =HALWorkspace


        BL      ClearPhysRAM

        BKPT    2

        LTORG

; int PhysAddrToPageNo(void *addr)
;
; Converts a physical address to the page number of the page containing it.
; Returns -1 if address is not in RAM.

PhysAddrToPageNo
        MOV     a4, #0
        LDR     ip, =ZeroPage + PhysRamTable
10      LDMIA   ip!, {a2, a3}                   ; get phys addr, size
        TEQ     a3, #0                          ; end of list? (size=0)
        BEQ     %FT90                           ;   then it ain't RAM
        SUB     a2, a1, a2                      ; a2 = amount into this bank
        CMP     a2, a3                          ; if more than size
        ADDHS   a4, a4, a3                      ;   increase counter by size of bank
        BHS     %BT10                           ;   and move to next
        ADD     a4, a4, a2                      ; add offset to counter
        MOV     a1, a4, LSR #12                 ; convert counter to a page number
        MOV     pc, lr

90      MOV     a1, #-1
        MOV     pc, lr

LogToPhys
        Push    "lr"
        LDR     ip, =L1PT
        MOV     lr, a1, LSR #20
        LDR     lr, [ip, lr, LSL #2]
        TST     lr, #1
        BEQ     %FT80
        LDR     ip, =L2PT
        MOV     lr, a1, LSR #12
        LDR     lr, [ip, lr, LSL #2]
        TST     lr, #1
        MOV     lr, lr, LSR #12
        MOV     lr, lr, LSL #12


80

; A routine to construct the soft CAM from the page tables. This is used
; after a soft reset, and also on a hard reset as it's an easy way of
; clearing up after the recursive page table allocaton.

        ROUT
ConstructCAMfromPageTables
        Push    "v1-v8, lr"
        MOV     a1, #ZeroPage
        LDR     a2, [a1, #MaxCamEntry]
        LDR     v1, =CAM                        ; v1 -> CAM (for whole routine)
        ADD     a2, a2, #1
        ADD     a2, v1, a2, LSL #3

        LDR     a3, =DuffEntry                  ; Clear the whole CAM, from
        MOV     a4, #AP_Duff                    ; the top down.
10      STMDB   a2!, {a3, a4}
        CMP     a2, v1
        BHI     %BT10

        MOV     v2, #0                          ; v2 = logical address
        LDR     v3, =L1PT                       ; v3 -> L1PT (not used much)
        LDR     v4, =L2PT                       ; v4 -> L2PT

30      LDR     v5, [v3, v2, LSR #18]           ; lr = first level descriptor
        ASSERT  L1_Fault = 0
        AND     a1, v5, #2_11                   ; move to next section if not
        TEQ     a1, #L1_Page                    ; a page table (we ignore section maps
        BEQ     %FT40                           ; and we don't do fine tables)
        ADDS    v2, v2, #&00100000
        BCC     %BT30
        Pull    "v1-v8, pc"

40      LDR     v5, [v4, v2, LSR #10]           ; lr = second level descriptor
        ASSERT  L2_Fault = 0
        ANDS    v6, v5, #2_11                   ; move to next page if fault
        TEQNE   v6, #L2_TinyPage                ; or tiny page (we don't do tiny pages)
        BEQ     %FT80

        MOV     a1, v5, LSR #12
        MOV     a1, a1, LSL #12                 ; a1 = address (flags stripped out),,,
        TEQ     v6, #L2_LargePage
        BICEQ   a1, a1, #&0000F000              ; large pages get bits 12-15
        ANDEQ   lr, v2, #&0000F000              ; from the virtual address
        ORREQ   a1, a1, lr
        MOV     v5, a1

        BL      PhysAddrToPageNo

        ADD     a2, v1, a1, LSL #3              ; a2 -> CAM entry

        AND     a1, lr, #&30                    ; a1 = access permission
        MOV     a1, a1, LSR #4
        ; ARM access goes 0 => all R/O, 1 => user none, 2 => user R/O, 3  => user R/W
        ; PPL access goes 0 => user R/W, 1 => user R/O, 2 => user none, (and let's say 3 all R/O)
        RSB     v6, a1, #3                      ; v4 = PPL access
        TST     lr, #L2_C
        ORREQ   v6, v6, #DynAreaFlags_NotCacheable
        TST     lr, #L2_B
        ORREQ   v6, v6, #DynAreaFlags_NotBufferable
        ORR     v6, v6, #PageFlags_Unavailable               ; ???? pages from scratch to cam only?
        STMIA   a2, {v2, v6}                    ; store logical address, PPL

80      ADDS    v2, v2, #&00001000
        TST     v2, #&000FF000
        BNE     %BT40
        BCC     %BT30

        Pull    "v1-v8, pc"



; Allocate a physical page from DRAM
;
; On entry:
;    v1 -> current entry in PhysRamTable
;    v2 -> end of last used physical page
; On exit:
;    a1 -> next free page
;    v1, v2 updated
;    ip corrupt (a2-a4 preserved)
;
; No out of memory check...

Init_ClaimPhysicalPage
        Push    "lr"
        MOV     a1, v2
        LDMIA   v1, {ip, lr}
        ADD     ip, ip, lr                      ; ip = end of this bank
        CMP     v2, ip                          ; advance v2 to next bank if
        LDRHS   a1, [v1, #8]!                   ; this bank is fully used
        ADD     v2, a1, #4096
        Pull    "pc"

; Allocate and map in some RAM.
;
; On entry:
;    a1 = logical address
;    a2 = access permissions
;    a3 = length
;    v1 -> current entry in PhysRamTable
;    v2 = next physical address
;    v3 -> L1PT
;
; On exit:
;    a1 -> physical address of start of RAM (deduce the rest from PhysRamTable)
;
; No out of memory check...
Init_MapInRAM ROUT
        Push    "v4-v8,lr"
        MOV     v8, #-1
        MOV     v5, a3                          ; v5 = amount of memory required
        MOV     v6, a1                          ; v6 = logical address
        MOV     v7, a2                          ; v7 = access permissions
10      LDMIA   v1, {ip, v4}                    ; ip = addr of bank, v4 = len
        SUB     ip, v2, ip                      ; ip = amount of bank used
        SUBS    v4, v4, ip                      ; v4 = amount of bank left
        LDREQ   v2, [v1, #8]!                   ; move to next bank if 0 left
        BEQ     %BT10

        CMP     v8, #-1                         ; is this the first bank?
        MOVEQ   v8, v2                          ; remember it

        CMP     v4, v5                          ; sufficient in this bank?
        MOVHS   a4, v5
        MOVLO   a4, v4                          ; a4 = amount to take

        MOV     a1, v2                          ; set up parameters for MapIn call
        MOV     a2, v6                          ; then move globals (in case MapIn
        MOV     a3, v7                          ; needs to allocate for L2PT)
        ADD     v2, v2, a4                      ; advance physaddr
        SUB     v5, v5, a4                      ; decrease wanted
        ADD     v6, v6, a4                      ; advance address pointer
        BL      Init_MapIn                      ; map in the RAM
        TEQ     v5, #0                          ; more memory still required?
        BNE     %BT10

        MOV     a1, v8
        Pull    "v4-v8,pc"

; Map a range of physical addresses to a range of logical addresses.
;
; On entry:
;    a1 = physical address
;    a2 = logical address
;    a3 = access permissions + C + B bits (bits 11-2 of descriptor)
;           (also set bit 1 as a request to make non-updateable -
;            only obeyed if section mapped)
;    a4 = area size
;    v1 -> current entry in PhysRamTable
;    v2 = last used physical address
;    v3 -> L1PT (or 0 if MMU on)

Init_MapIn ROUT
        Push    "lr"
        ORR     lr, a1, a2                      ; OR together, physaddr, logaddr
        ORR     lr, lr, a4                      ; and size.
        MOVS    ip, lr, LSL #12                 ; If all bottom 20 bits 0
        BEQ     Init_MapIn_Sections             ; it's section mapped

        MOVS    ip, lr, LSL #16                 ; If bottom 16 bits 0
        ORREQ   a3, a3, #L2_LargePage           ; then large pages (64K)
        ORRNE   a3, a3, #L2_SmallPage           ; else small pages (4K)

        Push    "v4-v7"
        MOV     v4, a1                          ; v4 = physaddr
        MOV     v5, a2                          ; v5 = logaddr
        MOV     v6, a3                          ; v6 = access permissions
        MOV     v7, a4                          ; v7 = area size

20      MOV     a1, v4
        MOV     a2, v5
        MOV     a3, v6
        BL      Init_MapInPage                  ; Loop through mapping in each
        ADD     v4, v4, #4096                   ; page in turn
        ADD     v5, v5, #4096
        SUBS    v7, v7, #4096
        BNE     %BT20
        Pull    "v4-v7,pc"

Init_MapIn_Sections
        MOVS    ip, v3                          ; is MMU on?
        LDREQ   ip, =L1PT                       ; then use virtual address
        BIC     a3, a3, #4_033300               ; Clear out unwanted permission indicators
  [ ARM6support
        ASSERT  AP_ROM = 0
        TST     a3, #4_300000                   ; If ROM permission
        ARM_6   lr,EQ                           ;   and ARM 6
        ORREQ   a3, a3, #AP_Read * L1_APMult    ;     then make it Read permission, non-updateable
        ORRNE   a3, a3, #L1_U
        ORR     a3, a3, #L1_Section             ; Add section indicator to permission
  |
        ORR     a3, a3, #L1_U+L1_Section        ; Add section + U indicators to permission
  ]
        ORR     a1, a1, a3                      ; Merge with physical address
        ADD     a2, ip, a2, LSR #18             ; a2 -> L1PT entry
70      STR     a1, [a2], #4                    ; And store in L1PT
        ADD     a1, a1, #1024*1024              ; Advance one megabyte
        SUBS    a4, a4, #1024*1024              ; and loop
        BNE     %BT70
        Pull    "pc"


; Map a logical page to a physical page, allocating L2PT as necessary.
;
; On entry:
;    a1 = physical address
;    a2 = logical address
;    a3 = access permissions + C + B bits + size (all of bits 11-0 of 2nd level descriptor)
;    v1 -> current entry in PhysRamTable
;    v2 = last used physical address
;    v3 -> L1PT (or 0 if MMU on)
; On exit:
;    a1 = logical address
;    a2-a4, ip corrupt
;    v1, v2 updated

Init_MapInPage
        Push    "v4-v6, lr"
        MOV     v4, a1                          ; v4 = physical address
        MOV     v5, a2                          ; v5 = logical address
        MOV     v6, a3                          ; v6 = access permissions
        MOV     a1, v5
        MOV     a2, #4096
        BL      AllocateL2PT
        TEQ     v3, #0                          ; if MMU on, access L2PT virtually...
        LDREQ   a1, =L2PT                       ; a1 -> L2PT virtual address
        MOVEQ   ip, v5                          ; index using whole address
        BEQ     %FT40
        MOV     ip, v5, LSR #20
        LDR     a1, [v3, ip, LSL #2]            ; a1 = level one descriptor
        MOV     a1, a1, LSR #10
        MOV     a1, a1, LSL #10                 ; a1 -> L2PT tables for this section
        AND     ip, v5, #&000FF000              ; extract L2 table index bits
40      ORR     lr, v4, v6                      ; lr = value for L2PT entry
  [ ARM6support
        ASSERT  AP_ROM = 0
        TST     lr, #4_333300                   ; if ROM permission
        ARM_6   a2,EQ                           ;   and ARM 6
        ORREQ   lr, lr, #AP_Read * L1_APMult    ;     then make it Read permission, non-updateable
  ]
        ASSERT  L2_LargePage = 2_01 :LAND: L2_SmallPage = 2_10 :LAND: L2_TinyPage = 2_11
        MOVS    a2, v6, LSL #30                 ; PL if large page (or fault...)
        BICPL   lr, lr, #&0000F000              ; these bits must be clear for large page
        STR     lr, [a1, ip, LSR #10]           ; update L2PT entry
        MOV     a1, v5
        Pull    "v4-v6, pc"



; On entry:
;    a1 = virtual address L2PT required for
;    a2 = number of bytes of virtual space
;    a3 = access permissions that will be placed in L2PT (bits 11-0 of 2nd level descriptor)
;    v1 -> current entry in PhysRamTable
;    v2 = last used physical address
;    v3 -> L1PT (or 0 if MMU on)
; On exit
;    a1-a4,ip corrupt
;    v1, v2 updated
AllocateL2PT
        Push    "a3,v4-v8,lr"
        MOV     v8, a1, LSR #20                 ; round base address down to 1M
        ADD     lr, a1, a2
        MOV     v7, lr, LSR #20
        TEQ     lr, v7, LSL #20
        ADDNE   v7, v7, #1                      ; round end address up to 1M

        MOVS    v6, v3
        LDREQ   v6, =L1PT                       ; v6->L1PT (whole routine)

05      LDR     v5, [v6, v8, LSL #2]            ; L1PT contains 1 word per M
        TEQ     v5, #0                          ; if non-zero, the L2PT has
                                                ; already been allocated
 [ ARM6support
        BEQ     %FT10

        TST     v5, #L1_U                       ; if section is already updateable
        BNE     %FT40                           ; leave it.
        LDR     a3, [sp, #0]
        TST     a3, #4_333300                   ; if they want anything other than
        ORRNE   v5, v5, #L1_U                   ; ROM access, make the section
        STRNE   v5, [v6, v8, LSL #2]            ; updateable.
        B       %FT40
10
 |
        BNE     %FT40
 ]

        BIC     lr, v8, #3                      ; round down to 4M - each page
        ADD     lr, v6, lr, LSL #2              ; of L2PT maps to 4 sections
        LDMIA   lr, {a3,a4,v5,ip}               ; check if any are page mapped
        ASSERT  L1_Fault = 2_00 :LAND: L1_Page = 2_01 :LAND: L1_Section = 2_10
        TST     a3, #1
        TSTEQ   a4, #1
        TSTEQ   v5, #1
        TSTEQ   ip, #1
        BEQ     %FT20                           ; nothing page mapped - claim a page

        TST     a4, #1                          ; at least one of the sections is page mapped
        SUBNE   a3, a4, #1*1024                 ; find out where it's pointing to and
        TST     v5, #1                          ; derive the corresponding address for our
        SUBNE   a3, v5, #2*1024                 ; section
        TST     ip, #1
        SUBNE   a3, ip, #3*1024

        AND     lr, v8, #3
        ORR     a3, a3, lr, LSL #10
  [ ARM6support
        ASSERT  AP_ROM = 0
        ARM_6   lr                              ; if ARM 6
        BNE     %FT15
        LDR     lr, [sp, #0]                    ; do they want ROM access?
        TST     lr, #4_333300
        BICEQ   a3, a3, #L1_U                   ; set the updateable bit accordingly
        ORRNE   a3, a3, #L1_U
15
  ]
        STR     a3, [v6, v8, LSL #2]            ; fill in the L1PT entry
        B       %FT40                           ; no more to do

20      BL      Init_ClaimPhysicalPage          ; Claim a page to put L2PT in
        MOV     v4, a1

  [ ARM6support
        ASSERT  AP_ROM = 0
        ARM_6   lr                              ; if ARM 6
        LDREQ   lr, [sp, #0]                    ; do they want ROM access?
        TSTEQ   lr, #4_333300
        ORREQ   a3, a1, #L1_Page                ; set the updateable bit accordingly
        ORRNE   a3, a1, #L1_Page + L1_U
  |
        ORR     a3, a1, #L1_Page + L1_U
  ]
        AND     lr, v8, #3
        ORR     v5, a3, lr, LSL #10
        STR     a3, [v6, v8, LSL #2]

        TEQ     v3, #0                          ; MMU on?
        MOVEQ   a1, a1                          ; then point at logical address
        MOVNE   a1, v4                          ; else point at physical address
        MOV     a2, #0
        MOV     a3, #4*1024
        BL      memset                          ; zero out 4K of L2PT

        SUB     a1, v4, #4*1024                 ; Map in the L2PT page itself
        LDR     a2, =L2PT                       ; (will recurse...)
        ADD     a2, a2, v8, LSL #10
        BIC     a2, a2, #&C00
        LDR     a3, =(AP_None * L2_APMult) + L2_SmallPage
        BL      Init_MapInPage

40      ADD     v8, v8, #1                      ; go back until all
        CMP     v8, v7                          ; pages allocated
        BLO     %BT05

        Pull    "a3,v4-v8,pc"


; void *RISCOS_AccessPhysicalAddress(unsigned int flags, void *addr, void **oldp)
RISCOS_AccessPhysicalAddress
        LDR     ip, =L1PT + (PhysicalAccess:SHR:18)     ; ip -> L1PT entry
        LDR     a4, =(AP_None * L1_APMult) + L1_U + L1_Section
        AND     a1, a1, #L1_B                           ; user can ask for bufferable
        ORR     a1, a4, a1                              ; a1 = flags for 2nd level descriptor
        MOV     a4, a2, LSR #20                         ; rounded to section
        ORR     a1, a1, a4, LSL #20                     ; a1 = complete descriptor
        TEQ     a3, #0
        LDRNE   a4, [ip]                                ; read old value (if necessary)
        STR     a1, [ip]                                ; store new one
        STRNE   a4, [a3]                                ; put old one in [oldp]

        LDR     a3, =PhysicalAccess
        MOV     a1, a2, LSL #12                         ; take bottom 20 bits of address
        ORR     a1, a3, a1, LSR #12                     ; and make an offset within PhysicalAccess
50
;        LDR     pc, =ZeroPage + InvalidateTLBentry
        MOV     a4, a1
        MOV     a3, lr
        BL      Init_ARMarch
        MOV     a1, a4
        MCREQ   ARM_config_cp,0,a1,ARMv3_TLBpurge_reg,C0; for ARMv3 (FAR write for v4)
        MCRNE   ARM_config_cp,0,a1,ARMv4_TLB_reg,C7,1   ; for ARMv4 (ignored by v3)
        MOV     pc, a3

; void RISCOS_ReleasePhysicalAddress(void *old)
RISCOS_ReleasePhysicalAddress
        LDR     ip, =L1PT + (PhysicalAccess:SHR:18)     ; ip -> L1PT entry
        STR     a1, [ip]
        LDR     a1, =PhysicalAccess
        B       %BT50


; void Init_PageTablesChanged(void)
;
; A TLB+cache invalidation that works on all known ARMs. Invalidate all I+D TLB is the _only_ TLB
; op that works on ARM720T, ARM920T and SA110. Ditto invalidate all I+D cache.
;
; DOES NOT CLEAN THE DATA CACHE. This is a helpful simplification, but requires that don't use
; this routine after we've started using normal RAM.
;
Init_PageTablesChanged
        MOV     a3, lr
        BL      Init_ARMarch
        MOV     ip, #0
        MCREQ   ARM_config_cp,0,ip,ARMv3_TLBflush_reg,C0
        MCRNE   ARM_config_cp,0,ip,ARMv4_TLB_reg,C7
        MCR     ARM_config_cp,0,ip,ARMv4_cache_reg,C7           ; works on ARMv3
        MOV     pc, a3

ARM_Analyse
        Push    "v1,v2,v5,v6,lr"
        ARM_read_ID v1
        ARM_read_cachetype v2
        MOV     v6, #ZeroPage

        ADR     lr, KnownCPUTable
FindARMloop
        LDMIA   lr!, {a1, a2}                   ; See if it's a known ARM
        CMP     a1, #-1
        BEQ     %FT20
        AND     a2, v1, a2
        TEQ     a1, a2
        ADDNE   lr, lr, #8
        BNE     FindARMloop
        TEQ     v2, v1                          ; If we don't have cache attributes, read from table
        LDREQ   v2, [lr]

20      TEQ     v2, v1
        BEQ     %BT20                           ; Cache unknown: panic

        ASSERT  CT_Isize_pos = 0
        MOV     a1, v2
        ADD     a2, v6, #ICache_Info
        BL      EvaluateCache
        MOV     a1, v2, LSR #CT_Dsize_pos
        ADD     a2, v6, #DCache_Info
        BL      EvaluateCache

        MOV     v5, #0

        ; Test abort timing (base restored or base updated)
        MOV     a1, #&8000
        LDR     a2, [a1], #4                    ; Will abort - DAb handler will continue execution
        TEQ     a1, #&8000
        ORREQ   v5, v5, #CPUFlag_BaseRestored

        ; Check store of PC
30      STR     pc, [sp, #-4]!
        ADR     a2, %BT30 + 8
        LDR     a1, [sp], #4
        TEQ     a1, a2
        ORREQ   v5, v5, #CPUFlag_StorePCplus8

        ; Check whether 26-bit mode is available
        MSR     CPSR_c, #F32_bit+I32_bit+SVC26_mode
        MRS     a1, CPSR
        AND     a1, a1, #M32_bits
        TEQ     a1, #SVC26_mode
        ORRNE   v5, v5, #CPUFlag_No26bitMode
        MSREQ   CPSR_c, #F32_bit+I32_bit+SVC32_mode
        BNE     %FT35

        ; Do we get vector exceptions on read?
        MOV     a1, #0
        LDR     a1, [a1]                        ; If this aborts a1 will be left unchanged
        TEQ     a1, #0
        ORREQ   v5, v5, #CPUFlag_VectorReadException



35

30      Pull    "v1,v2,v6,pc"


; This routine works out the values LINELEN, ASSOCIATIVITY, NSETS and CACHE_SIZE defined in section
; B2.3.3 of the ARMv5 ARM.
EvaluateCache
        AND     a3, a1, #CT_assoc_mask+CT_M
        TEQ     a3, #(CT_assoc_0:SHL:CT_assoc_pos)+CT_M
        BEQ     %FT80
        MOV     ip, #1
        ASSERT  CT_len_pos = 0
        AND     a4, a1, #CT_len_mask
        ADD     a4, a4, #3
        MOV     a4, ip, LSL a4                  ; LineLen = 1 << (len+3)
        STRB    a4, [a2, #ICache_LineLen-ICache_Info]
        MOV     a3, #2
        TST     a1, #CT_M
        ADDNE   a3, a3, #1                      ; Multiplier = 2 + M
        AND     a4, a1, #CT_assoc_mask
        RSB     a4, ip, a4, LSR #CT_assoc_pos
        MOV     a4, a3, LSL a4                  ; Associativity = Multiplier << (assoc-1)
        STRB    a4, [a2, #ICache_Associativity-ICache_Info]
        AND     a4, a1, #CT_size_mask
        MOV     a4, a4, LSR #CT_size_pos
        MOV     a3, a3, LSL a4
        MOV     a3, a3, LSL #8                  ; Size = Multiplier << (size+8)
        STR     a3, [a2, #ICache_Size-ICache_Info]
        ADD     a4, a4, #6
        AND     a3, a1, #CT_assoc_mask
        SUB     a4, a4, a3, LSR #CT_assoc_pos
        AND     a3, a1, #CT_len_mask
        ASSERT  CT_len_pos = 0
        SUB     a4, a4, a3
        MOV     a4, ip, LSL a4                  ; NSets = 1 << (size + 6 - assoc - len)
        STR     a4, [a2, #ICache_NSets-ICache_Info]
        MOV     pc, lr


80      MOV     a1, #0
        STR     a1, [a2, #ICache_NSets-ICache_Info]
        STR     a1, [a2, #ICache_Size-ICache_Info]
        STRB    a1, [a2, #ICache_LineLen-ICache_Info]
        STRB    a1, [a2, #ICache_Associativity-ICache_Info]
        MOV     pc, lr


; Create a list of CPUs, 16 bytes per entry:
;    ID bits (1 word)
;    Test mask for ID (1 word)
;    Cache type register value (1 word)
;    Processor type (1 byte)
;    Architecture type (1 byte)
;    Reserved (2 bytes)
        GBLA    tempcpu

        MACRO
        CPUDesc $proc, $id, $mask, $arch, $type, $s, $dsz, $das, $dln, $isz, $ias, $iln
        LCLA    type
type    SETA    (CT_ctype_$type:SHL:CT_ctype_pos)+($s:SHL:CT_S_pos)
tempcpu CSzDesc $dsz, $das, $dln
type    SETA    type+(tempcpu:SHL:CT_Dsize_pos)
        [ :LNOT:($s=0 :LAND: "$isz"="")
tempcpu CSzDesc $isz, $ias, $iln
        ]
type    SETA    type+(tempcpu:SHL:CT_Isize_pos)
        ASSERT  ($id :AND: :NOT: $mask) = 0
        DCD     $id, $mask, type
        DCB     $proc, $arch, 0, 0
        MEND

        MACRO
$var    CSzDesc $sz, $as, $ln
$var    SETA    (CT_size_$sz:SHL:CT_size_pos)+(CT_assoc_$as:SHL:CT_assoc_pos)+(CT_len_$ln:SHL:CT_len_pos)
$var    SETA    $var+(CT_M_$sz:SHL:CT_M_pos)
        MEND


KnownCPUTable
;                                             /---Cache Type register fields---\
;                      ID reg     Mask       Arch    Type    S  Dsz Das Dln Isz Ias Iln
        CPUDesc ARM6,  &00000000, &0000F000, ARMv3,  WT,     0,  4K, 64, 4
        CPUDesc ARM7,  &00007000, &00FFFFF0, ARMv3,  WT,     0,  8K,  4, 8              ; ARM700
        CPUDesc ARM7,  &00007100, &00FFFFF0, ARMv3,  WT,     0,  8K,  4, 8              ; ARM710
        CPUDesc ARM7,  &00047100, &00FDFFF0, ARMv3,  WT,     0,  8K,  4, 4              ; ARM710a
        CPUDesc ARM7,  &00027100, &00FFFFF0, ARMv3,  WT,     0,  4K,  4, 4              ; other ARM7
        CPUDesc ARM7T, &00807000, &0080F000, ARMv4T, WT,     0,  8K,  4, 4
        CPUDesc SA110, &0001A100, &000FFFF0, ARMv4,  WB_Crd, 1, 16K, 32, 8, 16K, 32, 8
        DCD     -1




HAL_Write0
        Push    "v1, lr"
        MOV     v1, a1
        LDRB    a1, [v1], #1
        TEQ     a1, #0





;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
;
;       ClearPhysRAM - Routine to clear "all" memory
;
; While this routine is running, keyboard IRQs may happen. For this reason
; it avoids the base of logical RAM (hardware IRQ vector and workspace).
;
; We also have to avoid the page tables and the PhysRamTable.
; The latter is also used to tell us which areas of memory we should clear.

; We don't have to worry about trampling on the ROM image as it's
; already been excluded from PhysRamTable.

; This routine must work in 32-bit mode.

; in:   r7 = memory speed
;       r8 = page size
;       r9 = MEMC control register
;       r13 = total RAM size
;
; None of the above are actually used by this routine
;
; out:  r7-r9, r13 preserved
;

ClearPhysRAM ROUT

      [ EmulatorSupport
       ; ARM_on_emulator r0
       ; BEQ     CPR_skipped
      ]

;now let us do the clear
        MOV     r0,#ZeroPage+InitClearRamWs             ;we can preserve r7-r9,r13 at logical address 52..67
        STMIA   r0,{r7-r9,r13,lr}
        MOV     r8, #0
        MOV     r9, #0
        MOV     r13,#0
        LDR     r7, =PhysRamTable                      ; point to 5 lots of (physaddr,size)
        ADR     r6, RamSkipTable
        ADD     r4, r7, #PhysRamTableEnd-PhysRamTable  ; r4 -> end of table
10
        LDR     r5, [r6], #4                            ; load first skip offset

        LDMIA   r7, {r10, r11}                          ; load next address, size
        TEQ     r11, #0
        BEQ     %FT50
15
        ASSERT  ZeroPage=0
        ADD     r11, r10, r11                           ; r11 = end address

        ; Need to check for RAM we've already used
        LDR     r0, [r8, #InitUsedStart]                ; check for intersection with used space
        CMP     r0, r10
        BLO     %FT18
        CMP     r0, r11
        BHS     %FT18

        BKPT    0

        LDR     r1, [r8, #InitUsedEnd]
        TEQ     r0, r10                                 ; if at start of block, easy
        BEQ     %FT17                                   ; just move start pointer forwards

        MOV     r11, r0                                 ; else set end pointer to start address
        B       %FT18                                   ; next time round, will be at start address


16      LDR     r1, [r8, #InitUsedEnd]
17      CMP     r1, r10                                 ; advance until we find block with end pointer in
        CMPHS   r11, r1                                 ; intersection if r10 <= r1 <= r11
        ADDLS   r7, r7, #8                              ; get next block if r1 = r11 or r1 outside [r10,r11]
        LDRLS   r5, [r6], #4
        LDMLSIA r7, {r10, r11}                          ; (if r1 = r11, will just move straight to next block)
        ADDLS   r11, r10, r11
        BLO     %BT17                                   ; check next block if r1 outside [r10,r11]
        MOVHI   r10, r1                                 ; if r11 > r1, advance r10 to r1 (do partial block)


18      MOV     r0, #L1_B                               ; map in the appropriate megabyte,
        MOV     r1, r10                                 ; making it bufferable
        MOV     r2, #0
        BL      RISCOS_AccessPhysicalAddress
        SUB     lr, r0, r10                             ; lr = offset from phys to log
        MOV     r10, r0
        ADD     r11, r11, lr

        MOV     r0, r10, LSR #20
        TEQ     r0, r11, LSR #20                        ; if end in different megabyte to start
        MOVNE   r11, r0, LSL #20                        ; then stop at end of megabyte
        ADDNE   r11, r11, #&00100000

        MOV     r0, #0
        MOV     r1, #0
        MOV     r2, #0
        MOV     r3, #0
        MOV     r12, #0

19      ADD     r5, r5, r10

20      TEQ     r10, r11                                ; *this* is the speed critical bit - executed
        TEQNE   r10, r5                                 ; 32768 times per outer loop
        STMNEIA r10!, {r0-r3,r8,r9,r12,r13}
        BNE     %BT20

        TEQ     r10, r11
        BEQ     %FT30

        LDR     r5, [r6], #4                            ; load skip amount
        ADD     r10, r10, r5                            ; and skip it
        LDR     r5, [r6], #4                            ; load next skip offset (NB relative to end of last skip)
        B       %BT19

30      LDMIA   r7, {r0, r1}
        SUB     r11, r11, lr                            ; turn r11 back into physical address
        SUB     r5, r5, r0                              ; r5 back into offset
        ADD     r0, r0, r1                              ; r0 = physical end address of block
        TEQ     r0, r11                                 ; did we reach the real end?
        MOVNE   r10, r11                                ; if not, carry on from where we stopped
        SUBNE   r11, r0, r11                            ; (this also deals with avoiding InitUsed area)
        BNE     %BT15

40      ADD     r7, r7, #8
        TEQ     r7, r4                                  ; have we done all areas?
        BNE     %BT10

50      MOV     r1, #0
        MOV     r4, #ZeroPage+InitClearRamWs
        LDMIA   r4, {r7-r9,r13,r14}                     ;restore

        MOV     r4, #ZeroPage+InitUsedStart             ;clear our speed up workspace
        ASSERT  InitUsedStart < InitUsedEnd
        ASSERT  InitUsedEnd < InitClearRamWs
        GBLA    finalclear
finalclear      SETA InitUsedStart
        WHILE   finalclear < InitWsEnd
        STMIA   r4!,{r1-r3,r12}
finalclear      SETA finalclear + 16
        WEND

CPR_skipped

        LDR     r0, =OsbyteVars + :INDEX: LastBREAK

        MOV     r1, #&80
        STRB    r1, [r0]                                ; flag the fact that RAM cleared

        MSR     CPSR_c, #F32_bit + FIQ32_mode           ; retrieve the MMU control register
        MOV     r0, #ZeroPage                           ; soft copy
        STR     v5, [r0, #MMUControlSoftCopy]
        MSR     CPSR_c, #F32_bit + SVC32_mode

        MOV     pc, lr

        LTORG

        GBLA    lastaddr
lastaddr SETA   0
        GBLA    lastregion
lastregion SETA 0

        MACRO
        MakeSkipTable $region, $addr, $size
 [ ($region)<>lastregion
        &       -1
lastaddr SETA   0
 ]
        &       ($addr)-lastaddr, $size
lastaddr SETA   ($addr)+($size)
lastregion SETA $region
        MEND

        MACRO
        EndSkipTables
        WHILE   lastregion < (PhysRamTableEnd-PhysRamTable)/8
        &       -1
lastregion SETA   lastregion +1
        WEND
        MEND


RamSkipTable
        MakeSkipTable   1, DRAMOffset_PageZero + 0, InitWsEnd
        MakeSkipTable   1, DRAMOffset_PageZero + SkippedTables, SkippedTablesEnd-SkippedTables
        EndSkipTables


        ROUT
memset
        TEQ     a3, #0                                  ; return immediately if zero bytes
        MOVEQ   pc, lr
        TST     a1, #3                                  ; check for unaligned start
        BNE     %FT80
05      SUBS    a3, a3, #32                             ; if at least 32, do 32 at a time
        BLO     %FT50

        Push    "v1-v5"
        AND     a2, a2, #&FF
        ORR     a2, a2, a2, LSL #8
        ORR     a2, a2, a2, LSL #16
        MOV     a4, a2
        MOV     v1, a2
        MOV     v2, a2
        MOV     v3, a2
        MOV     v4, a2
        MOV     v5, a2
        MOV     ip, a2
10      STMIA   a1!, {a2,a4,v1,v2,v3,v4,v5,ip}
        SUBS    a3, a3, #32
        BHS     %BT10
        Pull    "v1-v5"

50      ADDS    a3, a3, #32-4                           ; if at least 4, do 4 at a time
        BMI     %FT70
60      STR     a2, [a1], #4
        SUBS    a3, a3, #4
        BPL     %BT60

70      ADDS    a3, a3, #4
        MOVEQ   pc, lr

80      STRB    a2, [a1], #1                            ; byte at a time until finished or
        SUBS    a3, a3, #1                              ; aligned (if at end, will finish)
        MOVEQ   pc, lr
        TST     a1, #3
        BNE     %BT80
        B       %BT05


InitProcVecs
        BKPT    &C000                                   ; Reset
        BKPT    &C004                                   ; Undefined Instruction
        BKPT    &C008                                   ; SWI
        BKPT    &C00C                                   ; Prefetch Abort
        SUBS    pc, lr, #4                              ; ignore data aborts
        BKPT    &C014                                   ; Address Exception
        BKPT    &C018                                   ; IRQ
        BKPT    &C01C                                   ; FIQ
InitProcVec_FIQ
        DCD     0
InitProcVecsEnd

        END

@


1.1.2.2
log
@More HAL work. IOMD HAL work in progress. Lots of my own little build
scripts. Don't touch this.

Version 5.35, 4.79.2.2. Tagged as 'Kernel-5_35-4_79_2_2'
@
text
@d15 2
d18 1
a18 1
; Set sb up ready for CallHAL.
d20 8
a27 3
        AddressHAL
        LDR     sb, =ZeroPage
        LDR     sb, [sb, #HAL_Workspace]
d30 1
a30 1
; Calls the HAL. $rout is the routine. sb must have been set up by AddressHAL
d32 2
a33 1
        CallHAL $rout
d35 1
a35 1
        LDR     pc, [sb, #-(EntryNo_$rout+1) * 4]
d52 18
d78 1
d404 1
a404 1
        ADD     v2, a3, #DRAMOffset_LastFixed - DRAMOffset_L1PT
d424 7
d433 1
a433 3
        ADD     a4, v3, #DRAMOffset_PageZero - DRAMOffset_L1PT
        LDR     a3, [sp, #8]                            ; recover pushed HAL header
        LDR     a1, =HALWorkspace
d435 1
a435 3
        LDR     lr, [a3, #HALDesc_Workspace]            ; their workspace
        LDR     ip, [a3, #HALDesc_NumEntries]           ; plus 1 word per entry
        ADD     lr, lr, ip, LSL #2
d440 2
a441 3
        STR     a3, [a4, #HAL_WsSize]                   ; Make a note of allocated space
        ADD     ip, a1, ip, LSL #2                      ; Their workspace starts
        STR     ip, [a4, #HAL_Workspace]                ; after our table of entries
d444 6
d492 1
a492 1
        LDR     v7, [v5, #OSHdr_ImageSize]
d515 2
a516 2
        ; HAL is separate. (We should cope with larger images)
        LDR     a2, =ROM
a539 1
        STR     v1, [a1, #InitUsedBlock]
a666 5
; Store the logical address of the HAL descriptor
        LDR     a1, =ZeroPage
        LDR     a2, [sp, #8]
        STR     a2, [a1, #HAL_Descriptor]

d674 2
a675 1
        ASSERT  ZeroPage = 0
a676 14
        MOV     a2, #0
        LDR     a3, [a2, #HAL_WsSize]
        BL      memset

        LDR     a1, =IO
        MOV     a2, #ZeroPage
        STR     a1, [a2, #IOAllocPtr]

        BL      SetUpHALEntryTable

; Initialise the HAL
        LDR     a1, =RISCOS_Header
        AddressHAL
        CallHAL HAL_Init
a677 3
; Start timer zero, at 100 ticks per second
        MOV     a1, #0
        CallHAL HAL_TimerGranularity
a678 14
        MOV     a2, a1
        MOV     a1, #100
        BL      __rt_udiv

        MOV     a2, a1
        MOV     a1, #0
        CallHAL HAL_TimerSetPeriod

; Remember some stuff that's about to get zapped
        LDR     v8, =ZeroPage
        LDR     v5, [v8, #RAMLIMIT]
        LDR     v7, [v8, #MaxCamEntry]

; Clear the memory.
d681 1
a681 65
; Put it back
        STR     v5, [v8, #RAMLIMIT]
        STR     v7, [v8, #MaxCamEntry]


; Allocate the CAM
        LDR     a3, [v8, #SoftCamMapSize]
        LDR     a2, =(AP_None * L2_APMult) + L2_C + L2_B
        LDR     a1, =CAM
        BL      Init_MapInRAM

; Allocate the supervisor stack
        LDR     a1, =SVCStackAddress
        LDR     a2, =(AP_Read * L2_APMult) + L2_C + L2_B
        LDR     a3, =SVCStackSize
        BL      Init_MapInRAM

 [ HAL32
; Allocate the interrupt stack
        LDR     a1, =IRQStackAddress
        LDR     a2, =(AP_None * L2_APMult) + L2_C + L2_B
        LDR     a3, =IRQStackSize
        BL      Init_MapInRAM
 ]

; Allocate the abort stack
        LDR     a1, =ABTStackAddress
        LDR     a2, =(AP_None * L2_APMult) + L2_C + L2_B
        LDR     a3, =ABTStackSize
        BL      Init_MapInRAM

; Allocate the undefined stack
        LDR     a1, =UNDStackAddress
        LDR     a2, =(AP_None * L2_APMult) + L2_C + L2_B
        LDR     a3, =UNDStackSize
        BL      Init_MapInRAM

; Allocate the system heap
        LDR     a1, =SysHeapAddress
        LDR     a2, =(AP_Full * L2_APMult) + L2_C + L2_B
        LDR     a3, =32*1024
        BL      Init_MapInRAM

; Allocate the cursor/system/sound block
        LDR     a1, =CursorChunkAddress
        LDR     a2, =(AP_None * L2_APMult) + L2_C + L2_B
        LDR     a3, =32*1024
        BL      Init_MapInRAM


        MSR     CPSR_c, #F32_bit+I32_bit+IRQ32_mode
        LDR     sp, =IRQSTK
        MSR     CPSR_c, #F32_bit+I32_bit+ABT32_mode
        LDR     sp, =ABTSTK
        MSR     CPSR_c, #F32_bit+I32_bit+UND32_mode
        LDR     sp, =UNDSTK
        MSR     CPSR_c, #F32_bit+I32_bit+SVC2632
        LDR     sp, =SVCSTK

        BL      ConstructCAMfromPageTables

        MOV     a1, #4096
        STR     a1, [v8, #Page_Size]

        B       Continue_after_HALInit
d803 1
d808 1
d810 3
a812 3
        LDMIA   v1, {a2, a3}
        ADD     a2, a2, a3                      ; ip = end of this bank
        CMP     v2, a2                          ; advance v2 to next bank if
d815 1
a815 1
        MOV     pc, lr
d1058 2
a1059 2
        ORR     a3, a3, lr, LSL #10
        STR     a3, [v6, v8, LSL #2]            ; fill in the L1PT
d1061 6
a1066 5
        TEQ     v3, #0                          ; MMU off?
        MOVNE   a1, v4                          ; if so, zero out the L2PT
        MOVNE   a2, #0                          ; (if it's on then it will already
        MOVNE   a3, #4*1024                     ; be clear from ClearPhysRAM, and
        BLNE    memset                          ; it's not mapped in yet anyway)
d1068 1
a1068 1
        MOV     a1, v4                          ; Map in the L2PT page itself
a1161 4
        AND     a1, v2, #CT_ctype_mask
        MOV     a1, a1, LSR #CT_ctype_pos
        STRB    a1, [v6, #Cache_Type]

a1190 1
35      STR     v5, [v6, #ProcessorFlags]
d1192 5
a1196 1
30      Pull    "v1,v2,v5,v6,pc"
d1332 1
a1332 1
        STMIA   r0,{r4-r11,r13,lr}
d1356 1
a1356 1
        ;BKPT    0
d1425 2
a1426 2
        MOV     r0, #ZeroPage+InitClearRamWs
        LDMIA   r0, {r4-r11,r13,r14}                    ;restore
d1428 1
a1428 1
        MOV     r0, #ZeroPage+InitUsedStart             ;clear our speed up workspace
d1434 1
a1434 1
        STMIA   r0!,{r1-r3,r12}
d1485 42
a1538 55

RISCOS_MapInIO
        Entry   "v1-v4"
        MOV     a4, a3
        AND     a3, a1, #L2_C
        ORR     a3, a3, #AP_None * L2_APMult
        MOV     a1, a2
        Push    "a1,a3,a4"
        BL      FindIOVirtualAddress
        MOV     a2, a1
        MOV     v4, a1
        Pull    "a1,a3,a4"
        MOV     ip, #ZeroPage
        LDR     v1, [ip, #InitUsedBlock]
        LDR     v2, [ip, #InitUsedEnd]
        MOV     v3, #0
        BL      Init_MapIn
        MOV     a1, v4
        EXIT

; a1 = Physical address
; a3 = flags
; a4 = size
FindIOVirtualAddress
        MOV     a2, a4, LSR #12
        TEQ     a4, a2, LSL #12
        ADDNE   a2, a2, #1
        MOV     a4, a2, LSL #12                         ; a4 = size rounded up

        ORR     a2, a1, a4
        MOVS    ip, a2, LSL #12                         ; EQ if megabyte aligned

        MOV     ip, #ZeroPage
        LDR     a3, [ip, #IOAllocPtr]
        MOVEQ   a3, a3, LSR #20
        MOVEQ   a3, a3, LSL #20
        SUB     a1, a3, a4
        STR     a1, [ip, #IOAllocPtr]
        MOV     pc, lr


SetUpHALEntryTable ROUT
        LDR     a1, =ZeroPage
        LDR     a2, [a1, #HAL_Descriptor]
        LDR     a3, [a1, #HAL_Workspace]
        LDR     a4, [a2, #HALDesc_Entries]
        LDR     ip, [a2, #HALDesc_NumEntries]
        ADD     a4, a2, a4                              ; a4 -> entry table
        MOV     a2, a4                                  ; a2 -> entry table (increments)
10      LDR     a1, [a2], #4
        SUBS    ip, ip, #1                              ; decrement counter
        ADD     a1, a4, a1                              ; convert offset to absolute
        STR     a1, [a3, #-4]!                          ; store backwards below HAL workspace
        BHI     %BT10
        MOV     pc, lr
@


1.1.2.3
log
@More HAL work. IOMD HAL fleshed out somewhat - system gets most of the way
through initialisation.

Version 5.35, 4.79.2.5. Tagged as 'Kernel-5_35-4_79_2_5'
@
text
@d15 28
a42 2
        GBLL    MajorL2PThack
MajorL2PThack SETL {TRUE}
a283 3
 [ MajorL2PThack
        SUBEQS  v6, v6, #5*1024*1024
 |
a284 1
 ]
a449 9
  [ MajorL2PThack
;        BKPT    0 ; listing on
DoTheL2PThack
        MOV     a1, #0                          ; allocate ALL 4M of L2PT now. Wahey.
        MOV     a2, #&FFFFFFFF
        LDR     a3, =(AP_Full * L2_APMult) + L2_C + L2_B
        BL      AllocateL2PT
  ]

a507 4
        ; Note the HAL flags passed in.
        LDR     a2, [sp, #0]
        STR     a2, [a1, #InitHALFlags]

a730 3
        LDR     a1, =ZeroPage
        STR     v2, [a1, #InitUsedEnd]

a740 3
        LDR     ip, =CAM
        STR     ip, [a1, #CamEntriesPointer]

d851 1
a851 1
80      ADD     v2, v2, #&00001000
d854 1
a854 2
        TEQ     v2, #0                          ; yuck (could use C from ADDS but TST corrupts C
        BNE     %BT30                           ; because of big constant)
d1490 10
@


1.1.2.4
log
@  Added HAL NVRAM support
Detail:
  Added the HAL NVRAM entries.
  Modified i2cutils to use the HAL entries for NVRAM and behave sensibly if the HAL reports that there is no NVRAM, in which case there must be a forced reset_cmos call so that the cache gets set up sensibly.
Admin:
  Tested under the RPC emulator and appears to be working correctly, although some calls to IIC are still being made in the no nvram case.

Version 5.35, 4.79.2.8. Tagged as 'Kernel-5_35-4_79_2_8'
@
text
@d1240 1
a1240 1
    	[ 0=1
a1254 1
    	]
d1256 2
a1257 1
        Pull    "v1,v2,v5,v6,pc"
@


1.1.2.5
log
@More L7200 HAL work
@
text
@a1122 3
  [ EmulatorSupport
        ARM_on_emulator a1,NE                   ; and not on the emulator?
  ]
d1387 2
a1388 2
        ARM_on_emulator r0
        BEQ     CPR_skipped
d1599 1
a1599 3
        TEQ     a1, #0
        ADREQ   a1, NullHALEntry
        ADDNE   a1, a4, a1                              ; convert offset to absolute
a1602 65

NullHALEntry
        MOV     pc, lr

; Can freely corrupt r10-r12 (v7,v8,ip).
HardwareSWI
        AND     ip, v5, #&FF
        CMP     ip, #1
        BHI     HardwareBadReason
        BEQ     HardwareLookup

HardwareCall
        Push    "v1-v4,sb,lr"
        ADD     v8, sb, #1                              ; v8 = entry no + 1
        MOV     ip, #0
        LDR     v7, [ip, #HAL_Descriptor]
        AddressHAL ip                                   ; sb set up
        LDR     v7, [v7, #HALDesc_Entries]              ; v7 = number of entries
        CMP     v8, v7                                  ; entryno + 1 must be <= number of entries
        BHI     HardwareBadEntry
        LDR     ip, [sb, -v8, LSL #2]
        ADR     v7, NullHALEntry
        TEQ     ip, v7
        BEQ     HardwareBadEntry
        MOV     lr, pc
        MOV     pc, ip
        ADD     sp, sp, #4*4
        Pull    "sb,lr"
        ExitSWIHandler

HardwareLookup
        ADD     v8, sb, #1                              ; v8 = entry no + 1
        MOV     ip, #0
        LDR     v7, [ip, #HAL_Descriptor]
        AddressHAL ip
        LDR     v7, [v7, #HALDesc_Entries]
        CMP     v8, v7                                  ; entryno + 1 must be <= number of entries
        BHI     HardwareBadEntry
        LDR     a1, [sb, -v8, LSL #2]
        ADR     v7, NullHALEntry
        TEQ     a1, v7
        BEQ     HardwareBadEntry
        MOV     a2, sb
        ExitSWIHandler

HardwareBadReason
        ADR     r0, ErrorBlock_HardwareBadReason
 [ International
        Push    "lr"
        BL      TranslateError
        Pull    "lr"
 ]
        B       SLVK_SetV

HardwareBadEntry
        ADR     r0, ErrorBlock_HardwareBadEntry
 [ International
        Push    "lr"
        BL      TranslateError
        Pull    "lr"
 ]
        B       SLVK_SetV

        MakeErrorBlock HardwareBadReason
        MakeErrorBlock HardwareBadEntry
@


1.1.2.6
log
@More stuff. Up to the desktop now; cache on, working keyboard. Some source
restructuring to start to make splitting it up into several object files more
feasible.
@
text
@d28 43
a70 3
;        IMPORT  Init_ARMarch
;        IMPORT  ARM_Analyse

d81 6
d96 1
a96 1
        BIC     a1, a1, #MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S+MMUC_R
a106 6
        ; We assume that ARMs with an I cache can have it enabled while the MMU is off.
        [ :LNOT:CacheOff
        ORRNE   a1, a1, #MMUC_I
        ARM_write_control a1, NE                                ; whoosh
        ]

d109 3
a111 3
        ; Keep a soft copy of the CR in a banked register (R13_und)
        MSR     CPSR_c, #F32_bit+I32_bit+UND32_mode
        MOV     sp, a1
d497 1
a497 1
        STR     a2, [a1, #HAL_StartFlags]
a509 1

d537 1
a537 1
        MSR     CPSR_c, #F32_bit+I32_bit+UND32_mode ; Recover the soft copy of the CR
d539 1
a539 1
        ORR     v5, sp, #MMUC_M                 ; MMU on
d542 1
a542 1
        ORR     v5, sp, #MMUC_W+MMUC_C+MMUC_M   ; Write buffer, data cache, MMU on
a832 2
        CMP     a1, #-1
        BEQ     %FT80
d1158 12
a1169 6
        LDR     a1, =PhysicalAccess
        MOV     a3, a2, LSL #12                         ; take bottom 20 bits of address
        ORR     a3, a1, a3, LSR #12                     ; and make an offset within PhysicalAccess
        Push    "a3,lr"
        ARMop   TLB_InvalidateEntry                     ; sufficient, cause not cacheable
        Pull    "a1,pc"
d1176 1
a1176 1
        ARMop   TLB_InvalidateEntry,,tailcall           ; sufficient, cause not cacheable
d1196 155
a1387 9
        MSR     CPSR_c, #F32_bit+I32_bit+FIQ32_mode     ; get some extra registers
        MOV     r8, #0
        MOV     r9, #0
        MOV     r10, #0
        MOV     r11, #0
        MOV     r12, #0
        MOV     r13, #0
        MOV     r14, #0
        MSR     CPSR_c, #F32_bit+I32_bit+SVC32_mode
d1396 1
a1396 1
        STMIA   r0,{r4-r11,lr}
d1399 1
d1446 2
a1447 1
        ADD     r1, r11, lr
d1449 4
a1452 4
        MOV     r2, r0, LSR #20
        TEQ     r2, r1, LSR #20                         ; if end in different megabyte to start
        MOVNE   r1, r2, LSL #20                         ; then stop at end of megabyte
        ADDNE   r1, r1, #&00100000
d1454 2
a1455 1
        MSR     CPSR_c, #F32_bit+I32_bit+FIQ32_mode     ; switch to our bank o'zeros
d1457 2
d1460 1
a1460 1
19      ADD     r5, r5, r0
d1462 3
a1464 3
20      TEQ     r0, r1                                  ; *this* is the speed critical bit - executed
        TEQNE   r0, r5                                  ; 32768 times per outer loop
        STMNEIA r0!, {r2,r8-r14}
d1467 1
a1467 1
        TEQ     r0, r1
d1471 1
a1471 1
        ADD     r0, r0, r5                              ; and skip it
d1475 1
a1475 4
30      MSR     CPSR_c, #F32_bit+I32_bit+SVC32_mode     ; back to supervisor mode
        MOV     r10, r0
        MOV     r11, r1
        LDMIA   r7, {r0, r1}
d1490 1
a1490 1
        LDMIA   r0, {r4-r11,r14}                        ;restore
a1551 3
; In: a1 = flags
;     a2 = physical address
;     a3 = size
d1554 1
a1554 11
        MOV     v4, a2                                  ; v4 = original requested address
        ADD     a3, a2, a3                              ; a3 -> end
        MOV     a2, a2, LSR #12
        MOV     a2, a2, LSL #12                         ; round a2 down to a page boundary
        SUB     v4, v4, a2                              ; v4 = offset of original within page-aligned area
        MOV     lr, a3, LSR #12
        TEQ     a3, lr, LSL #12
        ADDNE   lr, lr, #1
        MOV     a3, lr, LSL #12                         ; round a3 up to a page boundary

        SUB     a4, a3, a2                              ; a4 = area size
d1556 2
a1557 2
        ORR     a3, a3, #AP_None * L2_APMult            ; a3 = access permissions
        MOV     a1, a2                                  ; a1 = physical base
d1559 3
a1561 3
        BL      FindIOLogicalAddress
        MOV     a2, a1                                  ; a2 = logical address of page-aligned area
        ADD     v4, a1, v4                              ; v4 = logical address of requested physical address
d1571 4
a1574 5
; In:  a1 = Physical address
;      a3 = flags
;      a4 = size
; Out: a1 = Logical address
FindIOLogicalAddress
d1600 2
a1601 3
10      SUBS    ip, ip, #1                              ; decrement counter
        LDRCS   a1, [a2], #4
        MOVCC   pc, lr
d1606 2
a1607 1
        B       %BT10
d1625 1
a1625 1
        LDR     v7, [v7, #HALDesc_NumEntries]           ; v7 = number of entries
d1627 1
a1627 1
        BHI     HardwareBadEntry2
d1631 1
a1631 1
        BEQ     HardwareBadEntry2
d1643 1
a1643 1
        LDR     v7, [v7, #HALDesc_NumEntries]
a1661 3
HardwareBadEntry2
        ADD     sp, sp, #4*4
        Pull    "sb,lr"
@


1.1.2.7
log
@Stuff. A bit of touchscreen, I expect, and probably some other bits too.
@
text
@d16 1
a16 1
MajorL2PThack SETL {FALSE}
a17 2
        GBLL    MinorL2PThack
MinorL2PThack SETL :LNOT:MajorL2PThack
a334 7
; Allocate the L2PT backing store for the logical L2PT space, to
; prevent recursion.
        LDR     a1, =L2PT
        MOV     a2, #&400000
        LDR     a3, =(AP_None * L2_APMult)
        BL      AllocateL2PT

a580 2
        LDR     a1, =ZeroPage

d583 1
a583 7
        ADD     v1, v1, a1                              ; turn v1 from PhysAddr to LogAddr

        LDR     a2, [a1, #InitUsedBlock]                ; turn this from Phys to Log too
        SUB     a2, a2, lr
        ADD     a2, a2, a1
        STR     a2, [a1, #InitUsedBlock]

d586 1
d609 1
a609 7
; Initialise the HAL. Due to its memory claiming we need to get our v1 and v2 values
; into workspace and out again around it.

        MOV     a1, #ZeroPage
        STR     v1, [a1, #InitUsedBlock]
        STR     v2, [a1, #InitUsedEnd]

a613 4
        MOV     a1, #ZeroPage
        LDR     v1, [a1, #InitUsedBlock]
        LDR     v2, [a1, #InitUsedEnd]

a682 18
 [ MinorL2PThack
; Allocate backing L2PT for the free pool
        MOV     a1, #FreePoolAddress
        LDR     a2, [v8, #RAMLIMIT]
        LDR     a3, =(AP_None * L2_APMult) + L2_B
        BL      AllocateL2PT
; And for application space
        MOV     a1, #0
        LDR     a2, [v8, #RAMLIMIT]             ; Not quite right, but the whole thing's wrong anyway
        LDR     a3, =(AP_Full * L2_APMult) + L2_C + L2_B
        BL      AllocateL2PT
; And for the system heap. Sigh
        LDR     a1, =SysHeapAddress
        LDR     a2, =SysHeapMaxSize
        LDR     a3, =(AP_Full * L2_APMult) + L2_C + L2_B
        BL      AllocateL2PT
 ]

d1085 8
a1092 15
; Need to zero the L2PT. Must do it before calling in MapInPage, as that may well
; want to put something in the thing we are clearing. If the MMU is off, no problem,
; but if the MMU is on, then the L2PT isn't accessible until we've called MapInPage.
; Solution is to use the AccessPhysicalAddress call.

        TEQ     v3, #0                          ; MMU on?
        MOVNE   a1, v4                          ; if not, just access v4
        MOVEQ   a1, #L1_B                       ; if so, map in v4
        MOVEQ   a2, v4
        MOVEQ   a3, #0
        BLEQ    RISCOS_AccessPhysicalAddress

        MOV     a2, #0
        MOV     a3, #4*1024
        BL      memset
d1095 2
a1096 2
        LDR     a2, =L2PT                       ; (can't recurse, because L2PT
        ADD     a2, a2, v8, LSL #10             ; backing for L2PT is preallocated)
a1100 1

a1390 3
        MOV     ip, #ZeroPage
        STR     v1, [ip, #InitUsedBlock]
        STR     v2, [ip, #InitUsedEnd]
@


1.1.2.8
log
@Check-in of the few last-minute changes for the Customer L demo. Nothing
exciting, apart from an extended touchscreen API.

Version 5.35, 4.79.2.13. Tagged as 'Kernel-5_35-4_79_2_13'
@
text
@d705 1
a705 1
        LDR     a2, =(AP_Read * L2_APMult) + L2_C + L2_B  ; Should be AP_None?
a746 6

        MOV     a1, #InitKbdWs
        MOV     a2, #0
        MOV     a3, #0
        STMIA   a1!, {a2,a3}
        STMIA   a1!, {a2,a3}
@


1.1.2.9
log
@Various L7200 tweaks, plus working ARM920T code.

Version 5.35, 4.79.2.17. Tagged as 'Kernel-5_35-4_79_2_17'
@
text
@d52 1
a52 1
        BIC     a1, a1, #MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S
d507 1
a507 5
        MOV     a4, a1
        BL      Init_ARMarch                    ; corrupts a1 and ip
        MOV     a1, a4
        MSREQ   CPSR_c, #F32_bit+I32_bit+UND32_mode ; Recover the soft copy of the CR
        ARM_read_control v5, NE
d509 1
a509 1
        ORR     v5, v5, #MMUC_M                 ; MMU on
d512 1
a512 1
        ORR     v5, v5, #MMUC_W+MMUC_C+MMUC_M   ; Write buffer, data cache, MMU on
d517 1
a517 4
        MSREQ   CPSR_c, #F32_bit+I32_bit+SVC32_mode

        MOV     ip, #0                                          ; junk MMU-off contents of I-cache
        MCR     ARM_config_cp,0,ip,ARMv4_cache_reg,C7           ; (works on ARMv3)
@


1.1.2.10
log
@spectacular new OS_Memory reason codes
13 map permanent I/O space, return logical address
14 access temporary physical mapping
15 release temporary physical mapping
DA creation and I/O space creation now avoid collision if address
space fills

Version 5.35, 4.79.2.28. Tagged as 'Kernel-5_35-4_79_2_28'
@
text
@d626 1
a627 3
        LDR     a1, =IOLimit
        STR     a1, [a2, #IOAllocLimit]
        LDR     a1, =IO
d1426 4
a1429 10
;
; In:  a1 = flags  (only L1_B currently allowed)
;      a2 = physical address
;      a3 = size
; Out: a1 = assigned logical address, or 0 if failed (no room)
;
; Will detect and return I/O space already mapped appropriately, or map and return new space
; For simplicity and speed of search, works on a section (1Mb) granularity
;
RISCOS_MapInIO ROUT
d1432 6
a1437 6
        ADD     a3, a2, a3                              ; a3 -> end (exclusive)
        MOV     a2, a2, LSR #20
        MOV     a2, a2, LSL #20                         ; round a2 down to a section boundary
        SUB     v4, v4, a2                              ; v4 = offset of original within section-aligned area
        MOV     lr, a3, LSR #20
        TEQ     a3, lr, LSL #20
d1439 1
a1439 1
        MOV     a3, lr, LSL #20                         ; round a3 up to a section boundary
d1441 32
a1472 5
        LDR     ip, =ZeroPage
        LDR     a4, =L1PT
        AND     a1, a1, #L1_B                           ; only allow bufferable as flags option
        LDR     v2, =IO                                 ; logical end (exclusive) of currently mapped IO
        LDR     v1, [ip, #IOAllocPtr]                   ; logical start (inclusive)
a1473 37
        SUB     v1, v1, #&100000
10
        ADD     v1, v1, #&100000                        ; next mapped IO section
        CMP     v1, v2
        BHS     %FT32                                   ; no more currently mapped IO
        LDR     v3, [a4, v1, LSR #(20-2)]               ; L1PT entry (must be for mapped IO) 
        MOV     lr, v3, LSR #20                         ; physical address bits
        CMP     lr, a2, LSR #20
        BNE     %BT10                                   ; no address match
        AND     lr, v3, #L1_B
        TEQ     lr, a1
        BNE     %BT10                                   ; no flags match
;
; alright, found start of requested IO already mapped, and with required L1_B
;
        Push    "a2, v1"
20
        ADD     a2, a2, #&100000
        CMP     a2, a3
        Pull    "a2, v1", HS
        BHS     %FT40                                  ; its all there already!
        ADD     v1, v1, #&100000                       ; next mapped IO section
        CMP     v1, v2
        BHS     %FT30                                  ; not all there in this mapping
        LDR     v3, [a4, v1, LSR #(20-2)]              ; L1PT entry
        MOV     lr, v3, LSR #20                        ; physical address bits
        CMP     lr, a2, LSR #20
        BNE     %FT30                                  ; address match failed
        AND     lr, v3, #L1_B
        TEQ     lr, a1
        BEQ     %BT20                                  ; address and flags match so far
30
        Pull    "a2, v1"
;
; request not currently mapped, only partially mapped, or mapped with wrong L1_B
;
32
d1475 6
a1480 22
        LDR     v2, [ip, #IOAllocPtr]
        ADD     v1, v2, a2
        SUB     v1, v1, a3                              ; attempt to allocate size of a3-a2
        LDR     v3, [ip, #IOAllocLimit]                 ; can't extend down below limit
        CMP     v1, v3
        MOVLS   a1, #0
        BLS     %FT90
        STR     v1, [ip, #IOAllocPtr]
        ORR     a2, a2, a1
        ORR     a2, a2, #AP_None * L1_APMult
        ORR     a2, a2, #L1_Section                     ; first L1PT value
34
        STR     a2, [a4, v1, LSR #(20-2)]
        ADD     a2, a2, #&100000
        ADD     v1, v1, #&100000                        ; next section
        CMP     a2, a3
        BLO     %BT34
        LDR     v1, [ip, #IOAllocPtr]
40
        ADD     a1, v1, v4                              ; logical address for request
90
        EXIT
@


1.1.2.11
log
@* Allows interrupt-driven use of PointerV (as well as polled).
* Allows HAL-driven software resets.
* Sound buffers corrected to be uncacheable.

Version 5.35, 4.79.2.33. Tagged as 'Kernel-5_35-4_79_2_33'
@
text
@d712 1
a712 1
; Allocate the cursor/system/sound block - first the cached bit
d715 1
a715 6
        LDR     a3, =SoundDMABuffers - CursorChunkAddress
        BL      Init_MapInRAM
; then the uncached bit
        LDR     a1, =SoundDMABuffers
        LDR     a2, =(AP_Read * L2_APMult) + L2_B  ; Should be AP_None?
        LDR     a3, =?SoundDMABuffers
d1460 1
a1460 1
        LDR     v3, [a4, v1, LSR #(20-2)]               ; L1PT entry (must be for mapped IO)
@


1.1.2.12
log
@Merge in long command line support from Ursula kernel.
Look for LongCommandLine flag, command line size currently
set at 1k.
For HAL/32bit builds, the kernel buffer space is at high
(top bit set) address, which may break some code using signed
comparisons. So *beware* that there may be some latent
bugs in old kernel code using these buffers, not yet found.
One such bug, in s.Arthur2 found and fixed.
Tested moderately on ARM9 desktop build.
Lovely to reimplement things I did two and half years ago.

Version 5.35, 4.79.2.37. Tagged as 'Kernel-5_35-4_79_2_37'
@
text
@a722 7
 [ LongCommandLines
        LDR     a1, =KbuffsBaseAddress
        LDR     a2, =(AP_Read * L2_APMult) + L2_C + L2_B
        LDR     a3, =(KbuffsSize + &FFF) :AND: &FFFFF000  ;(round to 4k)
        BL      Init_MapInRAM
 ]

@


1.1.2.13
log
@1) Bring IOMD HAL more up to date. Add support for new
call HAL_CleanerSpace (preparation for StrongARM kernel
support).

2) In kernel, add HAL_CleanerSpace call (preparation for
StrongARM and XScale core support). Fix bug found with
ARMv3 support during test on Risc PC.

3) Implement new API for kernel SWIs that have used top
bits of addresses as flags. The new API has an extra
flag that must be set, so kernel can distinguish and
support both APIs. The reason for all this is that
addresses are 32-bits now, people, keep up there. Briefly:

  OS_HeapSort
    bit 31 of r0 set for new API, r1 is full 32-bit address
    flags move from r1 bits 31-29 to r0 bits 30-28

  OS_ReadLine
    bit 31 of r1 set for new API, r0 is full 32-bit address
    flags move from bits 31,30 of r0 to bits 30,29 of r1

  OS_SubstituteArgs
    bit 31 of r2 set for new API, r0 is full 32-bit address
    flag moves from bit 31 of r0 to bit 30 of r2

Tested on Risc PC and briefly on Customer A 2

Ta

Version 5.35, 4.79.2.41. Tagged as 'Kernel-5_35-4_79_2_41'
@
text
@a510 1
        MOVEQ   v5, sp
a520 1
        MOVEQ   sp, v5
a672 18
; Set up the data cache cleaner space if necessary (eg. for StrongARM core)
        CallHAL HAL_CleanerSpace
        CMP     a1, #-1          ;-1 means none needed (HAL only knows this if for specific ARM core eg. system-on-chip)
        BEQ     %FT20
        LDR     a2, =DCacheCleanAddress
        LDR     a3, =(AP_None * L2_APMult) + L2_C + L2_B  ; ideally, svc read only, user none but hey ho
        ASSERT  DCacheCleanSize = 4*&10000                ; 64k of physical space used 4 times (allows large page mapping)
        MOV     a4, #&10000
        MOV     ip, #4              
        SUB     sp, sp, #5*4       ;room for a1-a4,ip
10
        STMIA   sp, {a1-a4, ip}
        BL      Init_MapIn
        LDMIA   sp, {a1-a4, ip}
        SUBS    ip, ip, #1
        ADD     a2, a2, #&10000
        BNE     %BT10
        ADD     sp, sp, #5*4
a673 1
20
d1387 1
a1387 1
        MSR     CPSR_c, #F32_bit + UND32_mode           ; retrieve the MMU control register
d1389 1
a1389 1
        STR     sp, [r0, #MMUControlSoftCopy]
@


1.1.2.14
log
@Lots of Tungsten work.

Version 5.35, 4.79.2.48. Tagged as 'Kernel-5_35-4_79_2_48'
@
text
@d15 3
d19 1
a19 1
MinorL2PThack SETL {TRUE}
a85 1
;             bit 1: video memory is not suitable for general use
a188 87
; Subtractv1v2fromRAMtable
;
; On entry: v1 = base of memory area
;           v2 = size of memory area
;           a4 = RAM table handle (ie pointer to terminator word containing number of entries)
;
; On exit:  a1-a3 preserved
;           a4 and RAM table updated
;           other registers corrupted
Subtractv1v2fromRAMtable
        ADD     v2, v1, v2              ; v2 = end address
        MOV     v1, v1, LSR #12
        MOV     v1, v1, LSL #12         ; round base down
        ADD     v2, v2, #4096
        SUB     v2, v2, #1
        MOV     v2, v2, LSR #12
        MOV     v2, v2, LSL #12         ; round end up

        LDR     v5, [a4]
        SUB     v8, a4, v5, LSL #3
10      TEQ     v8, a4
        MOVEQ   pc, lr
        LDMIA   v8!, {v3, v4}
        MOV     v6, v4, LSR #12
        ADD     v6, v3, v6, LSL #12     ; v6 = end of RAM block
        CMP     v2, v3                  ; if our end <= RAM block start
        CMPHI   v6, v1                  ; or RAM block end <= our start
        BLS     %BT10                   ; then no intersection

        MOV     v4, v4, LSL #20         ; extract flags

        CMP     v1, v3
        BHI     not_bottom

        ; our area is at the bottom
        CMP     v2, v6
        BHS     remove_block

        SUB     v6, v6, v2              ; v6 = new size
        ORR     v6, v6, v4, LSR #20     ; + flags
        STMDB   v8, {v2, v6}            ; store new base (= our end) and size
        B       %BT10

        ; we've completely covered a block. Remove it.
remove_block
        MOV     v6, v8
20      TEQ     v6, a4                  ; shuffle down subsequent blocks in table
        LDMNEIA v6, {v3, v4}
        STMNEDB v6, {v3, v4}
        ADDNE   v6, v6, #8
        BNE     %20
        SUB     v5, v5, #1
        SUB     a4, a4, #8
        STR     v5, [a4]
        SUB     v8, v8, #8
        B       %BT10

        ; our area is not at the bottom.
not_bottom
        CMP     v2, v6
        BLO     split_block

        ; our area is at the top
        SUB     v6, v1, v3              ; v6 = new size
        ORR     v6, v6, v4, LSR #20     ; + flags
        STMDB   v8, {v3, v6}            ; store original base and new size
        B       %BT10

split_block
        MOV     v6, a4
30      TEQ     v6, v8                  ; shuffle up subsequent blocks in table
        LDMNEDB v6, {v3, v4}
        STMNEIA v6, {v3, v4}
        SUBNE   v6, v6, #8
        BNE     %BT30
        ADD     v5, v5, #1
        ADD     a4, a4, #8
        STR     v5, [a4]

        SUB     v7, v1, v3              ; v7 = size of first half
        SUB     v6, v6, v2              ; v6 = size of second half
        ORR     v7, v7, v4, LSR #20
        ORR     v6, v6, v4, LSR #20     ; + flags
        STMDB   v8, {v3, v7}
        STMIA   v8!, {v2, v6}
        B       %BT10

a201 9
        ; subtract the HAL and OS from the list of RAM areas
        MOV     v1, a2
        LDR     v2, [a2, #OSHdr_ImageSize]
        BL      Subtractv1v2fromRAMtable
        LDR     v1, [a3, #HALDesc_Start]
        ADD     v1, a3, v1
        LDR     v2, [a3, #HALDesc_Size]
        BL      Subtractv1v2fromRAMtable

d220 3
d224 1
d226 2
a227 2
        CMP     v6, #32*1024*1024
        MOVHS   v6, #32*1024*1024               ; Limit allocation to 32M (arbitrary)
a267 1
        STR     v6, [ip, #VRAMFlags]
d340 2
a341 2
        MOV     a2, #&00400000
        LDR     a3, =(AP_None * L2X_APMult)
d349 1
a349 1
        LDR     a2, =(AP_None * L2X_APMult) + L2_C + L2_B
d371 1
a371 5
 [ ECC
        LDR     a3, =(AP_Read * L2X_APMult) + L2_C + L2_B + 1:SHL:31
 |
        LDR     a3, =(AP_Read * L2X_APMult) + L2_C + L2_B
 ]
d378 1
a378 5
  [ ECC
        LDR     a3, =(AP_Read * L2X_APMult) + L2_C + L2_B + 1:SHL:31
  |
        LDR     a3, =(AP_Read * L2X_APMult) + L2_C + L2_B
  ]
d385 1
a385 5
 [ ECC
        LDR     a3, =(AP_None * L2X_APMult) + 1:SHL:31
 |
        LDR     a3, =(AP_None * L2X_APMult)
 ]
d393 1
a393 5
 [ ECC
        LDR     a3, =(AP_None * L2X_APMult) + 1:SHL:31
 |
        LDR     a3, =(AP_None * L2X_APMult)
 ]
d397 9
d430 1
a430 1
        MOV     a3, #(AP_ROM * L2X_APMult) + L2_C + L2_B
d444 1
a444 1
        MOV     a3, #(AP_ROM * L2X_APMult) + L2_C + L2_B
d455 1
a455 1
        MOV     a3, #(AP_ROM * L2X_APMult) + L2_C + L2_B
a467 8
        ; Set up a reset IRQ handler (used during RAM clear for keyboard
        ; scan, and later for IIC CMOS access)
        MSR     CPSR_c, #IRQ32_mode + I32_bit + F32_bit
        LDR     sp_irq, =IRQSTK
        MSR     CPSR_c, #SVC32_mode + I32_bit + F32_bit
        LDR     a2, =Reset_IRQ_Handler
        STR     a2, [a1, #InitIRQHandler]

d478 1
a478 1
MMU_activation_zone
d489 4
a492 2
        MOV     a1, #4_3333333333333333         ; All domain manager - in case MMU already on
        ARM_MMU_domain a1                       ; (who knows what domains/permissions they are using)
d494 1
a494 1
        ADR     a1, MMU_activation_zone
a508 1

a519 6
        ARM_MMU_transbase v3                    ; Always useful to tell it where L1PT is...

        MOV     ip, #0
        MCREQ   p15, 0, ip, c5, c0              ; MMU may already be on (but flat mapped)
        MCRNE   p15, 0, ip, c8, c7              ; if HAL needed it (eg XScale with ECC)
                                                ; so flush TLBs now
a527 3
        MOV     ip, #4_0000000000000001                         ; domain 0 client only
        ARM_MMU_domain ip

d535 1
a535 1
        ADR     a4, MMU_activation_zone
a618 2
        ChangedProcVecs a1

a667 3
        MOV     a1, #0
        LDR     a1, [a1, #HAL_StartFlags]
        TST     a1, #OSStartFlag_RAMCleared
d669 1
a669 1
        BLEQ    ClearPhysRAM
a674 8
; Set v4 to XCB bits for default cacheable+bufferable
        MOV     v5, #0
        LDR     v4, [v5, #MMU_PCBTrans]
        LDRB    v4, [v4, #0]
; Set v5 to XCB bits for default bufferable
        LDR     v5, [v5, #MMU_PCBTrans]
        LDRB    v5, [v5, #XCB_NC]

a675 1
        MOV     a1, #-1
d680 1
a680 1
        ORR     a3, v4, #AP_None * L2X_APMult             ; ideally, svc read only, user none but hey ho
d683 1
a683 1
        MOV     ip, #4
d697 1
a697 1
        ORR     a2, v4, #AP_None * L2X_APMult
d703 1
a703 1
        ORR     a2, v4, #AP_Read * L2X_APMult
d710 1
a710 1
        ORR     a2, v4, #AP_None * L2X_APMult
d717 1
a717 1
        ORR     a2, v4, #AP_None * L2X_APMult
d723 1
a723 1
        ORR     a2, v4, #AP_None * L2X_APMult
d729 1
a729 1
        ORR     a2, v4, #AP_Full * L2X_APMult
d735 1
a735 1
        ORR     a2, v4, #AP_Read * L2X_APMult           ; Should be AP_None?
d740 1
a740 1
        ORR     a2, v5, #AP_Read * L2X_APMult           ; Should be AP_None?
d746 1
a746 1
        ORR     a2, v4, #AP_Read * L2X_APMult
d755 1
a755 1
        ORR     a3, v5, #AP_None * L2X_APMult
d759 2
a760 2
        MOV     a2, #AplWorkMaxSize             ; Not quite right, but the whole thing's wrong anyway
        ORR     a3, v4, #AP_Full * L2X_APMult
d765 1
a765 1
        ORR     a3, v4, #AP_Full * L2X_APMult
d790 1
a790 1
        MOV     a1, #InitIRQWs
a847 1
        LDR     ip, [a1, #MMU_PCBTrans]
d874 1
d877 1
a877 24
        TEQ     v6, #L2_SmallPage               ; convert small pages to extended pages
        BNE     %FT50
        MOV     lr, #0                          ; if we now know that CPU supports them
        LDR     a1, [lr, #ProcessorFlags]
        TST     a1, #CPUFlag_ExtendedPages
        BEQ     %FT50

        ASSERT  ZeroPage = 0
        LDR     a1, [lr, #MMU_PCBTrans]         ; reprocess C and B bits as per XCB table
        TST     v5, #L2_C                       ; (eg if C and B both set, replace with
        ORREQ   lr, lr, #DynAreaFlags_NotCacheable ; default cacheable+bufferable XCB)
        TST     v5, #L2_B
        ORREQ   lr, lr, #DynAreaFlags_NotBufferable
        LDRB    lr, [a1, lr, LSR #2]
        EOR     v5, v5, #L2_ExtPage:EOR:L2_SmallPage
        BIC     v5, v5, #2_111111000000         ; remove excess 3 AP fields
        BIC     v5, v5, #2_000000001100         ; remove old C+B
        BIC     a1, v5, #2_000000110011         ; remove all other bits for just address
        ORR     v5, v5, lr                      ; put in new XCB
        MOV     a2, #ZeroPage
        ARMop   MMU_ChangingEntry,,,a2
        STR     v5, [v4, v2, LSR #10]           ; update page table

50      MOV     a1, v5, LSR #12
d883 1
d891 1
a891 1
        AND     a1, v5, #&30                    ; a1 = access permission
d895 5
a899 20
        RSB     v6, a1, #3                      ; v6 = PPL access
        AND     a1, v5, #2_11                   ; a1 = page type
        CMP     a1, #L2_SmallPage
        ANDHI   a1, v5, #2_0000001111001100     ; Extended TEX and CB bits
        ANDLS   a1, v5, #2_0000000000001100     ; Small/Large CB bits only
        ANDLO   lr, v5, #2_1111000000000000     ; Large TEX bits
        ORRLO   a1, a1, lr, LSR #6              ; Move Large TEX back to Extended TEX position
        MOV     lr, #3                          ; lr = PCB value (funny loop to do NCNB first)
60      LDRB    a3, [ip, lr]                    ; look in XCBTrans table
        TEQ     a3, a1                          ; found a match for our XCB?
        BEQ     %FT70
        TST     lr, #2_11
        SUBNE   lr, lr, #1                      ; loop goes 3,2,1,0,7,6,5,4,...,31,30,29,28
        ADDEQ   lr, lr, #7
        TEQ     lr, #35
        BNE     %BT60
70      AND     a1, lr, #2_00011
        ORR     v6, v6, a1, LSL #4              ; extract NCNB bits
        AND     a1, lr, #2_11100
        ORR     v6, v6, a1, LSL #10             ; extract P bits
d937 1
a937 1
;    a2 = access permissions (see Init_MapIn)
a967 3
 [ ECC
        ORR     a3, v7, #1:SHL:31
 |
a968 1
 ]
d984 3
a986 3
;    a3 = access permissions + C + B bits (bits 11-2 of an extended descriptor)
;           (also set bit 31 to indicate that P bit in L1PT should
;            be set)
d999 4
a1002 13
        MOVS    ip, lr, LSL #16                 ; If bottom 16 bits not all 0
        ORRNE   a3, a3, #L2_ExtPage             ; then extended small pages (4K)
        BNE     %FT10

        ORR     a3, a3, #L2_LargePage           ; else large pages (64K)
        AND     lr, a3, #L2_TEX                 ; extract TEX from ext page flags
        AND     ip, a3, #L2X_AP                 ; extract AP from ext page flags
        BIC     a3, a3, #L2_AP                  ; clear way for large page AP
        ORR     ip, ip, ip, LSL #2              ; duplicate up AP to 4 sub-pages
        ORR     ip, ip, ip, LSL #4
        ORR     a3, a3, lr, LSL #6              ; replace TEX in large page position
        ORR     a3, a3, ip                      ; replace quadrupled AP
10
d1022 1
a1022 3
        AND     lr, a3, #4_033300               ; extract TEX and AP bits
        BIC     a3, a3, #4_033300               ; and clear them (now P, Domain and U bits)
        ORR     a3, a3, lr, LSL #6              ; put TEX and AP bits back in new position
d1029 1
a1029 1
        ORRS    a3, a3, #L1_Section             ; Add section indicator to permission
d1031 1
a1031 1
        ORRS    a3, a3, #L1_U+L1_Section        ; Add section + U indicators to permission
a1032 2
        ORRMI   a3, a3, #L1_P
        BICMI   a3, a3, #1:SHL:31
d1047 1
a1047 2
;    a3 = access permissions + C + B bits + size (all non-address bits, of appropriate type)
;           (also set bit 31 to indicate that P bit in L1PT should be set)
a1054 3
;
; ROM permission is caught if on an ARM 6 and turned into Read
; Extended pages are caught if not available (or MMU off) and turned into Small
d1073 1
a1073 18
40      AND     lr, v6, #3
        TEQ     lr, #L2_LargePage               ; strip out surplus address bits from
        BICEQ   v6, v6, #&0000F000              ; large page descriptors
        TEQ     lr, #L2_ExtPage
        BNE     %FT50
        TEQ     v3, #0                          ; if we've been given an extended page
        BNE     %FT45                           ; must check that (a) the MMU is on
        MOV     lr, #0
        LDR     lr, [lr, #ProcessorFlags]       ; and (b) the CPU supports them
        TST     lr, #CPUFlag_ExtendedPages
        BNE     %FT50
45      EOR     v6, v6, #L2_SmallPage :EOR: L2_ExtPage
        AND     lr, v6, #L2X_AP                 ; convert the extended page descriptor
        BIC     v6, v6, #L2_AP                  ; into a small page descriptor
        ORR     lr, lr, lr, LSL #2              ; (losing the TEX bits in the
        ORR     v6, v6, lr                      ; process, but they should have been 0)
        ORR     v6, v6, lr, LSL #4
50      ORR     lr, v4, v6                      ; lr = value for L2PT entry
d1078 1
a1078 1
        ORREQ   lr, lr, #AP_Read * L2_APMult    ;     then make it Read permission
d1080 3
d1092 1
a1092 2
;    a3 = access permissions that will be placed in L2PT (bits 11-2 of Extended L2 descriptor)
;           (also set bit 31 to indicate that P bit in L1PT should be set)
d1152 1
a1152 1
        TST     lr, #4_000300
d1167 1
a1167 1
        TSTEQ   lr, #4_000300
a1172 3
  [ ECC
        ORR     a3, a3, #L1_P
  ]
d1197 1
a1197 4
        LDR     a3, =(AP_None * L2X_APMult) + L2_ExtPage
 [ ECC
        ORR     a3, a3, #1:SHL:31
 ]
a1209 7
 [ ECC
        Push    "a1-a3,lr"
        MOV     a1, a2
        BL      PhysAddrToPageNo
        CMP     a1, #-1
        Pull    "a1-a3,lr"
 ]
a1212 3
 [ ECC
        ORRNE   a1, a1, #L1_P
 ]
d1455 1
a1455 1
        LDR     pc, InitProcVecs + InitIRQHandler       ; IRQ
a1469 4

        ASSERT  L1_B = 1:SHL:2
MapInFlag_DoublyMapped * 1:SHL:16

d1471 1
a1471 2
        Entry   "v1-v5"
        MOV     v5, a1                                  ; v5 = original flags
a1481 3
        ANDS    v5, v5, #MapInFlag_DoublyMapped
        SUBNE   v5, a3, a2                              ; v5 = offset of second mapping or 0

a1499 16

        TEQ     v5, #0                                  ; doubly mapped?
        BEQ     %FT19

        ADD     lr, v1, v5                              ; address of second copy
        CMP     lr, v2
        BHS     %FT32
        LDR     v3, [a4, lr, LSR #(20-2)]
        MOV     lr, v3, LSR #20                         ; physical address bits
        CMP     lr, a2, LSR #20
        BNE     %BT10                                   ; no address match
        AND     lr, v3, #L1_B
        TEQ     lr, a1
        BNE     %BT10                                   ; no flags match

19
d1511 1
a1511 1
        BHS     %FT30                                  ; no more currently mapped IO
d1515 1
a1515 1
        BNE     %FT29                                  ; address match failed
a1517 1
        TEQEQ   v5, #0                                 ; doubly mapped?
a1518 13
        ADD     lr, v1, v5                             ; where duplicate should be
        CMP     lr, v2
        BHS     %FT30                                  ; no more currently mapped IO
        LDR     v3, [a4, lr, LSR #(20-2)]
        MOV     lr, v3, LSR #20                        ; physical address bits
        CMP     lr, a2, LSR #20
        BNE     %FT29                                  ; address match failed
        AND     lr, v3, #L1_B
        TEQ     lr, a1
        BEQ     %BT20
29
        Pull    "a2, v1"
        B       %BT10
a1528 1
        SUB     v1, v1, v5                              ; double if necessary
a1538 3
        TEQ     v5, #0
        ADDNE   v2, v1, v5
        STRNE   a2, [a4, v2, LSR #(20-2)]
a1633 72

 [ DebugTerminal
DebugTerminal_Rdch
        Push    "a2-a4,sb,ip"
        WritePSRc SVC_mode, r1
        MOV     sb, ip
20
        CallHAL HAL_DebugRX
        CMP     a1, #27
        BNE     %FT25
        LDR     a2, =OsbyteVars + :INDEX: RS423mode
        LDRB    a2, [a2]
        TEQ     a2, #0                  ; is RS423 raw data,or keyb emulator?
        BNE     %FT25
        MOV     a2, #0
        LDRB    a1, [a2, #ESC_Status]
        ORR     a1, a1, #&40
        STRB    a1, [a2, #ESC_Status]   ; mark escape flag
        MOV     a1, #27
        SEC                             ; tell caller to look carefully at R0
        Pull    "a2-a4,sb,ip,pc"
25
        CMP     a1, #-1
        Pull    "a2-a4,sb,ip,pc",NE     ; claim it
        MOV     R0, #0
        LDRB    R14, [R0, #CallBack_Flag]
        TST     R14, #CBack_VectorReq
        BLNE    process_callback_chain
        B       %BT20


DebugTerminal_Wrch
        Push    "a1-a4,sb,ip,lr"
        MOV     sb, ip
        CallHAL HAL_DebugTX
        Pull    "a1-a4,sb,ip,pc"        ; don't claim it
 ]

 [ DebugHALTX
DebugHALPrint
        Push    "a1-a4,v1,sb,ip"
        AddressHAL
        MOV     v1, lr
10      LDRB    a1, [v1], #1
        TEQ     a1, #0
        BEQ     %FT20
        CallHAL HAL_DebugTX
        B       %BT10
20      MOV     a1, #13
        CallHAL HAL_DebugTX
        MOV     a1, #10
        CallHAL HAL_DebugTX
        ADD     v1, v1, #3
        BIC     lr, v1, #3
        Pull    "a1-a4,v1,sb,ip"
        MOV     pc, lr
 ]

Reset_IRQ_Handler
        SUB     lr, lr, #4
        Push    "a1-a4,sb,ip,lr"
        MRS     lr, SPSR
        Push    "lr"
        MOV     a1, #ZeroPage
        AddressHAL a1
        LDR     a2, [a1, #IICType]
        TST     a2, #IICFlag_Background
        MOVNE   ip, sb
        BLNE    IICIRQ
        Pull    "lr"
        MSR     SPSR_cxsf, lr
        Pull    "a1-a4,sb,ip,pc",,^
@


1.1.2.15
log
@  Mostly device stuff.
Detail:
  * Implemented OS_Hardware 2, 3 and 4 as described in Docs.HAL.NewAPI.
  * Added new OS->HAL and HAL->OS routines to register HAL devices with the
    OS during hard resets.
  * Updated Docs.HAL.NewAPI to correct inconsistencies, fill in missing
    definitions, and allow for interrupt sharing.
  * Now uses OS_LeaveOS to trigger callbacks after ROM module init.
Admin:
  Untested. Requires new HAL.

Version 5.35, 4.79.2.49. Tagged as 'Kernel-5_35-4_79_2_49'
@
text
@a1808 6
; void RISCOS_AddDevice(unsigned int flags, struct device *d)
RISCOS_AddDevice
        ADDS    a1, a2, #0      ; also clears V
        B       HardwareDeviceAdd_Common


d1833 1
a1833 1
        BLO     HardwareCall
a1834 6
        CMP     ip, #3
        BLO     HardwareDeviceAdd
        BEQ     HardwareDeviceRemove
        CMP     ip, #5
        BLO     HardwareDeviceEnumerate
        BHS     HardwareBadReason
a1867 104
        ExitSWIHandler

HardwareDeviceAdd
        Push    "lr"
        BL      HardwareDeviceAdd_Common
        Pull    "lr"
        B       SLVK_TestV

HardwareDeviceRemove
        Push    "lr"
        BL      HardwareDeviceRemove_Common
        Pull    "lr"
        B       SLVK_TestV

HardwareDeviceAdd_Common
        Entry   "r1-r3"
        BL      HardwareDeviceRemove_Common             ; first try to remove any device already at the same address
        EXIT    VS
        MOV     lr, #0
        LDR     r1, [lr, #DeviceCount]
        LDR     r2, [lr, #DeviceTable]
        TEQ     r2, #0
        BEQ     %FT80
        LDR     lr, [r2, #-4]                           ; word before heap block is length
        TEQ     r1, lr, LSR #2                          ; block already full?
        BEQ     %FT81
10      ADD     lr, r2, r1, LSL #2
11      LDR     r1, [lr, #-4]!                          ; copy existing devices up, so new ones get enumerated first
        STR     r1, [lr, #4]
        CMP     lr, r2
        BHI     %BT11
        STR     r0, [r2]
        MOV     r2, r0
        MOV     r1, #Service_Device
        MOV     r0, #0
        BL      Issue_Service
        ADDS    r0, r2, #0                              ; exit with V clear
        EXIT

80      ; Claim a system heap block for the device table
        MOV     r3, #16
        BL      ClaimSysHeapNode
        EXIT    VS
        MOV     r1, #0
        STR     r2, [r1, #DeviceTable]
        B       %BT10

81      ; Extend the system heap block
        MOV     r0, #HeapReason_ExtendBlock
        MOV     r3, #16
        BL      DoSysHeapOpWithExtension
        EXIT    VS
        MOV     lr, #0
        LDR     r1, [lr, #DeviceCount]
        STR     r2, [lr, #DeviceTable]
        B       %BT10

HardwareDeviceRemove_Common
        Entry   "r1-r4"
        MOV     lr, #0
        LDR     r3, [lr, #DeviceCount]
        LDR     r4, [lr, #DeviceTable]
        TEQ     r3, #0
        EXIT    EQ                                      ; no devices registered
01      LDR     r2, [r4], #4
        TEQ     r2, r0
        SUBNES  r3, r3, #1
        BNE     %BT01
        TEQ     r2, r0
        EXIT    NE                                      ; this device not registered
        MOV     r0, #1
        MOV     r1, #Service_Device
        BL      Issue_Service
        CMP     r1, #0                                  ; if service call claimed
        CMPEQ   r1, #1:SHL:31                           ; then set V
        EXIT    VS                                      ; and exit
        MOV     r0, r2
        SUBS    r3, r3, #1
02      LDRNE   r2, [r4], #4                            ; copy down remaining devices
        STRNE   r2, [r4, #-8]
        SUBS    r3, r3, #1
        BNE     %BT02
        EXIT

HardwareDeviceEnumerate
        Push    "r3-r4,lr"
        MOV     lr, #0
        LDR     r2, [lr, #DeviceCount]
        LDR     r3, [lr, #DeviceTable]
        SUBS    r4, r2, r1
        MOVLS   r1, #-1
        BLS     %FT90                                   ; if r1 is out of range then exit
        ADD     r3, r3, r1, LSL #2
10      ADD     r1, r1, #1
        LDR     r2, [r3], #4
        LDR     lr, [r2, #HALDevice_Type + 2]
        MOV     lr, lr, LSR #16
        TEQ     lr, r0
        SUBNES  r4, r4, #1
        BNE     %BT10
        TEQ     lr, r0
        MOVNE   r1, #-1
90
        Pull    "r3-r4,lr"
@


1.1.2.16
log
@  Commit of kernel as featured in release 5.00.
Detail:
  Lots of changes since last version, at least the following:
  * Updated OS timestamp, removed alpha status
  * Negative INKEY OS version changed to &AA
  * GraphicsV is now alocated vector number &2A
  * ROM moved up to &FC000000
  * Max application slot increased to 512 Mbytes (for now)
  * Max size of RMA increased to 256 Mbytes
  * RMA is now first-created dynamic area (so it gets lowest address after
    top of application slot)
  * OS_Memory 10 reimplemeted
  * New OS_ReadSysInfo 6 values 18-22 added
  * OS_ReadSysInfo 8 gains flag bit to indicate soft power-off
  * Misc internal top-bit-set-address fixes
  * *ChangeDynamicArea can take sizes in megabytes or gigabytes
  * Magic word "&off" in R0 passed to OS_Reset powers down if possible
  * Added acceleration: block copy; CLS; text window scroll up; rectangle
    fill
  * Disabled LED flashing in page mode (liable to crash)
  * Masked sprite plot and VDU 5 text avoids reading the screen if possible
  * Framestore made USR mode accessible
  * Fix for VDU 5,127 bug - now relies on font definitions being in extreme
    quarters of memory, rather than bottom half
  * Allocated 64-bit OS_Convert... SWIs
  * IIC errors use allocated error numbers
  * Looks for Dallas RTC before Philips RTC because we're using a Philips
    NVRAM device with the same ID
  * Fix to bug that meant the oscillator in the Dallas RTC wasn't enabled
  * Default mouse type (USB) changed to allocated number
  * Ram disc max size increased to 128 Mbytes (Ursula merge) and made
    cacheable for StrongARMs (not XScale)
  * Branch through zero handler now works in USR mode, by use of a
    trampoline in the system stack to allow PC-relative register storage
  * Address exception handler changed to not use 0 as workspace
  * OS_Memory 13 extended to allow specification of cacheability and access
    privileges
  * Added OS_Memory 16 to return important memory addresses
  * RISCOS_MapInIO() takes cacheable flag in bit 3, access permissions in
    bits 10 and 11, doubly-mapped flag in bit 20, and access permissions
    specified flag in bit 21
  * Bug fix in last version for application abort handlers didn't quite
    work; register shuffle required
  * "Module is not 32-bit compatible" error now reports the module name
  * Default configured language changed from 10 to 11 (now Desktop again)

Version 5.35, 4.79.2.51. Tagged as 'Kernel-5_35-4_79_2_51'
@
text
@a917 2
        BL      CountPageTablePages

a927 18
CountPageTablePages ROUT
        MOV     a1, #ZeroPage
        LDR     a2, =CAM
        LDR     a3, [a1, #MaxCamEntry]
        MOV     a1, #0
        ADD     a3, a3, #1
        ADD     a4, a2, a3, LSL #3
10      LDR     ip, [a4, #-8]!
        ASSERT  (L2PT :AND: &3FFFFF) = 0
        MOV     ip, ip, LSR #22
        TEQ     ip, #L2PT :SHR: 22
        ADDEQ   a1, a1, #4096
        TEQ     a4, a2
        BNE     %BT10
        MOV     a2, #0
        STR     a1, [a2, #L2PTUsed]
        MOV     pc, lr

d1679 1
a1679 1
; In:  a1 = flags  (L1_B,L1_C,L1_AP)
d1689 1
a1689 4
        ASSERT  L1_C = 1:SHL:3
        ASSERT  L1_AP = 3:SHL:10
MapInFlag_DoublyMapped * 1:SHL:20
MapInFlag_APSpecified * 1:SHL:21
d1692 1
a1692 3
        Entry   "v1-v5,v7"
        MOV     v7, #L1_B:OR:L1_C
        ORR     v7, v7, #L1_AP                          ; v7 = user-specifiable flags
a1703 4
        TST     v5, #MapInFlag_APSpecified
        BICEQ   a1, a1, #L1_AP
        ORREQ   a1, a1, #L1_APMult * AP_None

d1709 1
a1709 1
        AND     a1, a1, v7                              ; only allow bufferable as flags option
d1722 1
a1722 1
        AND     lr, v3, v7
d1736 1
a1736 1
        AND     lr, v3, v7
d1742 1
a1742 1
; alright, found start of requested IO already mapped, and with required flags
d1757 1
a1757 1
        AND     lr, v3, v7
d1768 1
a1768 1
        AND     lr, v3, v7
d1777 1
a1777 1
; request not currently mapped, only partially mapped, or mapped with wrong flags
d1791 1
d1913 1
a1913 1
        MOV     r1, #Service_Hardware
d1951 1
a1951 1
        MOV     r1, #Service_Hardware
@


1.1.2.17
log
@  HAL device support, and a couple of new service calls.
Detail:
  * Rejigged documented meaning of device "Location" field so that we can
    fit full PCI locations in.
  * Defined lots of device "Type" values in Hdr:HALDevice.
  * Removed obsolete DMA-related HAL entries in Hdr:HALEntries (no longer
    required by DMAManager 0_15-4_4_2_6, no longer provided by Tungsten HAL
    0.07).
  * OS_Hardware 2 and 3 actually work now.
  * Changed OS_Hardware 4 to take a maximum major version number to match.
  * HAL workspace is now USR mode readable.
  * Service calls issued after module initialisation/finalisation (see
    Docs.ModPostServ).
Admin:
  OS_Hardware tested, service calls not tested.

Version 5.35, 4.79.2.52. Tagged as 'Kernel-5_35-4_79_2_52'
@
text
@d440 1
a440 1
        LDR     a2, =(AP_Read * L2X_APMult) + L2_C + L2_B
d1911 1
a1911 1
        Push    "r1-r3,lr"
d1913 1
a1913 1
        Pull    "r1-r3,lr"
d1917 1
a1917 1
        Push    "r1-r3,lr"
d1919 1
a1919 1
        Pull    "r1-r3,lr"
d1923 1
a1923 1
        Entry
d1931 1
a1931 2
        ADD     r1, r1, #1                              ; increment DeviceCount
        LDR     lr, [r2, #-4]                           ; word before heap block is length including length word
d1934 1
a1934 4
        MOV     lr, #0
10      STR     r1, [lr, #DeviceCount]
        ADD     lr, r2, r1, LSL #2
        SUB     lr, lr, #4
a1947 1
        Push    "r0"
a1949 1
        ADDVS   sp, sp, #4
d1951 2
a1952 4
        Pull    "r0"
        MOV     lr, #0
        MOV     r1, #1
        STR     r2, [lr, #DeviceTable]
a1955 1
        Push    "r0"
a1958 1
        ADDVS   sp, sp, #4
a1959 1
        Pull    "r0"
a1962 1
        ADD     r1, r1, #1
d1966 1
a1966 1
        Entry   "r4"
d1973 2
a1974 2
        SUBS    r3, r3, #1
        TEQNE   r2, r0
d1982 1
a1982 1
        CMPEQ   r1, #1:SHL:31                           ; then set V (r0 already points to error block)
d1986 4
a1989 8
02      LDRCS   r2, [r4], #4                            ; copy down remaining devices
        STRCS   r2, [r4, #-8]
        SUBCSS  r3, r3, #1
        BCS     %BT02
        MOV     lr, #0
        LDR     r3, [lr, #DeviceCount]
        SUB     r3, r3, #1
        STR     r3, [lr, #DeviceCount]
d2003 3
a2005 3
        LDR     lr, [r2, #HALDevice_Type]
        EOR     lr, lr, r0
        MOVS    lr, lr, LSL #16                         ; EQ if types match
d2008 1
a2008 1
        TEQ     lr, #0
a2009 5
        BNE     %FT90
        LDR     lr, [r2, #HALDevice_Version]
        MOV     lr, lr, LSR #16
        CMP     lr, r0, LSR #16                         ; newer than our client understands?
        BHI     %BT10
@


1.1.2.18
log
@Support for keys held down in the HAL at power on.
*Configure ANYTHINGsize was broken due to not setting R0 to ReadUnsigned
IIC ack message uninternationalised
OS_Memory was saying we only had 4M of RAM
VDU4 scrolling when output was switched to sprite was causing corruption
on use of CTRL-J and CTRL-K
Default SystemSize CMOS set to 32k

Version 5.35, 4.79.2.55. Tagged as 'Kernel-5_35-4_79_2_55'
@
text
@a452 8
        LDR     a3, [sp, #8]                            ; recover pushed HAL header
        LDR     lr, [a3, #HALDesc_Flags]
        TST     lr, #HALFlag_NCNBWorkspace              ; do they want uncacheable
        LDRNE   a1, =HALWorkspaceNCNB                   ; workspace?
        LDRNE   a2, =(AP_None * L2X_APMult)
        LDRNE   a3, =32*1024
        BLNE    Init_MapInRAM

d569 1
a569 1
        LDR     sp_irq, =ScratchSpace + ScratchSpaceSize/2
a759 1
        LDR     a2, =HALWorkspaceNCNB
a778 8
        MOV     a1, #InitIRQWs
        MOV     a2, #1
        STRB    a2, [a1, #KbdScanActive]

        CallHAL HAL_KbdScanSetup

        MSR     CPSR_c, #F32_bit+SVC32_mode             ; enable IRQs for scan

a919 1
 [ {FALSE}
a924 1
 ]
d970 16
d1527 1
a1527 1
        MSR     CPSR_c, #F32_bit+FIQ32_mode             ; get some extra registers
d1535 1
a1535 1
        MSR     CPSR_c, #F32_bit+SVC32_mode
d1600 1
a1600 1
        MSR     CPSR_c, #F32_bit+FIQ32_mode             ; switch to our bank o'zeros
d1618 1
a1618 1
30      MSR     CPSR_c, #F32_bit+SVC32_mode             ; back to supervisor mode
a1841 10
; uint32_t RISCOS_LogToPhys(const void *log)
RISCOS_LogToPhys
        Push    "r4,r5,r8,r9,lr"
        MOV     r4, a1
        LDR     r8, =L2PT
        BL      logical_to_physical
        MOVCC   a1, r5
        MOVCS   a1, #-1
        Pull    "r4,r5,r8,r9,pc"

a2126 4
        MOV     a1, #InitIRQWs
        LDRB    a1, [a1, #KbdScanActive]
        TEQ     a1, #0
        CallHAL HAL_KbdScanInterrupt,NE
@


1.1.2.19
log
@Huge update to L7200 HAL for Customer M 2 demo - now runs with 5.02 Kernel
used in Tungsten.
Added "fast" flash tool for Customer L, allowing ROMs to be sent serially at
115200 baud not 9600 baud.
Fix to VDU despatch for ARMv4 and later.
Fixes to power on delete keyboard and keyboard timeout
Implemented MemoryReadPhys and MemoryAmounts with the HAL.

Version 5.35, 4.79.2.59. Tagged as 'Kernel-5_35-4_79_2_59'
@
text
@a538 1
        MOV     a3, v6
a562 1
        MOV     a3, v5
a568 1
        STR     a3, [a1, #ROMPhysAddr]
a797 1
        LDR     v4, [v8, #ROMPhysAddr]
a807 1
        STR     v4, [v8, #ROMPhysAddr]
d924 1
a924 1
        MSR     CPSR_c, #F32_bit+SVC2632
@


1.1.2.20
log
@* HAL can choose to limit amount of screen memory to allocate
  [Not fully implemented - for now leaves at least 16MB free if only
  one RAM area; was 1MB].
* Added HAL_USBControllerInfo, HAL_MonitorLeadID and HAL_Video_Render.
* Added HAL->OS call OS_IICOpV.
* OS_MMUControl now allows independent control of I and C bits.
* Added facility to deactivate keyboard debounce (magic word "NoKd" in
  R2 in KeyV 0).
* Fixed problem with RAM amounts not a multiple of 4MB.
* Supremacy bit (in VDU 19) now sets all 8 bits of supremacy.
* Added PaletteV 14 (reads gamma tables).
* Added Supremacy transfer functions (like gamma correction, but for
  supremacy). Allows easy global supremacy effects in a mode-independent
  fashion. Controlled with PaletteV 15,16.
* Added modes 50-53 (320x240, 1,2,4,8bpp). Intended for small LCD.
* Added 13.5kHz versions of TV modes (selected by Hdr:Machine).
* Upped desktop version to 5.06.

Version 5.35, 4.79.2.66. Tagged as 'Kernel-5_35-4_79_2_66'
@
text
@d313 2
a314 2
        TEQ     v8, a4                          ; Was this the only block? If so, leave 16M
        SUBEQS  v6, v6, #16*1024*1024
a904 10
        ; Need to round this up to 4M boundary, as AllocateL2PT only does
        ; individual (1M) sections, rather than 4 at a time, corresponding
        ; to a L2PT page. The following space is available for dynamic areas,
        ; and ChangeDyn.s will get upset if it sees only some out of a set of 4
        ; section entries pointing to the L2PT page.
        ASSERT  FreePoolAddress :MOD: (4*1024*1024) = 0
        ADD     a2, a2, #4*1024*1024
        SUB     a2, a2, #1
        MOV     a2, a2, LSR #22
        MOV     a2, a2, LSL #22
a1859 6
; kernel_oserror *RISCOS_IICOpV(int ndesc, IICDesc *descs)
RISCOS_IICOpV
        Push    "lr"
        BL      IIC_OpV
        MOVVC   a1, #0
        Pull    "pc"
@


1.1.2.21
log
@Now pads its HAL entry table with NullHALEntry if the HAL is providing fewer
entries than the Kernel needs. Helps internal Kernel use of CallHAL, but
doesn't affect external OS_Hardware users.

Version 5.35, 4.79.2.67. Tagged as 'Kernel-5_35-4_79_2_67'
@
text
@a442 2
        CMP     ip, #KnownHALEntries
        MOVLO   ip, #KnownHALEntries
d1887 1
a1887 1
        BCC     %FT20
a1892 8
20      LDR     a1, =ZeroPage                           ; pad table with NullHALEntries
        LDR     a4, =HALWorkspace                       ; in case where HAL didn't supply enough
        ADR     a1, NullHALEntry
30      CMP     a3, a4
        STRHI   a1, [a3, #-4]!
        BHI     %BT30
        MOV     pc, lr

@


1.1.2.22
log
@Merge Cortex kernel into HAL branch
Detail:
  This is a full merge of the Cortex kernel back into the HAL branch. Since the Cortex kernel is/was just a superset of the HAL branch, at this point in time both branches are identical.
  Main features the HAL branch gains from this merge:
  - ARMv6/ARMv7 support
  - High processor vectors/zero page relocation support
  - objasm 4 warning fixes
  - Improved HAL related functionality:
    - Support for HAL-driven RTCs instead of kernel-driven IIC based ones
    - Support for arbitrary size machine IDs
    - Support for multiple IIC busses
    - Support for any HAL size, instead of hardcoded 64k size
    - Probably some other stuff I've forgotten
  - Probably a few bug fixes here and there
Admin:
  Tested on BB-xM & Iyonix.
  Was successfully flashed to ROM on an Iyonix to test the Cortex branch implementation of the 2010 RTC bug fix.
  IOMD build untested - but has been known to work in the past.


Version 5.35, 4.79.2.123. Tagged as 'Kernel-5_35-4_79_2_123'
@
text
@d30 1
a30 27
 [ MEMM_Type = "VMSAv6"
mmuc_init_new
        ; MMUC initialisation flags for ARMv6/ARMv7
        ; This tries to leave the reserved/unpredictable bits untouched, while initialising everything else to what we want
                ; ARMv7MP (probably) wants SW. ARMv6 wants U+XP (which should both be fixed at 1 on ARMv7)
        DCD     MMUC_SW+MMUC_U+MMUC_XP
                ; M+C+W+Z+I+L2 clear to keep MMU/caches off.
                ; A to keep alignment exceptions off (for now at least)
                ; B+EE clear for little-endian
                ; S+R+RR clear to match mmuc_init_old
                ; V+VE clear to keep processor vectors at &0
                ; FI clear to disable fast FIQs (interruptible LDM/STM)
                ; TRE+AFE clear for our VMSAv6 implementation
                ; TE clear for processor vectors to run in ARM mode
        DCD     MMUC_M+MMUC_A+MMUC_C+MMUC_W+MMUC_B+MMUC_S+MMUC_R+MMUC_Z+MMUC_I+MMUC_V+MMUC_RR+MMUC_FI+MMUC_VE+MMUC_EE+MMUC_L2+MMUC_TRE+MMUC_AFE+MMUC_TE
mmuc_init_old
        ; MMUC initialisation flags for ARMv5 and below, as per ARM600 MMU code
                ; Late abort (ARM6 only), 32-bit Data and Program space. No Write buffer (ARM920T
                ; spec says W bit should be set, but I reckon they're bluffing).
                ;
                ; The F bit's tricky. (1 => CPCLK=FCLK, 0=>CPCLK=FCLK/2). The only chip using it was the
                ; ARM700, it never really reached the customer, and it's always been programmed with
                ; CPCLK=FCLK. Therefore we'll keep it that way, and ignore the layering violation.
        DCD     MMUC_F+MMUC_L+MMUC_D+MMUC_P
                ; All of these bits should be off already, but just in case...
        DCD     MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M+MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S
 ]
d38 2
a39 11
        MOVEQ   a3, #0
        ARM_read_control a3, NE
 [ MEMM_Type = "VMSAv6"
        CMP     a1, #ARMv6
        CMPNE   a1, #ARMvF
        ADREQ   a2, mmuc_init_new
        ADRNE   a2, mmuc_init_old
        LDMIA   a2, {a2, lr}
        ORR     a3, a3, a2
        BIC     a3, a3, lr     
 |
d46 1
a46 1
        ORR     a3, a3, #MMUC_F+MMUC_L+MMUC_D+MMUC_P
d48 2
a49 3
        BIC     a3, a3, #MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M
        BIC     a3, a3, #MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S
 ]
d52 1
a52 5
        ARM_write_control a3
        MOV     a2, #0
 [ MEMM_Type = "VMSAv6"
        myISB   ,a2,,y ; Ensure the update went through
 ]
d55 2
a56 7
 [ MEMM_Type = "VMSAv6"
        CMP     a1, #ARMvF
        ; Assume that all ARMvF ARMs have multi-level caches and thus no single MCR op for invalidating all the caches
        MCRNE   ARM_config_cp,0,a2,ARMv4_cache_reg,C7           ; invalidate I+D caches
        BLEQ    HAL_InvalidateCache_ARMvF
 ]
        CMP     a1, #ARMv3
a58 4
 [ MEMM_Type = "VMSAv6"
        myDSB   ,a2,,y
        myISB   ,a2,,y
 ]
d62 2
a63 5
        ORRNE   a3, a3, #MMUC_I
        ARM_write_control a3, NE                                ; whoosh
 [ MEMM_Type = "VMSAv6"
        myISB   ,a2,,y ; Ensure the update went through
 ]
d70 1
a70 1
        MOV     sp, a3
d471 2
a472 2
        LDR     a2, =ZeroPage
  [ ECC
d474 1
a474 1
  |
d476 1
a476 1
  ]
a513 1

d537 1
d598 1
a614 3
 [ MEMM_Type = "VMSAv6"
        LDR     ip, =(AP_ROM * L1_APMult) + L1_Section
 |
a619 1
 ]
d626 1
a626 2
        MOV     ip, a1 ; Remember architecture for later
        
d640 3
a642 3
        MOV     lr, #0
        MCREQ   p15, 0, lr, c5, c0              ; MMU may already be on (but flat mapped)
        MCRNE   p15, 0, lr, c8, c7              ; if HAL needed it (eg XScale with ECC)
a643 16
  [ MEMM_Type = "VMSAv6"
        CMP     ip, #ARMv6
        MCRGE   p15, 0, lr, c2, c0, 2           ; Ensure only TTBR0 is used (v6)
        MCRGT   p15, 0, lr, c12, c0, 0          ; Ensure exception vector base is 0 (Cortex)
        myISB   ,lr,,y
        ORRGE   v5, v5, #MMUC_XP ; Extended pages enabled (v6)
        BICGE   v5, v5, #MMUC_TRE+MMUC_AFE ; TEX remap, Access Flag disabled
        BICGE   v5, v5, #MMUC_EE+MMUC_TE+MMUC_VE ; Exceptions = nonvectored LE ARM
        CMP     ip, #0
  ]
  [ NoUnaligned
        ORR     v5, v5, #MMUC_A ; Alignment exceptions on
  ]
  [ HiProcVecs
        ORR     v5, v5, #MMUC_V ; High processor vectors enabled
  ]
a645 4
  [ MEMM_Type = "VMSAv6"
        MOV     lr, #0
        myISB   ,lr,,y ; Just in case
  ]
d649 2
a650 12
  [ MEMM_Type = "VMSAv6"
        CMP     ip, #ARMvF
        MCRNE   ARM_config_cp,0,lr,ARMv4_cache_reg,C7           ; junk MMU-off contents of I-cache (works on ARMv3)
        MCREQ   p15, 0, lr, c7, c5, 0           ; invalidate instruction cache
        MCREQ   p15, 0, lr, c8, c7, 0           ; invalidate TLBs
        MCREQ   p15, 0, lr, c7, c5, 6           ; invalidate branch predictor
        myISB   ,lr,,y ; Ensure below branch works
        BLEQ    HAL_InvalidateCache_ARMvF       ; invalidate data cache (and instruction+TLBs again!)
  |
        MOV     lr, #0                                          ; junk MMU-off contents of I-cache
        MCR     ARM_config_cp,0,lr,ARMv4_cache_reg,C7           ; (works on ARMv3)
  ]
a677 3
 [ MEMM_Type = "VMSAv6"
        LDR     ip, =(AP_ROM * L1_APMult) + L1_Section
 |
a682 1
 ]
d751 1
d753 1
a753 1
        LDR     a2, =ZeroPage
a754 3
      [ ZeroPage <> 0
        MOV     a2, #0
      ]
d757 1
a757 1
        LDR     a2, =ZeroPage
d768 1
a768 1
        LDR     a1, =ZeroPage
d777 1
a777 7
 [ DebugHALTX
        BL      DebugHALPrint
        = "HAL initialised",0
        ALIGN
 ]

        LDR     a1, =ZeroPage
d793 1
a793 1
        LDR     a1, =ZeroPage+InitIRQWs
d807 2
a808 1
        LDR     a1, [v8, #HAL_StartFlags]
d819 2
a820 1
        LDR     v4, [v8, #MMU_PCBTrans]
d823 1
a823 1
        LDR     v5, [v8, #MMU_PCBTrans]
a902 8
 [ HiProcVecs
        ; Map in DebuggerSpace
        LDR     a1, =DebuggerSpace
        ORR     a2, v5, #AP_Read * L2X_APMult
        LDR     a3, =(DebuggerSpace_Size + &FFF) :AND: &FFFFF000
        BL      Init_MapInRAM
 ]

a965 71
 [ MEMM_Type = "VMSAv6"
HAL_InvalidateCache_ARMvF
        ; Cache invalidation for ARMs with multiple cache levels, used before ARMop initialisation
        ; This function gets called before we have a stack set up, so we've got to preserve as many registers as possible
        ; The only register we can safely change is ip, but we can switch into FIQ mode with interrupts disabled and use the banked registers there
        MRS     ip, CPSR
        MSR     CPSR_c, #F32_bit+I32_bit+FIQ32_mode
        MOV     r9, #0
        MCR     p15, 0, r9, c7, c5, 0           ; invalidate instruction cache
        MCR     p15, 0, r9, c8, c7, 0           ; invalidate TLBs
        MCR     p15, 0, r9, c7, c5, 6           ; invalidate branch target predictor
        myDSB   ,r9,,y                          ; Wait for completion
        myISB   ,r9,,y
        ; Check whether we're ARMv7 (and thus multi-level cache) or ARMv6 (and thus single-level cache)
        MRC     p15, 0, r8, c0, c0, 1
        TST     r8, #&80000000 ; EQ=ARMv6, NE=ARMv7
        MCREQ   ARM_config_cp,0,r9,ARMv4_cache_reg,C7 ; ARMv3-ARMv6 I+D cache flush
        BEQ     %FT50 ; Skip to the end

        ; This is basically the same algorithm as the MaintainDataCache_WB_CR7_Lx macro, but tweaked to use less registers and to read from CP15 directly
        TST     r8, #&07000000
        BEQ     %FT50
        MOV     r11, #0 ; Current cache level
10 ; Loop1
        ADD     r10, r11, r11, LSR #1 ; Work out 3 x cachelevel
        MOV     r9, r8, LSR r10 ; bottom 3 bits are the Cache type for this level
        AND     r9, r9, #7 ; get those 3 bits alone
        CMP     r9, #2
        BLT     %FT40 ; no cache or only instruction cache at this level
        MCR     p15, 2, r11, c0, c0, 0 ; write CSSELR from r11
        myISB   ,r9
        MRC     p15, 1, r9, c0, c0, 0 ; read current CSSIDR to r9
        AND     r10, r9, #&7 ; extract the line length field
        ADD     r10, r10, #4 ; add 4 for the line length offset (log2 16 bytes)
        LDR     r8, =&3FF
        AND     r8, r8, r9, LSR #3 ; r8 is the max number on the way size (right aligned)
        CLZ     r13, r8 ; r13 is the bit position of the way size increment
        LDR     r12, =&7FFF
        AND     r12, r12, r9, LSR #13 ; r12 is the max number of the index size (right aligned)
20 ; Loop2
        MOV     r9, r12 ; r9 working copy of the max index size (right aligned)
30 ; Loop3
        ORR     r14, r11, r8, LSL r13 ; factor in the way number and cache number into r14
        ORR     r14, r14, r9, LSL r10 ; factor in the index number
        MCR     p15, 0, r14, c7, c6, 2 ; Invalidate
        SUBS    r9, r9, #1 ; decrement the index
        BGE     %BT30
        SUBS    r8, r8, #1 ; decrement the way number
        BGE     %BT20
        MRC     p15, 0, r8, c0, c0, 1
40 ; Skip
        ADD     r11, r11, #2
        AND     r14, r8, #&07000000
        CMP     r14, r11, LSL #23
        BGT     %BT10        

50 ; Finished
        ; Wait for clean to complete
        MOV     r8, #0
        myDSB   ,r8,,y
        MCR     p15, 0, r8, c7, c5, 0           ; invalidate instruction cache
        MCR     p15, 0, r8, c8, c7, 0           ; invalidate TLBs
        MCR     p15, 0, r8, c7, c5, 6           ; invalidate branch target predictor
        myDSB   ,r8,,y                          ; Wait for completion
        myISB   ,r8,,y
        ; All caches clean; switch back to SVC, then recover the stored PSR from ip (although we can be fairly certain we started in SVC anyway)
        MSR     CPSR_c, #F32_bit+I32_bit+SVC32_mode
        MSR     CPSR_cxsf, ip
        MOV     pc, lr
 ] ; MEMM_Type = "VMSAv6"

d967 1
a967 1
        LDR     a1, =ZeroPage
a969 1
      [ ZeroPage <> 0
a970 1
      ]
d980 1
a980 1
        LDR     a2, =ZeroPage
d1014 1
a1014 1
        LDR     a1, =ZeroPage
a1044 1
 [ MEMM_Type <> "VMSAv6"
d1047 2
a1048 2
        LDR     a2, =ZeroPage                   ; if we now know that CPU supports them
        LDR     a1, [a2, #ProcessorFlags]
d1052 1
a1053 1
        MOV     lr, #0
d1064 1
a1066 1
 ]
a1080 31
 [ MEMM_Type = "VMSAv6"
        AND     a1, v5, #L2_AP                  ; a1 = access permission
        MOV     a1, a1, LSR #L2_APShift
        ; Map AP_ROM to 0
        CMP     a1, #AP_ROM
        MOVEQ   a1, #0
        ; Now ARM access goes 0 => all R/O, 1 => user none, 2 => user R/O, 3  => user R/W
        ; PPL access goes 0 => user R/W, 1 => user R/O, 2 => user none, (and let's say 3 all R/O)
        RSB     v6, a1, #3                      ; v6 = PPL access
        AND     a1, v5, #2_11                   ; a1 = page type
        CMP     a1, #L2_ExtPage
        ANDHS   a1, v5, #L2_TEX+L2_C+L2_B       ; Extended TEX and CB bits
        ANDLO   a1, v5, #L2_C+L2_B              ; Large CB bits only
        ANDLO   lr, v5, #L2L_TEX                ; Large TEX bits
        ORRLO   a1, a1, lr, LSR #L2L_TEXShift-L2_TEXShift ; Move Large TEX back to Extended TEX position
        MOV     lr, #3                          ; lr = PCB value (funny loop to do NCNB first)
60      LDRB    a3, [ip, lr]                    ; look in XCBTrans table
        TEQ     a3, a1                          ; found a match for our XCB?
        BEQ     %FT70
        TST     lr, #2_11
        SUBNE   lr, lr, #1                      ; loop goes 3,2,1,0,7,6,5,4,...,31,30,29,28
        ADDEQ   lr, lr, #7
        TEQ     lr, #35
        BNE     %BT60
70      AND     a1, lr, #2_00011
        ORR     v6, v6, a1, LSL #4              ; extract NCNB bits
        AND     a1, lr, #2_11100
        ORR     v6, v6, a1, LSL #10             ; extract P bits
        ORR     v6, v6, #PageFlags_Unavailable               ; ???? pages from scratch to cam only?
        STMIA   a2, {v2, v6}                    ; store logical address, PPL
 |
d1107 1
a1107 2
 ]
 
d1193 1
a1193 1
;    a3 = access permissions+C+B bits (bits 11-2 of an L2 extended small page)
a1211 6
 [ MEMM_Type = "VMSAv6"
        ORR     a3, a3, #L2_LargePage           ; else large pages (64K)
        AND     lr, a3, #L2_TEX                 ; extract TEX from ext page flags
        BIC     a3, a3, #L2_TEX                 ; small page TEX bits SBZ for large pages
        ORR     a3, a3, lr, LSL #L2L_TEXShift-L2_TEXShift ; replace TEX in large page position     
 |
a1219 1
 ]
a1236 16
 [ MEMM_Type = "VMSAv6"
Init_MapIn_Sections
        MOVS    ip, v3                          ; is MMU on?
        LDREQ   ip, =L1PT                       ; then use virtual address
        AND     lr, a3, #L2_TEX + L2_AP         ; extract TEX, AP, APX bits (input is extended small page)
        BIC     a3, a3, #L2_TEX + L2_AP         ; and clear them
        ORR     a3, a3, lr, LSL #6              ; put TEX and AP bits back in new position
        ORR     a3, a3, #L1_Section             ; Mark as section
        ORR     a1, a1, a3                      ; Merge with physical address
        ADD     a2, ip, a2, LSR #18             ; a2 -> L1PT entry
70      STR     a1, [a2], #4                    ; And store in L1PT
        ADD     a1, a1, #1024*1024              ; Advance one megabyte
        SUBS    a4, a4, #1024*1024              ; and loop
        BNE     %BT70
        Pull    "pc"
 |
d1262 1
a1262 1
 ]
a1301 1
 [ MEMM_Type <> "VMSAv6"
d1306 1
a1306 1
        LDR     lr, =ZeroPage
a1315 1
 ]
a1403 3
 [ MEMM_Type = "VMSAv6"
        ORR     a3, a1, #L1_Page
 |
a1416 1
 ]
a1464 3
 [ MEMM_Type = "VMSAv6"
        LDR     a4, =(AP_None * L1_APMult) + L1_Section
 |
a1465 1
 ]
a1506 5
 [ MEMM_Type = "VMSAv6"
        CMP     a1, #ARMvF
        MCRNE   ARM_config_cp,0,ip,ARMv4_cache_reg,C7           ; works on ARMv3
        BLEQ    HAL_InvalidateCache_ARMvF
 |
a1507 1
 ]
d1563 1
a1563 1
        LDR     r0,=ZeroPage+InitClearRamWs             ;we can preserve r7-r9,r13 at logical address 52..67
d1565 1
a1565 1
        LDR     r8, =ZeroPage
d1567 1
a1567 1
        LDR     r7, =ZeroPage+PhysRamTable              ; point to 5 lots of (physaddr,size)
d1576 2
a1577 1
15        
d1627 1
a1627 2
        STMNEIA r0!, {r2,r8-r10}
        STMNEIA r0!, {r2,r8-r10}
d1655 1
a1655 1
        LDR     r0, =ZeroPage+InitClearRamWs
d1660 1
a1660 1
        LDR     r0, =ZeroPage+OsbyteVars + :INDEX: LastBREAK
d1666 1
a1666 1
        LDR     r0, =ZeroPage                           ; soft copy
d1719 1
a1719 1
; In:  a1 = flags  (L1_B,L1_C,L1_AP,L1_APX)
a1729 3
 [ MEMM_Type = "VMSAv6"
        ASSERT  L1_AP = 2_100011 :SHL: 10
 |
a1730 1
 ]
a1750 1
        ; For VMSAv6, assume HAL knows what it's doing and requests correct settings for AP_ROM
d1829 1
a1829 1
        LDR     ip, =ZeroPage
d1872 1
a1872 1
; kernel_oserror *RISCOS_IICOpV(IICDesc *descs, uint32_t ndesc_and_bus)
d1923 1
a1923 1
        LDR     ip, =ZeroPage
d1941 1
a1941 1
        LDR     ip, =ZeroPage
d1970 1
a1970 1
        LDR     lr, =ZeroPage
d1979 1
a1979 1
        LDR     lr, =ZeroPage
d2002 1
a2002 1
        LDR     lr, =ZeroPage
d2015 1
a2015 1
        LDR     lr, =ZeroPage
d2023 1
a2023 1
        LDR     lr, =ZeroPage
d2046 1
a2046 1
        LDR     lr, =ZeroPage
d2054 1
a2054 1
        LDR     lr, =ZeroPage
d2112 1
a2112 1
        LDR     a2, =ZeroPage + OsbyteVars + :INDEX: RS423mode
d2116 1
a2116 1
        LDR     a2, =ZeroPage
d2126 1
a2126 1
        LDR     R0, =ZeroPage
d2162 6
a2167 14
        Push    "a1-a4,v1-v2,sb,ip,lr"
        MRS     a1, SPSR
        MRS     a2, CPSR
        ORR     a3, a2, #SVC32_mode
        MSR     CPSR_c, a3
        Push    "a1-a2,lr"
        ; If it's not an IIC interrupt, pass it on to the keyboard scan code
        LDR     v2, =ZeroPage
        AddressHAL v2
        CallHAL HAL_IRQSource
        ADD     v1, v2, #IICBus_Base
        MOV     ip, #0
10
        LDR     a2, [v1, #IICBus_Type]
d2169 5
a2173 12
        BEQ     %FT20
        LDR     a2, [v1, #IICBus_Device]
        CMP     a2, a1
        ADREQ   lr, Reset_IRQ_Exit
        BEQ     IICIRQ
20
        ADD     ip, ip, #1
        ADD     v1, v1, #IICBus_Size
        CMP     ip, #IICBus_Count
        BNE     %BT10
        LDRB    a2, [v2, #InitIRQWs+KbdScanActive]
        TEQ     a2, #0
d2175 3
a2177 10
        ; Keyboard scan code will have return -1 if it handled the IRQ
        ; If it didn't handle it, or keyboard scanning is inactive, something
        ; bad has happened
        CMP     a1, #-1
        CallHAL HAL_IRQDisable,NE ; Stop the rogue device from killing us completely
Reset_IRQ_Exit
        Pull    "a1-a2,lr"
        MSR     CPSR_c, a2
        MSR     SPSR_cxsf, a1
        Pull    "a1-a4,v1-v2,sb,ip,pc",,^
@


1.1.2.23
log
@Add compressed ROM support. Make more use of ARMv5+ instructions. Other misc tweaks.
Detail:
  hdr/OSEntries, s/HAL, s/Kernel - Add compressed ROM support.
  With the current scheme, a compressed ROM will have everything except the HAL and kernel compressed.
  During the keyboard scan period the kernel will allocate some temporary decompression workspace and call the decompression stub that was appended to the ROM.
  The decompression stub is expected to perform in-place decompression of the ROM. Once decompression is complete the workspace will be freed and the page tables updated to make the ROM image readonly.
  It's the HAL's responsibility to make sure any compressed ROM is located in an area of physically contiguous RAM large enough to hold the uncompressed image.
  More info here: http://www.riscosopen.org/wiki/documentation/show/Compressed%20ROMs
  Makefile, h/OSEntries - Add C export of hdr/OSEntries
  hdr/HALDevice - Add device ID for Tungsten video device. Convert tabs to spaces for consistency.
  hdr/HALEntries, s/NewReset - Moved KbdFlag_* definitions to hdr/HALEntries so HALs can use them in their keyboard scan code
  s/ArthurSWIs, S/HAL, s/HeapSort, s/Kernel, s/MemInfo, s/Middle, s/NewIRQs, s/TickEvents, s/vdu/vdugrafb - Make use of BLX, BFI and long multiplies if the CPU supports them. Don't support SWI calls from thumb mode if the CPU doesn't support thumb.
  s/HAL - Made the LDMIA in Init_MapInRAM more sensible (register order was backwards). The old code did work, but wasn't doing what the comments described. Removed unused/unfinished HAL_Write0 function. Improve RISCOS_LogToPhys to check L1PT for any section mappings if the logical_to_physical call fails
  s/ModHand - Save one instruction by using ADR instead of MOV+ADD to compute lr
  s/NewReset, s/PMF/key - Pass L1PT to HAL_Reset to allow machines without hardware reset (e.g. IOMD) to perform resets by manually disabling the MMU and restarting the ROM
  s/vdu/vdudriver, s/vdu/vdugrafv - Use GVEntry macro borrowed from NVidia module for setting up the GraphicsV jump table. Make GraphicsV_ReadPaletteEntry call HAL_Video_ReadPaletteEntry if left unclaimed. Fixup GV_Render to only call HAL_Video_Render if the HAL call is implemented.
Admin:
  Tested with OMAP3, IOMD & Tungsten ROMs/softloads.


Version 5.35, 4.79.2.138. Tagged as 'Kernel-5_35-4_79_2_138'
@
text
@d590 1
a590 4
        LDR     a3, [v5, #OSHdr_DecompressHdr]  ; check if ROM is compressed, and if so, make writeable
        CMP     a3, #0                          
        MOVNE   a3, #(AP_Full * L2X_APMult) + L2_C + L2_B
        MOVEQ   a3, #(AP_ROM * L2X_APMult) + L2_C + L2_B
d616 1
a616 4
        LDR     a3, [v5, #OSHdr_DecompressHdr]
        CMP     a3, #0
        MOVNE   a3, #(AP_Full * L2X_APMult) + L2_C + L2_B
        MOVEQ   a3, #(AP_ROM * L2X_APMult) + L2_C + L2_B
d634 1
a634 1
        LDR     sp_irq, =ScratchSpace + 1024    ; 1K is plenty since Reset_IRQ_Handler now runs in SVC mode
a942 128
; Decompress the ROM
        LDR     a1, =RISCOS_Header
        LDR     a2, [a1, #OSHdr_DecompressHdr]
        CMP     a2, #0
        BEQ     %FT30
        ADD     ip, a1, a2
        ASSERT  OSDecompHdr_WSSize = 0
        ASSERT  OSDecompHdr_Code = 4
        LDMIA   ip, {a3-a4}
        ADRL    a2, SyncCodeAreas
        CMP     a3, #0 ; Any workspace required?
        ADD     a4, a4, ip
   [ DebugHALTX
        BNE     %FT25
        DebugTX "Decompressing ROM, no workspace required"
      [ NoARMv5
        MOV     lr, pc
        MOV     pc, a4
      |
        BLX     a4
      ]
        DebugTX "Decompression complete"
        B       %FT27
25
   |
        ADREQ   lr, %FT27
        MOVEQ   pc, a4
   ]
        Push    "a1-a4,v1-v2,v5-v7"
; Allocate workspace for decompression code
; Workspace is located at a 4MB-aligned log addr, and is a multiple of 1MB in
; size. This greatly simplifies the code required to free the workspace, since
; we can guarantee it will have been section-mapped, and won't hit any
; partially-allocated L2PT blocks (where 4 L1PT entries point to subsections of
; the same L2PT page)
; This means all we need to do to free the workspace is zap the L1PT entries
; and rollback v1 & v2
; Note: This is effectively a MB-aligned version of Init_MapInRAM
        DebugTX "Allocating decompression workspace"
        LDR     v5, =(1<<20)-1
        ADD     v7, a3, v5
        BIC     v7, v7, v5 ; MB-aligned size
        STR     v7, [sp, #8] ; Overwrite stacked WS size
        MOV     v6, #4<<20 ; Current log addr
26
        ADD     v2, v2, v5
        BIC     v2, v2, v5 ; MB-aligned physram
        LDMIA   v1, {a2, a3}
        SUB     a2, v2, a2 ; Amount of bank used
        SUB     a2, a3, a2 ; Amount of bank remaining
        MOVS    a2, a2, ASR #20 ; Round down to nearest MB
        LDRLE   v2, [v1, #8]! ; Move to next bank if 0MB left
        BLE     %BT26
        CMP     a2, v7, LSR #20
        MOVHS   a4, v7
        MOVLO   a4, a2, LSL #20 ; a4 = amount to take
        MOV     a1, v2 ; set up parameters for MapIn call
        MOV     a2, v6
        ORR     a3, v4, #AP_None * L2X_APMult
        SUB     v7, v7, a4 ; Decrease amount to allocate
        ADD     v2, v2, a4 ; Increase physram ptr
        ADD     v6, v6, a4 ; Increase logram ptr
        BL      Init_MapIn
        CMP     v7, #0
        BNE     %BT26
        Pull    "a1-a2,v1-v2" ; Pull OS header, IMB func ptr, workspace size, decompression code
        MOV     a3, #4<<20
        DebugTX "Decompressing ROM"
      [ NoARMv5
        MOV     lr, pc
        MOV     pc, v2
      |
        BLX     v2
      ]
        DebugTX "Decompression complete"
; Before we free the workspace, make sure we zero it
        MOV     a1, #4<<20
        MOV     a2, #0
        MOV     a3, v1
        BL      memset
; Flush the workspace from the cache & TLB so we can unmap it
        MOV     a1, #4<<20
        MOV     a2, v1, LSR #12
        ARMop   MMU_ChangingEntries
; Zero each L1PT entry
        LDR     a1, =L1PT+(4<<2)
        MOV     a2, #0
        MOV     a3, v1, LSR #18
        BL      memset
; Pop our registers and we're done
        Pull    "v1-v2,v5-v7"
        DebugTX "ROM decompression workspace freed"
27
; Now that the ROM is decompressed we need to change the ROM page mapping to
; read-only. The easiest way to do this is to make another call to Init_MapIn.
; But before we can do that we need to work out if the HAL+OS are contiguous in
; physical space. To do this we can just check if the L1PT entry for the OS is a
; section mapping.
        LDR     a1, =L1PT+(ROM>>18)
        LDR     a1, [a1]
        ASSERT  L1_Section = 2
        ASSERT  L1_Page = 1
        TST     a1, #2
; Section mapped, get address from L1PT
        MOVNE   a1, a1, LSR #20
        MOVNE   a1, a1, LSL #20
        MOVNE   a2, #ROM
        MOVNE   a4, #OSROM_ImageSize*1024
        BNE     %FT29
; Page/large page mapped, get address from L2PT
        LDR     a2, =RISCOS_Header
        LDR     a1, =L2PT
        LDR     a1, [a1, a2, LSR #10]
        LDR     a4, [a2, #OSHdr_ImageSize]
        MOV     a1, a1, LSR #12
        MOV     a1, a1, LSL #12
29
        Push    "a2,a4"
        MOV     a3, #(AP_ROM * L2X_APMult) + L2_C + L2_B
        BL      Init_MapIn
; Flush & invalidate cache/TLB to ensure everything respects the new page access
; Putting a flush here also means the decompression code doesn't have to worry
; about IMB'ing the decompressed ROM
        Pull    "a1,a2"
        MOV     a2, a2, LSR #12
        ARMop   MMU_ChangingEntries
        DebugTX "ROM access changed to read-only"
30
d1353 1
a1353 1
;    a3 = length (4K multiple)
d1368 3
a1370 3
10      LDMIA   v1, {v4, ip}                    ; v4 = addr of bank, ip = len
        SUB     v4, v2, v4                      ; v4 = amount of bank used
        SUBS    v4, ip, v4                      ; v4 = amount of bank left
d1406 1
a1406 1
;    a4 = area size (4K multiple)
d1760 8
d2117 1
a2117 1
RISCOS_LogToPhys ROUT
d2123 1
a2123 21
        BCC     %FT10
        ; Try checking L1PT for any section mappings (logical_to_physical only
        ; deals with regular 4K page mappings)
        ; TODO - Add large page support
        LDR     r9, =L1PT
        MOV     r5, r4, LSR #20
        LDR     a1, [r9, r5, LSL #2]
        ASSERT  L1_Section = 2
        EOR     a1, a1, #2
        TST     a1, #3
        MOVNE   a1, #-1
        BNE     %FT10
        ; Apply offset from bits 0-19 of logical addr
      [ NoARMT2
        MOV     a1, a1, LSR #20
        ORR     a1, a1, r4, LSL #12
        MOV     a1, a1, ROR #12
      |
        BFI     a1, r4, #0, #20
      ]  
10
a2186 1
      [ NoARMv5
a2188 3
      |
        BLX     ip
      ]
@


1.1.2.24
log
@Make Mike's macros permanent.
While the HAL and kernel were being split some temporary macros were used for the bits being worked on, after 12 years of use they're probably safe to adopt.
mjsCallHAL -> CallHAL; mjsAddressHAL -> AddressHAL; mjsHAL -> HAL.
OS_VIDCDividerSWI code now always does NoSuchSWI (had been switched out previously).
File vduhint.s no longer assembled (was empty).


Version 5.35, 4.79.2.150. Tagged as 'Kernel-5_35-4_79_2_150'
@
text
@d875 5
a879 1
        DebugTX "HAL initialised"
@


1.1.2.25
log
@Fix failure to boot with exactly 16MB of RAM
With no VRAM, in Kernerl.s.HAL line 370, the less than 16M case sets aside half the RAM as available for video (more than, it uses no more than 32M) but the exactly equals 16M case set aside none.
Add some exports to hdr.HALEntries to define the subreasons to OS_Hardware.

Version 5.35, 4.79.2.154. Tagged as 'Kernel-5_35-4_79_2_154'
@
text
@d367 1
a367 1
        MOVLS   v6, v2, LSR #1                  ; If that overflowed, take half the bank.
d2306 4
a2309 8

        CMP     ip, #OSHW_LookupRoutine
        ASSERT  OSHW_CallHAL < OSHW_LookupRoutine
        BLO     HardwareCallHAL
        BEQ     HardwareLookupRoutine

        CMP     ip, #OSHW_DeviceRemove
        ASSERT  OSHW_DeviceAdd < OSHW_DeviceRemove
d2312 1
a2312 3

        CMP     ip, #OSHW_MaxSubreason
        ASSERT  OSHW_DeviceEnumerate < OSHW_MaxSubreason 
d2316 1
a2316 1
HardwareCallHAL
d2339 1
a2339 1
HardwareLookupRoutine
@


1.1.2.26
log
@  Added OS_Hardware 5
Detail:
  This functions like OS_Hardware 4, but enumerates devices in the
  chronological order of registration.
Admin:
  Builds, but untested.

Version 5.35, 4.79.2.160. Tagged as 'Kernel-5_35-4_79_2_160'
@
text
@d2317 2
a2318 3
        CMP     ip, #OSHW_DeviceEnumerateChrono
        ASSERT  OSHW_DeviceEnumerate < OSHW_DeviceEnumerateChrono
        ASSERT  OSHW_DeviceEnumerateChrono < OSHW_MaxSubreason 
d2320 1
a2320 2
        BEQ     HardwareDeviceEnumerateChrono
        BHI     HardwareBadReason
a2484 27
HardwareDeviceEnumerateChrono
        Push    "r3-r4,lr"
        LDR     lr, =ZeroPage
        LDR     r2, [lr, #DeviceCount]
        LDR     r3, [lr, #DeviceTable]
        SUBS    r4, r2, r1
        MOVLS   r1, #-1
        BLS     %FT90                                   ; if r1 is out of range then exit
        ADD     r3, r3, r4, LSL #2
10      ADD     r1, r1, #1
        LDR     r2, [r3, #-4]!
        LDR     lr, [r2, #HALDevice_Type]
        EOR     lr, lr, r0
        MOVS    lr, lr, LSL #16                         ; EQ if types match
        SUBNES  r4, r4, #1
        BNE     %BT10
        TEQ     lr, #0
        MOVNE   r1, #-1
        BNE     %FT90
        LDR     lr, [r2, #HALDevice_Version]
        MOV     lr, lr, LSR #16
        CMP     lr, r0, LSR #16                         ; newer than our client understands?
        BHI     %BT10
90
        Pull    "r3-r4,lr"
        ExitSWIHandler

@


1.1.2.27
log
@Fix crash in OS_Hardware 4/5 when encountering devices which are newer than the caller supports
Detail:
  s/HAL - Fix crash in OS_Hardware 4/5 when encountering devices which are newer than the caller supports
Admin:
  Untested, but same fix as Pi branch


Version 5.35, 4.79.2.163. Tagged as 'Kernel-5_35-4_79_2_163'
@
text
@a2481 2
        BLS     %FT90
        SUBS    r4, r4, #1
a2482 1
        MOV     r1, #-1
a2508 2
        BLS     %FT90
        SUBS    r4, r4, #1
a2509 1
        MOV     r1, #-1
@


1.1.2.28
log
@Merge with RPi branch
Detail:
  Merge the RPi branch with the HAL branch, ending RPi branch development
  Brief summary of changes brought in:
  * Added HAL_VideoStartupMode to allow the HAL to specify a startup mode for the OS
  * Fixed addresses being sent to GraphicsV_SetDMAAddress being wrong for external framestores (addresses were given as if internal framestore was in use)
  * Add InverseTextTransparency option for limited compile-time support for targets where framebuffer alpha channel is important
  * Fix ConfiguredLanguage for non-Tungsten builds
  * Update ARMv6 CPU detection to read cache parameters from cache type register instead of using KnownCPUTable
  * Add HALDebugHexTX/TX2/TX4 debug routines for writing out numbers via HAL
  * Use HAL_TimerIRQClear when clearing timer 0 interrupt instead of just HAL_IRQClear
  * Initialise FileLangCMOS using defines from Hdr:FSNumbers instead of magic numbers. Use SDFS on M_ARM11ZF.
  * Improved software mouse pointer support; software pointer now removed & restored in some of the same places the text cursor is
  * Improve support for external framestores; driver is now able to grow/shrink/move the framestore on mode changes if bit 5 of GraphicsV_DisplayFeatures R0 is set
  * GraphicsV_FramestoreAddress now has a default claimant which calls HAL_VideoFramestoreAddress
Admin:
  Tested on Raspberry Pi, Iyonix, OMAP3, IOMD


Version 5.35, 4.79.2.165. Tagged as 'Kernel-5_35-4_79_2_165'
@
text
@d2581 19
a2641 62
 [ DebugHALTX
DebugHALPrint
        Push    "a1-a4,v1,sb,ip"
        AddressHAL
        MOV     v1, lr
10      LDRB    a1, [v1], #1
        TEQ     a1, #0
        BEQ     %FT20
        CallHAL HAL_DebugTX
        B       %BT10
20      MOV     a1, #13
        CallHAL HAL_DebugTX
        MOV     a1, #10
        CallHAL HAL_DebugTX
        ADD     v1, v1, #3
        BIC     lr, v1, #3
        Pull    "a1-a4,v1,sb,ip"
        MOV     pc, lr
 ]


 [ DebugHALTX
HALDebugHexTX
       stmfd    r13!, {r0-r3,sb,ip,lr}
       AddressHAL
       b        jbdt1
HALDebugHexTX2
       stmfd    r13!, {r0-r3,sb,ip,lr}
       AddressHAL
       b        jbdt2
HALDebugHexTX4
       stmfd    r13!, {r0-r3,sb,ip,lr}
       AddressHAL
       mov      r0,r0,ror #24          ; hi byte
       bl       jbdtxh
       mov      r0,r0,ror #24
       bl       jbdtxh
jbdt2
       mov      r0,r0,ror #24
       bl       jbdtxh
       mov      r0,r0,ror #24
jbdt1
       bl       jbdtxh
       mov      r0,#' '
       CallHAL  HAL_DebugTX
       ldmfd    r13!, {r0-r3,sb,ip,pc}

jbdtxh stmfd    r13!,{a1,v1,lr}        ; print byte as hex. corrupts a2-a4, ip, assumes sb already AddressHAL'd
       and      v1,a1,#&f              ; get low nibble
       and      a1,a1,#&f0             ; get hi nibble
       mov      a1,a1,lsr #4           ; shift to low nibble
       cmp      a1,#&9                 ; 9?
       addle    a1,a1,#&30
       addgt    a1,a1,#&37             ; convert letter if needed
       CallHAL  HAL_DebugTX
       cmp      v1,#9
       addle    a1,v1,#&30
       addgt    a1,v1,#&37
       CallHAL  HAL_DebugTX
       ldmfd    r13!,{a1,v1,pc}
 ]

@


1.1.2.29
log
@Initialise IIC earlier in the startup sequence. Add ID for Pandora audio HAL device
Detail:
  s/HAL, s/NewReset - Moved IIC initialisation to just after timer initialisation, and crucially, before keyboard scan initialisation. This makes things a lot easier for the HAL if it wants to use IIC during the keyboard scan (previously IIC would be enabled inbetween HAL_KbdScanSetup and the first call to HAL_KbdScan)
  hdr/HALDevice - Added a device ID for the Pandora audio controller
Admin:
  Tested on Pandora


Version 5.35, 4.79.2.168. Tagged as 'Kernel-5_35-4_79_2_168'
@
text
@a892 2
        BL      IICInit

@


1.1.2.30
log
@Add new HAL call, HAL_IRQMax, to allow the kernel to determine the number of IRQ lines/devices at runtime
Detail:
  hdr/HALEntries - Reuse the old HAL_MonitorLeadID call number for HAL_IRQMax
  hdr/KernelWS - Rearrange CursorChunkAddress workspace a bit. Removed unused OldOscliBuffs and a couple of pre-HAL allocations, and made DefIRQ1Vspace the same size for all build configs. Add an IRQMax var to zero page workspace to cache the value returned by HAL_IRQMax.
  s/HAL - Initialise IRQMax shortly after HAL initialisation. Revise ClearPhysRAM comment to reflect which vars are preserved in the current version of the code.
  s/NewIRQs - Strip out a fair bit of pre-HAL code to make the file more readable. Update OS_ClaimDeviceVector/OS_ReleaseDeviceVector to check against IRQMax instead of the MaxInterrupts compile-time limit.
Admin:
  Tested on BB-xM, Iyonix, RiscPC, Pi
  Although the OS will now nominally adapt at runtime to how many IRQ devices there are, it's still using MaxInterrupts as an upper limit as the device claimant table has a fixed memory allocation.


Version 5.35, 4.79.2.182. Tagged as 'Kernel-5_35-4_79_2_182'
@
text
@d877 3
a879 9
        MOV     a1, #64 ; Old limit prior to OMAP3 port
        CallHAL HAL_IRQMax
        CMP     a1, #MaxInterrupts
        MOVHI   a1, #MaxInterrupts ; Avoid catastrophic failure if someone forgot to increase MaxInterrupts
        LDR     a2, =ZeroPage
        STR     a1, [a2, #IRQMax]        

        LDR     v1, [a2, #InitUsedBlock]
        LDR     v2, [a2, #InitUsedEnd]
a909 1
        LDR     v8, [v8, #IRQMax]
a914 2
        MOV     a1, v8
        LDR     v8, =ZeroPage
a917 1
        STR     a1, [v8, #IRQMax]
d1909 6
d1916 1
a1916 1
; out:  r4-r11, r13 preserved
@


1.1.2.31
log
@Clarify reaching up the stack
Not tagged.
@
text
@d151 1
a151 1
        LDR     v4, [sp, #5*4]          ; Get ref
@


1.1.2.32
log
@Teach the kernel about different memory attributes
Detail:
  Briefly, this set of changes:
  * Adjusts PhysRamTable so that it retains the flags passed in by the HAL from OS_AddRAM (by storing them in the lower 12 bits of the size field)
  * Sorts the non-VRAM entries of PhysRamTable by speed and DMA capability, to ensure optimal memory allocation during OS startup.
  * Adjust the initial memory allocation logic to allow the cursor/sound chunk and HAL noncacheable workspace to come from DMA capable memory
  * Extends OS_Memory 12 to accept a 'must be DMA capable' flag in bit 8 of R0. This is the same as available in ROL's OS.
  * Extends OS_DynamicArea 0 to allow the creation of dynamic areas that automatically allocate from DMA capable memory. In ROL's OS this was done by setting bit 12 of R4, but we're using bits 12-14 for specifying the cache policy, so instead bit 15 is used.
  * Fixes OS_ReadSysInfo 6 to return the correct DevicesEnd value now that the IRQ/device limit is computed at runtime
  File changes:
  * hdr/OSEntries - Add definitions of the various flags passed to OS_AddRAM by the HAL. Add a new flag, NoDMA, for memory which can't be used for DMA.
  * hdr/KernelWS - Tidy PhysRamTable definition a bit by removing all the DRAM bank definitions except the first - this makes it easier to search for code which is interacting with the table. Remove VRAMFlags, it's redundant now that the flags are kept in the table. Add DMA allocation info to InitWs.
  * s/AMBControl/memmap - Updated to mask out the flags from PhysRamTable when reading RAM block sizes.
  * s/ARM600 - Strip out a lot of IOMD specific pre-HAL code.
  * s/ChangeDyn - Updated to cope with the flags stored in PhysRamTable. Implement support for DMA-capable dynamic areas. Rewrite InitDynamicAreas to insert pages into the free pool in the right order so that the fastest memory will be taken from it first.
  * s/GetAll, s/Middle - Fix OS_ReadSysInfo 6 to return the correct HAL-specific DevicesEnd value
  * s/HAL - Significant rework of initial RAM allocation code to allow the kernel workspace to come from the fastest DMA incapable RAM, while also allowing allocation of DMA capable memory for HAL NCNB workspace & kernel cursor/sound chunks. ClearPhysRAM rewritten as part of this.
  * s/MemInfo - Updated to cope with the flags stored in PhysRamTable. Add support for the new OS_Memory 12 flag. Update OS_Memory 7 to not assume PhysRamTable entries are sorted in address order, and rip out the old pre-HAL IOMD implementation.
  * s/NewReset - Remove GetPagesFromFreePool option, assume TRUE (as this has been the case for the past 10+ years). Revise a few comments and strip dead code. Update to cope with PhysRamTable flags.
  * s/VMSAv6 - Remove a couple of unused definitions
  * s/vdu/vdudriver - Update to cope with PhysRamTable flags
Admin:
  Tested in Kinetic RiscPC ROM softload, Iyonix softload, & OMAP3


Version 5.35, 4.79.2.186. Tagged as 'Kernel-5_35-4_79_2_186'
@
text
@a135 1
;             bit 2: memory can't be used for DMA (sound, video, or other)
d353 1
a353 1
        TST     v2, #OSAddRAM_IsVRAM            ; Is it VRAM?
d358 1
a358 1
        ; Extract some pseudo-VRAM from first DMA-capable RAM block
d360 1
a360 4
06      LDMIA   v8!, {v1, v2}
        TEQ     v8, a4                          ; End of list?
        TSTNE   v2, #OSAddRAM_NoDMA             ; DMA capable?
        BNE     %BT06
d363 1
a363 13
        ; Is this the only DMA-capable block?
        MOV     v4, v8
        MOV     v6, #OSAddRAM_NoDMA
07      TEQ     v4, a4
        BEQ     %FT08
        LDR     v6, [v4, #4]
        ADD     v4, v4, #8
        TST     v6, #OSAddRAM_NoDMA
        BNE     %BT07
08
        ; v6 has NoDMA set if v8 was the only block
        TST     v6, #OSAddRAM_NoDMA
        MOV     v4, v1                          ; Allocate block as video memory
d365 2
a366 2
        BEQ     %FT09
        SUBS    v6, v6, #16*1024*1024           ; Leave 16M if it was the only DMA-capable block
a367 1
09
d373 1
a373 7
        BEQ     %FT22                           ; pack array tighter if this block is all gone
        STR     v1, [v8, #-8]                   ; update base
        LDR     v1, [v8, #-4]
        MOV     v1, v1, LSL #20
        ORR     v1, v1, v2, LSR #12
        MOV     v1, v1, ROR #20                 ; merge flags back into size
        STR     v1, [v8, #-4]                   ; update size
d387 2
a388 1
30      SUB     v8, a4, v5, LSL #3              ; Rewind to start of list
d390 3
a392 20
        ; Scan forwards to find the fastest block of non-DMAable memory which is at least DRAMOffset_LastFixed size
        LDMIA   v8!, {v1, v2}
31
        TEQ     v8, a4
        BEQ     %FT32
        LDMIA   v8!, {v7, ip}
        CMP     ip, #DRAMOffset_LastFixed
        ANDHS   sp, ip, #&F*OSAddRAM_Speed+OSAddRAM_NoDMA
        ANDHS   lr, v2, #&F*OSAddRAM_Speed+OSAddRAM_NoDMA
        ASSERT  OSAddRAM_Speed = 1:SHL:8
        ASSERT  OSAddRAM_NoDMA < OSAddRAM_Speed
        MOVHS   sp, sp, ROR #8                  ; Give NoDMA flag priority over speed when sorting
        CMPHS   sp, lr, ROR #8
        MOVHI   v1, v7
        MOVHI   v2, ip
        B       %BT31
32        
        ; Fill in the Kernel's permanent memory table, sorting by speed and DMA ability
        ; Non-DMAable RAM is preferred over DMAable, as the kernel requires very little DMAable RAM, and we don't want to permanently claim DMAable RAM if we're not actually using it for DMA (in case machine only has a tiny amount available)
        ADD     ip, v1, #DRAMOffset_PageZero
a397 1
        SUB     v8, a4, v5, LSL #3              ; Rewind to start of list
d400 4
a403 9
        
        ; First put the VRAM information in to free up some regs
        ADD     v7, ip, #VideoPhysAddr
        STMIA   v7!, {v4, v6}

        ; Now fill in the rest
        ASSERT  DRAMPhysAddrA = VideoPhysAddr+8
        STMIA   v7!, {v1, v2}                   ; workspace block must be first
33
d405 10
a414 33
        BEQ     %FT39
        LDMIA   v8!, {v1, v2}
        CMP     v2, #4096                       ; skip zero-length sections
        BLO     %BT33
        ; Perform insertion sort
        ; a1-a3, v3-v6, ip, lr free
        ADD     a1, ip, #DRAMPhysAddrA
        LDMIA   a1!, {a2, a3}
        TEQ     v1, a2
        BEQ     %BT33                           ; don't duplicate the initial block
        AND     v3, v2, #&F*OSAddRAM_Speed+OSAddRAM_NoDMA
        ASSERT  OSAddRAM_Speed = 1:SHL:8
        ASSERT  OSAddRAM_NoDMA < OSAddRAM_Speed
        MOV     v3, v3, ROR #8                  ; Give NoDMA flag priority over speed when sorting
34
        AND     v4, a3, #&F*OSAddRAM_Speed+OSAddRAM_NoDMA
        CMP     v3, v4, ROR #8
        BHI     %FT35
        TEQ     a1, v7
        LDMNEIA a1!, {a2, a3}
        BNE     %BT34
        ADD     a1, a1, #8
35
        ADD     v7, v7, #8
        ; Insert at a1-8, overwriting {a2, a3}
36
        STMDB   a1, {v1, v2}                   ; store new entry
        TEQ     a1, v7
        MOVNE   v1, a2                         ; if not at end, shuffle
        MOVNE   v2, a3                         ; overwritten entry down one,
        LDMNEIA a1!, {a2, a3}                  ; load next to be overwritten,
        BNE     %BT36                          ; and loop
        B       %BT33
a415 1
39
d418 1
a418 2
        ADD     v6, ip, #PhysRamTable
        MOV     a3, v6
d421 1
a421 1
        ADD     a2, a2, v2, LSR #12             ; add on size
a423 1
        MOV     a2, a2, LSL #12
a424 52
        ; Work out how much DMAable RAM the HAL/kernel needs
        LDR     a1, [sp, #8]
        LDR     a1, [a1, #HALDesc_Flags]
        TST     a1, #HALFlag_NCNBWorkspace              ; do they want uncacheable workspace?
        LDRNE   a1, =SoundDMABuffers-CursorChunkAddress + ?SoundDMABuffers + 32*1024 + DRAMOffset_LastFixed
        LDREQ   a1, =SoundDMABuffers-CursorChunkAddress + ?SoundDMABuffers + DRAMOffset_LastFixed
        ; Scan PhysRamTable for a DMAable block of at least this size, extract it, and stash it in InitDMABlock
        ; Once the initial memory claiming is done we can re-insert it
        ADD     a4, a3, #DRAMPhysAddrA-VideoPhysAddr    ; don't claim VRAM
        
        ; First block needs special treatment as we've already claimed some of it
        LDMIA   a4!, {v1, v2}
        TST     v2, #OSAddRAM_NoDMA
        BNE     %FT41
        CMP     v2, a1
        BLO     %FT41
        ; Oh crumbs, the first block is a match for our DMA block
        ; Claim it as normal, but set InitDMAEnd to v1+DRAMOffset_LastFixed so
        ; that the already used bit won't get used for DMA
        ; We also need to be careful later on when picking the initial v2 value
        ADD     lr, v1, #DRAMOffset_LastFixed
        STR     lr, [ip, #InitDMAEnd]
        B       %FT43
41
        ; Go on to check the rest of PhysRamTable
        SUB     a1, a1, #DRAMOffset_LastFixed
42
        LDMIA   a4!, {v1, v2}
        TST     v2, #OSAddRAM_NoDMA
        BNE     %BT42
        CMP     v2, a1
        BLO     %BT42
        ; Make a note of this block
        STR     v1, [ip, #InitDMAEnd]
43
        STR     v1, [ip, #InitDMABlock]
        STR     v2, [ip, #InitDMABlock+4]
        SUB     lr, a4, a3
        STR     lr, [ip, #InitDMAOffset]
        ; Now shrink/remove this memory from PhysRamTable
        SUB     v2, v2, a1
        ADD     v1, v1, a1
        CMP     v2, #4096               ; Block all gone?
        STMHSDB a4, {v1, v2}            ; no, just shrink it
        BHS     %FT55
45
        CMP     a4, v7
        LDMNEIA a4, {v1, v2}
        STMNEDB a4, {v1, v2}
        ADDNE   a4, a4, #8
        BNE     %BT45
        SUB     v7, v7, #8
d443 2
a444 2
        LDR     a4, =DRAMOffset_L1PT+16*1024-(PhysRamTable+DRAMOffset_PageZero) ; offset from a3 to L1PT end
        ADD     a3, a3, a4
a474 7
        ; Detect if the DMA claiming adjusted the first block
        ; If so, we'll need to reset v2 to the start of the block at v1
        LDR     a1, [v1]
        ADD     lr, a1, #DRAMOffset_LastFixed
        TEQ     lr, v2
        MOVNE   v2, a1

d513 1
a513 1
        BLNE    Init_MapInRAM_DMA
a898 2
        DebugTX "IICInit"

a904 2
        DebugTX "HAL_KbdScanSetup"

a914 2
        DebugTX "ClearPhysRAM"

a935 2
        DebugTX "HAL_CleanerSpace"

d1127 1
a1127 1
        BL      Init_MapInRAM_DMA
d1132 1
a1132 1
        BL      Init_MapInRAM_DMA
d1177 2
a1178 1
        STR     v2, [v8, #InitUsedEnd]
a1179 22
        ; Put InitDMABlock back into PhysRamTable
        Push    "v1-v7"
        ASSERT  InitDMAOffset = InitDMABlock+8
        ADD     v1, v8, #InitDMABlock
        LDMIA   v1, {v1-v3}
        ADD     v3, v3, #PhysRamTable
        ADD     v3, v3, v8
        ; Work out whether the block was removed or merely shrunk
        LDMDB   v3, {v4-v5}
        ADD     v6, v1, v2
        ADD     v7, v4, v5
        STMDB   v3, {v1-v2}
        TEQ     v6, v7
        BEQ     %FT40                   ; End addresses match, it was shrunk
35
        LDMIA   v3, {v1-v2}             ; Shuffle following entries down
        STMIA   v3!, {v4-v5}
        MOV     v4, v1
        MOVS    v5, v2
        BNE     %BT35
40
        Pull    "v1-v7"
d1191 1
a1191 1
        STR     ip, [v8, #CamEntriesPointer]
d1312 1
a1312 1
        MOVS    a3, a3, LSR #12                 ; end of list? (size=0)
d1315 2
a1316 2
        CMP     a2, a3, LSL #12                 ; if more than size
        ADDHS   a4, a4, a3, LSL #12             ;   increase counter by size of bank
d1484 1
a1484 2
        MOV     a3, a3, LSR #12
        ADD     a2, a2, a3, LSL #12             ; ip = end of this bank
d1510 1
a1510 2
10      LDMIA   v1, {v4, ip}                    ; v4 = addr of bank, ip = len+flags
        MOV     ip, ip, LSR #12
d1512 1
a1512 1
        RSBS    v4, v4, ip, LSL #12             ; v4 = amount of bank left
a1539 45
; Allocate and map a physically contigous chunk of some DMAable RAM.
;
; On entry:
;    a1 = logical address
;    a2 = access permissions (see Init_MapIn)
;    a3 = length (4K multiple)
;    v1 -> current entry in PhysRamTable
;    v2 = next physical address
;    v3 -> L1PT
;
; On exit:
;    a1 -> physical address of start of RAM (deduce the rest from PhysRamTable)
;
; Use this routine with caution - correct total amount of required DMA memory
; must have been calculated beforehand and stashed in InitDMABlock
Init_MapInRAM_DMA ROUT
        Push    "a1,a3,v4-v5,ip,lr"
        TEQ     v3, #0                          ; MMU on?
        LDREQ   v4, =ZeroPage                   ; get workspace directly
        ADDNE   v4, v3, #DRAMOffset_PageZero-DRAMOffset_L1PT ; deduce from L1PT
        LDR     v5, [v4, #InitDMAEnd]
        ADD     lr, v5, a3                      ; claim the RAM
        STR     lr, [v4, #InitDMAEnd]

        MOV     a4, a3
 [ ECC
        ORR     a3, a2, #1:SHL:31
 |
        MOV     a3, a2
 ]
        MOV     a2, a1
        MOV     a1, v5
        BL      Init_MapIn                      ; map it in
        ; DMA regions won't get cleared by ClearPhysRam, so do it manually
        ; Could potentially skip this if the HAL says RAM is already clear, but
        ; for now do it anyway (especially since startup flags haven't been set
        ; when we're first called)
        Pull    "a1,a3"
        TEQ     v3, #0
        MOVNE   a1, v5
        MOV     a2, #0
        BL      memset        
        MOV     a1, v5
        Pull    "v4-v5,ip,pc"

d1911 7
a1917 11
; We also have to avoid anything between InitUsedStart and InitUsedEnd - i.e.
; the page tables, HAL workspace, etc.
;
; Note that zero page workspace isn't included in InitUsedStart/InitUsedEnd.
; Sensitive areas of it (e.g. PhysRamTable, IRQ vector) are skipped via the
; help of RamSkipTable

; We don't have to worry about trampling on the ROM image as it's already been
; excluded from PhysRamTable. We also don't have to worry about skipping the
; special DMA block, because at this point in time that won't be listed in
; PhysRamTable either.
a1928 1
   [ {FALSE} ; Not used at the moment
a1931 1
   ]
d1940 1
a1940 1
        LDR     r0,=ZeroPage+InitClearRamWs             ;we can preserve r4-r11,lr in one of the skipped regions
d1942 3
a1944 6

        ; Start off by clearing zero page + scratch space, as these:
        ; (a) are already mapped in and
        ; (b) may require the use of the skip table
        LDR     r0, =ZeroPage
        ADD     r1, r0, #16*1024
d1946 1
a1946 2
        MSR     CPSR_c, #F32_bit+FIQ32_mode             ; switch to our bank o'zeros
        LDR     r5, [r6], #4                            ; load first skip addr
d1948 34
a1981 19
        TEQ     r0, r1
        TEQNE   r0, r5
        STMNEIA r0!, {r8-r11}
        STMNEIA r0!, {r8-r11}
        BNE     %BT10
        TEQ     r0, r1
        BEQ     %FT20
        LDR     r5, [r6], #4                            ; load skip amount
        ADD     r0, r0, r5                              ; and skip it
        LDR     r5, [r6], #4                            ; load next skip addr
        B       %BT10
20
        LDR     r0, =ScratchSpace
        ADD     r1, r0, #ScratchSpaceSize
30
        TEQ     r0, r1
        STMNEIA r0!, {r8-r11}
        STMNEIA r0!, {r8-r11}
        BNE     %BT30
a1982 6
        ; Now walk PhysRamTable and clear everything else, except for the stuff
        ; between InitUsedStart and InitUsedEnd.
        ;
        ; To skip these areas properly, we convert their addresses to physical
        ; page numbers. This is because PhysRamTable isn't guaranteed to be in
        ; ascending address order.
d1984 2
a1985 22
        MSR     CPSR_c, #F32_bit+SVC32_mode
        LDR     r9, =ZeroPage
        LDR     r5, [r9, #InitUsedStart]
        SUB     r0, r5, #DRAMOffset_L1PT                ; Scratch space + zero page are already cleared, so add them to InitUsedStart..InitUsedEnd
        BL      PhysAddrToPageNo
        ; If the DMA region was taken from the first RAM block, we won't be able to look up the page number
        ; Instead, let's use the first DRAM page for InitUsedStart - as this will correspond to the first page that isn't hidden inside the DMA region
        CMP     r0, #-1
        LDREQ   r0, [r9, #DRAMPhysAddrA]
        BLEQ    PhysAddrToPageNo
        MOV     r5, r0
        LDR     r0, [r9, #InitUsedEnd]
        BL      PhysAddrToPageNo
        SUB     r6, r0, #1
        ADD     r9, r9, #PhysRamTable
        MOV     r4, #0                                  ; current page no
        LDMIA   r9!, {r10, r11}
        MOV     r11, r11, LSR #12                       ; get rid of flags
50
        ; Map in this area
        MOV     r0, #L1_B
        MOV     r1, r10
d1988 8
a1995 2
        ; Inner loop will process one page at a time to keep things simple
        MOV     r3, r11
d1997 10
a2006 9
60
        CMP     r4, r5
        CMPHS   r6, r4
        BHS     %FT80
        ; Clear this page
        ADD     r1, r0, #4096
70
        STMDB   r1!, {r8-r11}
        STMDB   r1!, {r8-r11}
d2008 6
a2013 7
        BNE     %BT70
80
        ADD     r0, r0, #4096                           ; increment log addr
        ADD     r4, r4, #1                              ; increment page no
        SUBS    r3, r3, #1                              ; decrement length
        MOVNES  r1, r0, LSL #12                         ; check for MB limit
        BNE     %BT60
d2015 11
a2025 1
        MSR     CPSR_c, #F32_bit+SVC32_mode
d2027 3
a2029 6
        ADD     r10, r10, r11, LSL #12                  ; r10+(r11-r3) = next MB
        MOVS    r11, r3                                 ; next block needed? also resets r11 ready for next pass
        SUBNE   r10, r10, r3, LSL #12
        LDMEQIA r9!, {r10, r11}                         ; grab next block if necessary
        MOVEQS  r11, r11, LSR #12                       ; anything left to do?
        BNE     %BT50
d2031 1
d2051 5
d2057 8
a2064 2
        MakeSkipTable $addr, $size
        &       $addr, $size
d2069 1
d2071 2
d2077 2
a2078 2
        MakeSkipTable   ZeroPage, InitWsEnd
        MakeSkipTable   ZeroPage+SkippedTables, SkippedTablesEnd-SkippedTables
@


1.1.2.33
log
@Allow bursting during ClearPhysRAM on StrongARM
The StrongARM TRM (and hints from ARM600.s revision 4.3.2.2) show that the StrongARM will only do burst writes to memory marked as C=1 B=1, but by default RISCOS_AccessPhysicalAddress only allows bufferable.
So, checking for StrongARM first, two extra snippets are enabled - first mark as C=1 B=1, then afterwards clean the cache before moving onto the next 1MB.

On a StrongARM Kinetic these burst writes improve the RAM clear from ~60ms per MB to 40ms per MB. For a 256MB SODIMM that's over 5s knocked off the boot time.
Other memory configurations will be similarly improved, though 256MB is of course the maximum the motherboard can hold.

Tested in ROM on a Risc PC with StrongARM and ARM710.

Version 5.35, 4.79.2.189. Tagged as 'Kernel-5_35-4_79_2_189'
@
text
@d1044 6
a1049 5
        LDR     ip, =ZeroPage
        LDR     v4, [ip, #ROMPhysAddr]
        LDR     v5, [ip, #RAMLIMIT]
        LDR     v7, [ip, #MaxCamEntry]
        LDR     v8, [ip, #IRQMax]
d1051 2
a1052 1
        LDR     a1, [ip, #HAL_StartFlags]
d1054 3
a1056 2
        BLEQ    ClearPhysRAM            ; Only clear the memory if the HAL didn't
         
d1058 6
a1063 5
        LDR     ip, =ZeroPage
        STR     v4, [ip, #ROMPhysAddr]
        STR     v5, [ip, #RAMLIMIT]
        STR     v7, [ip, #MaxCamEntry]
        STR     v8, [ip, #IRQMax]
d1066 2
d1069 2
a1070 4
        MOV     v8, ip
        LDR     ip, [v8, #MMU_PCBTrans]
        LDRB    v4, [ip, #XCB_CB]
        LDRB    v5, [ip, #XCB_NC]
d2043 1
a2043 3
RISCOS_AccessPhysicalAddress ROUT
        AND     a1, a1, #L1_B                           ; user can ask for bufferable
RISCOS_AccessPhysicalAddressUnchecked                   ; well OK then, I trust you know what you're doing
a2049 1
        ORRNE   a1, a1, #L1_P
d2057 4
a2154 2
        DebugTX "ClearPhysRAM"

a2211 6
      [ StrongARM
        ARM_read_ID r2
        AND     r2, r2, #&F000
        CMP     r2, #&A000
        ORREQ   r0, r0, #L1_B+L1_C                      ; StrongARM can't burst to B=1 C=0
      ]
d2214 1
a2214 2
        BL      RISCOS_AccessPhysicalAddressUnchecked

a2237 15
      [ StrongARM
        ; If it was B=1 C=1 for StrongARM, clean the cache before the 1MB window closes.
        ; We're careful to only ever write the PhysicalAccess area so only a clean is needed,
        ; no flush. The next user of PhysicalAccess will restore everything to bufferable.
        ARM_read_ID r2
        AND     r2, r2, #&F000
        CMP     r2, #&A000
        BNE     %FT90
85
        SUB     r0, r0, #32                             ; rewind 1 cache line
        ARMA_clean_DCentry r0
        MOVS    r1, r0, LSL #12                         ; start of the MB?
        BNE     %BT85
90
      ]
@


1.1.2.34
log
@Clean up of RISCOS_IICOpV
No longer returns a RISC OS specific error block to the HAL, instead uses a platform agnostic IICStatus value.

Version 5.35, 4.79.2.196. Tagged as 'Kernel-5_35-4_79_2_196'
@
text
@d2492 2
a2493 2
; int RISCOS_IICOpV(IICDesc *descs, uint32_t ndesc_and_bus)
RISCOS_IICOpV ROUT
d2496 1
a2496 10
        MOVVC   a1, #IICStatus_Completed
        Pull    "pc", VC
        ; Map from RISC OS error numbers to abstract IICStatus return values
        LDR     a1, [a1]
        LDR     lr, =ErrorNumber_IIC_NoAcknowledge
        SUB     a1, a1, lr              ; 0/1/2 = NoAck/Error/Busy
        CMP     a1, #3
        MOVCS   a1, #3                  ; 3+ => unknown, either way it's an Error
        ADR     lr, %FT10
        LDRB    a1, [lr, a1]
d2498 1
a2498 6
10
        ASSERT    (ErrorNumber_IIC_Error - ErrorNumber_IIC_NoAcknowledge) = 1
        ASSERT    (ErrorNumber_IIC_Busy - ErrorNumber_IIC_NoAcknowledge) = 2
        DCB       IICStatus_NoACK, IICStatus_Error, IICStatus_Busy, IICStatus_Error
        ALIGN
        
@


1.1.2.35
log
@  Fix long-standing, insidious memory management bug
Detail:
  Early in OS startup, certain kernel memory areas (zero page, the privileged
  mode stacks, system heap, etc) are initially allocated only in the L2PT.
  The soft CAM is initialised later, using the L2PT as it is at that point.
  However, the pointer to the table that maps from page table cache policy
  bit layouts for the current CPU back to the platform-independent
  (OS_DynamicArea style) flags was corrupted, meaning that the CAM entries
  for these memory areas were initialised with a default value, which was
  non-cacheable non-bufferable. Application slots and most dynamic areas were
  unaffected, because once enough of the kernel was initialised to be able to
  use AMBControl or OS_ChangeDynamicArea, this function was no longer used.
  The problem comes when those pages come to be remapped; this could be due
  to requests for specific physical RAM pages, or (what I was actually
  investigating) DDT changing the access permissions on them. The problem was
  that as far as the CPU was concerned, the pages were cacheable/bufferable,
  but BangCam examined the soft CAM to decide what cache management
  operations to perform, and so got it wrong. The subsequent poke of the L2PT
  resulted in undefined CPU behaviour; in particular it seems to cause L2
  caches to throw a strop (enough so that disabling the L2 cache was enough
  to make DDT significantly more reliable).
Admin:
  It looks like this bug has been present on all HAL versions of RISC OS.
  Tested with DDT on a Beagleboard, previously the most crashy platform.
  There remains an IRQ (and FIQ) hole in OS_SetMemMapEntries when changing
  permissions on the page containing the processor vectors, which I haven't
  attempted to fix. Arguably, it should also issue Service_PagesSafe/Unsafe,
  in case anyone is DMAing to/from the remapped pages.

Version 5.35, 4.79.2.212. Tagged as 'Kernel-5_35-4_79_2_212'
@
text
@a1549 1
        Push    "ip"
a1550 1
        Pull    "ip"
@


1.1.2.36
log
@  Correction to code to add GraphicsV_PixelFormats call to
  HAL graphics driver calls.  Added further debug capability
Detail:
  Added additional HAL call. minor code correction in hal graphicsv dispatcher
  Added DebugReg macro to aid debugging
Admin:
  (highlight level of testing that has taken place)
  (bugfix number if appropriate)


Version 5.35, 4.79.2.215. Tagged as 'Kernel-5_35-4_79_2_215'
@
text
@d2874 1
a2874 1
;        CallHAL HAL_DebugTX
d2876 1
a2876 1
;        CallHAL HAL_DebugTX
d2885 2
a2886 6
DebugHALPrintReg ; Output number on top of stack to the serial port
        Push    "a1-a4,v1-v4,sb,ip,lr"   ; this is 11 regs
        LDR     v2, [sp,#11*4]           ; find TOS value on stack
        ADR     v3, hextab
        MOV     v4, #8
05
d2888 19
a2906 1
10      LDRB    a1, [v3, v2, LSR #28]
d2908 9
a2916 4
        MOV     v2, v2, LSL #4
        SUBS    v4, v4, #1
        BNE     %BT10
        MOV     a1, #13
d2918 3
a2920 1
        MOV     a1, #10
d2922 2
a2924 51
        Pull    "a1-a4,v1-v4,sb,ip,lr"
        ADD     sp, sp, #4
        MOV     pc, lr

hextab  DCB "0123456789abcdef"


 ]
;
;
; [ DebugHALTX
;HALDebugHexTX
;       stmfd    r13!, {r0-r3,sb,ip,lr}
;       AddressHAL
;       b        jbdt1
;HALDebugHexTX2
;       stmfd    r13!, {r0-r3,sb,ip,lr}
;       AddressHAL
;       b        jbdt2
;HALDebugHexTX4
;       stmfd    r13!, {r0-r3,sb,ip,lr}
;       AddressHAL
;       mov      r0,r0,ror #24          ; hi byte
;       bl       jbdtxh
;       mov      r0,r0,ror #24
;       bl       jbdtxh
;jbdt2
;       mov      r0,r0,ror #24
;       bl       jbdtxh
;       mov      r0,r0,ror #24
;jbdt1
;       bl       jbdtxh
;       mov      r0,#' '
;       CallHAL  HAL_DebugTX
;       ldmfd    r13!, {r0-r3,sb,ip,pc}
;
;jbdtxh stmfd    r13!,{a1,v1,lr}        ; print byte as hex. corrupts a2-a4, ip, assumes sb already AddressHAL'd
;       and      v1,a1,#&f              ; get low nibble
;       and      a1,a1,#&f0             ; get hi nibble
;       mov      a1,a1,lsr #4           ; shift to low nibble
;       cmp      a1,#&9                 ; 9?
;       addle    a1,a1,#&30
;       addgt    a1,a1,#&37             ; convert letter if needed
;       CallHAL  HAL_DebugTX
;       cmp      v1,#9
;       addle    a1,v1,#&30
;       addgt    a1,v1,#&37
;       CallHAL  HAL_DebugTX
;       ldmfd    r13!,{a1,v1,pc}
; ]
;
@


1.1.2.37
log
@Fix corrupt L2PT page flags being generated on Iyonix
Detail:
  s/ARMops - If extended pages aren't supported, make sure we use a PCBTrans table which doesn't use L2_X, otherwise the AP flags for some of the sub-pages will be corrupted when the PCB flags get merged in. Add some more comments to the PCBTrans tables so it's easier to see what the different columns are.
  s/ARM600 - Fix BangCam to use extended pages if they're supported; otherwise (assuming ARMops has selected the right PCBTrans table) we'll end up corrupting the AP flags again
  s/HAL - Fix ConstructCAMfromPageTables using the wrong register for ZeroPage when looking up MMU_PCBTrans. Correct a few comments.
Admin:
  Tested on Iyonix
  Page table examination now shows that all subpages have the correct (i.e. identical) AP flags. Previously some pages would have incorrect access (e.g. every 4th subpage in some FileCore disc map/dir buffer DAs were writable in user mode)
  ARMops fix will presumably mean extended pages will now work correctly on IOP 80200, as before it would have been using small pages with corrupt AP flags


Version 5.35, 4.79.2.221. Tagged as 'Kernel-5_35-4_79_2_221'
@
text
@d1505 1
a1505 1
30      LDR     v5, [v3, v2, LSR #18]           ; v5 = first level descriptor
d1514 1
a1514 1
40      LDR     v5, [v4, v2, LSR #10]           ; v5 = second level descriptor
d1527 1
a1527 1
        LDR     a1, [a2, #MMU_PCBTrans]         ; reprocess C and B bits as per XCB table
@


1.1.2.38
log
@Add OS_Memory 24 implementation. Change OS_ValidateAddress to use it. Fix kernel leaving the physical access MB in a messy state. Try and protect against infinite abort loops caused by bad environment handlers.
Detail:
  s/MemInfo - Added an implementation of ROL's OS_Memory 24 call. Unlike the old OS_ValidateAddress call, this call should successfully report the presence of all memory areas known to the kernel. It should also correctly indicate which parts of a sparse DA are mapped in, unlike the old OS_ValidateAddress implementation.
  s/ChangeDyn - Update dynamic area handling to construct a lookup table for mapping logical addresses to dynamic areas; this is used by OS_Memory 24 to quickly locate which DA(s) hit a given region
  s/AMBControl/main - Make sure lazy task swapping is marked as disabled when AMB_LazyMapIn is {FALSE} - required so that OS_Memory 24 will give application space the correct flags
  s/ArthurSWIs - Switch OS_ValidateAddress over to using OS_Memory 24, as per ROL. For compatibility, Service_ValidateAddress is still issued for any areas which the kernel doesn't recognise (currently, OS_Memory 24 doesn't issue any service calls itself)
  s/Convrsions - ADR -> ADRL to keep things happy
  s/HAL - Fix L2PT page allocation and RAM clear to release the physical access region once they're done with it
  s/Kernel - Make the error dispatcher validate the error handler code ptr & error buffer using OS_Memory 24 before attempting to use them. If they look bad, reset to default. Should prevent getting stuck in an infinite abort loop in some situations (e.g. as was the case with ticket 279). The system might not fully recover, but it's better than a hard crash.
  s/Middle - Rework data/prefetch/etc. abort handlers so that DumpyTheRegisters can validate the exception dump area via OS_Memory 24 before anything gets written to it. Should also help to prevent some infinite abort loops. Strip 26bit/pre-HAL code to make things a bit more readable.
  hdr/KernelWS - Update comment
Admin:
  Tested on BB-xM, Raspberry Pi


Version 5.35, 4.79.2.222. Tagged as 'Kernel-5_35-4_79_2_222'
@
text
@d2015 1
a2015 2
        SUBEQ   sp, sp, #4
        MOVEQ   a3, sp
a2021 4
        TEQ     v3, #0
        LDREQ   a1, [sp], #4
        BLEQ    RISCOS_ReleasePhysicalAddress

a2265 3
        MOV     a1, #L1_Fault
        BL      RISCOS_ReleasePhysicalAddress           ; reset to default

@


1.1.2.39
log
@Fix wildly inaccurate sizes in PhysRamTable when split_block needed
When Subtractv1v2fromRAMtable is called to remove a region that results in one of the RAM blocks being split in the middle the resulting size was incorrect.
The shuffle up loop was reusing v6 as an iterator not realising that it's needed to calculate the size of the 2nd half later, the error introduced was the difference between the physical address where PhysRamTable is located and the block being split - these could be a long way apart for example when there are two SDRAM banks.
Even if the PhysRamTable is nearby (eg. 1 SDRAM bank) the result would be some weird sized entries which ultimately mean some dynamic area address space is "leaked".

Fixed by swapping to v7, and for symmetry also adjusted the shuffle down loop to match.

Version 5.35, 4.79.2.224. Tagged as 'Kernel-5_35-4_79_2_224'
@
text
@d285 6
a290 6
        MOV     v7, v8
20      TEQ     v7, a4                  ; shuffle down subsequent blocks in table
        LDMNEIA v7, {v3, v4}
        STMNEDB v7, {v3, v4}
        ADDNE   v7, v7, #8
        BNE     %BT20
d309 5
a313 5
        MOV     v7, a4
30      TEQ     v7, v8                  ; shuffle up subsequent blocks in table
        LDMNEDB v7, {v3, v4}
        STMNEIA v7, {v3, v4}
        SUBNE   v7, v7, #8
@


1.1.2.40
log
@Improve ClearPhysRAM performance
Detail:
  s/HAL - Change ClearPhysRAM to always map in memory as cacheable + bufferable instead of only on StrongARM, as it's an optimisation that can help other platforms as well.
Admin:
  Tested on BB-xM, StrongARM RiscPC


Version 5.35, 4.79.2.231. Tagged as 'Kernel-5_35-4_79_2_231'
@
text
@d2141 1
d2145 1
d2214 8
a2221 5
        ; Map in this area, cacheable + bufferable to ensure burst writes are
        ; performed. We're careful to not partially overwrite any pages which
        ; are being used, so this shouldn't cause any issues due to being
        ; cachable + potentially doubly mapped.
        MOV     r0, #L1_B+L1_C
a2228 1
        MOV     r2, #0
a2231 1
        ADD     r1, r0, #4096
d2234 1
d2236 2
a2237 2
        STMIA   r0!, {r2,r8-r14}
        STMIA   r0!, {r2,r8-r14}
d2241 1
a2241 1
        MOV     r0, r1                                  ; increment log addr
a2248 1
        ; Clean & invalidate the cache before the 1MB window closes
d2250 3
a2252 3
        ; StrongARM requires special clean code, because we haven't mapped in
        ; DCacheCleanAddress yet. Cheat and only perform a clean, not full
        ; clean + invalidate (should be safe as we've only been writing)
a2261 1
        B       %FT91
a2263 3
        ARMop Cache_CleanInvalidateAll
91

a2294 2
        ASSERT  ($addr :AND: 31) = 0
        ASSERT  ($size :AND: 31) = 0
@


1.1.2.41
log
@Another fix to split_block
Following hot on the heels of revision 1.1.2.39, when there's more than one block in existance the shuffle up loop trashes v3 & v4, which we need in the calculation just below.
Could just use other registers in the shuffle loop, but we only have ip free at that point, so be lazy and just reload & reextract the flags.
Tested on a softload Kinetic, now the RAM speed flags look sensible and the RAM clear doesn't fall off the end.

Version 5.35, 4.79.2.244. Tagged as 'Kernel-5_35-4_79_2_244'
@
text
@d311 1
a311 1
        LDMDB   v7, {v3, v4}
a318 2
        MOV     v4, v4, LSL #20         ; (re)extract flags

@


1.1.2.42
log
@Add workaround for Cortex-A7 errata 814220
Detail:
  s/ARMops, s/HAL - Errata 814220 states that the Cortex-A7 set/way cache maintenance operations violate the usual operation ordering rules, such that an L2 maintenance operation which is started after an L1 operation may actually complete before it, causing data corruption if the L1 data was to be evicted to the L2 entry. Implement the suggested workaround of performing a DSB when switching cache levels, rather than just at the end of the combined L1+L2 group of operations.
Admin:
  Tested on Raspberry Pi 2


Version 5.35, 4.79.2.257. Tagged as 'Kernel-5_35-4_79_2_257'
@
text
@a1417 1
        myDSB   ,r8        ; Cortex-A7 errata 814220: DSB required when changing cache levels when using set/way operations. This also counts as our end-of-maintenance DSB.
d1428 1
@


1.1.2.43
log
@Ensure IO memory is marked as non-executable
Detail:
  s/HAL - The VMSAv6/v7 memory model allows speculative instruction fetches from any memory (including device/strongly-ordered), unless the memory is marked as non-executable. So to prevent interference with read-sensitive devices we must make sure all appropriate IO memory is marked as non-executable.
Admin:
  Tested on IGEPv5
  Fixes data corruption seen when reading from SD card


Version 5.35, 4.79.2.265. Tagged as 'Kernel-5_35-4_79_2_265'
@
text
@d2061 1
a2061 1
        LDR     a4, =(AP_None * L1_APMult) + L1_Section + L1_XN
d2065 1
a2065 1
        ORR     a1, a4, a1                              ; a1 = flags for 1st level descriptor
a2373 3
 [ MEMM_Type = "VMSAv6"
        ORR     a1, a1, #L1_XN                          ; force non-executable to prevent speculative instruction fetches
 ]
@


1.1.2.44
log
@Improve support for VMSAv6 cache policies & memory types. Expose raw ARMops via OS_MMUControl & cache information via OS_PlatformFeatures.
Detail:
  Docs/HAL/ARMop_API - Document two new ARMops: Cache_Examine and IMB_List
  hdr/KernelWS - Shuffle workspace round a bit to allow space for the two new ARMops. IOSystemType now deleted (has been deprecated and fixed at 0 for some time)
  s/ARM600 - Cosmetic changes to BangCam to make it clearer what's going on. Add OS_MMUControl 2 (get ARMop) implementation.
  s/ARMops - Switch out different ARMop implementations and XCB tables depending on MMU model - helps reduce assembler warnings and make it clearer what code paths are and aren't possible. Add implementations of the two new ARMops. Simplify ARM_Analyse_Fancy by removing some tests which we know will have certain results. Use CCSIDR constants in ARMv7 ARMops instead of magic numbers. Update XCB table comments, and add a new table for VMSAv6
  s/ChangeDyn - Define constant for the new NCB 'idempotent' cache policy (VMSAv6 normal, non-cacheable memory)
  s/HAL - Use CCSIDR constants instead of magic numbers. Extend RISCOS_MapInIO to allow the TEX bits to be specified.
  s/Kernel - OS_PlatformFeatures 33 (read cache information) implementation (actually, just calls through to an ARMop)
  s/MemInfo - Modify VMSAv6 OS_Memory 0 cache/uncache implementation to use the XCB table instead of modifying L2_C directly. This allows the cacheability to be changed without affecting the memory type - important for e.g. unaligned accesses to work correctly. Implement cache policy support for OS_Memory 13.
  s/Middle - Remove IOSystemType from OS_ReadSysInfo 6.
  s/VMSAv6 - Make sure BangCam uses the XCB table for working out the attributes of temp-uncacheable pages instead of manipulating L2_C directly. Add OS_MMUControl 2 implementation.
  s/AMBControl/memmap - Update VMSAv6 page table pokeing to use XCB table
  s/PMF/osinit - Remove IOSystemType reference, and switch out some pre-HAL code that was trying to use IOSystemType.
Admin:
  Tested on Iyonix, ARM11, Cortex-A7, -A8, -A9, -A15
  Note that contrary to the comments in the source the default NCB policy currently maps to VMSAv6 Device memory type (as per previous kernel versions). This is just a temporary measure, and it will be switched over to Normal, non-cacheable once appropriate memory barriers have been added to the affected IO code.


Version 5.35, 4.79.2.273. Tagged as 'Kernel-5_35-4_79_2_273'
@
text
@d1401 1
a1401 1
        AND     r10, r9, #CCSIDR_LineSize_mask ; extract the line length field
d1403 2
a1404 2
        LDR     r8, =CCSIDR_Associativity_mask:SHR:CCSIDR_Associativity_pos
        AND     r8, r8, r9, LSR #CCSIDR_Associativity_pos ; r8 is the max number on the way size (right aligned)
d1406 2
a1407 2
        LDR     r12, =CCSIDR_NumSets_mask:SHR:CCSIDR_NumSets_pos
        AND     r12, r12, r9, LSR #CCSIDR_NumSets_pos ; r12 is the max number of the index size (right aligned)
d2329 1
a2329 3
; In:  a1 = flags  (L1_B,L1_C,L1_TEX)
;           bit 20 set if doubly mapped
;           bit 21 set if L1_AP specified (else default to AP_None)
a2341 1
        ASSERT  L1_TEX = 2_111 :SHL: 12
a2343 1
        ASSERT  L1_TEX = 2_1111 :SHL: 12
d2350 2
a2351 1
        LDR     v7, =L1_B:OR:L1_C:OR:L1_AP:OR:L1_TEX    ; v7 = user-specifiable flags
d2373 1
a2373 1
        AND     a1, a1, v7                              ; mask out unsupported attributes
@


1.1.2.45
log
@Fix DebuggerSpace page to be cacheable
Detail:
  s/HAL - A typo seems to have resulted in the HiProcVecs DebuggerSpace page being mapped in as NCB instead of CB. Fix it.
Admin:
  Untested.


Version 5.35, 4.79.2.274. Tagged as 'Kernel-5_35-4_79_2_274'
@
text
@d1280 1
a1280 1
        ORR     a2, v4, #AP_Read * L2X_APMult
@


1.1.2.46
log
@Add initial support for "physical memory pools"
Detail:
  This set of changes adds support for "physical memory pools" (aka PMPs), a new type of dynamic area which allow physical pages to be claimed/allocated without mapping them in to the logical address space. PMPs have full control over which physical pages they use (similar to DAs which request specific physical pages), and also have full control over the logical mapping of their pages (which pages go where, and per-page access/cacheability control).
  Currently the OS makes use of two PMPs: one for the free pool (which now has a logical size of zero - freeing up gigabytes of logical space), and one for the RAM disc (logical size of 1MB, allowing for a physical size limited only by the amount of free memory)
  Implementing these changes has required a number of other changes to be made:
  * The CAM has been expanded from 8 bytes per entry to 16 bytes per entry, in order to allow each RAM page to store information about its PMP association
  * The system heap has been expanded to 32MB in size (from just under 4MB), in order to allow it to be used to store PMP page lists (1 word needed per page, but PMP pages may not always have physical pages assigned to them - so to allow multiple large PMPs to exist we need more than just 1 word per RAM page)
  * The &FA000000-&FBFFFFFF area of fixed kernel workspace has been shuffled around to accomodate the larger CAM, and the system heap is now located just above the RMA.
  * SoftResets code stripped out (unlikely we'll ever want to fix and re-enable it)
  * A couple of FastCDA options are now permanently on
  * Internal page flags shuffled around a bit. PageFlags_Unavailable now publicly exposed so that PMP clients can lock/unlock pages at will.
  * When OS_ChangeDynamicArea is asked to grow or shrink the free pool, it now implicitly converts it into a shrink or grow of application space (which is what would happen anyway). This simplifies the implementation; during a grow, pages (or replacement pages) are always sourced from the free pool, and during a shrink pages are always sent to the free pool.
  File changes:
  - hdr/KernelWS - Extend DANode structure. Describe CAM format. Adjust kernel workspace.
  - hdr/OSRSI6, s/Middle - Add new item to expose the CAM format
  - hdr/Options - Remove SoftResets switch. Add some PMP switches.
  - s/ARM600, s/VMSAv6 - Updated for new CAM format. Note that although the CAM stores PMP information, BangCamUpdate currently doesn't deal with updating that data - it's the caller's responsibility to do so where appropriate.
  - s/ChangeDyn - Lots of changes to implement PMP support, and to cope with the new CAM format.
  - s/HAL - Updated to cope with new CAM format, and lack of logical mapping of free pool.
  - s/MemInfo - Updated to cope with new CAM format. OS_Memory 0 updated to cope with converting PPN to PA for pages which are mapped out. OS_Memory 24 updated to decode the access permissions on a per-page basis for PMPs, and fixed its HWM usage for sparse DAs.
  - s/NewReset - Soft reset code and unused AddCamEntries function removed. Updated to cope with new CAM format, PMP free pool, PMP RAMFS
  - s/AMBControl/allocate - Update comment (RMA hasn't been used for AMBControl nodes for a long time)
  - s/AMBControl/growp, s/AMBControl/memmap, s/AMBControl/shrinkp - Update for new CAM format + PMP free pool
  - s/vdu/vdudriver - Strip out soft reset code.
Admin:
  Tested on Pandaboard
  This is just a first iteration of the PMP feature, with any luck future changes will improve functionality. This means APIs are subject to change as well.


Version 5.35, 4.79.2.284. Tagged as 'Kernel-5_35-4_79_2_284'
@
text
@d590 2
a591 2
        MOV     lr, a2, LSR #12-CAM_EntrySizeLog2+12
        CMP     a2, lr, LSL #12-CAM_EntrySizeLog2+12
d1286 16
a1301 6
; Allocate backing L2PT for application space
; Note that ranges must be 4M aligned, as AllocateL2PT only does individual
; (1M) sections, rather than 4 at a time, corresponding to a L2PT page. The
; following space is available for dynamic areas, and ChangeDyn.s will get
; upset if it sees only some out of a set of 4 section entries pointing to the
; L2PT page.
a1303 1
        ASSERT  AplWorkMaxSize :MOD: (4*1024*1024) = 0
a1308 2
        ASSERT  SysHeapAddress :MOD: (4*1024*1024) = 0 
        ASSERT  SysHeapMaxSize :MOD: (4*1024*1024) = 0
a1440 1
        Entry
d1448 2
a1449 1
        ADD     a4, a2, a3, LSL #CAM_EntrySizeLog2
d1451 2
a1452 3
        LDR     lr, =L2PT :SHR: 22
10      LDR     ip, [a4, #CAM_LogAddr-CAM_EntrySize]!
        TEQ     lr, ip, LSR #22
d1458 1
a1458 1
        EXIT
d1495 1
a1495 1
        ADD     a2, v1, a2, LSL #CAM_EntrySizeLog2
d1499 1
a1499 8
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
        ASSERT  CAM_PMP=8
        ASSERT  CAM_PMPIndex=12
        ASSERT  CAM_EntrySize=16
        MOV     v2, #0
        MOV     v3, #-1
10      STMDB   a2!, {a3, a4, v2, v3}
d1558 1
a1558 1
        ADD     a2, v1, a1, LSL #CAM_EntrySizeLog2 ; a2 -> CAM entry
a1588 2
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
a1615 2
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
@


1.1.2.47
log
@  Support for ARMv8
Detail:
  * Filled in CPU tables for publicly documented ARMv8 cores (Cortex-A53,57,72).
  * Recent ARM ARMs (e.g. section B1.9.2 of the ARMv7AR ARM) permit the core to
    take an undefined instruction exception upon encountering even not-taken
    conditional undefined instructions. This option is exercised by the
    Cortex-A53, unlike all ARMv7 cores previously supported by RISC OS. This
    unfortunately trips up a lot of kernel code that adapts to different
    architectures at runtime. These have now all been replaced with branches
    over the affected code on the opposite condition.
  * Fixed bug in HAL_InvalidateCache_ARMvF: for the main body of the loop,
    which was written as though to act on the CLIDR register, r8 actually
    contained the CTR register instead.
Admin:
  Tested on Raspberry Pi 3

Version 5.35, 4.79.2.304. Tagged as 'Kernel-5_35-4_79_2_304'
@
text
@a97 2
        ADREQ   lr, %FT01
        BEQ     HAL_InvalidateCache_ARMvF
d99 1
a99 1
01
a101 1
        BNE     %FT01
d103 1
a103 3
        B       %FT02
01      MCRNE   ARM_config_cp,0,a2,ARMv4_TLB_reg,C7             ; flush TLBs
02
a834 1
        BNE     %FT01
d836 2
a837 3
        B       %FT02
01      MCRNE   p15, 0, lr, c8, c7              ; if HAL needed it (eg XScale with ECC)
02                                              ; so flush TLBs now
a839 1
        BLT     %FT01
a840 1
        BLE     %FT01
a841 1
01
a864 1
        BEQ     %FT01
d866 1
a866 2
        B       %FT02
01      MCREQ   p15, 0, lr, c7, c5, 0           ; invalidate instruction cache
a870 1
02
d1378 2
a1379 1
        BEQ     %FT80
a1381 1
        MRC     p15, 1, r8, c0, c0, 1           ; Read CLIDR to r8
d1412 1
a1412 1
        MRC     p15, 1, r8, c0, c0, 1
a1430 3
80 ; ARMv6 case
        MCR     ARM_config_cp,0,r9,ARMv4_cache_reg,C7 ; ARMv3-ARMv6 I+D cache flush
        B       %BT50
a2104 1
        BNE     %FT01
d2106 1
a2106 3
        B       %FT02
01      MCRNE   ARM_config_cp,0,ip,ARMv4_TLB_reg,C7
02
a2108 2
        ADREQ   lr, %FT01
        BEQ     HAL_InvalidateCache_ARMvF
d2110 1
a2110 1
01
@


1.1.2.48
log
@OS_Memory 13/14/15 fixes
Detail:
  s/HAL - Change RISCOS_AccessPhysicalAddress & RISCOS_ReleasePhysicalAddress (aka OS_Memory 14 & 15) to use the MMU_ChangingUncached ARMop instead of TLB_InvalidateEntry, as on ARMv6+ the MMU version ensures the write has been flushed to be visible by the TLB, while the TLB invalidate call doesn't.
  Fix RISCOS_MapInIO (aka OS_Memory 13) not detecting regions which have already been mapped in due to L1_XN flag masking issue. Also issue DSB+ISB after the page table write(s) to ensure it's visible by the TLB hardware.
Admin:
  Tested on IGEPv5


Version 5.35, 4.79.2.305. Tagged as 'Kernel-5_35-4_79_2_305'
@
text
@d2098 1
a2098 1
        ARMop   MMU_ChangingUncached                    ; sufficient, cause not cacheable
d2106 1
a2106 1
        ARMop   MMU_ChangingUncached,,tailcall          ; sufficient, cause not cacheable
a2404 1
        ORR     v7, v7, #L1_XN
a2496 6
      [ MEMM_Type = "VMSAv6"
        ; DSB + ISB required to ensure effect of page table write is fully
        ; visible (after overwriting a faulting entry)
        myDSB   ,a1
        myISB   ,a1,,y
      ]
@


1.1.2.49
log
@Cache maintenance fixes
Detail:
  This set of changes tackles two main issues:
  * Before mapping out a cacheable page or making it uncacheable, the OS performs a cache clean+invalidate op. However this leaves a small window where data may be fetched back into the cache, either accidentally (dodgy interrupt handler) or via agressive prefetch (as allowed for by the architecture). This rogue data can then result in coherency issues once the pages are mapped out or made uncacheable a short time later.
    The fix for this is to make the page uncacheable before performing the cache maintenance (although this isn't ideal, as prior to ARMv7 it's implementation defined whether address-based cache maintenance ops affect uncacheable pages or not - and on ARM11 it seems that they don't, so for that CPU we currently force a full cache clean instead)
  * Modern ARMs generally ignore unexpected cache hits, so there's an interrupt hole in the current OS_Memory 0 "make temporarily uncacheable" implementation where the cache is being flushed after the page has been made uncacheable (consider the case of a page that's being used by an interrupt handler, but the page is being made uncacheable so it can also be used by DMA). As well as affecting ARMv7+ devices this was found to affect XScale (and ARM11, although untested for this issue, would have presumably suffered from the "can't clean uncacheable pages" limitation)
    The fix for this is to disable IRQs around the uncache sequence - however FIQs are currently not being dealt with, so there's still a potential issue there.
  File changes:
  - Docs/HAL/ARMop_API, hdr/KernelWS, hdr/OSMisc - Add new Cache_CleanInvalidateRange ARMop
  - s/ARM600, s/VMSAv6 - BangCam updated to make the page uncacheable prior to flushing the cache. Add GetTempUncache macro to help with calculating the page flags required for making pages uncacheable. Fix abort in OS_MMUControl on Raspberry Pi - MCR-based ISB was resetting ZeroPage pointer to 0
  - s/ARMops - Cache_CleanInvalidateRange implementations. PL310 MMU_ChangingEntry/MMU_ChangingEntries refactored to rely on Cache_CleanInvalidateRange_PL310, which should be a more optimal implementation of the cache cleaning code that was previously in MMU_ChangingEntry_PL310.
  - s/ChangeDyn - Rename FastCDA_UpFront to FastCDA_Bulk, since the cache maintenance is no longer performed upfront. CheckCacheabilityR0ByMinusR2 now becomes RemoveCacheabilityR0ByMinusR2. PMP LogOp implementation refactored quite a bit to perform cache/TLB maintenance after making page table changes instead of before. One flaw with this new implementation is that mapping out large areas of cacheable pages will result in multiple full cache cleans while the old implementation would have (generally) only performed one - a two-pass approach over the page list would be needed to solve this.
  - s/GetAll - Change file ordering so GetTempUncache macro is available earlier
  - s/HAL - ROM decompression changed to do full MMU_Changing instead of MMU_ChangingEntries, to make sure earlier cached data is truly gone from the cache. ClearPhysRAM changed to make page uncacheable before flushing cache.
  - s/MemInfo - OS_Memory 0 interrupt hole fix
  - s/AMBControl/memmap - AMB_movepagesout_L2PT now split into cacheable+non-cacheable variants. Sparse map out operation now does two passes through the page list so that they can all be made uncacheable prior to the cache flush + map out.
Admin:
  Tested on StrongARM, XScale, ARM11, Cortex-A7, Cortex-A9, Cortex-A15, Cortex-A53
  Appears to fix the major issues plaguing SATA on IGEPv5


Version 5.35, 4.79.2.306. Tagged as 'Kernel-5_35-4_79_2_306'
@
text
@a1186 2
; Really we should make the pages uncacheable first, but for simplicity we just
; do a full cache clean+invalidate later on when changing the ROM permissions
d1223 1
d1229 3
a1231 1
        ARMop   MMU_Changing ; Perform full clean+invalidate to ensure any lingering cache lines for the decompression workspace are gone
a2272 8
        ; Make page uncacheable so the following is safe
        Push    "r0-r3"
        MOV     r0, #L1_B
        MOV     r1, r10
        MOV     r2, #0
        BL      RISCOS_AccessPhysicalAddress
        Pull    "r0-r3"

@


1.1.2.50
log
@Fix CPU features being clobbered by software RAM clear
Detail:
  s/ARMops, s/HAL - Move CPU feature init to after the RAM clear, to prevent the cached values being clobbered on platforms where the HAL doesn't perform the RAM clear
  s/CPUFeatures - Update/clarify comment
Admin:
  Tested on Raspberry Pi
  Fixes issue spotted by Sprow


Version 5.35, 4.79.2.320. Tagged as 'Kernel-5_35-4_79_2_320'
@
text
@a1075 3
; Calculate CPU feature flags (if moving this to before ClearPhysRAM, ensure the workspace also gets moved into the skipped region)
        BL      ReadCPUFeatures

@


1.1.2.51
log
@Clear the exclusive monitor when returning to pre-empted code
Detail:
  s/Kernel - Add macro for CLREX, which uses a dummy STREX on basic ARMv6 machines. Clear the exclusive monitor after issuing transient callbacks, to cope with callbacks being triggered on exit from IRQ
  s/ArthurSWIs, s/HAL, s/NewIRQs - Clear the exclusive monitor on exit from IRQ handlers & default FIQ handler
  s/VMSAv6 - Clear the exclusive monitor on entry to the data abort pre-veneer
Admin:
  Tested on Raspberry Pi
  Non-transient callback handlers, custom abort handlers, FIQ handlers, and anything else which returns directly to interrupted user code is responsible for issuing its own CLREX if the code has done something that could have left the local monitor in the exclusive state (e.g. calling a SWI counts towards this, as there's no guarantee the monitor will be open on exit from the SWI)


Version 5.35, 4.79.2.327. Tagged as 'Kernel-5_35-4_79_2_327'
@
text
@a2919 1
        MyCLREX a1, a2
@


1.1.2.23.2.1
log
@  Merge of Raspberry Pi support code against latest kernel
Detail:
  This is a new branch from the current tip of the HAL branch, incorporating
  the changes received from Adrian Lees. The same caveats apply - this is a
  work in progress and will not work on any other platform at present.
Admin:
  Builds, but not tested.

Version 5.35, 4.79.2.147.2.1. Tagged as 'Kernel-5_35-4_79_2_147_2_1'
@
text
@a840 6
        ! 0, "FIXME: temporary code"
;!!! STILL DON'T KNOW WHY THE BODGE IN ARM_ANALYSE ITSELF DOESN'T WORK!
  MOV a1,#0
  MOV a2,#32
  STRB a2,[a1,#DCache_LineLen]
  STRB a2,[a1,#ICache_LineLen]
@


1.1.2.23.2.2
log
@  Removed one more place where the cache parameters were hard-coded
Detail:
  This one evaded the cull in my last commit
Admin:
  Still works on a Raspberry Pi

Version 5.35, 4.79.2.147.2.3. Tagged as 'Kernel-5_35-4_79_2_147_2_3'
@
text
@d841 6
@


1.1.2.23.2.3
log
@Fix crash in OS_Hardware 4 when encountering devices which are newer than the caller supports. Add new device IDs for Raspberry Pi audio devices.
Detail:
  s/HAL - Fixed crash in OS_Hardware 4 when encountering devices which are newer than the caller supports
  hdr/HALDevice - Added new HAL devices for Raspberry Pi audio devices
Admin:
  Tested on Raspberry Pi with high processor vectors


Version 5.35, 4.79.2.147.2.17. Tagged as 'Kernel-5_35-4_79_2_147_2_17'
@
text
@a2477 2
        BLS     %FT90
        SUBS    r4, r4, #1
a2478 1
        MOV     r1, #-1
@


1.1.2.23.2.4
log
@Preperation for working Raspberry Pi video driver
Detail:
  hdr/HALEntries - Add new HAL_Video_StartupMode HAL entry to allow the HAL to specify a startup mode
  s/HAL, s/Kernel - Tweaked debug routines
  s/vdu/vdudriver - Make use of HAL_Video_StartupMode in InitialiseMode to decide what initial mode should be. Clean up some hacks & debug. Improve handling of external framestores; if bit 5 of GraphicsV_DisplayFeatures r0 is set, the kernel will now allow the display driver to grow/shrink/move its framestore in response to mode changes.
  s/vdu/vdugrafv - Adjust default GV_FramestoreAddress implementation to only claim vector if HAL returns a framestore
  s/vdu/vduswis - Re-enable FindOKMode
Admin:
  Tested on Raspberry Pi with high processor vectors


Version 5.35, 4.79.2.147.2.18. Tagged as 'Kernel-5_35-4_79_2_147_2_18'
@
text
@d2547 19
a2607 78
 [ DebugHALTX
DebugHALPrint
        Push    "a1-a4,v1,sb,ip"
        AddressHAL
        MOV     v1, lr
10      LDRB    a1, [v1], #1
        TEQ     a1, #0
        BEQ     %FT20
        CallHAL HAL_DebugTX
        B       %BT10
20      MOV     a1, #13
        CallHAL HAL_DebugTX
        MOV     a1, #10
        CallHAL HAL_DebugTX
        ADD     v1, v1, #3
        BIC     lr, v1, #3
        Pull    "a1-a4,v1,sb,ip"
        MOV     pc, lr
 ]


 [ DebugHALTX
HALDebugHexTX
       stmfd    r13!, {r0-r3,sb,ip,lr}
       AddressHAL
       b        jbdt1
HALDebugHexTX2
       stmfd    r13!, {r0-r3,sb,ip,lr}
       AddressHAL
       b        jbdt2
HALDebugHexTX4
       stmfd    r13!, {r0-r3,sb,ip,lr}
       AddressHAL
       mov      r0,r0,ror #24          ; hi byte
       bl       jbdtxh
       mov      r0,r0,ror #24
       bl       jbdtxh
jbdt2
       mov      r0,r0,ror #24
       bl       jbdtxh
       mov      r0,r0,ror #24
jbdt1
       bl       jbdtxh
       mov      r0,#' '
       CallHAL  HAL_DebugTX
       ldmfd    r13!, {r0-r3,sb,ip,pc}

jbdtxh stmfd    r13!,{r0-r3,lr}        ; print byte as hex
       and      a4,a1,#&f              ; get low nibble
       and      a1,a1,#&f0             ; get hi nibble
       mov      a1,a1,lsr #4           ; shift to low nibble
       cmp      a1,#&9                 ; 9?
       addle    a1,a1,#&30
       addgt    a1,a1,#&37             ; convert letter if needed
       CallHAL  HAL_DebugTX
       cmp      a4,#9
       addle    a1,a4,#&30
       addgt    a1,a4,#&37
       CallHAL  HAL_DebugTX
       ldmfd    r13!,{r0-r3,pc}

 |

HALDebugHexTX
HALDebugHexTX2
HALDebugHexTX4
       MOV     pc, lr

 ]









@


1.1.2.23.2.5
log
@Clean up remaining kernel hacks
Detail:
  Docs/RPiNotes - Deleted, contents no longer relevant
  s/HAL, s/Kernel, s/vdu/vduswis, s/pmf/key - Cleaned up debug code
  s/NewIRQs - No need to piggy back on timer 0 IRQ to generate a fake VSync; PushModeInfo already claims/releases TickerV as appropriate if video driver doesn't provide a VSync IRQ.
  s/NewReset - Re-enable LookForHALRTC call, the stack imbalance bug was fixed before the Pi changes were merged in
  s/vdu/vducursoft - Streamline PostWrchCursor a bit by only preserving R14 around RestorePointer if the software pointer is in use
  s/vdu/vdudriver - Amend ModeChangeSub improvements to ensure old external framestore handling logic is used if driver doesn't support framestore growth/realloc
Admin:
  Tested on Raspberry Pi with high processor vectors
  Kernel now looks to be in a good state for merging back into HAL branch
  Note - Software mouse pointer support in vducursoft only checks HALVideoFeatures, so doesn't take into account the capabilities of any GraphicsV driver that may be in use.


Version 5.35, 4.79.2.147.2.20. Tagged as 'Kernel-5_35-4_79_2_147_2_20'
@
text
@d2636 2
a2637 2
jbdtxh stmfd    r13!,{a1,v1,lr}        ; print byte as hex. corrupts a2-a4, ip, assumes sb already AddressHAL'd
       and      v1,a1,#&f              ; get low nibble
d2644 3
a2646 3
       cmp      v1,#9
       addle    a1,v1,#&30
       addgt    a1,v1,#&37
d2648 9
a2656 1
       ldmfd    r13!,{a1,v1,pc}
d2659 8
@


1.1.2.23.2.6
log
@Merge with HAL branch
Detail:
  Merge the HAL branch into the RPi branch, prior to merging RPi to HAL
  Brief summary of main changes brought in:
  * Added *cache functionality previously provided by ARM module
  * Added "CMOS RAM reset" message on startup when CMOS has been wiped by keypress
  * Renamed HAL Video entries from HAL_Video_XXX to HAL_VideoXXX
  * Dropped mjsHAL macros, GRAB/STASH macros
  * Fixed pseudo-VRAM allocation when machine has exactly 16MB of RAM
  * Added OS_Hardware 5
  * Use OS_SerialOp GetDeviceName for getting serial device name
  * Drop HAL_MonitorLeadID
  * Rework default GraphicsV_IICOp handler
Admin:
  Tested on Raspberry Pi with high processor vectors


Version 5.35, 4.79.2.147.2.23. Tagged as 'Kernel-5_35-4_79_2_147_2_23'
@
text
@d367 1
a367 1
        MOVLS   v6, v2, LSR #1                  ; If that overflowed, take half the bank.
d875 5
a879 1
        DebugTX "HAL initialised"
d2310 4
a2313 8

        CMP     ip, #OSHW_LookupRoutine
        ASSERT  OSHW_CallHAL < OSHW_LookupRoutine
        BLO     HardwareCallHAL
        BEQ     HardwareLookupRoutine

        CMP     ip, #OSHW_DeviceRemove
        ASSERT  OSHW_DeviceAdd < OSHW_DeviceRemove
d2316 1
a2316 4

        CMP     ip, #OSHW_DeviceEnumerateChrono
        ASSERT  OSHW_DeviceEnumerate < OSHW_DeviceEnumerateChrono
        ASSERT  OSHW_DeviceEnumerateChrono < OSHW_MaxSubreason 
d2318 1
a2318 2
        BEQ     HardwareDeviceEnumerateChrono
        BHI     HardwareBadReason
d2320 1
a2320 1
HardwareCallHAL
d2343 1
a2343 1
HardwareLookupRoutine
a2485 30
HardwareDeviceEnumerateChrono
        Push    "r3-r4,lr"
        LDR     lr, =ZeroPage
        LDR     r2, [lr, #DeviceCount]
        LDR     r3, [lr, #DeviceTable]
        SUBS    r4, r2, r1
        MOVLS   r1, #-1
        BLS     %FT90                                   ; if r1 is out of range then exit
        ADD     r3, r3, r4, LSL #2
10      ADD     r1, r1, #1
        LDR     r2, [r3, #-4]!
        LDR     lr, [r2, #HALDevice_Type]
        EOR     lr, lr, r0
        MOVS    lr, lr, LSL #16                         ; EQ if types match
        SUBNES  r4, r4, #1
        BNE     %BT10
        TEQ     lr, #0
        MOVNE   r1, #-1
        BNE     %FT90
        LDR     lr, [r2, #HALDevice_Version]
        MOV     lr, lr, LSR #16
        CMP     lr, r0, LSR #16                         ; newer than our client understands?
        BLS     %FT90
        SUBS    r4, r4, #1
        BHI     %BT10
        MOV     r1, #-1
90
        Pull    "r3-r4,lr"
        ExitSWIHandler

@


1.1.2.21.2.1
log
@  Add support for Cortex cache type. Extend ARM_Analyse to, where appropriate, use CPU feature registers to identify CPU capabilities.
Detail:
  s/ARMops - Support for Cortex multi-level cache (CT_ctype_WB_CR7_Lx). New ARM_Analyse_Fancy to identify CPU capabilities using feature registers.
  s/HAL - Modify pre-ARMop cache code to handle Cortex-syle caches.
  s/MemInfo - Replace ARM_flush_TLB macro call with appropriate ARMop to provide Cortex compatability
  hdr/ARMops - Update list of ARM architectures
  hdr/CoPro15ops - Deprecate ARM_flush_* macros for HAL kernels, as they are no longer capable of flushing all cache types. ARMops should be used instead.
  hdr/KernelWS - Add storage space for multi-level cache properties required for new cache cleaning code.
Admin:
  Tested under qemu-omap3. Still unable to verify on real hardware due to lack of appropriate MMU code. However new OMAP3 HAL code that uses similar cache management functions appears to work fine on real hardware.


Version 5.35, 4.79.2.98.2.2. Tagged as 'Kernel-5_35-4_79_2_98_2_2'
@
text
@d38 2
a39 2
        MOVEQ   a3, #0
        ARM_read_control a3, NE
d46 1
a46 1
        ORR     a3, a3, #MMUC_F+MMUC_L+MMUC_D+MMUC_P
d48 2
a49 2
        BIC     a3, a3, #MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M
        BIC     a3, a3, #MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S
d52 1
a52 1
        ARM_write_control a3
d56 1
a56 5
        CMP     a1, #ARMvF
        ; Assume that all ARMvF ARMs have multi-level caches and thus no single MCR op for invalidating all the caches
        MCRNE   ARM_config_cp,0,a2,ARMv4_cache_reg,C7           ; invalidate I+D caches
        BLEQ    HAL_InvalidateCache_ARMvF
        CMP     a1, #ARMv3
d62 2
a63 2
        ORRNE   a3, a3, #MMUC_I
        ARM_write_control a3, NE                                ; whoosh
d70 1
a70 1
        MOV     sp, a3
d626 1
a626 2
        MOV     ip, a1 ; Remember architecture for later
        
d640 3
a642 3
        MOV     lr, #0
        MCREQ   p15, 0, lr, c5, c0              ; MMU may already be on (but flat mapped)
        MCRNE   p15, 0, lr, c8, c7              ; if HAL needed it (eg XScale with ECC)
d649 5
a653 8
        CMP     ip, #ARMvF
        MOV     lr, #0                                          ; junk MMU-off contents of I-cache
        MCRNE   ARM_config_cp,0,lr,ARMv4_cache_reg,C7           ; (works on ARMv3)
        BLEQ    HAL_InvalidateCache_ARMvF

;       HACK HACK HACK - all domains remain in manager mode
;        MOV     ip, #4_0000000000000001                         ; domain 0 client only
;        ARM_MMU_domain ip
a776 6
 [ DebugHALTX
        BL      DebugHALPrint
        = "HAL initialised",0
        ALIGN
 ]

a965 47
HAL_InvalidateCache_ARMvF
        ; Cache invalidation for ARMs with multiple cache levels, used before ARMop initialisation
        ; This function gets called before we have a stack set up, so we've got to preserve as many registers as possible
        ; The only register we can safely change is ip, but we can switch into FIQ mode with interrupts disabled and use the banked registers there
        MRS     ip, CPSR
        MSR     CPSR_c, #F32_bit+I32_bit+FIQ32_mode
        MRC     p15, 1, r8, c0, c0, 1 ; Cache level ID register
        BIC     r8, r8, #&FF000000 ; Discard unification/coherency bits
        MOV     r9, #0 ; Current cache level
20
        TST     r8, #7 ; Get flags
        BEQ     %FT10 ; Cache clean complete
        MCR     p15, 2, r9, c0, c0, 0 ; Program cache size selection register
        MRC     p15, 1, r10, c0, c0, 0 ; Get size info
        AND     r11, r10, #&7 ; log2(Line size)-2
        BIC     r10, r10, #&F0000007 ; Clear flags & line size
        MOV     r12, r10, LSL #19 ; Number of ways-1 in upper 10 bits
        MOV     r10, r10, LSR #13 ; Number of sets-1 in upper 15 bits
        ; Way number needs to be packed right up at the high end of the data word; shift it up
        CLZ     r14, r12
        MOV     r12, r12, LSL r14
        ; Set number needs to start at log2(Line size)
        MOV     r10, r10, LSR #15 ; Start at bit 2
        MOV     r10, r10, LSL r11 ; Start at log2(Line size)
        ; Now calculate the offset numbers we will use to increment sets & ways
        BIC     r12, r12, r12, LSL #1 ; Way increment
        BIC     r11, r10, r10, LSL #1 ; Set increment
        ; Now we can finally clean this cache!
        ORR     r14, r9, r10 ; Current way (0), set (max), and level
30
        MCR     p15, 0, r14, c7, c6, 2 ; Invalidate
        ADDS    r14, r14, r12 ; Increment way
        BCC     %BT30 ; Overflow will occur once ways are enumerated
        TST     r14, r10 ; Are set bits all zero?
        SUBNE   r14, r14, r11 ; No, so decrement set and loop around again
        BNE     %BT30
        ; This cache is now clean. Move on to the next level.
        ADD     r9, r9, #2
        MOVS    r8, r8, LSR #3
        BNE     %BT20
10
        ; All caches clean; switch back to SVC, then recover the stored PSR from ip (although we can be fairly certain we started in SVC anyway)
        MSR     CPSR_c, #F32_bit+I32_bit+SVC32_mode
        MSR     CPSR_cxsf, ip
        MOV     pc, lr


d1507 1
a1507 3
        CMP     a1, #ARMvF
        MCRNE   ARM_config_cp,0,ip,ARMv4_cache_reg,C7           ; works on ARMv3
        BLEQ    HAL_InvalidateCache_ARMvF
@


1.1.2.21.2.2
log
@Add VMSAv6 MMU support, fixes to allow booting on beagleboard
Detail:
  s/ARM600 - fix to SyncCodeAreasRange to correctly read cache line length for WB_CR7_Lx caches
  s/ARMops - Cortex cache handling fixes. Enable L2 cache for Cortex.
  s/ChangeDyn - VMSAv6 support in AllocateBackingLevel2
  s/HAL - Improve RISCOS_InitARM to set/clear correct CP15 flags for ARMv6/v7. VMSAv6 support in code to generate initial page tables.
  s/NewReset - Extra DebugTX calls during OS startup. Disable pre-HAL Processor_Type for HAL builds.
  s/VMSAv6 - Main VMSAv6 MMU code - stripped down version of s/ARM600 with support for basic VMSAv6 features.
  hdr/Options - Use VMSAv6 MMU code, not ARM600. Disable ARM6support since current VMSAv6 code will conflict with it.
Admin:
  Tested basic OS functionality under qemu-omap3 and revision B6 beagleboard.


Version 5.35, 4.79.2.98.2.3. Tagged as 'Kernel-5_35-4_79_2_98_2_3'
@
text
@a30 14
 [ MEMM_Type = "VMSAv6"
mmuc_table ; Table of MMUC init values. First word is value to ORR, second is value to BIC
 DCD MMUC_F+MMUC_L+MMUC_D+MMUC_P, MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M+MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S ; ARMv3
 DCD MMUC_F+MMUC_L+MMUC_D+MMUC_P, MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M+MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S ; ARMv4
 DCD MMUC_F+MMUC_L+MMUC_D+MMUC_P, MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M+MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S ; ARMv4T
 DCD MMUC_F+MMUC_L+MMUC_D+MMUC_P, MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M+MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S ; ARMv5
 DCD MMUC_F+MMUC_L+MMUC_D+MMUC_P, MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M+MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S ; ARMv5T
 DCD MMUC_F+MMUC_L+MMUC_D+MMUC_P, MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M+MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S ; ARMv5TE
 DCD MMUC_F+MMUC_L+MMUC_D+MMUC_P, MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M+MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S ; ARMv5TEJ
 DCD MMUC_F+MMUC_L+MMUC_D+MMUC_P, MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M+MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S+MMUC_VE+MMUC_EE+MMUC_L2 ; ARMv6
 ; Skip undefined architecture numbers
mmuc_table_armvf
 DCD 0, MMUC_C+MMUC_A+MMUC_M+MMUC_V+MMUC_I+MMUC_Z+MMUC_RR+MMUC_TRE+MMUC_AFE+MMUC_TE ; ARMvF (Cortex)
 ]
a39 11
 [ MEMM_Type = "VMSAv6"
        ; Use a lookup table to get the correct control register set/clear mask
        CMP     a1, #ARMvF
        ADREQ   a2, mmuc_table_armvf
        ADRLT   a2, mmuc_table
        ADDLT   a2, a2, a1, LSL #3
        LDMIA   a2, {a2, a4}
        ORR     a3, a3, a2
        BIC     a3, a3, a4
        CMP     a1,#0     
 |
a49 1
 ]
a517 1

d541 1
d602 1
a618 3
 [ MEMM_Type = "VMSAv6"
        LDR     ip, =(AP_ROM * L1_APMult) + L1_Section
 |
a623 1
 ]
a648 9
  [ MEMM_Type = "VMSAv6"
        CMP     ip, #ARMv6
        MCRGE   p15, 0, lr, c2, c0, 2           ; Ensure only TTBR0 is used (v6)
        MCRGT   p15, 0, lr, c12, c0, 0          ; Ensure exception vector base is 0 (Cortex)
        ORRGE   v5, v5, #MMUC_XP ; Extended pages enabled (v6)
        BICGT   v5, v5, #MMUC_TRE+MMUC_AFE ; TEX remap, Access Flag disabled (Cortex)
        BICGE   v5, v5, #MMUC_EE+MMUC_TE+MMUC_VE ; Exceptions = nonvectored LE ARM
        CMP     ip, #0
  ]
d657 1
a657 3
        MCREQ   p15, 0, lr, c7, c5, 0           ; invalidate instruction cache
        MCREQ   p15, 0, lr, c8, c7, 0           ; invalidate TLBs
        BLEQ    HAL_InvalidateCache_ARMvF       ; invalidate data cache (and instruction+TLBs again!)
d659 3
a661 2
        MOV     ip, #4_0000000000000001                         ; domain 0 client only
        ARM_MMU_domain ip
a685 3
 [ MEMM_Type = "VMSAv6"
        LDR     ip, =(AP_ROM * L1_APMult) + L1_Section
 |
a690 1
 ]
a985 3
        MOV     r8, #0
        MCR     p15, 0, r8, c7, c5, 0           ; invalidate instruction cache
        MCR     p15, 0, r8, c8, c7, 0           ; invalidate TLBs
a1105 1
 [ MEMM_Type <> "VMSAv6"
a1127 1
 ]
a1141 31
 [ MEMM_Type = "VMSAv6"
        AND     a1, v5, #L2_AP                  ; a1 = access permission
        MOV     a1, a1, LSR #L2_APShift
        ; Map AP_ROM to 0
        CMP     a1, #AP_ROM
        MOVEQ   a1, #0
        ; Now ARM access goes 0 => all R/O, 1 => user none, 2 => user R/O, 3  => user R/W
        ; PPL access goes 0 => user R/W, 1 => user R/O, 2 => user none, (and let's say 3 all R/O)
        RSB     v6, a1, #3                      ; v6 = PPL access
        AND     a1, v5, #2_11                   ; a1 = page type
        CMP     a1, #L2_ExtPage
        ANDHS   a1, v5, #L2_TEX+L2_C+L2_B       ; Extended TEX and CB bits
        ANDLO   a1, v5, #L2_C+L2_B              ; Large CB bits only
        ANDLO   lr, v5, #L2L_TEX                ; Large TEX bits
        ORRLO   a1, a1, lr, LSR #L2L_TEXShift-L2_TEXShift ; Move Large TEX back to Extended TEX position
        MOV     lr, #3                          ; lr = PCB value (funny loop to do NCNB first)
60      LDRB    a3, [ip, lr]                    ; look in XCBTrans table
        TEQ     a3, a1                          ; found a match for our XCB?
        BEQ     %FT70
        TST     lr, #2_11
        SUBNE   lr, lr, #1                      ; loop goes 3,2,1,0,7,6,5,4,...,31,30,29,28
        ADDEQ   lr, lr, #7
        TEQ     lr, #35
        BNE     %BT60
70      AND     a1, lr, #2_00011
        ORR     v6, v6, a1, LSL #4              ; extract NCNB bits
        AND     a1, lr, #2_11100
        ORR     v6, v6, a1, LSL #10             ; extract P bits
        ORR     v6, v6, #PageFlags_Unavailable               ; ???? pages from scratch to cam only?
        STMIA   a2, {v2, v6}                    ; store logical address, PPL
 |
d1168 1
a1168 2
 ]
 
d1254 1
a1254 1
;    a3 = access permissions+C+B bits (bits 11-2 of an L2 extended small page)
a1272 6
 [ MEMM_Type = "VMSAv6"
        ORR     a3, a3, #L2_LargePage           ; else large pages (64K)
        AND     lr, a3, #L2_TEX                 ; extract TEX from ext page flags
        BIC     a3, a3, #L2_TEX                 ; small page TEX bits SBZ for large pages
        ORR     a3, a3, lr, LSL #L2L_TEXShift-L2_TEXShift ; replace TEX in large page position     
 |
a1280 1
 ]
a1297 16
 [ MEMM_Type = "VMSAv6"
Init_MapIn_Sections
        MOVS    ip, v3                          ; is MMU on?
        LDREQ   ip, =L1PT                       ; then use virtual address
        AND     lr, a3, #L2_TEX + L2_AP         ; extract TEX, AP, APX bits (input is extended small page)
        BIC     a3, a3, #L2_TEX + L2_AP         ; and clear them
        ORR     a3, a3, lr, LSL #6              ; put TEX and AP bits back in new position
        ORR     a3, a3, #L1_Section             ; Mark as section
        ORR     a1, a1, a3                      ; Merge with physical address
        ADD     a2, ip, a2, LSR #18             ; a2 -> L1PT entry
70      STR     a1, [a2], #4                    ; And store in L1PT
        ADD     a1, a1, #1024*1024              ; Advance one megabyte
        SUBS    a4, a4, #1024*1024              ; and loop
        BNE     %BT70
        Pull    "pc"
 |
d1323 1
a1323 1
 ]
a1362 1
 [ MEMM_Type <> "VMSAv6"
a1376 1
 ]
a1464 3
 [ MEMM_Type = "VMSAv6"
        ORR     a3, a1, #L1_Page
 |
a1477 1
 ]
a1525 3
 [ MEMM_Type = "VMSAv6"
        LDR     a4, =(AP_None * L1_APMult) + L1_Section
 |
a1526 1
 ]
a1779 28
; Debug InitProcVecs - on an exception they output the PC to a UART
;        GET ADFS::4.$.work.riscos.omap3dev.riscos.sources.hal.omap3.hdr.UART
;InitProcVecs
;        NOP ; Reset
;        NOP ; Undefined instruction
;        NOP ; SWI
;        NOP ; Prefetch abort
;        NOP ; data abort
;        NOP ; address exception
;        NOP ; IRQ
;        NOP ; FIQ
;        ADR     a3,hextab
;        LDR     a2,[a3],#4
;        MOV     a4, #8
;10
;        LDRB    v1, [a2, #UART_LSR]
;        TST     v1, #THRE
;        BEQ     %BT10
;        LDRB    v1, [a3, lr, LSR #28]
;        STRB    v1, [a2, #UART_THR]
;        MOV     lr, lr, LSL #4
;        SUBS    a4, a4, #1
;        BNE     %BT10
;20
;        B %BT20
;hextab  DCD &49020000 ; UART address
;        DCB "0123456789abcdef"
;InitProcVecsEnd
d1782 1
a1782 1
; In:  a1 = flags  (L1_B,L1_C,L1_AP,L1_APX)
a1792 3
 [ MEMM_Type = "VMSAv6"
        ASSERT  L1_AP = 2_100011 :SHL: 10
 |
a1793 1
 ]
a1813 1
        ; For VMSAv6, assume HAL knows what it's doing and requests correct settings for AP_ROM
@


1.1.2.21.2.3
log
@Fix kernel cache clean/invalidate operations for Cortex CPUs
Detail:
  s/ARMops - Fix set/way-based cache ops for cache type WB_CR7_Lx to iterate sets/ways/cache levels properly
  s/HAL - Fix HAL_InvalidateCache_ARMvF to iterate sets/ways/cache levels properly
Admin:
  Tested on rev C2 beagleboard


Version 5.35, 4.79.2.98.2.4. Tagged as 'Kernel-5_35-4_79_2_98_2_4'
@
text
@d1043 1
a1043 1
        MOV     r10, r10, LSR #13 ; Number of sets-1 in lower 15 bits
d1047 3
a1049 3
        ; Set number needs to start at log2(Line size)+2
        MOV     r10, r10, LSL #4 ; Start at bit 4
        MOV     r10, r10, LSL r11 ; Start at log2(Line size)+2
@


1.1.2.21.2.4
log
@Assorted kernel fixes for ARMv6/ARMv7
Detail:
  s/ARMops - Fix IMB_Range_WB_CR7_Lx to clean the correct number of cache lines
  s/HAL - Change CP15 control register flags so unaligned loads are enabled on ARMv6 (to simplify support for ARMv7 where unaligned loads are always enabled, and to match the behaviour expected by the example code in Hdr:CPU.Arch)
  s/AMBControl/memmap - Make AMB_LazyFixUp use the correct L2PT protection flags depending on ARM600/VMSAv6 MMU model. Also guard against problems caused by future L2PT flag changes.
  s/vdu/vdugrafj - Fix previously undiscovered 32bit incompatability in GetSprite (OS_SpriteOp 14/16)
Admin:
  Tested on rev C2 beagleboard


Version 5.35, 4.79.2.98.2.5. Tagged as 'Kernel-5_35-4_79_2_98_2_5'
@
text
@d40 1
a40 1
 DCD MMUC_F+MMUC_L+MMUC_D+MMUC_P+MMUC_U, MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M+MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S+MMUC_VE+MMUC_EE+MMUC_L2 ; ARMv6
@


1.1.2.21.2.5
log
@Fix bug when creating code variables via OS_SetVarVal, remove errant line from s.ARM600, automatically enable alignment exceptions if NoUnaligned is TRUE (Cortex branch)
Detail:
  s/ARM600 - Removed an errant line that could have caused problems if the ARM600 MMU model was used with the WB_CR7_Lx cache type
  s/Arthur2 - OS_SetVarVal was failing to call XOS_SynchroniseCodeAreas after copying the code variables code block into the system heap. This has now been fixed.
  s/HAL - Alignment exceptions are now automatically enabled when the kernel is built with the NoUnaligned option turned on.
Admin:
  Tested on rev C2 beagleboard. OS_SetVarVal fix means the Debugger module now shows the right register names instead of ofla!


Version 5.35, 4.79.2.98.2.15. Tagged as 'Kernel-5_35-4_79_2_98_2_15'
@
text
@a686 3
  [ NoUnaligned
        ORR     v5, v5, #MMUC_A ; Alignment exceptions on
  ]
@


1.1.2.21.2.6
log
@Update Cortex kernel to use correct instruction/memory barriers and to perform branch target predictor maintenance. Plus tweak default CMOS settings.
Detail:
  hdr/Copro15ops - Added myISB, myDSB, myDMB macros to provide barrier functionality on ARMv6+
  s/ARMops, s/HAL, s/VMSAv6, s/AMBControl/memmap - Correct barrier operations are now performed on ARMv6+ following CP15 writes. Branch predictors are now also maintained properly.
  s/NewReset - Change default CMOS settings so number of CDFS drives is 0 in Cortex builds. Fixes rogue CDFS icon on iconbar.
Admin:
  Tested on rev C2 beagleboard


Version 5.35, 4.79.2.98.2.27. Tagged as 'Kernel-5_35-4_79_2_98_2_27'
@
text
@a78 3
 [ MEMM_Type = "VMSAv6"
        myISB ; Ensure the update went through
 ]
a88 4
 [ MEMM_Type = "VMSAv6"
        myDSB
        myISB
 ]
a93 3
 [ MEMM_Type = "VMSAv6"
        myISB ; Ensure the update went through
 ]
a681 1
        myISB
a691 3
  [ MEMM_Type = "VMSAv6"
        myISB ; Just in case
  ]
a699 4
        MCREQ   p15, 0, lr, c7, c5, 6           ; invalidate branch predictor
  [ MEMM_Type = "VMSAv6"
        myISB ; Ensure below branch works
  ]
a1034 3
        MCR     p15, 0, r8, c7, c5, 6           ; invalidate branch target predictor
        myDSB                                   ; Wait for completion
        myISB
a1041 1
        myISB
a1069 8
        ; Wait for clean to complete
        myDSB
        MOV     r8, #0
        MCR     p15, 0, r8, c7, c5, 0           ; invalidate instruction cache
        MCR     p15, 0, r8, c8, c7, 0           ; invalidate TLBs
        MCR     p15, 0, r8, c7, c5, 6           ; invalidate branch target predictor
        myDSB                                   ; Wait for completion
        myISB
@


1.1.2.21.2.7
log
@Fix some issues preventing the Cortex kernel from being used on non-Cortex machines
Detail:
  hdr/Options - ARM6support and GetKernelMEMC values are now derived from the value of MEMM_Type
  s/ARMops, s/HAL - Code to detect and handle ARMv7 CPUs is now only enabled when using VMSAv6 MMU model. Saves us from having to deal with lack of myIMB, myDSB, etc. implementations on pre-ARMv6.
  s/HAL - Removed some debug code
  s/NewReset - Fix bug spotted by Tom Walker where R12 wasn't being restored by LookForHALRTC if a non-HAL RTC had already been found
  s/AMBControl/memmap - correct the assert clause that was checking that &FFE are the correct L2PT protection bits for non-VMSAv6 machines
Admin:
  Tested this kernel on a rev C2 beagleboard & Iyonix softload. Also compiled it into an IOMD ROM, but didn't try running it.


Version 5.35, 4.79.2.98.2.32. Tagged as 'Kernel-5_35-4_79_2_98_2_32'
@
text
@a84 1
 [ MEMM_Type = "VMSAv6"
a88 1
 ]
a708 1
  [ MEMM_Type = "VMSAv6"
d715 1
d717 1
a718 4
  |
        MOV     lr, #0                                          ; junk MMU-off contents of I-cache
        MCR     ARM_config_cp,0,lr,ARMv4_cache_reg,C7           ; (works on ARMv3)
  ]
a1043 1
 [ MEMM_Type = "VMSAv6"
d1104 1
a1104 1
 ] ; MEMM_Type = "VMSAv6"
a1713 1
 [ MEMM_Type = "VMSAv6"
a1716 3
 |
        MCR     ARM_config_cp,0,ip,ARMv4_cache_reg,C7           ; works on ARMv3
 ]
d1926 28
@


1.1.2.21.2.8
log
@Update OS_IICOp to support multiple IIC buses
Detail:
  OS_IICOp (and in turn, RISCOS_IICOpV) now treat the top byte of R1 as containing the IIC bus number, allowing multiple buses to be used.
  hdr/KernelWS - Changed workspace a bit so that the kernel can support up to IICBus_Count buses (currently 3), each with its own IICBus_* block.
  s/HAL - Update Reset_IRQ_Handler to cope with interrupts from all IIC buses instead of just the first. Fix/update RISCOS_IICOpV description.
  s/NewIRQs - Update InitialiseIRQ1Vtable to set up interrupt handlers for all IRQ-supporting IIC buses
  s/NewReset - Get rid of the IICAbort call that was just before IICInit. IICInit now calls IICAbort itself.
  s/PMF/IIC - Bulk of the changes. Code now uses the IICBus_ structures instead of the IICStatus and IICType variables. Re-entrancy code has been updated to take into account the possiblity of multiple buses; when OS_IICOp calls are nested, the IIC transfers will be added to bus-specific queues instead of all going in the same queue. However only one queue will be processed at a time.
  s/ChangeDyn - Workspace shuffling means a couple of MOV's needed to be swapped with LDR's when getting immediate constants
Admin:
  Tested with OMAP & IOMD ROM builds.
  Both high & low-level bus types seem to work OK, along with re-entrancy, both on the same bus and on a different bus.


Version 5.35, 4.79.2.98.2.33. Tagged as 'Kernel-5_35-4_79_2_98_2_33'
@
text
@d2096 1
a2096 1
; kernel_oserror *RISCOS_IICOpV(IICDesc *descs, uint32_t ndesc_and_bus)
d2386 1
a2386 1
        Push    "a1-a4,v1-v2,sb,ip,lr"
d2389 3
a2391 5
        MOV     v2, #0
        AddressHAL v2
        MOV     v1, #IICBus_Base
10
        LDR     a2, [v1, #IICBus_Type]
d2393 1
a2393 1
        MOVNE   ip, v2
a2394 4
        ADD     v2, v2, #1
        ADD     v1, v1, #IICBus_Size
        CMP     v2, #IICBus_Count
        BNE     %BT10
d2401 1
a2401 1
        Pull    "a1-a4,v1-v2,sb,ip,pc",,^
@


1.1.2.21.2.9
log
@Update Cortex branch of kernel to support HALSize env variable. Export C version of hdr.OSEntries.
Detail:
  Makefile - Now exports a C version of hdr.OSEntries, for use by the new HAL USB drivers
  s/GetAll, s/Kernel - The HALSize env variable is now used in place of hard-coded values for the HAL size
  s/HAL - Reset_IRQ_Handler now switches to SVC mode before calling HAL_KbdScanInterrupt, to allow the HAL USB drivers to re-enable interrupts if they wish.
  s/VMSAv6 - Deleted some obsolete definitions
Admin:
  Tested on rev C2 BB, A2 BB-xM, C1 TouchBook
  Needs latest BuildSys, Env, HdrSrc


Version 5.35, 4.79.2.98.2.37. Tagged as 'Kernel-5_35-4_79_2_98_2_37'
@
text
@d2387 2
a2388 5
        MRS     a1, SPSR
        MRS     a2, CPSR
        ORR     a3, a2, #SVC32_mode
        MSR     CPSR_c, a3
        Push    "a1-a2,lr"
d2405 2
a2406 3
        Pull    "a1-a2,lr"
        MSR     CPSR_c, a2
        MSR     SPSR_cxsf, a1
@


1.1.2.21.2.10
log
@Add hdr.Variables to the C header export, fix ARMv6 issues
Detail:
  Makefile - Added hdr.Variables to the C header export list
  hdr/ARMops, s/ARMops - Added ARM1176JZF-S to the list of known CPUs
  s/ARMops - Fix unaligned memory access in ARM_PrintProcessorType
  hdr/Copro15ops, s/ARMops, s/HAL, s/VMSAv6, s/AMBControl/memmap - Fixed all myDSB/myISB/etc. macro instances to specify a temp register, so that they work properly when building an ARMv6 version of the kernel
Admin:
  Fixes build errors with the latest Draw module.
  Should also allow the kernel to work properly with the new S3C6410 port.
  ARMv6 version builds OK, but no other builds or runtime tests have been made.


Version 5.35, 4.79.2.98.2.38. Tagged as 'Kernel-5_35-4_79_2_98_2_38'
@
text
@a78 1
        MOV     a2, #0
d80 1
a80 1
        myISB   ,a2,,y ; Ensure the update went through
d84 1
d95 2
a96 2
        myDSB   ,a2,,y
        myISB   ,a2,,y
d104 1
a104 1
        myISB   ,a2,,y ; Ensure the update went through
d694 1
a694 1
        myISB   ,lr,,y
d706 1
a706 2
        MOV     lr, #0
        myISB   ,lr,,y ; Just in case
d713 2
a714 1
        MCRNE   ARM_config_cp,0,lr,ARMv4_cache_reg,C7           ; junk MMU-off contents of I-cache (works on ARMv3)
d718 1
a718 1
        myISB   ,lr,,y ; Ensure below branch works
d1060 2
a1061 2
        myDSB   ,r8,,y                          ; Wait for completion
        myISB   ,r8,,y
d1069 1
a1069 1
        myISB   ,r8,,y
d1099 1
a1100 1
        myDSB   ,r8,,y
d1104 2
a1105 2
        myDSB   ,r8,,y                          ; Wait for completion
        myISB   ,r8,,y
@


1.1.2.21.2.11
log
@Kernel fixes for ARMv6
Detail:
  hdr/ARMops - Amended ARMvF description to state that an ARMvF CPU can be ARMv6 or ARMv7
  s/ARMops - Move ARM11JZF_S CPUDesc to KnownCPUTable_Fancy, since it's ARMvF. Update ARM_Analyse_Fancy to detect whether ARMv6 or ARMv7 style cache control is in use, and react accordingly.
  s/HAL - Simplified system control register/MMUC initialisation. There are now just two types of setup - one for ARMv3-ARMv5 and one for ARMv6-ARMv7. Modified HAL_InvalidateCache_ARMvF to use the appropriate cache flush instructions depending on whether it's an ARMv6 or ARMv7 style cache.
Admin:
  S3C6410 and other ARMv6 machines should work now.
  Tested on BB-xM rev A2.


Version 5.35, 4.79.2.98.2.39. Tagged as 'Kernel-5_35-4_79_2_98_2_39'
@
text
@d30 1
d32 12
a43 25
mmuc_init_new
        ; MMUC initialisation flags for ARMv6/ARMv7
        ; This tries to leave the reserved/unpredictable bits untouched, while initialising everything else to what we want
                ; ARMv7MP (probably) wants SW. ARMv6 wants U+XP (which should both be fixed at 1 on ARMv7)
        DCD     MMUC_SW+MMUC_U+MMUC_XP
                ; M+C+W+Z+I+L2 clear to keep MMU/caches off.
                ; A to keep alignment exceptions off (for now at least)
                ; B+EE clear for little-endian
                ; S+R+RR clear to match mmuc_init_old
                ; V+VE clear to keep processor vectors at &0
                ; FI clear to disable fast FIQs (interruptible LDM/STM)
                ; TRE+AFE clear for our VMSAv6 implementation
                ; TE clear for processor vectors to run in ARM mode
        DCD     MMUC_M+MMUC_A+MMUC_C+MMUC_W+MMUC_B+MMUC_S+MMUC_R+MMUC_Z+MMUC_I+MMUC_V+MMUC_RR+MMUC_FI+MMUC_VE+MMUC_EE+MMUC_L2+MMUC_TRE+MMUC_AFE+MMUC_TE
mmuc_init_old
        ; MMUC initialisation flags for ARMv5 and below, as per ARM600 MMU code
                ; Late abort (ARM6 only), 32-bit Data and Program space. No Write buffer (ARM920T
                ; spec says W bit should be set, but I reckon they're bluffing).
                ;
                ; The F bit's tricky. (1 => CPCLK=FCLK, 0=>CPCLK=FCLK/2). The only chip using it was the
                ; ARM700, it never really reached the customer, and it's always been programmed with
                ; CPCLK=FCLK. Therefore we'll keep it that way, and ignore the layering violation.
        DCD     MMUC_F+MMUC_L+MMUC_D+MMUC_P
                ; All of these bits should be off already, but just in case...
        DCD     MMUC_B+MMUC_W+MMUC_C+MMUC_A+MMUC_M+MMUC_RR+MMUC_V+MMUC_I+MMUC_Z+MMUC_R+MMUC_S
d55 6
a60 5
        CMP     a1, #ARMv6
        CMPNE   a1, #ARMvF
        ADREQ   a2, mmuc_init_new
        ADRNE   a2, mmuc_init_old
        LDMIA   a2, {a2, lr}
d62 2
a63 1
        BIC     a3, a3, lr     
d696 1
a696 1
        BICGE   v5, v5, #MMUC_TRE+MMUC_AFE ; TEX remap, Access Flag disabled
a1061 6
        ; Check whether we're ARMv7 (and thus multi-level cache) or ARMv6 (and thus single-level cache)
        MRC     p15, 0, r9, c0, c0, 1
        TST     r9, #&80000000 ; EQ=ARMv6, NE=ARMv7
        MCREQ   ARM_config_cp,0,r8,ARMv4_cache_reg,C7 ; ARMv3-ARMv6 I+D cache flush
        BEQ     %FT10 ; Skip to the end
        
@


1.1.2.21.2.12
log
@Add zero page relocation support
Detail:
  A whole mass of changes to add high processor vectors + zero page relocation support to the Cortex branch of the kernel
  At the moment the code can only cope with two ZeroPage locations, &0 and &FFFF0000. But with a bit more tweaking those restrictions can probably be lifted, allowing ZeroPage to be hidden at almost any address (assuming it's fixed at compile time). If I've done my job right, these restrictions should all be enforced by asserts.
  There's a new option, HiProcVecs, in hdr/Options to control whether high processor vectors are used. When enabling it and building a ROM, remember:
  * FPEmulator needs to be built with the FPEAnchor=High option specified in the components file (not FPEAnchorType=High as my FPEmulator commit comments suggested)
  * ShareFS needs unplugging/removing since it can't cope with it yet
  * Iyonix users will need to use the latest ROOL boot sequence, to ensure the softloaded modules are compatible (OMAP, etc. don't really softload much so they're OK with older sequences)
  * However VProtect also needs patching to fix a nasty bug there - http://www.riscosopen.org/tracker/tickets/294
  The only other notable thing I can think of is that the ProcessTransfer code in s/ARM600 & s/VMSAv6 is disabled if high processor vectors are in use (it's fairly safe to say that code is obsolete in HAL builds anyway?)
  Fun challenge for my successor: Try setting ZeroPage to &FFFF00FF (or similar) so its value can be loaded with MVN instead of LDR. Then use positive/negative address offsets to access the contents.
  File changes:
  - hdr/ARMops - Modified ARMop macro to take the ZeroPage pointer as a parameter instead of 'zero'
  - hdr/Copro15ops - Corrected $quick handling in myISB macro
  - hdr/Options - Added ideal setting for us to use for HiProcVecs
  - s/AMBControl/allocate, s/AMBControl/growp, s/AMBControl/mapslot, s/AMBControl/memmap, s/AMBControl/service, s/AMBControl/shrinkp, s/Arthur2, s/Arthur3, s/ArthurSWIs, s/ChangeDyn, s/ExtraSWIs, s/HAL, s/HeapMan, s/Kernel, s/MemInfo, s/Middle, s/ModHand, s/MoreSWIs, s/MsgCode, s/NewIRQs, s/NewReset, s/Oscli, s/PMF/buffer, s/PMF/IIC, s/PMF/i2cutils, s/PMF/key, s/PMF/mouse, s/PMF/osbyte, s/PMF/oseven, s/PMF/osinit, s/PMF/osword, s/PMF/oswrch, s/SWINaming, s/Super1, s/SysComms, s/TickEvents, s/Utility, s/vdu/vdu23, s/vdu/vdudriver, s/vdu/vdugrafl, s/vdu/vdugrafv, s/vdu/vdupalxx, s/vdu/vdupointer, s/vdu/vduswis, s/vdu/vduwrch - Lots of updates to deal with zero page relocation
  - s/ARM600 - UseProcessTransfer option. Zero page relocation support. Deleted pre-HAL ClearPhysRAM code to tidy the file up a bit.
  - s/ARMops - Zero page relocation support. Set CPUFlag_HiProcVecs when high vectors are in use.
  - s/KbdResPC - Disable compilation of dead code
  - s/VMSAv6 - UseProcessTransfer option. Zero page relocation support.
Admin:
  Tested with OMAP & Iyonix ROM softloads, both with high & low zero page.
  High zero page hasn't had extensive testing, but boot sequence + ROM apps seem to work.


Version 5.35, 4.79.2.98.2.48. Tagged as 'Kernel-5_35-4_79_2_98_2_48'
@
text
@d523 2
a524 2
        LDR     a2, =ZeroPage
  [ ECC
d526 1
a526 1
  |
d528 1
a528 1
  ]
a712 3
  [ HiProcVecs
        ORR     v5, v5, #MMUC_V ; High processor vectors enabled
  ]
d838 1
d840 1
a840 1
        LDR     a2, =ZeroPage
a841 3
      [ ZeroPage <> 0
        MOV     a2, #0
      ]
d844 1
a844 1
        LDR     a2, =ZeroPage
d855 1
a855 1
        LDR     a1, =ZeroPage
d870 1
a870 1
        LDR     a1, =ZeroPage
d886 1
a886 1
        LDR     a1, =ZeroPage+InitIRQWs
d900 2
a901 1
        LDR     a1, [v8, #HAL_StartFlags]
d912 2
a913 1
        LDR     v4, [v8, #MMU_PCBTrans]
d916 1
a916 1
        LDR     v5, [v8, #MMU_PCBTrans]
a995 8
 [ HiProcVecs
        ; Map in DebuggerSpace
        LDR     a1, =DebuggerSpace
        ORR     a2, v5, #AP_Read * L2X_APMult
        LDR     a3, =(DebuggerSpace_Size + &FFF) :AND: &FFFFF000
        BL      Init_MapInRAM
 ]

d1129 1
a1129 1
        LDR     a1, =ZeroPage
a1131 1
      [ ZeroPage <> 0
a1132 1
      ]
d1142 1
a1142 1
        LDR     a2, =ZeroPage
d1176 1
a1176 1
        LDR     a1, =ZeroPage
d1210 2
a1211 2
        LDR     a2, =ZeroPage                   ; if we now know that CPU supports them
        LDR     a1, [a2, #ProcessorFlags]
d1215 1
a1216 1
        MOV     lr, #0
d1227 1
d1526 1
a1526 1
        LDR     lr, =ZeroPage
d1798 1
a1798 1
        LDR     r0,=ZeroPage+InitClearRamWs             ;we can preserve r7-r9,r13 at logical address 52..67
d1800 1
a1800 1
        LDR     r8, =ZeroPage
d1802 1
a1802 1
        LDR     r7, =ZeroPage+PhysRamTable              ; point to 5 lots of (physaddr,size)
d1811 2
a1812 1
15        
d1890 1
a1890 1
        LDR     r0, =ZeroPage+InitClearRamWs
d1895 1
a1895 1
        LDR     r0, =ZeroPage+OsbyteVars + :INDEX: LastBREAK
d1901 1
a1901 1
        LDR     r0, =ZeroPage                           ; soft copy
d2069 1
a2069 1
        LDR     ip, =ZeroPage
d2163 1
a2163 1
        LDR     ip, =ZeroPage
d2181 1
a2181 1
        LDR     ip, =ZeroPage
d2210 1
a2210 1
        LDR     lr, =ZeroPage
d2219 1
a2219 1
        LDR     lr, =ZeroPage
d2242 1
a2242 1
        LDR     lr, =ZeroPage
d2255 1
a2255 1
        LDR     lr, =ZeroPage
d2263 1
a2263 1
        LDR     lr, =ZeroPage
d2286 1
a2286 1
        LDR     lr, =ZeroPage
d2294 1
a2294 1
        LDR     lr, =ZeroPage
d2352 1
a2352 1
        LDR     a2, =ZeroPage + OsbyteVars + :INDEX: RS423mode
d2356 1
a2356 1
        LDR     a2, =ZeroPage
d2366 1
a2366 1
        LDR     R0, =ZeroPage
d2408 1
a2408 1
        LDR     v2, =ZeroPage
d2410 1
a2410 4
        ADD     v1, v2, #IICBus_Base
      [ ZeroPage <> 0
        MOV     v2, #0
      ]
d2420 1
a2420 1
        LDR     a1, =ZeroPage+InitIRQWs
@


1.1.2.21.2.13
log
@Improve Reset_IRQ_Handler
Detail:
  s/HAL - Reset_IRQ_Handler now uses HAL_IRQSource to determine the cause of the interrupt, using that value to work out which IIC bus (if any) generated the IRQ. If it's unrecognised it passes it to HAL_KbdScanInterrupt, and if that fails to do anything it'll disable the IRQ.
  This aims to fix the spurious "No XStart!" debug spam that the OMAP IIC drivers produce when the keyboard scan is running, and to fix the potential IIC breakage that could occur by the IIC code trying to clear the non-existant interrupt.
  Note that behaviour of HAL_KbdScanInterrupt has now been changed; it now accepts the device number in a1, and is expected to return either -1 (if the interrupt was handled) or the device number given as input (if the interrupt wasn't handled, e.g. not from a device managed by the keyboard scan code).
Admin:
  Tested on rev C2 BB


Version 5.35, 4.79.2.98.2.49. Tagged as 'Kernel-5_35-4_79_2_98_2_49'
@
text
@a2418 1
        ; If it's not an IIC interrupt, pass it on to the keyboard scan code
a2420 1
        CallHAL HAL_IRQSource
d2422 3
a2424 1
        MOV     ip, #0
d2428 3
a2430 7
        BEQ     %FT20
        LDR     a2, [v1, #IICBus_Device]
        CMP     a2, a1
        ADREQ   lr, Reset_IRQ_Exit
        BEQ     IICIRQ
20
        ADD     ip, ip, #1
d2432 1
a2432 1
        CMP     ip, #IICBus_Count
d2434 3
a2436 2
        LDRB    a2, [v2, #InitIRQWs+KbdScanActive]
        TEQ     a2, #0
a2437 6
        ; Keyboard scan code will have return -1 if it handled the IRQ
        ; If it didn't handle it, or keyboard scanning is inactive, something
        ; bad has happened
        CMP     a1, #-1
        CallHAL HAL_IRQDisable,NE ; Stop the rogue device from killing us completely
Reset_IRQ_Exit
@


1.1.2.21.2.14
log
@ARMv7 fixes
Detail:
  hdr/Copro15ops:
    - Fixed incorrect encodings of ISH/ISHST variants of DMB/DSB instructions
  s/ARMops, s/HAL, hdr/KernelWS:
    - Replace the ARMv7 cache maintenance code with the example code from the ARMv7 ARM. This allows it to deal with caches with non power-of-two set/way counts, and caches with only one way.
    - Fixed Analyse_WB_CR7_Lx to use the cache level ID register to work out how many caches to query instead of just looking for a 0 result from CSSIDR.
    - Also only look for 7 cache levels, since level 8 doesn't exist according to the ARMv7 ARM.
  s/NewReset:
    - Removed some incorrect/misleading debug output
Admin:
  Tested on rev A2 BB-xM


Version 5.35, 4.79.2.98.2.51. Tagged as 'Kernel-5_35-4_79_2_98_2_51'
@
text
@d1077 6
a1082 6
        MOV     r9, #0
        MCR     p15, 0, r9, c7, c5, 0           ; invalidate instruction cache
        MCR     p15, 0, r9, c8, c7, 0           ; invalidate TLBs
        MCR     p15, 0, r9, c7, c5, 6           ; invalidate branch target predictor
        myDSB   ,r9,,y                          ; Wait for completion
        myISB   ,r9,,y
d1084 30
a1113 30
        MRC     p15, 0, r8, c0, c0, 1
        TST     r8, #&80000000 ; EQ=ARMv6, NE=ARMv7
        MCREQ   ARM_config_cp,0,r9,ARMv4_cache_reg,C7 ; ARMv3-ARMv6 I+D cache flush
        BEQ     %FT50 ; Skip to the end

        ; This is basically the same algorithm as the MaintainDataCache_WB_CR7_Lx macro, but tweaked to use less registers and to read from CP15 directly
        TST     r8, #&07000000
        BEQ     %FT50
        MOV     r11, #0 ; Current cache level
10 ; Loop1
        ADD     r10, r11, r11, LSR #1 ; Work out 3 x cachelevel
        MOV     r9, r8, LSR r10 ; bottom 3 bits are the Cache type for this level
        AND     r9, r9, #7 ; get those 3 bits alone
        CMP     r9, #2
        BLT     %FT40 ; no cache or only instruction cache at this level
        MCR     p15, 2, r11, c0, c0, 0 ; write CSSELR from r11
        myISB   ,r9
        MRC     p15, 1, r9, c0, c0, 0 ; read current CSSIDR to r9
        AND     r10, r9, #&7 ; extract the line length field
        ADD     r10, r10, #4 ; add 4 for the line length offset (log2 16 bytes)
        LDR     r8, =&3FF
        AND     r8, r8, r9, LSR #3 ; r8 is the max number on the way size (right aligned)
        CLZ     r13, r8 ; r13 is the bit position of the way size increment
        LDR     r12, =&7FFF
        AND     r12, r12, r9, LSR #13 ; r12 is the max number of the index size (right aligned)
20 ; Loop2
        MOV     r9, r12 ; r9 working copy of the max index size (right aligned)
30 ; Loop3
        ORR     r14, r11, r8, LSL r13 ; factor in the way number and cache number into r14
        ORR     r14, r14, r9, LSL r10 ; factor in the index number
d1115 10
a1124 12
        SUBS    r9, r9, #1 ; decrement the index
        BGE     %BT30
        SUBS    r8, r8, #1 ; decrement the way number
        BGE     %BT20
        MRC     p15, 0, r8, c0, c0, 1
40 ; Skip
        ADD     r11, r11, #2
        AND     r14, r8, #&07000000
        CMP     r14, r11, LSL #23
        BGT     %BT10        

50 ; Finished
@


1.1.2.21.2.14.2.1
log
@  Support for Raspberry Pi / BCM2835
Detail:
  Falls into two main areas: graphics support and ARM11 core support.
  A work in progress - in many cases the code changes need to be replaced
  with an alternative mechanism which will permit the kernel to still function
  on other platforms. Adrian marked these with "!!!" comments - I have added
  ! directives as well so that they don't get forgotten about.
Admin:
  Changes received from Adrian Lees. This revision represents the code largely
  as delivered, and is placed on its own branch (forked off from the version
  from which he worked). It is intended for reference. It doesn't build against
  current headers - this is likely to require a merge with the other changes
  to the kernel since that time.

Version 5.35, 4.79.2.98.2.52.2.1. Tagged as 'Kernel-5_35-4_79_2_98_2_52_2_1'
@
text
@a834 6
        ! 0, "FIXME: temporary code"
;!!! STILL DON'T KNOW WHY THE BODGE IN ARM_ANALYSE ITSELF DOESN'T WORK!
  MOV a1,#0
  MOV a2,#32
  STRB a2,[a1,#DCache_LineLen]
  STRB a2,[a1,#ICache_LineLen]
@


1.1.2.21.2.15
log
@Fix objasm 4 warnings
Detail:
  s/Arthur3, s/ChangeDyn, s/HAL, s/HeapMan, s/Middle, s/MoreSWIs, s/NewIRQs, s/Utility, s/VMSAv6, s/PMF/key, s/PMF/osbyte, s/PMF/osword, s/vdu/vdudecl, s/vdu/vdudriver, s/vdu/vduplot, s/vdu/vduwrch - Tweaked lots of LDM/STM instructions in order to get rid of the depracation/performance warnings
Admin:
  Tested on rev A2 BB-xM


Version 5.35, 4.79.2.98.2.53. Tagged as 'Kernel-5_35-4_79_2_98_2_53'
@
text
@d1875 1
a1875 2
        STMNEIA r0!, {r2,r8-r10}
        STMNEIA r0!, {r2,r8-r10}
@


