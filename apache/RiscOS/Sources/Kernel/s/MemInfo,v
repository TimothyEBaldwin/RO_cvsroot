head	4.15;
access;
symbols
	Kernel-6_14:4.15
	Kernel-6_01-3:4.13
	Kernel-6_13:4.15
	Kernel-6_12:4.15
	Kernel-6_11:4.15
	Kernel-6_10:4.15
	Kernel-6_09:4.14
	Kernel-6_08-4_129_2_10:4.11.2.3
	Kernel-6_08-4_129_2_9:4.11.2.3
	Kernel-6_08:4.13
	Kernel-6_07:4.13
	Kernel-6_06:4.13
	Kernel-6_05-4_129_2_8:4.11.2.3
	Kernel-6_05:4.13
	Kernel-6_04:4.13
	Kernel-6_03:4.13
	Kernel-6_01-2:4.13
	Kernel-6_01-4_146_2_1:4.13
	Kernel-6_02:4.13
	Kernel-6_01-1:4.13
	Kernel-6_01:4.13
	Kernel-6_00:4.13
	Kernel-5_99:4.13
	Kernel-5_98:4.13
	Kernel-5_97-4_129_2_7:4.11.2.3
	Kernel-5_97:4.13
	Kernel-5_96:4.13
	Kernel-5_95:4.13
	Kernel-5_94:4.13
	Kernel-5_93:4.13
	Kernel-5_92:4.13
	Kernel-5_91:4.13
	Kernel-5_90:4.13
	Kernel-5_89-4_129_2_6:4.11.2.2
	Kernel-5_89:4.12
	Kernel-5_88-4_129_2_5:4.11.2.2
	Kernel-5_88-4_129_2_4:4.11.2.2
	Kernel-5_88:4.12
	Kernel-5_87:4.12
	Kernel-5_86-4_129_2_3:4.11.2.1
	Kernel-5_86-4_129_2_2:4.11
	Kernel-5_86-4_129_2_1:4.11
	Kernel-5_86:4.11
	SMP:4.11.0.2
	SMP_bp:4.11
	Kernel-5_85:4.11
	Kernel-5_54-1:4.7
	Kernel-5_84:4.11
	Kernel-5_83:4.11
	Kernel-5_82:4.11
	Kernel-5_81:4.11
	Kernel-5_80:4.11
	Kernel-5_79:4.11
	Kernel-5_78:4.11
	Kernel-5_77:4.11
	Kernel-5_76:4.11
	Kernel-5_75:4.11
	Kernel-5_74:4.11
	Kernel-5_73:4.11
	Kernel-5_72:4.11
	Kernel-5_71:4.11
	Kernel-5_70:4.10
	Kernel-5_69:4.9
	Kernel-5_68:4.9
	Kernel-5_67:4.9
	Kernel-5_66:4.9
	Kernel-5_65:4.8
	Kernel-5_64:4.8
	Kernel-5_63:4.8
	Kernel-5_62:4.8
	Kernel-5_61:4.8
	Kernel-5_60:4.8
	Kernel-5_59:4.8
	Kernel-5_58:4.8
	Kernel-5_57:4.8
	Kernel-5_56:4.8
	Kernel-5_55:4.8
	Kernel-5_54:4.7
	Kernel-5_53:4.7
	Kernel-5_52:4.7
	Kernel-5_51:4.7
	Kernel-5_50:4.7
	Kernel-5_49:4.7
	HAL_merge:4.4.2.34
	Kernel-5_48:4.6
	Kernel-5_35-4_79_2_327:4.4.2.34
	Kernel-5_35-4_79_2_326:4.4.2.34
	Kernel-5_35-4_79_2_325:4.4.2.34
	Kernel-5_35-4_79_2_324:4.4.2.34
	Kernel-5_35-4_79_2_323:4.4.2.34
	Kernel-5_35-4_79_2_322:4.4.2.34
	Kernel-5_35-4_79_2_321:4.4.2.34
	Kernel-5_35-4_79_2_320:4.4.2.34
	Kernel-5_35-4_79_2_319:4.4.2.34
	Kernel-5_35-4_79_2_318:4.4.2.33
	Kernel-5_35-4_79_2_317:4.4.2.33
	Kernel-5_35-4_79_2_316:4.4.2.33
	Kernel-5_35-4_79_2_315:4.4.2.33
	Kernel-5_35-4_79_2_314:4.4.2.33
	Kernel-5_35-4_79_2_313:4.4.2.33
	Kernel-5_35-4_79_2_312:4.4.2.33
	Kernel-5_35-4_79_2_311:4.4.2.33
	Kernel-5_35-4_79_2_310:4.4.2.32
	Kernel-5_35-4_79_2_309:4.4.2.32
	Kernel-5_35-4_79_2_308:4.4.2.32
	Kernel-5_35-4_79_2_307:4.4.2.32
	Kernel-5_35-4_79_2_306:4.4.2.31
	Kernel-5_35-4_79_2_305:4.4.2.30
	Kernel-5_35-4_79_2_304:4.4.2.30
	Kernel-5_35-4_79_2_303:4.4.2.30
	Kernel-5_35-4_79_2_302:4.4.2.30
	Kernel-5_35-4_79_2_301:4.4.2.30
	Kernel-5_35-4_79_2_300:4.4.2.30
	Kernel-5_35-4_79_2_299:4.4.2.30
	Kernel-5_35-4_79_2_298:4.4.2.30
	Kernel-5_35-4_79_2_297:4.4.2.30
	Kernel-5_35-4_79_2_296:4.4.2.30
	Kernel-5_35-4_79_2_295:4.4.2.30
	Kernel-5_35-4_79_2_294:4.4.2.30
	Kernel-5_35-4_79_2_293:4.4.2.30
	Kernel-5_35-4_79_2_292:4.4.2.30
	Kernel-5_35-4_79_2_291:4.4.2.30
	Kernel-5_35-4_79_2_290:4.4.2.30
	Kernel-5_35-4_79_2_289:4.4.2.30
	Kernel-5_35-4_79_2_288:4.4.2.30
	Kernel-5_35-4_79_2_287:4.4.2.30
	Kernel-5_35-4_79_2_286:4.4.2.30
	Kernel-5_35-4_79_2_285:4.4.2.30
	Kernel-5_35-4_79_2_284:4.4.2.29
	Kernel-5_35-4_79_2_283:4.4.2.28
	Kernel-5_35-4_79_2_282:4.4.2.28
	Kernel-5_35-4_79_2_281:4.4.2.28
	Kernel-5_35-4_79_2_280:4.4.2.28
	Kernel-5_35-4_79_2_279:4.4.2.28
	Kernel-5_35-4_79_2_278:4.4.2.28
	Kernel-5_35-4_79_2_277:4.4.2.28
	Kernel-5_35-4_79_2_276:4.4.2.28
	Kernel-5_35-4_79_2_275:4.4.2.28
	Kernel-5_35-4_79_2_274:4.4.2.28
	Kernel-5_35-4_79_2_273:4.4.2.28
	Kernel-5_35-4_79_2_272:4.4.2.27
	Kernel-5_35-4_79_2_271:4.4.2.27
	Kernel-5_35-4_79_2_270:4.4.2.26
	Kernel-5_35-4_79_2_269:4.4.2.26
	Kernel-5_35-4_79_2_268:4.4.2.26
	Kernel-5_35-4_79_2_267:4.4.2.26
	Kernel-5_35-4_79_2_266:4.4.2.26
	Kernel-5_35-4_79_2_265:4.4.2.26
	Kernel-5_35-4_79_2_264:4.4.2.26
	Kernel-5_35-4_79_2_263:4.4.2.26
	Kernel-5_35-4_79_2_262:4.4.2.26
	Kernel-5_35-4_79_2_261:4.4.2.26
	Kernel-5_35-4_79_2_260:4.4.2.26
	Kernel-5_35-4_79_2_259:4.4.2.26
	Kernel-5_35-4_79_2_258:4.4.2.26
	Kernel-5_35-4_79_2_257:4.4.2.26
	Kernel-5_35-4_79_2_256:4.4.2.26
	Kernel-5_35-4_79_2_255:4.4.2.26
	Kernel-5_35-4_79_2_254:4.4.2.25
	Kernel-5_35-4_79_2_253:4.4.2.25
	Kernel-5_35-4_79_2_252:4.4.2.25
	Kernel-5_35-4_79_2_251:4.4.2.25
	Kernel-5_35-4_79_2_250:4.4.2.25
	Kernel-5_35-4_79_2_249:4.4.2.25
	Kernel-5_35-4_79_2_248:4.4.2.25
	Kernel-5_35-4_79_2_247:4.4.2.24
	Kernel-5_35-4_79_2_246:4.4.2.24
	Kernel-5_35-4_79_2_245:4.4.2.24
	Kernel-5_35-4_79_2_244:4.4.2.24
	Kernel-5_35-4_79_2_243:4.4.2.24
	Kernel-5_35-4_79_2_242:4.4.2.24
	Kernel-5_35-4_79_2_241:4.4.2.24
	Kernel-5_35-4_79_2_240:4.4.2.24
	Kernel-5_35-4_79_2_239:4.4.2.24
	Kernel-5_35-4_79_2_238:4.4.2.24
	Kernel-5_35-4_79_2_237:4.4.2.24
	Kernel-5_35-4_79_2_236:4.4.2.24
	Kernel-5_35-4_79_2_235:4.4.2.24
	Kernel-5_35-4_79_2_234:4.4.2.24
	Kernel-5_35-4_79_2_233:4.4.2.24
	Kernel-5_35-4_79_2_232:4.4.2.24
	Kernel-5_35-4_79_2_231:4.4.2.24
	Kernel-5_35-4_79_2_230:4.4.2.24
	Kernel-5_35-4_79_2_229:4.4.2.24
	Kernel-5_35-4_79_2_228:4.4.2.24
	Kernel-5_35-4_79_2_227:4.4.2.24
	Kernel-5_35-4_79_2_226:4.4.2.24
	Kernel-5_35-4_79_2_225:4.4.2.23
	Kernel-5_35-4_79_2_224:4.4.2.23
	Kernel-5_35-4_79_2_223:4.4.2.23
	Kernel-5_35-4_79_2_222:4.4.2.23
	Kernel-5_35-4_79_2_221:4.4.2.22
	Kernel-5_35-4_79_2_220:4.4.2.22
	Kernel-5_35-4_79_2_219:4.4.2.22
	Kernel-5_35-4_79_2_218:4.4.2.22
	Kernel-5_35-4_79_2_217:4.4.2.22
	Kernel-5_35-4_79_2_216:4.4.2.22
	Kernel-5_35-4_79_2_215:4.4.2.22
	Kernel-5_35-4_79_2_214:4.4.2.22
	Kernel-5_35-4_79_2_213:4.4.2.22
	Kernel-5_35-4_79_2_212:4.4.2.22
	Kernel-5_35-4_79_2_211:4.4.2.22
	Kernel-5_35-4_79_2_210:4.4.2.22
	Kernel-5_35-4_79_2_209:4.4.2.22
	Kernel-5_35-4_79_2_208:4.4.2.22
	Kernel-5_35-4_79_2_207:4.4.2.22
	Kernel-5_35-4_79_2_206:4.4.2.22
	Kernel-5_35-4_79_2_205:4.4.2.21
	Kernel-5_35-4_79_2_204:4.4.2.21
	Kernel-5_35-4_79_2_203:4.4.2.21
	Kernel-5_35-4_79_2_202:4.4.2.21
	Kernel-5_35-4_79_2_201:4.4.2.21
	Kernel-5_35-4_79_2_200:4.4.2.21
	Kernel-5_35-4_79_2_199:4.4.2.21
	Kernel-5_35-4_79_2_198:4.4.2.21
	Kernel-5_35-4_79_2_197:4.4.2.21
	Kernel-5_35-4_79_2_196:4.4.2.21
	Kernel-5_35-4_79_2_195:4.4.2.21
	Kernel-5_35-4_79_2_194:4.4.2.21
	Kernel-5_35-4_79_2_193:4.4.2.21
	Kernel-5_35-4_79_2_192:4.4.2.21
	Kernel-5_35-4_79_2_191:4.4.2.21
	Kernel-5_35-4_79_2_190:4.4.2.21
	Kernel-5_35-4_79_2_189:4.4.2.21
	Kernel-5_35-4_79_2_188:4.4.2.20
	Kernel-5_35-4_79_2_187:4.4.2.20
	Kernel-5_35-4_79_2_186:4.4.2.20
	Kernel-5_35-4_79_2_185:4.4.2.19
	Kernel-5_35-4_79_2_184:4.4.2.19
	Kernel-5_35-4_79_2_183:4.4.2.19
	Kernel-5_35-4_79_2_182:4.4.2.19
	Kernel-5_35-4_79_2_181:4.4.2.19
	Kernel-5_35-4_79_2_180:4.4.2.19
	Kernel-5_35-4_79_2_179:4.4.2.19
	Kernel-5_35-4_79_2_178:4.4.2.19
	Kernel-5_35-4_79_2_177:4.4.2.19
	Kernel-5_35-4_79_2_176:4.4.2.19
	Kernel-5_35-4_79_2_175:4.4.2.19
	Kernel-5_35-4_79_2_174:4.4.2.19
	Kernel-5_35-4_79_2_173:4.4.2.19
	Kernel-5_35-4_79_2_172:4.4.2.19
	Kernel-5_35-4_79_2_171:4.4.2.19
	Kernel-5_35-4_79_2_170:4.4.2.19
	Kernel-5_35-4_79_2_169:4.4.2.19
	Kernel-5_35-4_79_2_168:4.4.2.19
	Kernel-5_35-4_79_2_167:4.4.2.19
	Kernel-5_35-4_79_2_166:4.4.2.19
	Kernel-5_35-4_79_2_165:4.4.2.19
	RPi_merge:4.4.2.17.2.1
	Kernel-5_35-4_79_2_147_2_23:4.4.2.17.2.1
	Kernel-5_35-4_79_2_147_2_22:4.4.2.17.2.1
	Kernel-5_35-4_79_2_147_2_21:4.4.2.17.2.1
	Kernel-5_35-4_79_2_147_2_20:4.4.2.17.2.1
	Kernel-5_35-4_79_2_147_2_19:4.4.2.17.2.1
	Kernel-5_35-4_79_2_147_2_18:4.4.2.17.2.1
	Kernel-5_35-4_79_2_164:4.4.2.19
	Kernel-5_35-4_79_2_163:4.4.2.19
	Kernel-5_35-4_79_2_147_2_17:4.4.2.17.2.1
	Kernel-5_35-4_79_2_147_2_16:4.4.2.17.2.1
	Kernel-5_35-4_79_2_147_2_15:4.4.2.17.2.1
	Kernel-5_35-4_79_2_162:4.4.2.19
	Kernel-5_35-4_79_2_161:4.4.2.19
	Kernel-5_35-4_79_2_147_2_14:4.4.2.17
	Kernel-5_35-4_79_2_147_2_13:4.4.2.17
	Kernel-5_35-4_79_2_160:4.4.2.18
	Kernel-5_35-4_79_2_159:4.4.2.18
	Kernel-5_35-4_79_2_158:4.4.2.18
	Kernel-5_35-4_79_2_157:4.4.2.18
	Kernel-5_35-4_79_2_156:4.4.2.18
	Kernel-5_35-4_79_2_147_2_12:4.4.2.17
	Kernel-5_35-4_79_2_147_2_11:4.4.2.17
	Kernel-5_35-4_79_2_155:4.4.2.18
	Kernel-5_35-4_79_2_147_2_10:4.4.2.17
	Kernel-5_35-4_79_2_154:4.4.2.18
	Kernel-5_35-4_79_2_153:4.4.2.18
	Kernel-5_35-4_79_2_147_2_9:4.4.2.17
	Kernel-5_35-4_79_2_152:4.4.2.17
	Kernel-5_35-4_79_2_151:4.4.2.17
	Kernel-5_35-4_79_2_147_2_8:4.4.2.17
	Kernel-5_35-4_79_2_147_2_7:4.4.2.17
	Kernel-5_35-4_79_2_150:4.4.2.17
	Kernel-5_35-4_79_2_147_2_6:4.4.2.17
	Kernel-5_35-4_79_2_147_2_5:4.4.2.17
	Kernel-5_35-4_79_2_149:4.4.2.17
	Kernel-5_35-4_79_2_147_2_4:4.4.2.17
	Kernel-5_35-4_79_2_147_2_3:4.4.2.17
	Kernel-5_35-4_79_2_148:4.4.2.17
	Kernel-5_35-4_79_2_147_2_2:4.4.2.17
	Kernel-5_35-4_79_2_147_2_1:4.4.2.17
	RPi:4.4.2.17.0.2
	RPi_bp:4.4.2.17
	Kernel-5_35-4_79_2_98_2_52_2_1:4.4.2.14.2.3
	alees_Kernel_dev:4.4.2.14.2.3.0.2
	alees_Kernel_dev_bp:4.4.2.14.2.3
	Kernel-5_35-4_79_2_147:4.4.2.17
	Kernel-5_35-4_79_2_146:4.4.2.17
	Kernel-5_35-4_79_2_145:4.4.2.17
	Kernel-5_35-4_79_2_144:4.4.2.17
	Kernel-5_35-4_79_2_143:4.4.2.17
	Kernel-5_35-4_79_2_142:4.4.2.17
	Kernel-5_35-4_79_2_141:4.4.2.17
	Kernel-5_35-4_79_2_140:4.4.2.17
	Kernel-5_35-4_79_2_139:4.4.2.17
	Kernel-5_35-4_79_2_138:4.4.2.17
	Kernel-5_35-4_79_2_137:4.4.2.17
	Kernel-5_35-4_79_2_136:4.4.2.17
	Kernel-5_35-4_79_2_135:4.4.2.17
	Kernel-5_35-4_79_2_134:4.4.2.17
	Kernel-5_35-4_79_2_133:4.4.2.17
	Kernel-5_35-4_79_2_132:4.4.2.17
	Kernel-5_35-4_79_2_131:4.4.2.17
	Kernel-5_35-4_79_2_130:4.4.2.17
	Kernel-5_35-4_79_2_129:4.4.2.17
	Kernel-5_35-4_79_2_128:4.4.2.17
	Kernel-5_35-4_79_2_127:4.4.2.17
	Kernel-5_35-4_79_2_126:4.4.2.17
	Kernel-5_35-4_79_2_125:4.4.2.17
	Kernel-5_35-4_79_2_124:4.4.2.17
	Kernel-5_35-4_79_2_123:4.4.2.17
	Cortex_merge:4.4.2.14.2.3
	Kernel-5_35-4_79_2_122:4.4.2.16
	Kernel-5_35-4_79_2_98_2_54:4.4.2.14.2.3
	Kernel-5_35-4_79_2_98_2_53:4.4.2.14.2.3
	Kernel-5_35-4_79_2_98_2_52:4.4.2.14.2.3
	Kernel-5_35-4_79_2_98_2_51:4.4.2.14.2.3
	Kernel-5_35-4_79_2_98_2_50:4.4.2.14.2.3
	Kernel-5_35-4_79_2_98_2_49:4.4.2.14.2.3
	Kernel-5_35-4_79_2_98_2_48:4.4.2.14.2.3
	Kernel-5_35-4_79_2_121:4.4.2.16
	Kernel-5_35-4_79_2_98_2_47:4.4.2.14.2.2
	Kernel-5_35-4_79_2_120:4.4.2.16
	Kernel-5_35-4_79_2_98_2_46:4.4.2.14.2.2
	Kernel-5_35-4_79_2_119:4.4.2.16
	Kernel-5_35-4_79_2_98_2_45:4.4.2.14.2.2
	Kernel-5_35-4_79_2_98_2_44:4.4.2.14.2.2
	Kernel-5_35-4_79_2_118:4.4.2.16
	Kernel-5_35-4_79_2_98_2_43:4.4.2.14.2.2
	Kernel-5_35-4_79_2_117:4.4.2.16
	Kernel-5_35-4_79_2_116:4.4.2.16
	Kernel-5_35-4_79_2_98_2_42:4.4.2.14.2.1
	Kernel-5_35-4_79_2_115:4.4.2.16
	Kernel-5_35-4_79_2_98_2_41:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_40:4.4.2.14.2.1
	Kernel-5_35-4_79_2_114:4.4.2.16
	Kernel-5_35-4_79_2_98_2_39:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_38:4.4.2.14.2.1
	Kernel-5_35-4_79_2_113:4.4.2.15
	Kernel-5_35-4_79_2_112:4.4.2.15
	Kernel-5_35-4_79_2_98_2_37:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_36:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_35:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_34:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_33:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_32:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_31:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_30:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_29:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_28:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_27:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_26:4.4.2.14.2.1
	Kernel-5_35-4_79_2_111:4.4.2.15
	Kernel-5_35-4_79_2_98_2_25:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_24:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_23:4.4.2.14.2.1
	Kernel-5_35-4_79_2_110:4.4.2.15
	Kernel-5_35-4_79_2_98_2_22:4.4.2.14.2.1
	Kernel-5_35-4_79_2_109:4.4.2.15
	Kernel-5_35-4_79_2_98_2_21:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_20:4.4.2.14.2.1
	Kernel-5_35-4_79_2_108:4.4.2.15
	Kernel-5_35-4_79_2_107:4.4.2.15
	Kernel-5_35-4_79_2_98_2_19:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_18:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_17:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_16:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_15:4.4.2.14.2.1
	Kernel-5_35-4_79_2_106:4.4.2.15
	Kernel-5_35-4_79_2_105:4.4.2.15
	Kernel-5_35-4_79_2_104:4.4.2.15
	Kernel-5_35-4_79_2_98_2_14:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_13:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_12:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_11:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_10:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_9:4.4.2.14.2.1
	Kernel-5_35-4_79_2_103:4.4.2.15
	Kernel-5_35-4_79_2_102:4.4.2.14
	Kernel-5_35-4_79_2_98_2_8:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_7:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_6:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_5:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_4:4.4.2.14.2.1
	Kernel-5_35-4_79_2_101:4.4.2.14
	Kernel-5_35-4_79_2_100:4.4.2.14
	Kernel-5_35-4_79_2_99:4.4.2.14
	Kernel-5_35-4_79_2_98_2_3:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_2:4.4.2.14.2.1
	Kernel-5_35-4_79_2_98_2_1:4.4.2.14
	Cortex:4.4.2.14.0.2
	Cortex_bp:4.4.2.14
	Kernel-5_35-4_79_2_98:4.4.2.14
	Kernel-5_35-4_79_2_97:4.4.2.14
	Kernel-5_35-4_79_2_96:4.4.2.14
	Kernel-5_35-4_79_2_95:4.4.2.14
	Kernel-5_35-4_79_2_94:4.4.2.14
	Kernel-5_35-4_79_2_93:4.4.2.14
	Kernel-5_35-4_79_2_92:4.4.2.14
	Kernel-5_35-4_79_2_91:4.4.2.14
	Kernel-5_35-4_79_2_90:4.4.2.14
	Kernel-5_35-4_79_2_89:4.4.2.14
	Kernel-5_35-4_79_2_88:4.4.2.14
	Kernel-5_35-4_79_2_87:4.4.2.14
	Kernel-5_35-4_79_2_86:4.4.2.14
	Kernel-5_35-4_79_2_85:4.4.2.14
	Kernel-5_35-4_79_2_84:4.4.2.14
	Kernel-5_35-4_79_2_83:4.4.2.14
	Kernel-5_35-4_79_2_82:4.4.2.14
	Kernel-5_35-4_79_2_81:4.4.2.14
	Kernel-5_35-4_79_2_80:4.4.2.14
	Kernel-5_35-4_79_2_79:4.4.2.14
	Kernel-5_35-4_79_2_78:4.4.2.14
	Kernel-5_35-4_79_2_77:4.4.2.14
	RO_5_07:4.4.2.14
	Kernel-5_35-4_79_2_76:4.4.2.14
	Kernel-5_35-4_79_2_75:4.4.2.14
	Kernel-5_35-4_79_2_74:4.4.2.14
	Kernel-5_35-4_79_2_73:4.4.2.14
	Kernel-5_35-4_79_2_72:4.4.2.14
	Kernel-5_35-4_79_2_71:4.4.2.14
	Kernel-5_35-4_79_2_70:4.4.2.14
	Kernel-5_35-4_79_2_69:4.4.2.14
	Kernel-5_35-4_79_2_68:4.4.2.14
	Kernel-5_35-4_79_2_67:4.4.2.14
	Kernel-5_35-4_79_2_66:4.4.2.14
	Kernel-5_35-4_79_2_65:4.4.2.14
	Kernel-5_35-4_79_2_64:4.4.2.14
	Kernel-5_35-4_79_2_63:4.4.2.14
	Kernel-5_35-4_79_2_62:4.4.2.14
	Kernel-5_35-4_79_2_61:4.4.2.14
	Kernel-5_35-4_79_2_59:4.4.2.14
	Kernel-5_35-4_79_2_58:4.4.2.13
	Kernel-5_35-4_79_2_57:4.4.2.13
	Kernel-5_35-4_79_2_56:4.4.2.13
	Kernel-5_35-4_79_2_55:4.4.2.13
	Kernel-5_35-4_79_2_54:4.4.2.12
	Kernel-5_35-4_79_2_53:4.4.2.12
	Kernel-5_35-4_79_2_52:4.4.2.12
	Kernel-5_35-4_79_2_51:4.4.2.12
	Kernel-5_35-4_79_2_50:4.4.2.11
	Kernel-5_35-4_79_2_49:4.4.2.11
	Kernel-5_35-4_79_2_48:4.4.2.11
	Kernel-5_47:4.5
	Kernel-5_46-4_90_2_1:4.5
	nbingham_Kernel_FastNC_dev_bp:4.5
	nbingham_Kernel_FastNC_dev:4.5.0.2
	Kernel-5_46:4.5
	Kernel-5_45:4.5
	Kernel-5_35-4_79_2_47:4.4.2.10
	Kernel-5_35-4_79_2_46:4.4.2.10
	Kernel-5_35-4_79_2_45:4.4.2.10
	Kernel-5_35-4_79_2_44:4.4.2.10
	Kernel-5_35-4_79_2_25_2_2:4.4.2.5
	Kernel-5_35-4_79_2_43:4.4.2.10
	Kernel-5_35-4_79_2_42:4.4.2.10
	Kernel-5_35-4_79_2_41:4.4.2.10
	Kernel-5_35-4_79_2_40:4.4.2.10
	Kernel-5_35-4_79_2_39:4.4.2.10
	Kernel-5_35-4_79_2_38:4.4.2.10
	Kernel-5_35-4_79_2_37:4.4.2.9
	Kernel-5_35-4_79_2_36:4.4.2.9
	Kernel-5_35-4_79_2_35:4.4.2.9
	Kernel-5_35-4_79_2_34:4.4.2.9
	Kernel-5_35-4_79_2_33:4.4.2.9
	Kernel-5_35-4_79_2_32:4.4.2.9
	Kernel-5_44:4.5
	Kernel-5_35-4_79_2_25_2_1:4.4.2.5
	Kernel-5_43:4.5
	Kernel-5_35-4_79_2_31:4.4.2.8
	Kernel-5_35-4_79_2_30:4.4.2.7
	Kernel-5_35-4_79_2_29:4.4.2.6
	Kernel-5_35-4_79_2_28:4.4.2.6
	Kernel-5_35-4_79_2_27:4.4.2.5
	Kernel-5_35-4_79_2_26:4.4.2.5
	Kernel-5_42:4.5
	Kernel-5_41:4.5
	Kernel-5_40:4.5
	Kernel-5_35-4_79_2_25:4.4.2.5
	Kernel-5_35-4_79_2_24:4.4.2.5
	Kernel-5_35-4_79_2_23:4.4.2.5
	Kernel-5_35-4_79_2_22:4.4.2.5
	Kernel-5_35-4_79_2_21:4.4.2.5
	Kernel-5_35-4_79_2_20:4.4.2.5
	Kernel-5_35-4_79_2_19:4.4.2.5
	Kernel-5_35-4_79_2_18:4.4.2.5
	Kernel-5_35-4_79_2_17:4.4.2.5
	Kernel-5_35-4_79_2_16:4.4.2.5
	Kernel-5_35-4_79_2_15:4.4.2.5
	Kernel-5_35-4_79_2_14:4.4.2.5
	Kernel-5_39:4.5
	Kernel-5_13-4_52_2_1:4.3
	Bethany:4.3.0.2
	Kernel-5_38:4.5
	Kernel-5_35-4_79_2_13:4.4.2.4
	Kernel-5_35-4_79_2_12:4.4.2.4
	Kernel-5_35-4_79_2_11:4.4.2.4
	Kernel-5_37:4.5
	Kernel-5_35-4_79_2_10:4.4.2.4
	Kernel-5_35-4_79_2_9:4.4.2.4
	Kernel-5_36:4.5
	Kernel-5_35-4_79_2_8:4.4.2.1
	Kernel-5_35-4_79_2_7:4.4.2.1
	Kernel-5_35-4_79_2_6:4.4.2.1
	Kernel-5_35-4_79_2_5:4.4.2.1
	Kernel-5_35-4_79_2_4:4.4.2.1
	Kernel-5_35-4_79_2_3:4.4.2.1
	Kernel-5_35-4_79_2_2:4.4.2.1
	dellis_autobuild_BaseSW:4.4
	Kernel-5_35-4_79_2_1:4.4.2.1
	HAL:4.4.0.2
	Kernel-5_35:4.4
	Kernel-5_34:4.4
	Kernel-5_33:4.4
	Kernel-5_32:4.4
	Kernel-5_31:4.4
	Kernel-5_30:4.4
	Kernel-5_29:4.4
	Kernel-5_28:4.4
	Kernel-5_27:4.4
	Kernel-5_26:4.4
	Kernel-5_25:4.4
	Kernel-5_24:4.4
	Kernel-5_23:4.4
	Kernel-5_22:4.3
	sbrodie_sedwards_16Mar2000:4.3
	Kernel-5_21:4.3
	Kernel-5_20:4.3
	Kernel-5_19:4.3
	Kernel-5_18:4.3
	Kernel-5_17:4.3
	Kernel-5_16:4.3
	Kernel-5_15:4.3
	Kernel-5_14:4.3
	Kernel-5_13:4.3
	Kernel-5_12:4.3
	Kernel-5_11:4.3
	Kernel-5_10:4.3
	Kernel-5_09:4.3
	Kernel-5_08:4.3
	Kernel-5_07:4.3
	Kernel-5_06:4.3
	Kernel-5_05:4.3
	Kernel-5_04:4.3
	Kernel-5_03:4.3
	Kernel-5_02:4.3
	Kernel-5_01:4.3
	Kernel-5_00:4.3
	Kernel-4_99:4.3
	Kernel-4_98:4.3
	Kernel-4_97:4.3
	Kernel-4_96:4.3
	Kernel-4_95:4.3
	Kernel-4_94:4.3
	Kernel-4_93:4.3
	Kernel-4_92:4.3
	Kernel-4_91:4.3
	Kernel-4_90:4.3
	dcotton_autobuild_BaseSW:4.5
	Kernel-4_89:4.3
	Kernel-4_88:4.3
	Kernel-4_87:4.3
	Kernel-4_86:4.3
	Kernel-4_85:4.3
	sbrodie_UrsulaRiscPC_Kernel_19Aug99:4.2.2.2.2.1
	Kernel-4_84:4.3
	sbrodie_UrsulaRiscPC_Kernel_18Aug99:4.2.2.2.2.1
	Ursula_RiscPC_bp:4.2.2.2
	Kernel-4_83:4.3
	Kernel-4_82:4.3
	Kernel-4_81:4.3
	Kernel-4_80:4.3
	Kernel-4_79:4.3
	Kernel-4_78:4.3
	Kernel-4_77:4.3
	Kernel-4_76:4.3
	Kernel-4_75:4.2
	Kernel-4_74:4.2
	Kernel-4_73:4.2
	Kernel-4_72:4.2
	Kernel-4_71:4.2
	Kernel-4_70:4.2
	Kernel-4_69:4.2
	Kernel-4_68:4.2
	mstphens_UrsulaRiscPCBuild_20Nov98:4.2.2.2.2.1
	Ursula_RiscPC:4.2.2.2.0.2
	Kernel-4_63-1_1_2_5:4.1.7.1
	Kernel-4_63-1_1_2_4:4.1.7.1
	Kernel-4_67:4.2
	Kernel-4_66:4.2
	Kernel-4_63-1_1_2_3:4.1.7.1
	Kernel-4_65:4.2
	Ursula_merge:4.2
	Kernel-4_64:4.2
	mstphens_Kernel-3_81:4.2.2.3
	Kernel-4_63-1_1_2_2:4.1.7.1
	nicke_Kernel_4_62:4.1.7.1
	rthornb_UrsulaBuild-19Aug1998:4.2.2.2
	UrsulaBuild_FinalSoftload:4.2.2.2
	rthornb_UrsulaBuild-12Aug1998:4.2.2.2
	aglover_UrsulaBuild-05Aug1998:4.2.2.2
	rthornb_UrsulaBuild-29Jul1998:4.2.2.2
	rthornb_UrsulaBuild-22Jul1998:4.2.2.2
	nturton_v459:4.1.7.1
	nturton_v460:4.1.7.1
	rthornb_UrsulaBuild-15Jul1998:4.2.2.2
	rthornb_UrsulaBuild-07Jul1998:4.2.2.2
	rthornb_UrsulaBuild-17Jun1998:4.2.2.2
	rthornb_UrsulaBuild-03Jun1998:4.2.2.2
	rthornb_UrsulaBuild-27May1998:4.2.2.2
	mstphens_Kernel-3_80:4.2.2.2
	rthornb_UrsulaBuild-21May1998:4.2.2.2
	afrost_Boca-1_2-Beta:4.1.7.1
	rthornb_UrsulaBuild_01May1998:4.2.2.2
	afrost_NC2_Generic:4.1.7.1
	Spinner_B20_2:4.1.7.1
	Spinner_19_3:4.1.7.1
	Spinner_B18:4.1.7.1
	Spinner_B17:4.1.7.1
	Spinner_B15:4.1.7.1
	Spinner_B14:4.1.7.1
	Spinner_B13:4.1.7.1
	Spinner_B12:4.1.7.1
	Spinner_B10:4.1.7.1
	Daytona:4.2.0.6
	Daytona_bp:4.2
	Ursula_bp:4.2
	Ursula:4.2.0.2
	Spinner_B7:4.1.7.1
	RO_3_71:4.1.3.1
	ARTtmp_merge:4.1.7.1
	Spin_3Apr97:4.1.7.1
	ARTtmp:4.1.7.1.0.2
	Spin_merge:4.1.7.1
	MergeFiles:4.1.3.1
	RO_3_70:4.1.3.1
	NC_1_06:4.1.7.1
	Spinner:4.1.7
	Spin_xx:4.1.5
	NC_xx:4.1.5.1
	RO_3_60:4.1.1.1
	StrongARM:4.1.3
	Black:4.1.1;
locks; strict;
comment	@# @;


4.15
date	2018.07.08.12.52.55;	author jlee;	state Exp;
branches;
next	4.14;
commitid	0EEuEFgNyuhXGkJA;

4.14
date	2018.07.07.14.06.29;	author jlee;	state Exp;
branches;
next	4.13;
commitid	VD8qInwgaJB98dJA;

4.13
date	2017.10.07.12.11.51;	author jlee;	state Exp;
branches;
next	4.12;
commitid	afNukMjf0Y8Ug7aA;

4.12
date	2017.08.19.16.43.35;	author jlee;	state Exp;
branches;
next	4.11;
commitid	43sQ2BtQwXrLlQ3A;

4.11
date	2016.12.13.19.41.12;	author jlee;	state Exp;
branches
	4.11.2.1;
next	4.10;
commitid	XeVhUEC50BLVkRxz;

4.10
date	2016.12.13.19.03.34;	author jlee;	state Exp;
branches;
next	4.9;
commitid	dvbJa4TQHit18Rxz;

4.9
date	2016.12.13.16.42.51;	author jlee;	state Exp;
branches;
next	4.8;
commitid	aGog9bB8f4QKlQxz;

4.8
date	2016.08.02.22.10.44;	author jlee;	state Exp;
branches;
next	4.7;
commitid	CnQYuUGzojQfrMgz;

4.7
date	2016.06.30.20.28.56;	author jlee;	state Exp;
branches;
next	4.6;
commitid	lMnWzoE9eJz3Wwcz;

4.6
date	2016.06.30.20.08.08;	author jlee;	state Exp;
branches;
next	4.5;
commitid	IWoXxARWeuLDOwcz;

4.5
date	2000.10.10.10.16.31;	author sbrodie;	state Exp;
branches;
next	4.4;

4.4
date	2000.04.04.14.27.31;	author kbracey;	state Exp;
branches
	4.4.2.1;
next	4.3;

4.3
date	99.04.30.15.18.53;	author kbracey;	state Exp;
branches;
next	4.2;

4.2
date	97.01.21.14.07.07;	author nturton;	state Exp;
branches
	4.2.2.1;
next	4.1;

4.1
date	96.11.05.09.41.21;	author nturton;	state Exp;
branches
	4.1.1.1
	4.1.3.1
	4.1.5.1
	4.1.7.1;
next	;

4.11.2.1
date	2017.08.12.16.22.48;	author jlee;	state Exp;
branches;
next	4.11.2.2;
commitid	FtabPHwDaEOAsW2A;

4.11.2.2
date	2017.08.31.18.47.03;	author jlee;	state Exp;
branches;
next	4.11.2.3;
commitid	e0d3gZHfENrcEo5A;

4.11.2.3
date	2018.02.16.00.01.40;	author jlee;	state Exp;
branches;
next	;
commitid	L7HYXYTsWSFlZ0rA;

4.4.2.1
date	2000.09.15.12.38.01;	author kbracey;	state Exp;
branches;
next	4.4.2.2;

4.4.2.2
date	2000.10.06.09.08.11;	author kbracey;	state Exp;
branches;
next	4.4.2.3;

4.4.2.3
date	2000.10.09.15.59.15;	author kbracey;	state Exp;
branches;
next	4.4.2.4;

4.4.2.4
date	2000.10.10.10.37.10;	author sbrodie;	state Exp;
branches;
next	4.4.2.5;

4.4.2.5
date	2001.01.09.17.17.32;	author mstephen;	state Exp;
branches;
next	4.4.2.6;

4.4.2.6
date	2001.05.01.14.10.59;	author mstephen;	state Exp;
branches;
next	4.4.2.7;

4.4.2.7
date	2001.05.17.10.51.11;	author kbracey;	state Exp;
branches;
next	4.4.2.8;

4.4.2.8
date	2001.05.22.15.27.54;	author mstephen;	state Exp;
branches;
next	4.4.2.9;

4.4.2.9
date	2001.06.06.14.24.00;	author mstephen;	state Exp;
branches;
next	4.4.2.10;

4.4.2.10
date	2001.06.18.14.49.44;	author mstephen;	state Exp;
branches;
next	4.4.2.11;

4.4.2.11
date	2002.10.07.17.29.41;	author kbracey;	state Exp;
branches;
next	4.4.2.12;

4.4.2.12
date	2002.11.30.00.31.08;	author bavison;	state Exp;
branches;
next	4.4.2.13;

4.4.2.13
date	2003.01.27.15.25.34;	author kbracey;	state Exp;
branches;
next	4.4.2.14;

4.4.2.14
date	2003.03.31.09.44.10;	author kbracey;	state Exp;
branches
	4.4.2.14.2.1;
next	4.4.2.15;

4.4.2.15
date	2009.06.07.22.23.36;	author bavison;	state Exp;
branches;
next	4.4.2.16;

4.4.2.16
date	2011.07.18.22.46.02;	author jlee;	state Exp;
branches;
next	4.4.2.17;
commitid	62XuglNpufjlQ3sv;

4.4.2.17
date	2011.11.26.21.11.15;	author jlee;	state Exp;
branches
	4.4.2.17.2.1;
next	4.4.2.18;
commitid	cI3W0zbtALQG6TIv;

4.4.2.18
date	2012.06.18.20.17.55;	author rsprowson;	state Exp;
branches;
next	4.4.2.19;
commitid	KeuVX14bDnORde9w;

4.4.2.19
date	2012.07.09.07.10.51;	author rsprowson;	state Exp;
branches;
next	4.4.2.20;
commitid	gcLIBE0WziF1cRbw;

4.4.2.20
date	2013.03.28.21.36.24;	author jlee;	state Exp;
branches;
next	4.4.2.21;
commitid	UN0GP6eB0LlNyBJw;

4.4.2.21
date	2013.05.27.09.49.03;	author rsprowson;	state Exp;
branches;
next	4.4.2.22;
commitid	17yY0YiXwOOAIfRw;

4.4.2.22
date	2013.12.19.00.08.57;	author jlee;	state Exp;
branches;
next	4.4.2.23;
commitid	sFhm1gbZFIB4TFhx;

4.4.2.23
date	2014.04.20.17.00.22;	author jlee;	state Exp;
branches;
next	4.4.2.24;
commitid	6eesW4yWEAvSyrxx;

4.4.2.24
date	2014.06.01.10.15.45;	author jlee;	state Exp;
branches;
next	4.4.2.25;
commitid	QTz6wSc2iYaoYNCx;

4.4.2.25
date	2014.12.06.09.42.44;	author rsprowson;	state Exp;
branches;
next	4.4.2.26;
commitid	IIeZ2aoECRgpKX0y;

4.4.2.26
date	2015.01.20.20.21.26;	author jlee;	state Exp;
branches;
next	4.4.2.27;
commitid	XTFnnthpWkiPPN6y;

4.4.2.27
date	2015.07.17.00.28.26;	author jlee;	state Exp;
branches;
next	4.4.2.28;
commitid	s5cS0yKIca1Nvzty;

4.4.2.28
date	2015.08.05.21.51.31;	author jlee;	state Exp;
branches;
next	4.4.2.29;
commitid	SpZpzVH47zb408wy;

4.4.2.29
date	2015.08.31.19.28.37;	author jlee;	state Exp;
branches;
next	4.4.2.30;
commitid	Ni3KL17bG70fnszy;

4.4.2.30
date	2015.09.01.21.14.47;	author jlee;	state Exp;
branches;
next	4.4.2.31;
commitid	6XNouVrEaXcGVAzy;

4.4.2.31
date	2016.03.10.22.57.41;	author jlee;	state Exp;
branches;
next	4.4.2.32;
commitid	DAXUqMY2ucjim9Yy;

4.4.2.32
date	2016.03.12.01.38.11;	author jlee;	state Exp;
branches;
next	4.4.2.33;
commitid	SpAXij222A7qdiYy;

4.4.2.33
date	2016.03.27.02.00.54;	author jlee;	state Exp;
branches;
next	4.4.2.34;
commitid	nHwGXjCPCyRkRd0z;

4.4.2.34
date	2016.05.19.21.03.49;	author jlee;	state Exp;
branches;
next	;
commitid	7f3vfXP8BWCNt87z;

4.4.2.14.2.1
date	2009.02.21.17.41.25;	author jlee;	state Exp;
branches;
next	4.4.2.14.2.2;

4.4.2.14.2.2
date	2011.08.01.19.54.10;	author jlee;	state Exp;
branches;
next	4.4.2.14.2.3;
commitid	m0TvirYe9M1urQtv;

4.4.2.14.2.3
date	2011.08.08.23.28.26;	author jlee;	state Exp;
branches;
next	;
commitid	D7rzILnwRRSXoLuv;

4.4.2.17.2.1
date	2012.07.20.00.51.55;	author bavison;	state Exp;
branches;
next	;
commitid	ELcSZZYPVgQ6Kedw;

4.2.2.1
date	97.09.09.13.33.16;	author mstphens;	state Exp;
branches;
next	4.2.2.2;

4.2.2.2
date	97.10.21.15.31.18;	author mstphens;	state Exp;
branches
	4.2.2.2.2.1;
next	4.2.2.3;

4.2.2.3
date	98.09.24.13.17.13;	author mstphens;	state Exp;
branches;
next	;

4.2.2.2.2.1
date	98.11.23.14.59.12;	author mstphens;	state Exp;
branches;
next	;

4.1.1.1
date	96.11.05.09.41.21;	author nturton;	state Exp;
branches;
next	;

4.1.3.1
date	96.11.06.02.00.33;	author nturton;	state Exp;
branches;
next	;

4.1.5.1
date	96.11.21.12.11.18;	author nturton;	state Exp;
branches;
next	;

4.1.7.1
date	96.11.29.21.03.40;	author nturton;	state Exp;
branches;
next	;


desc
@@


4.15
log
@Fix OS_Memory 0 "make temporarily uncacheable" not reporting errors
Detail:
  s/MemInfo - The wrapper around OS_Memory 0 introduced in Kernel-5_35-4_79_2_311 was preserving the wrong PSR field on exit, causing any error generated by the core code to be lost.
Admin:
  Tested on Iyonix
  Fixes *screensave saving mostly white pixels (address translation for "external" VRAM should have failed and caused ADFS to fall back to a bounce buffer)
  Is also likely to be the cause of https://www.riscosopen.org/forum/forums/5/topics/11713 (address translation should have failed for soft ROM)


Version 6.10. Tagged as 'Kernel-6_10'
@
text
@; Copyright 1996 Acorn Computers Ltd
;
; Licensed under the Apache License, Version 2.0 (the "License");
; you may not use this file except in compliance with the License.
; You may obtain a copy of the License at
;
;     http://www.apache.org/licenses/LICENSE-2.0
;
; Unless required by applicable law or agreed to in writing, software
; distributed under the License is distributed on an "AS IS" BASIS,
; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
; See the License for the specific language governing permissions and
; limitations under the License.
;
; > MemInfo

        LTORG

;----------------------------------------------------------------------------------------
; MemorySWI
;
;       In:     r0 = reason code and flags
;                       bits 0-7  = reason code
;                       bits 3-31 = reason specific flags
;       Out:    specific to reason codes
;
;       Perform miscellaneous operations for memory management.
;
MemorySWI       ROUT
        Push    lr                              ; Save real return address.
        AND     lr, r0, #&FF                    ; Get reason code.
        CMP     lr, #(%40-%30):SHR:2            ; If valid reason code then
        ADDCC   lr, lr, #(%30-%10):SHR:2        ;   determine where to jump to in branch table,
        ADDCC   lr, pc, lr, LSL #2
        Push    lr, CC                          ;   save address so we can
10
        ADRCC   lr, MemReturn                   ;   set up default return address for handler routines
        Pull    pc, CC                          ;   and jump into branch table.
20
        ADRL    r0, ErrorBlock_HeapBadReason    ; Otherwise, unknown reason code.
        SETV
        ; Drop through to...

MemReturn
 [ International
        BLVS    TranslateError
 ]
        Pull    lr                              ; Get back real return address.
        BVS     SLVK_SetV
        ExitSWIHandler

30
        B       MemoryConvertFIQCheck           ; 0
        B       %BT20                           ; Reason codes 1-5 are reserved.
        B       %BT20
        B       %BT20
        B       %BT20
        B       %BT20
        B       MemoryPhysSize                  ; 6
        B       MemoryReadPhys                  ; 7
        B       MemoryAmounts                   ; 8
        B       MemoryIOSpace                   ; 9
        B       %BT20                           ; Reason code 10 reserved (for free pool locking)
        B       %BT20                           ; Reason code 11 reserved (for PCImapping).
        B       RecommendPage                   ; 12
        B       MapIOpermanent                  ; 13
        B       AccessPhysAddr                  ; 14
        B       ReleasePhysAddr                 ; 15
        B       MemoryAreaInfo                  ; 16
        B       MemoryAccessPrivileges          ; 17
        B       FindAccessPrivilege             ; 18
        B       DMAPrep                         ; 19
        B       ChangeCompatibility             ; 20
        B       %BT20                           ; 21 |
        B       %BT20                           ; 22 | Reserved for us
        B       %BT20                           ; 23 |
        B       CheckMemoryAccess               ; 24
                                                ; 25+ reserved for ROL
40


;----------------------------------------------------------------------------------------
; MemoryConvert
;
;       In:     r0 = flags
;                       bit     meaning
;                       0-7     0 (reason code)
;                       8       page number provided when set
;                       9       logical address provided when set
;                       10      physical address provided when set
;                       11      fill in page number when set
;                       12      fill in logical address when set
;                       13      fill in physical address when set
;                       14-15   0,1=don't change cacheability
;                               2=disable caching on these pages
;                               3=enable caching on these pages
;                       16-31   reserved (set to 0)
;               r1 -> page block
;               r2 = number of 3 word entries in page block
;
;       Out:    r1 -> updated page block
;
;       Converts between representations of memory addresses. Can also set the
;       cacheability of the specified pages.
;

; Declare symbols used for decoding flags (given and wanted are used
; so that C can be cleared by rotates of the form a,b). We have to munge
; the flags a bit to make the rotates even.
;
ppn             *       1:SHL:0         ; Bits for address formats.
logical         *       1:SHL:1
physical        *       1:SHL:2
all             *       ppn :OR: logical :OR: physical
given           *       24              ; Rotate for given fields.
wanted          *       20              ; Rotate for wanted fields.
ppn_bits        *       ((ppn :SHL: 4) :OR: ppn)
logical_bits    *       ((logical :SHL: 4) :OR: logical)
physical_bits   *       ((physical :SHL: 4) :OR: physical)
cacheable_bit   *       1:SHL:15
alter_cacheable *       1:SHL:16

; Small wrapper to make sure FIQs are disabled if we're making pages uncacheable
; (Modern ARMs ignore unexpected cache hits, so big coherency issues if we make
; a page uncacheable which is being used by FIQ).
MemoryConvertFIQCheck ROUT
        AND     r11, r0, #3:SHL:14
        TEQ     r11, #2:SHL:14
        BNE     MemoryConvertNoFIQCheck
        Entry   "r0-r1"
        MOV     r1, #Service_ClaimFIQ
        SWI     XOS_ServiceCall
        LDMIA   sp, {r0-r1}
        BL      MemoryConvertNoFIQCheck
        FRAMSTR r0
        MRS     r11, CPSR
        MOV     r1, #Service_ReleaseFIQ
        SWI     XOS_ServiceCall
        MSR     CPSR_f, r11
        EXIT

MemoryConvertNoFIQCheck   ROUT
        Entry   "r0-r11"                ; Need lots of registers!!

;        MRS     lr, CPSR
;        Push    "lr"
;        ORR     lr, lr, #I32_bit+F32_bit
;        MSR     CPSR_c, lr

        BIC     lr, r0, #all,given      ; Need to munge r0 to get rotates to work (must be even).
        AND     r0, r0, #all,given
        ORR     r0, r0, lr, LSL #1      ; Move bits 11-30 to 12-31.

        TST     r0, #all,given          ; Check for invalid argument (no fields provided)
        TEQNE   r2, #0                  ;   (no entries in table).
        ADREQL  r0, ErrorBlock_BadParameters
        BEQ     %FT95

        EOR     lr, r0, r0, LSL #given-wanted   ; If flag bits 8-10 and 12-14 contain common bits then
        AND     lr, lr, #all,wanted             ;   clear bits in 12-14 (ie. don't fill in fields already given).
        EOR     lr, lr, #all,wanted
        BIC     r0, r0, lr

        LDR     r6, =ZeroPage
        LDR     r7, [r6, #MaxCamEntry]
        LDR     r6, [r6, #CamEntriesPointer]
        LDR     r8, =L2PT
10
        SUBS    r2, r2, #1
        BCC     %FT70

        LDMIA   r1!, {r3-r5}            ; Get next three word entry (PN,LA,PA) and move on pointer.

   [ AMB_LazyMapIn
        BL      handle_AMBHonesty       ; may need to make page honest (as if not lazily mapped)
   ]

        TST     r0, #physical,wanted    ; If PA not wanted
        BEQ     %FT20                   ;   then skip.
        TST     r0, #logical,given      ; If LA given (rotate clears C) then
        ADR     lr, %FT15
        BNE     logical_to_physical     ; Get PA from LA
        BL      ppn_to_logical          ; Else get LA from PN (PA wanted (not given) & LA not given => PN given).
        BLCC    ppn_to_physical         ; And get PA from PN (more accurate than getting PA from LA - page may be mapped out)
15
        BCS     %FT80
        TST     r0, #logical,wanted
        STRNE   r4, [r1, #-8]           ; Store back LA if wanted.
        STR     r5, [r1, #-4]           ; Store back PA.
20
        TST     r0, #alter_cacheable    ; If altering cacheability
        EORNE   lr, r0, #ppn,given      ;   and PN not given
        TSTNE   lr, #ppn,given
        TSTEQ   r0, #ppn,wanted         ;   OR PN wanted then don't skip
        BEQ     %FT30                   ; else skip.
        TST     r0, #physical_bits,given        ; If PA not given and PA not wanted (rotate clears C) then
        BLEQ    logical_to_physical             ;   get it from LA (PN wanted/not given & PA not given => LA given).
        BLCC    physical_to_ppn         ; Get PN from PA.
        BCS     %FT80
        TST     r0, #ppn,wanted
        STRNE   r3, [r1, #-12]          ; Store back PN if wanted.
30
        TST     r0, #logical,wanted     ; If LA wanted
        EORNE   lr, r0, #physical,wanted
        TSTNE   lr, #physical,wanted    ;   and PA not wanted then don't skip
        BEQ     %FT40                   ; else skip.
        TST     r0, #alter_cacheable    ; If not changing cacheability (already have PN)
        TSTEQ   r0, #ppn_bits,given     ;   and PN not given and PN not wanted (rotate clears C) then
        BLEQ    physical_to_ppn         ;   get it from PA (LA wanted (not given) & PN not given => PA given).
        BLCC    ppn_to_logical          ; Get LA from PN.
        BCS     %FT80
        STR     r4, [r1, #-8]           ; Store back LA.
40
        TST     r0, #alter_cacheable
        BEQ     %BT10

        CMP     r7, r3                  ; Make sure page number is valid (might not have done any conversion).
        BCC     %FT80

        ADD     r3, r6, r3, LSL #CAM_EntrySizeLog2 ; Point to CAM entry for this page.
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
        LDMIA   r3, {r4,r5}             ; Get logical address and PPL.

        AND     lr, r5, #PageFlags_TempUncacheableBits
        TST     r0, #cacheable_bit
        BNE     %FT50

        TEQ     lr, #PageFlags_TempUncacheableBits      ; Make uncacheable (increment count).
        BEQ     %BT10                                   ; If count has reached max then go no further (should not happen).
        TEQ     lr, #0                                  ; EQ => we have to change L2.
        ADD     r5, r5, #1:SHL:TempUncacheableShift
        B       %FT60
50
        TEQ     lr, #0                                  ; Make cacheable (decrement count).
        BEQ     %BT10                                   ; If count is already 0 then go no further (page already cacheable).
        SUB     r5, r5, #1:SHL:TempUncacheableShift
        TST     r5, #PageFlags_TempUncacheableBits      ; EQ => we have to change L2.
60
        STR     r5, [r3, #CAM_PageFlags] ; Write back new PPL.
        BNE     %BT10                   ; Do next entry if we don't have to change L2.

        MOV     r4, r4, LSR #12
        LDR     r3, =ZeroPage
        ADD     r4, r8, r4, LSL #2      ; Address of L2 entry for logical address.
 [ MEMM_Type = "VMSAv6"
        ; VMSAv6 is hard, use XCBTable/PCBTrans
        ASSERT  DynAreaFlags_CPBits = 7*XCB_P :SHL: 10
        ASSERT  DynAreaFlags_NotCacheable = XCB_NC :SHL: 4
        ASSERT  DynAreaFlags_NotBufferable = XCB_NB :SHL: 4
        TST     r0, #cacheable_bit      ; n.b. must match EQ/NE used by ARMop calls
        AND     lr, r5, #DynAreaFlags_NotCacheable + DynAreaFlags_NotBufferable
        AND     r5, r5, #DynAreaFlags_CPBits
        ORR     lr, lr, r5, LSR #10-4
        LDR     r5, [r3, #MMU_PCBTrans]
        ORREQ   lr, lr, #XCB_TU<<4      ; if temp uncache, set TU bit
        MOV     lr, lr, LSR #3
        LDRH    lr, [r5, lr]            ; convert to C, B and TEX bits for this CPU
        LDR     r5, [r4]                ; Get L2 entry (safe as we know address is valid).
        BIC     r5, r5, #TempUncache_L2PTMask ; Knock out existing attributes (n.b. assumed to not be large page!)
        ORR     r5, r5, lr              ; Set new attributes
 |
        LDR     r5, [r4]                ; Get L2 entry (safe as we know address is valid).
        TST     r0, #cacheable_bit
        BICEQ   r5, r5, #L2_C           ; Disable/enable cacheability.
        ORRNE   r5, r5, #L2_C
 ]
        BNE     %FT63
        ; Making page non-cacheable
        ; There's a potential interrupt hole here - many ARMs ignore cache hits
        ; for pages which are marked as non-cacheable (seen on XScale,
        ; Cortex-A53, Cortex-A15 to name but a few, and documented in many TRMs)
        ; We can't be certain that this page isn't being used by an interrupt
        ; handler, so if we're making it non-cacheable we have to take the safe
        ; route of disabling interrupts around the operation.
        ; Note - currently no consideration is given to FIQ handlers.
        ; Note - we clean the cache as the last step (as opposed to doing it at
        ; the start) to make sure prefetching doesn't pull data back into the
        ; cache.
        MRS     r11, CPSR
        ORR     lr, r11, #I32_bit       ; IRQs off
        ; Yuck, we also need to deal with the case where we're making the
        ; current SVC stack page uncacheable (coherency issue when calling the
        ; ARMops if cache hits to uncacheable pages are ignored). Deal with this
        ; by temporarily dropping into IRQ mode (and thus a different stack) if
        ; we think this is going to happen.
        MOV     r10, r4, LSL #10
        SUB     r10, sp, r10
        CMP     r10, #8192              ; Be extra cautious
        EORLO   lr, lr, #SVC32_mode :EOR: IRQ32_mode
        MSR     CPSR_c, lr              ; Switch mode
        Push    "r0, lr"                ; Preserve OS_Memory flags and (potential) IRQ lr
        STR     r5, [r4]                ; Write back new L2 entry.
        ASSERT  (L2PT :SHL: 10) = 0     ; Ensure we can convert r4 back to the page log addr
        MOV     r0, r4, LSL #10
        ARMop   MMU_ChangingEntry,,,r3  ; Clean TLB+cache
        Pull    "r5, lr"                ; Restore OS_Memory flags + IRQ lr
        MSR     CPSR_c, r11             ; Back to original mode + IRQ state
        B       %FT65
63
        ; Making page cacheable again
        ; Shouldn't be any cache maintenance worries
        STR     r5, [r4]                ; Write back new L2 entry.
        MOV     r5, r0
        ASSERT  (L2PT :SHL: 10) = 0     ; Ensure we can convert r4 back to the page log addr
        MOV     r0, r4, LSL #10
        ARMop   MMU_ChangingUncachedEntry,,,r3   ; Clean TLB
65
        MOV     r0, r5
        B       %BT10

70
        CLRV
        EXIT

80
        TST     r0, #alter_cacheable    ; If we haven't changed any cacheability stuff then
        BEQ     %FT90                   ;   just return error.

        AND     lr, r0, #all,wanted             ; Get wanted flags.
        LDMIA   sp, {r0,r1,r3}                  ; Get back original flags, pointer and count.
        ORR     r0, r0, lr, LSR #given-wanted   ; Wanted fields are now also given as we have done the conversion.
        BIC     r0, r0, #all:SHL:11             ; Clear wanted flags, we only want to change cacheability.
        EOR     r0, r0, #cacheable_bit          ; If we made them uncacheable then make them cacheable again & v.v.
        SUB     r2, r3, r2
        SUBS    r2, r2, #1              ; Change back the entries we have changed up to (but excluding) the error entry.
        BLNE    MemoryConvertNoFIQCheck
90
        ADRL    r0, ErrorBlock_BadAddress
95
        STR     r0, [sp, #Proc_RegOffset+0]
        SETV
        EXIT

   [ AMB_LazyMapIn
;
;  entry: r3,r4,r5 = provided PN,LA,PA triple for entry to make honest (at least one given)
;         r0 bits flag which of PN,LA,PA are given
;  exit:  mapping made honest (as if not lazily mapped) if necessary
handle_AMBHonesty  ROUT
        Push    "r0, r3-r5, lr"
        TST     r0, #logical,given
        BEQ     %FT10
        MOV     r0, r4
        BL      AMB_MakeHonestLA
        B       %FT90
10
        TST     r0, #ppn,given
        BEQ     %FT20
15
        MOV     r0, r3
        BL      AMB_MakeHonestPN
        B       %FT90
20
        TST     r0, #physical,given
        BEQ     %FT90
        Push    "r7, r9-r11"
        LDR     r14, =ZeroPage
        LDR     r7, [r14, #MaxCamEntry]
        BL      physical_to_ppn
        Pull    "r7, r9-r11"
        BCC     %BT15
90
        Pull    "r0, r3-r5, pc"

   ] ;AMB_LazyMapIn


;----------------------------------------------------------------------------------------
; ppn_to_logical
;
;       In:     r3 = page number
;               r5 = physical address if given
;               r6 = CamEntriesPointer
;               r7 = MaxCamEntry
;
;       Out:    r9 corrupted
;               CC => r4 = logical address
;               CS => invalid page number
;
;       Convert physical page number to logical address.
;
ppn_to_logical
        CMP     r7, r3                  ; Validate page number.
        BCC     meminfo_returncs        ; Invalid so return C set.

        ASSERT  CAM_LogAddr=0
        LDR     r4, [r6, r3, LSL #CAM_EntrySizeLog2] ; If valid then lookup logical address.
        TST     r0, #physical,given     ; If physical address was given then
        LDRNE   r9, =&FFF
        ANDNE   r9, r5, r9              ;   mask off page offset
        ORRNE   r4, r4, r9              ;   and combine with logical address.
        CLC
        MOV     pc, lr


;----------------------------------------------------------------------------------------
; logical_to_physical
;
;       In:     r4 = logical address
;               r8 = L2PT
;
;       Out:    r9 corrupted
;               CC => r5 = physical address
;               CS => invalid logical address, r5 corrupted
;
;       Convert logical address to physical address.
;
logical_to_physical
        MOV     r9, r4, LSR #12         ; r9 = logical page number
        ADD     r9, r8, r9, LSL #2      ; r9 -> L2PT entry for logical address
        MOV     r5, r9, LSR #12         ; r5 = page offset to L2PT entry for logical address
        LDR     r5, [r8, r5, LSL #2]    ; r5 = L2PT entry for L2PT entry for logical address
      [ MEMM_Type = "ARM600"
        ASSERT  ((L2_SmallPage :OR: L2_ExtPage) :AND: 2) <> 0
        ASSERT  (L2_LargePage :AND: 2) = 0
      |
        ASSERT  L2_SmallPage = 2
        ASSERT  L2_XN = 1               ; Because XN is bit 0, bit 1 is the only bit we can check when looking for small pages
      ]
        TST     r5, #2                  ; Check for valid (4K) page.
        BEQ     meminfo_returncs

        LDR     r5, [r9]                ; r5 = L2PT entry for logical address
        TST     r5, #2                  ; Check for valid (4K) page.
        BEQ     meminfo_returncs

        LDR     r9, =&FFF               ; Valid so
        BIC     r5, r5, r9              ;   mask off bits 0-11,
        AND     r9, r4, r9              ;   get page offset from logical page
        ORR     r5, r5, r9              ;   combine with physical page address.
        CLC
        MOV     pc, lr

meminfo_returncs_pullr5
        Pull    "r5"
meminfo_returncs
        SEC
        MOV     pc, lr

;----------------------------------------------------------------------------------------
; physical_to_ppn
;
;       In:     r5 = physical address
;               r7 = MaxCamEntry
;
;       Out:    r9-r11 corrupted
;               CC => r3 = page number
;               CS => invalid physical address, r3 corrupted
;
;       Convert physical address to physical page number.
;
physical_to_ppn ROUT
        Push    "r5"
        LDR     r9, =ZeroPage+PhysRamTable
        MOV     r3, #0                  ; Start at page 0.
        MOV     r5, r5, LSR #12
10
        CMP     r7, r3                  ; Stop if we run out of pages
        BCC     meminfo_returncs_pullr5

        LDMIA   r9!, {r10,r11}          ; Get start address and size of next block.
        SUB     r10, r5, r10, LSR #12   ; Determine if given address is in this block.
        CMP     r10, r11, LSR #12
        ADDCS   r3, r3, r11, LSR #12    ; Move on to next block.
        BCS     %BT10

        Pull    "r5"

        ADD     r3, r3, r10
        CLC
        MOV     pc, lr

;----------------------------------------------------------------------------------------
; ppn_to_physical
;
;       In:     r3 = page number
;
;       Out:    r9 corrupted
;               CC => r5 = physical address
;               CS => invalid page number, r5 corrupted
;
;       Convert physical page number to physical address.
;
ppn_to_physical ROUT
        Push    "r3,lr"
        LDR     r9, =ZeroPage+PhysRamTable
10
        LDMIA   r9!, {r5,lr}            ; Get start address and size of next block.
        MOVS    lr, lr, LSR #12
        BEQ     %FT20
        CMP     r3, lr
        SUBHS   r3, r3, lr
        BHS     %BT10

        ADD     r5, r5, r3, LSL #12
        Pull    "r3,pc"
20
        SEC
        Pull    "r3,pc"


;----------------------------------------------------------------------------------------
; Symbols used in MemoryPhysSize and MemoryReadPhys
;

; Shifts to determine number of bytes/words to allocate in table.
BitShift        *       10
ByteShift       *       BitShift + 3
WordShift       *       ByteShift + 2

; Bit patterns for different types of memory.
NotPresent      *       &00000000
DRAM_Pattern    *       &11111111
VRAM_Pattern    *       &22222222
ROM_Pattern     *       &33333333
IO_Pattern      *       &44444444
NotAvailable    *       &88888888


;----------------------------------------------------------------------------------------
; MemoryPhysSize
;
;       In:     r0 = 6 (reason code with flag bits 8-31 clear)
;
;       Out:    r1 = table size (in bytes)
;               r2 = page size (in bytes)
;
;       Returns information about the memory arrangement table.
;
MemoryPhysSize
        Entry   "r0-r1,r3,sb,ip"
        AddressHAL
        MOV     r0, #PhysInfo_GetTableSize
        ADD     r1, sp, #4
        CallHAL HAL_PhysInfo
        MOV     r2, #4*1024
        CLRV
        EXIT


;----------------------------------------------------------------------------------------
; MemoryReadPhys
;
;       In:     r0 = 7 (reason code with flag bits 8-31 clear)
;               r1 -> memory arrangement table to be filled in
;
;       Out:    r1 -> filled in memory arrangement table
;
;       Returns the physical memory arrangement table in the given block.
;
MemoryReadPhys  ROUT

        Entry   "r0-r12"
        AddressHAL
        MOV     r0, #PhysInfo_WriteTable
        SUB     sp, sp, #8
        MOV     r2, sp
        CallHAL HAL_PhysInfo            ; fills in everything except DRAM
        LDR     r0, [sp], #4
        LDR     r11, [sp], #4

        ; r0 to r11 is DRAM or not present.
        LDR     r1, [sp, #4]            ; Get table address back
        ADD     r1, r1, r0, LSR #ByteShift
        MOV     r2, r0                  ; Current physical address.
        MOV     r3, #0                  ; Next word to store in table.
        MOV     r4, #32                 ; How much more we have to shift r3 before storing it.
        LDR     r6, =ZeroPage+CamEntriesPointer
        LDR     r7, [r6]
        ADD     r7, r7, #CAM_PageFlags  ; Point to PPL entries.
        LDR     r8, [r6, #MaxCamEntry-CamEntriesPointer]
        MOV     r5, #0                  ; last block address processed + 1
        Push    "r5"

        ; Ugly logic to process PhysRamTable entries in address order instead of physical page order
10
        Pull    "r12"
        MVN     lr, #0
        MOV     r5, #0                  ; Current page number.
        Push    "r5,lr"
        LDR     r6, =ZeroPage+PhysRamTable
        MOV     r10, #0
11
        ADD     r5, r5, r10, LSR #12
        LDMIA   r6!, {r9,r10}           ; Get physical address and size of next block.
        CMP     r10, #0
        BEQ     %FT12

        CMP     r9, r0                  ; If not DRAM then
        CMPHS   r11, r9
        BLO     %BT11                   ; try next block.

        CMP     r9, r12                 ; have we processed this entry?
        CMPHS   lr, r9                  ; is it the lowest one we've seen?
        BLO     %BT11                   ; yes, try the next
        ; This is the best match so far
        STMIA   sp, {r5,r6}             ; Remember page number & details ptr
        MOV     lr, r9                  ; Remember base address
        B       %BT11
12
        Pull    "r5,r6"
        CMP     r6, #-1                 ; did we find anything?
        BEQ     %FT40
        LDMDB   r6,{r9,r10}
        ADD     r12, r9, #1        
        Push    "r12"                   ; Remember that we've processed up to here

        ; Now process this entry
        MOV     r10, r10, LSR #12
        ADD     r10, r9, r10, LSL #12   ; Add amount of unused space between current and start of block.
        SUB     r10, r10, r2            ; size = size + (physaddr - current)
20
        SUBS    r4, r4, #4              ; Reduce shift.
        MOVCS   r3, r3, LSR #4          ; If more space in current word then shift it.
        STRCC   r3, [r1], #4            ; Otherwise, store current word
        MOVCC   r3, #0                  ;   and start a new one.
        MOVCC   r4, #28

        CMP     r2, r9                  ; If not reached start of block then page is not present.
        ORRCC   r3, r3, #(NotPresent :OR: NotAvailable) :SHL: 28
        BCC     %FT30
        LDR     lr, [r7, r5, LSL #CAM_EntrySizeLog2] ; Page is there so get PPL and determine if it's available or not.
        TST     lr, #PageFlags_Unavailable
        ORREQ   r3, r3, #DRAM_Pattern :SHL: 28
        ORRNE   r3, r3, #(DRAM_Pattern :OR: NotAvailable) :SHL: 28
        ADD     r5, r5, #1              ; Increment page count.
30
        ADD     r2, r2, #&1000          ; Increase current address.
        SUBS    r10, r10, #&1000        ; Decrease size of block.
        BGT     %BT20                   ; Stop if no more block left.

        B       %BT10

40
        TEQ     r3, #0                          ; If not stored last word then
        MOVNE   r3, r3, LSR r4                  ;   put bits in correct position
        ADDNE   r2, r2, r4, LSL #BitShift       ;   adjust current address
        RSBNE   r4, r4, #32                     ;   rest of word is not present
        LDRNE   lr, =NotPresent :OR: NotAvailable
        ORRNE   r3, r3, lr, LSL r4
        STRNE   r3, [r1], #4                    ;   and store word.

        ; End of last block of DRAM to r11 is not present.
        MOV     r6, r0
        ADD     lr, r11, #1
        RSBS    r2, r2, lr
        MOVNE   r0, r1
        LDRNE   r1, =NotPresent :OR: NotAvailable
        MOVNE   r2, r2, LSR #ByteShift
        BLNE    memset

        ; If softloaded (ie ROM image is wholely within DRAM area returned
        ; by HAL_PhysInfo), mark that as unavailable DRAM.
        LDR     r0, =ZeroPage
        LDR     r0, [r0, #ROMPhysAddr]
        LDR     r1, [sp, #4]
        CMP     r0, r6
        ADDHS   lr, r0, #OSROM_ImageSize*1024
        SUBHS   lr, lr, #1
        CMPHS   r11, lr
        ADDHS   r0, r1, r0, LSR #ByteShift
        LDRHS   r1, =DRAM_Pattern :OR: NotAvailable
        MOVHS   r2, #(OSROM_ImageSize*1024) :SHR: ByteShift
        BLHS    memset

        CLRV
        EXIT


fill_words
        STR     r3, [r1], #4
        SUBS    r2, r2, #1
        BNE     fill_words
        MOV     pc, lr


;----------------------------------------------------------------------------------------
; MemoryAmounts
;
;       In:     r0 = flags
;                       bit     meaning
;                       0-7     8 (reason code)
;                       8-11    1=return amount of DRAM (excludes any soft ROM)
;                               2=return amount of VRAM
;                               3=return amount of ROM
;                               4=return amount of I/O space
;                               5=return amount of soft ROM (ROM loaded into hidden DRAM)
;                       12-31   reserved (set to 0)
;
;       Out:    r1 = number of pages of the specified type of memory
;               r2 = page size (in bytes)
;
;       Return the amount of the specified type of memory.
;
MemoryAmounts   ROUT
        Entry   "r3"

        BICS    lr, r0, #&FF            ; Get type of memory required (leave bits 12-31, non-zero => error).
        CMP     lr, #6:SHL:8
        ADDCC   pc, pc, lr, LSR #8-2
        NOP
        B       %FT99                   ; Don't understand 0 (so the spec says).
        B       %FT10                   ; DRAM
        B       %FT20                   ; VRAM
        B       %FT30                   ; ROM
        B       %FT40                   ; I/O
        B       %FT50                   ; Soft ROM

10
        LDR     r1, =ZeroPage
        LDR     r3, [r1, #VideoSizeFlags]
        TST     r3, #OSAddRAM_IsVRAM
        MOVNE   r3, r3, LSR #12         ; Extract size from flags when genuine VRAM
        MOVNE   r3, r3, LSL #12
        MOVEQ   r3, #0
        LDR     r1, [r1, #RAMLIMIT]
        SUB     r1, r1, r3              ; DRAM = RAMLIMIT - VRAMSize
        B       %FT97
20
        LDR     r1, =ZeroPage
        LDR     r1, [r1, #VideoSizeFlags]
        TST     r1, #OSAddRAM_IsVRAM
        MOVNE   r1, r1, LSR #12
        MOVNE   r1, r1, LSL #12         ; VRAM = VRAMSize
        MOVEQ   r1, #0
        B       %FT97
30
        Push    "r0, sb, ip"
        AddressHAL
        MOV     r0, #PhysInfo_HardROM
        SUB     sp, sp, #8
        MOV     r2, sp
        CallHAL HAL_PhysInfo
        LDMIA   sp!, {r0-r1}
        SUBS    r1, r1, r0
        ADDNE   r1, r1, #1              ; ROM = ROMPhysTop + 1 - ROMPhysBot
        Pull    "r0, sb, ip"
        B       %FT97
40
        LDR     r1, =ZeroPage
        LDR     r1, [r1, #IOAllocLimit]
        LDR     r3, =IO
        SUB     r1, r3, r1              ; IO = IO ceiling - IO floor
        B       %FT97
50
        Push    "r0"
        MOV     r0, #8
        SWI     XOS_ReadSysInfo         ; Are we softloaded?
        Pull    "r0"
        AND     r1, r1, r2
        ANDS    r1, r1, #1:SHL:4        ; Test OS-runs-from-RAM flag
        MOVNE   r1, #OSROM_ImageSize*1024
        B       %FT97
97
        MOV     r1, r1, LSR #12         ; Return as number of pages.
        MOV     r2, #4*1024             ; Return page size.
        CLRV
        EXIT
99
        PullEnv
        ; Fall through...
MemoryBadParameters
        ADRL    r0, ErrorBlock_BadParameters ; n.b. MemReturn handles internationalisation
        SETV
        MOV     pc, lr


;----------------------------------------------------------------------------------------
; MemoryIOSpace
;
;       In:     r0 = 9 (reason code with flag bits 8-31 clear)
;               r1 = controller ID
;                       bit     meaning
;                       0-7     controller sequence number
;                       8-31    controller type:
;                               0 = EASI card access speed control
;                               1 = EASI space(s)
;                               2 = VIDC1
;                               3 = VIDC20
;                               4 = S space (IOMD,podules,NICs,blah blah)
;                               5 = Extension ROM(s)
;                               6 = Tube ULA
;                               7-31 = Reserved (for us)
;                               32 = Primary ROM
;                               33 = IOMD
;                               34 = FDC37C665/SMC37C665/82C710/SuperIO/whatever
;                               35+ = Reserved (for ROL)
;
;       Out:    r1 = controller base address or 0 if not present
;
;       Return the location of the specified controller.
;

MemoryIOSpace   ROUT
        Entry   "r0,r2,r3,sb,ip"
        AddressHAL
        CallHAL HAL_ControllerAddress
        CMP     r0, #-1
        MOVNE   r1, r0
        PullEnv
        MOVNE   pc, lr
        B       MemoryBadParameters

;----------------------------------------------------------------------------------------
; MemoryFreePoolLock - removed now that free pool is a PMP

;----------------------------------------------------------------------------------------
;PCImapping - reserved for Acorn use (PCI manager)
;
; See code on Ursula branch


;----------------------------------------------------------------------------------------
;RecommendPage
;
;       In:     r0 bits 0..7  = 12 (reason code 12)
;               r0 bit 8 = 1 if region must be DMAable
;               r0 bits 9..31 = 0 (reserved flags)
;               r1 = size of physically contiguous RAM region required (bytes)
;               r2 = log2 of required alignment of base of region (eg. 12 = 4k, 20 = 1M)
;
;       Out:    r3 = page number of first page of recommended region that could be
;                    grown as specific pages by dynamic area handler (only guaranteed
;                    if grow is next page claiming operation)
;        - or error if not possible (eg too big, pages unavailable)
;
RecommendPage ROUT
        Push    "r0-r2,r4-r11,lr"
        CMP     r2,#30
        BHI     RP_failed         ;refuse to look for alignments above 1G
        ANDS    r11,r0,#1:SHL:8   ;convert flag into something usable in the loop
        MOVNE   r11,#OSAddRAM_NoDMA
;
        ADD     r1,r1,#&1000
        SUB     r1,r1,#1
        MOV     r1,r1,LSR #12
        MOVS    r1,r1,LSL #12     ;size rounded up to whole no. of pages
;
        CMP     r2,#12
        MOVLO   r2,#12            ;log2 alignment must be at least 12 (4k pages)
        MOV     r0,#1
        MOV     r4,r0,LSL r2      ;required alignment-1
;
        LDR     r0,=ZeroPage+PhysRamTable
        MOV     r3,#0            ;page number, starts at 0
        LDR     r5,=ZeroPage+CamEntriesPointer
        LDR     r5,[r5]
        ADD     r5,r5,#CAM_PageFlags ; [r5,<page no.>,LSL #3] addresses flags word in CAM
        LDMIA   r0!,{r7,r8}      ;address,size of video chunk (skip this one)
;
RP_nextchunk
        ADD     r3,r3,r8,LSR #12 ;page no. of first page of next chunk
        LDMIA   r0!,{r7,r8}      ;address,size of next physical chunk
        CMP     r8,#0
        BEQ     RP_failed
        TST     r8,r11           ;ignore non-DMA regions if bit 8 of R0 was set
        BNE     RP_nextchunk
;
        MOV     r8,r8,LSR #12
        ADD     r6,r7,r4
        MOV     r8,r8,LSL #12
        SUB     r6,r6,#1         ;round up
        MOV     r6,r6,LSR r2
        MOV     r6,r6,LSL r2
        SUB     r6,r6,r7         ;adjustment to first address of acceptable alignment
        CMP     r6,r8
        BHS     RP_nextchunk     ;negligible chunk
        ADD     r7,r3,r6,LSR #12 ;first page number of acceptable alignment
        SUB     r9,r8,r6         ;remaining size of chunk
;
;find first available page
RP_nextpage
        CMP     r9,r1
        BLO     RP_nextchunk
        LDR     r6,[r5,r7,LSL #CAM_EntrySizeLog2] ;page flags from CAM
        ;must not be marked Unavailable or Required
        TST     r6,#PageFlags_Unavailable :OR: PageFlags_Required
        BEQ     RP_checkotherpages
RP_nextpagecontinue
        CMP     r9,r4
        BLS     RP_nextchunk
        ADD     r7,r7,r4,LSR #12   ;next page of suitable alignment
        SUB     r9,r9,r4
        B       RP_nextpage
;
RP_checkotherpages
        ADD     r10,r7,r1,LSR #12
        SUB     r10,r10,#1         ;last page required
RP_checkotherpagesloop
        LDR     r6,[r5,r10,LSL #CAM_EntrySizeLog2] ;page flags from CAM
        TST     r6,#PageFlags_Unavailable :OR: PageFlags_Required
        BNE     RP_nextpagecontinue
        SUB     r10,r10,#1
        CMP     r10,r7
        BHI     RP_checkotherpagesloop
;
;success!
;
        MOV     r3,r7
        Pull    "r0-r2,r4-r11,pc"

RP_failed
        MOV     r3,#0
        ADR     r0,ErrorBlock_NoMemChunkAvailable
        SETV
        STR     r0,[sp]
        Pull    "r0-r2,r4-r11,pc"

        MakeErrorBlock NoMemChunkAvailable

;----------------------------------------------------------------------------------------
;MapIOpermanent - map IO space (if not already mapped) and return logical address
;
;       In:     r0 bits 0..7  = 13 (reason code 13)
;               r0 bit  8     = 1 to map bufferable space (0 is normal, non-bufferable)
;               r0 bit  9     = 1 to map cacheable space (0 is normal, non-cacheable)
;               r0 bits 10..12 = cache policy
;               r0 bits 13..15 = 0 (reserved flags)
;               r0 bit  16    = 1 to doubly map
;               r0 bit  17    = 1 if access privileges specified
;               r0 bits 18..23 = 0 (reserved flags)
;               r0 bits 24..27 = access privileges (if bit 17 set)
;               r0 bits 28..31 = 0 (reserved flags)
;               r1 = physical address of base of IO space required
;               r2 = size of IO space required (bytes)
;
;       Out:    r3 = logical address of base of IO space
;        - or error if not possible (no room)
;
MapIOpermanent ROUT
        Entry   "r0-r2,r12"
        ; Convert the input flags to some DA flags
        TST     r0, #1:SHL:17
        MOVEQ   r12, #2                 ; Default AP: SVC RW, USR none
        MOVNE   r12, r0, LSR #24        ; Else use given AP
        ANDNE   r12, r12, #DynAreaFlags_APBits
        AND     lr, r0, #&300
        EOR     lr, lr, #&300
        ASSERT  DynAreaFlags_NotBufferable = 1:SHL:4
        ASSERT  DynAreaFlags_NotCacheable = 1:SHL:5
        ORR     r12, r12, lr, LSR #4
        AND     lr, r0, #7:SHL:10
        ASSERT  DynAreaFlags_CPBits = 7:SHL:12
        ORR     r12, r12, lr, LSL #2
        ; Calculate the extra flags needed for RISCOS_MapInIO
        AND     r0, r0, #1:SHL:16
        ASSERT  MapInFlag_DoublyMapped = 1:SHL:20
        MOV     r0, r0, LSL #4
        ; Convert DA flags to page table entry
        GetPTE  r0, 1M, r0, r12
 [ MEMM_Type = "VMSAv6"
        ORR     r0, r0, #L1_XN          ; force non-executable to prevent speculative instruction fetches
 ]
        ; Map in the region
        BL      RISCOS_MapInIO_PTE
        MOV     r3, r0
        PullEnv
        CMP     r3, #0              ;MOV,CMP rather than MOVS to be sure to clear V
        ADREQ   r0, ErrorBlock_NoRoomForIO
        SETV    EQ
        MOV     pc, lr

        MakeErrorBlock NoRoomForIO

;----------------------------------------------------------------------------------------
;AccessPhysAddr - claim temporary access to given physical address (in fact,
;                 controls access to the 1Mb aligned space containing the address)
;                 The access remains until the next AccessPhysAddr or until a
;                 ReleasePhysAddr (although interrupts or subroutines may temporarily
;                 make their own claims, but restore on Release before returning)
;
;       In:     r0 bits 0..7  = 14 (reason code 14)
;               r0 bit  8     = 1 to map bufferable space, 0 for unbufferable
;               r0 bits 9..31 = 0 (reserved flags)
;               r1 = physical address
;
;       Out:    r2 = logical address corresponding to phys address r1
;               r3 = old state (for ReleasePhysAddr)
;
; Use of multiple accesses: it is fine to make several Access calls, and
; clean up with a single Release at the end. In this case, it is the old state
; (r3) of the *first* Access call that should be passed to Release in order to
; restore the state before any of your accesses. (The r3 values of the other
; access calls can be ignored.)
;
AccessPhysAddr ROUT
        Push    "r0,r1,r12,lr"
        TST     r0, #&100           ;test bufferable bit
        MOVNE   r0, #L1_B
        MOVEQ   r0, #0
        SUB     sp, sp, #4          ; word for old state
        MOV     r2, sp              ; pointer to word
        BL      RISCOS_AccessPhysicalAddress
        MOV     r2, r0
        Pull    r3                  ; old state
        Pull    "r0,r1,r12,pc"

;----------------------------------------------------------------------------------------
;ReleasePhysAddr - release temporary access that was claimed by AccessPhysAddr
;
;       In:     r0 bits 0..7  = 15 (reason code 15)
;               r0 bits 8..31 = 0 (reserved flags)
;               r1 = old state to restore
;
ReleasePhysAddr
        Push    "r0-r3,r12,lr"
        MOV     r0, r1
        BL      RISCOS_ReleasePhysicalAddress
        Pull    "r0-r3,r12,pc"

        LTORG

;----------------------------------------------------------------------------------------
;
;        In:    r0 = flags
;                       bit     meaning
;                       0-7     16 (reason code)
;                       8-15    1=cursor/system/sound
;                               2=IRQ stack
;                               3=SVC stack
;                               4=ABT stack
;                               5=UND stack
;                               6=Soft CAM
;                               7=Level 1 page tables
;                               8=Level 2 page tables
;                               9=HAL workspace
;                               10=Kernel buffers
;                               11=HAL uncacheable workspace
;                               12=Kernel 'ZeroPage' workspace
;                               13=Processor vectors
;                               14=DebuggerSpace
;                               15=Scratch space
;                               16=Compatibility page
;                       16-31   reserved (set to 0)
;
;       Out:    r1 = base of area
;               r2 = address space allocated for area (whole number of pages)
;               r3 = actual memory used by area (whole number of pages)
;               all values 0 if not present, or incorporated into another area
;
;       Return size of various low-level memory regions
MemoryAreaInfo ROUT
        Entry   "r0"
        MOV     r1, #0
        MOV     r2, #0
        MOV     r3, #0
        MOV     lr, r0, LSR #8
        AND     lr, lr, #&FF
        CMP     lr, #(MAI_TableEnd - MAI_TableStart)/4
        ADDLO   pc, pc, lr, LSL #2
        B       %FT70
MAI_TableStart
        B       %FT70
        B       MAI_CursSysSound
        B       MAI_IRQStk
        B       MAI_SVCStk
        B       MAI_ABTStk
        B       MAI_UNDStk
        B       MAI_SoftCAM
        B       MAI_L1PT
        B       MAI_L2PT
        B       MAI_HALWs
        B       MAI_Kbuffs
        B       MAI_HALWsNCNB
        B       MAI_ZeroPage
        B       MAI_ProcVecs
        B       MAI_DebuggerSpace
        B       MAI_ScratchSpace
        B       MAI_CompatibilityPage
MAI_TableEnd

70
        PullEnv
        B       MemoryBadParameters

MAI_CursSysSound
        LDR     r1, =CursorChunkAddress
        MOV     r2, #32*1024
        MOV     r3, r2
        EXIT

MAI_IRQStk
 [ IRQSTK < CursorChunkAddress :LOR: IRQSTK > CursorChunkAddress+32*1024
        LDR     r1, =IRQStackAddress
        MOV     r2, #IRQSTK-IRQStackAddress
        MOV     r3, r2
 ]
        EXIT

MAI_SVCStk
        LDR     r1, =SVCStackAddress
        MOV     r2, #SVCSTK-SVCStackAddress
        MOV     r3, r2
        EXIT

MAI_ABTStk
        LDR     r1, =ABTStackAddress
        MOV     r2, #ABTSTK-ABTStackAddress
        MOV     r3, r2
        EXIT

MAI_UNDStk
        LDR     r1, =UNDSTK :AND: &FFF00000
        LDR     r2, =UNDSTK :AND: &000FFFFF
        MOV     r3, r2
        EXIT

MAI_SoftCAM
        LDR     r0, =ZeroPage
        LDR     r1, [r0, #CamEntriesPointer]
        LDR     r2, =CAMspace
        LDR     r3, [r0, #SoftCamMapSize]
        EXIT

MAI_L1PT
        LDR     r1, =L1PT
        MOV     r2, #16*1024
        MOV     r3, r2
        EXIT

MAI_L2PT
        LDR     r0, =ZeroPage
        LDR     r1, =L2PT
        MOV     r2, #4*1024*1024
        LDR     r3, [r0, #L2PTUsed]
        EXIT

MAI_HALWs
        LDR     r0, =ZeroPage
        LDR     r1, =HALWorkspace
        MOV     r2, #HALWorkspaceSize
        LDR     r3, [r0, #HAL_WsSize]
        EXIT

MAI_HALWsNCNB
        LDR     r0, =ZeroPage
        LDR     r1, =HALWorkspaceNCNB
        MOV     r2, #32*1024
        LDR     r3, [r0, #HAL_Descriptor]
        LDR     r3, [r3, #HALDesc_Flags]
        ANDS    r3, r3, #HALFlag_NCNBWorkspace
        MOVNE   r3, r2
        EXIT

MAI_Kbuffs
        LDR     r1, =KbuffsBaseAddress
        MOV     r2, #KbuffsMaxSize
        LDR     r3, =(KbuffsSize + &FFF) :AND: :NOT: &FFF
        EXIT

MAI_ZeroPage
        LDR     r1, =ZeroPage
        MOV     r2, #16*1024
        MOV     r3, #16*1024
        EXIT

MAI_ProcVecs
      [ ZeroPage != ProcVecs
        LDR     r1, =ProcVecs
        MOV     r2, #4096
        MOV     r3, #4096
      ]
        EXIT

MAI_DebuggerSpace
        ; Only report if DebuggerSpace is a standalone page. The debugger module
        ; finds DebuggerSpace via OS_ReadSysInfo 6, this call is only for the
        ; benefit of the task manager.
      [ DebuggerSpace_Size >= &1000
        LDR     r1, =DebuggerSpace
        MOV     r2, #DebuggerSpace_Size
        MOV     r3, #DebuggerSpace_Size
      ]
        EXIT

MAI_ScratchSpace
        LDR     r1, =ScratchSpace
        MOV     r2, #16*1024
        MOV     r3, #16*1024
        EXIT

MAI_CompatibilityPage
      [ CompatibilityPage
        MOV     r1, #0
        MOV     r2, #4096
        LDR     r0, =ZeroPage
        LDRB    r3, [r0,#CompatibilityPageEnabled]
        CMP     r3, #0
        MOVNE   r3, #4096
      ]
        EXIT

;----------------------------------------------------------------------------------------
;
;        In:    r0 = flags
;                       bit     meaning
;                       0-7     17 (reason code)
;                       8-31    reserved (set to 0)
;               r1 = AP number to start search from (0 to start enumeration)
;                    increment by 1 on each call to enumerate all values
;
;       Out:    r1 = AP number (-1 if end of list reached)
;               r2 = Permissions:
;               bit 0: executable in user mode
;               bit 1: writable in user mode
;               bit 2: readable in user mode
;               bit 3: executable in privileged modes
;               bit 4: writable in privileged modes
;               bit 5: readable in privileged modes
;               bits 6+: reserved
;
;       Returns permission information for a given AP / enumerates all AP
MemoryAccessPrivileges ROUT
        CMP     r0, #17
        BNE     MemoryBadParameters
        Entry   "r3-r4"
        LDR     r3, =ZeroPage
        MOV     lr, r1
        LDR     r3, [r3, #MMU_PPLAccess]
        ; Currently we don't have any gaps in the table, so we can just index the r1'th element (being careful to not go past the table end)
10
        LDR     r4, [r3], #4
        CMP     r4, #-1
        BEQ     %FT98
        SUBS    lr, lr, #1
        BGE     %BT10
        BL      PPL_CMA_to_RWX             
        EXIT
98
        MOV     r1, #-1
        MOV     r2, #0
        EXIT

; In: r4 = CMA-style AP/PPL access flags (from MMU_PPLAccess)
; Out: r2 = RWX-style AP/PPL access flags (for OS_Memory 17/18)
PPL_CMA_to_RWX ROUT
        Entry
        AND     r2, r4, #CMA_Partially_UserR
        ASSERT  CMA_Partially_UserR = 1<<4
        ASSERT  MemPermission_UserR = 1<<2
        MOV     r2, r2, LSR #4-2
        AND     lr, r4, #CMA_Partially_UserW
        ASSERT  CMA_Partially_UserW = 1<<5
        ASSERT  MemPermission_UserW = 1<<1
        ORR     r2, r2, lr, LSR #5-1
        AND     lr, r4, #CMA_Partially_UserXN ; (internally, XN flags are stored inverted)
        ASSERT  CMA_Partially_UserXN = 1<<14
        ASSERT  MemPermission_UserX = 1<<0
        ORR     r2, r2, lr, LSR #14-0
        AND     lr, r4, #CMA_Partially_PrivR
        ASSERT  CMA_Partially_PrivR = 1<<6
        ASSERT  MemPermission_PrivR = 1<<5
        ORR     r2, r2, lr, LSR #6-5
        AND     lr, r4, #CMA_Partially_PrivW
        ASSERT  CMA_Partially_PrivW = 1<<7
        ASSERT  MemPermission_PrivW = 1<<4
        ORR     r2, r2, lr, LSR #7-4
        AND     lr, r4, #CMA_Partially_PrivXN
        ASSERT  CMA_Partially_PrivXN = 1<<15
        ASSERT  MemPermission_PrivX = 1<<3
        ORR     r2, r2, lr, LSR #15-3
        EXIT

;----------------------------------------------------------------------------------------
;
;        In:    r0 = flags
;                       bit     meaning
;                       0-7     18 (reason code)
;                       8-31    reserved (set to 0)
;               r1 = Permission flag values (as per OS_Memory 17)
;               r2 = Permission flag mask
;
;       Out:    r0 = AP number that gives closest permissions
;               r2 = Permission flags of that AP (== r1 if exact match)
;               Error if no suitable AP found
;
;       Searches for an AP where ((permissions AND r2) == r1), and which
;       grants the least extra permissions
;
;       Extra permissions are weighted as follows (least acceptable first):
;       * User write
;       * User execute
;       * User read
;       * Privileged write
;       * Privileged execute
;       * Privileged read
FindAccessPrivilege ROUT
        CMP     r0, #18 ; No extra flags in r0
        BICEQS  r0, r1, r2 ; r1 must be a subset of r2
        BICEQS  r0, r2, #63 ; Only 6 known permission flags
        BNE     MemoryBadParameters
        ; n.b. r0 is now 0
        Entry   "r3-r11"
        LDR     r3, =ZeroPage
        MOV     r5, r1
        LDR     r3, [r3, #MMU_PPLAccess]
        MOV     r6, r2
        MOV     r7, #-1 ; Best AP
        MOV     r8, #0 ; Best flags
        MOV     r9, #-1 ; Best difference
        ; Magic constants for weighting the difference
        LDR     r10, =(1<<1)+(1<<6)+(1<<12)+(1<<18)+(1<<24)+(1<<30)
        LDR     r11, =(MemPermission_PrivR<<1)+(MemPermission_PrivX<<6)+(MemPermission_PrivW<<12)+(MemPermission_UserR<<18)+(MemPermission_UserX<<24)+(MemPermission_UserW<<30)
10
        LDR     r4, [r3], #4
        CMP     r4, #-1
        BEQ     %FT50
        BL      PPL_CMA_to_RWX ; -> r2 = flags
        ; Check it satisfies the mask
        AND     lr, r2, r6
        CMP     lr, r5
        BNE     %FT40
        ; Calculate diff
        BIC     lr, r2, r6
        MUL     lr, r10, lr ; Replicate the six bits six times
        AND     lr, r11, lr ; Select just the bits that we care about
        CMP     lr, r9
        BEQ     %FT80       ; Exact match found
        MOVLO   r7, r0      ; Remember new result if better
        MOVLO   r8, r2
        MOVLO   r9, lr
40
        ADD     r0, r0, #1
        B       %BT10
50
        MOVS    r0, r7
        BMI     %FT90
        MOV     r2, r8
80
        CLRV
        EXIT

90
        MOV     r2, r6 ; Restore original r2        
        ADR     r0, ErrorBlock_AccessPrivilegeNotFound
        SETV
        EXIT

        MakeErrorBlock AccessPrivilegeNotFound

;----------------------------------------------------------------------------------------
;
;        In:    r0 = flags
;                       bit     meaning
;                       0-7     19 (reason code)
;                       8       Input function provides physical addresses
;                       9       DMA is writing to RAM
;                       10      DMA is complete, perform any post-op cache maintenance
;                       11-31   reserved (set to 0)
;               r1 = R12 value to provide to called functions
;               r2 = Initial R9 value to provide to input function
;               r3 -> Input function:
;                      in:  r9 = r2 from SWI / value from previous call
;                           r12 = r1 from SWI
;                      out: r0 = start address of region
;                           r1 = length of region (0 if end of transfer)
;                           r2 = flags:
;                                bit 0: Bounce buffer will be used
;                           r9 = new r9 for next input call
;                           r12 corrupt
;               r4 = Initial R9 value to provide to output function
;               r5 -> Output function (if bit 10 of R0 clear):
;                      in: r0 = logical address of start of region
;                          r1 = physical address of start of region
;                          r2 = length of region
;                          r3 = flags:
;                               bit 0: Bounce buffer must be used
;                          r9 = r4 from SWI / value from previous call
;                          r12 = r1 from SWI
;                      out: r9 = new r9 value for next output call
;                           r0-r3, r12 corrupt
;
;       Out:    r2, r4 updated to match values returned by input/output calls
;               All other regs preserved
;
; Performs address translation and cache maintenance necessary to allow for DMA
; to be performed to/from cacheable memory.
;
; To allow Service_PagesUnsafe to be dealt with in a straightforward manner, we
; have to be careful not to cache the results of any address translations over
; calls to the input/output functions. E.g. if the output function tries to
; allocate from PCI RAM, that may trigger claiming of a specific page by the
; PCI DA, potentially invalidating any existing logical -> physical translation.
; This restriction hampers the routines ability to merge together input and
; output blocks, and to perform minimal cache maintenance. However for typical
; scatter lists of low to medium complexity it should still produce acceptable
; output.
;
; Note that if the input function provides physical addresses, the caller must
; take care to abort the entire operation if one of the physical pages involved
; in the request becomes claimed by someone else while the OS_Memory call is in
; progress. This is because we have no sensible way of dealing with this case
; ourselves (even if we didn't attempt to call the input function multiple times
; and merge together the blocks, we'd still have to buffer things internally to
; deal with when blocks need splitting for cache alignment)
;
; Internally, blocks are stored in the following format:
;
; Word 0 = Start logical address (incl.)
; Word 1 = Logical -> physical address offset (low bits) + flags (high bits)
; Word 2 = End logical address (excl.)
;
; This minimises the number of registers needed to hold a block, and simplifies
; the merge calculation (blocks can be merged if words 2 + 1 of first block
; match words 0 + 1 of second block)

; Workspace struct that's stored on the stack
                    ^ 0
DMAPrepW_InHold     # 12
DMAPrepW_InChunk    # 12
DMAPrepW_PhyChunk   # 12
DMAPrepW_CacheMask  # 4 ; Cache line length - 1
DMAPrepW_ARMop      # 4 ; Cache maintenenace ARMop to use
DMAPrepW_CamEntriesPointer # 4 ; CamEntriesPointer copy
DMAPrepW_Size       # 0
                        ; These next few correspond directly to the input registers in the stack frame
DMAPrepW_Flags      # 4
DMAPrepW_R12        # 4
DMAPrepW_InR9       # 4
DMAPrepW_InFunc     # 4
DMAPrepW_OutR9      # 4
DMAPrepW_OutFunc    # 4

DMAPrep_FlagOffset * 20
DMAPrep_NonCacheable * 1:SHL:21 ; Internal flag used for tracking non-cacheable pages

DMAPrep ROUT
        CMP     r0, #1<<11
        BHS     MemoryBadParameters
        ; The end of a read from RAM is a no-op (no cache maintenance required)
        AND     r11, r0, #DMAPrep_Write :OR: DMAPrep_End
        TEQ     r11, #DMAPrep_End
        MOVEQ   pc, lr
        Entry   "r0-r9", DMAPrepW_Size
        ; Determine the cache maintenance function we need to use
        CMP     r11, #DMAPrep_Write
        LDR     r10, =ZeroPage
        ASSERT  DMAPrep_End > DMAPrep_Write
        LDRLE   r11, [r10, #Proc_Cache_CleanRange] ; Start of DMA (read or write)
        LDRGT   r11, [r10, #Proc_Cache_InvalidateRange] ; End of DMA write
        STR     r11, [sp, #DMAPrepW_ARMop]
        ; Get the params needed for address translation
        LDR     r6, [r10, #CamEntriesPointer]
        LDR     r7, [r10, #MaxCamEntry]
        LDR     r8, =L2PT
        ; Init workspace
        STR     r6, [sp, #DMAPrepW_CamEntriesPointer]
        ; Get the cache line mask value
      [ MEMM_Type == "ARM600"
        LDRB    r1, [r10, #DCache_LineLen]
      |
        ; Yuck, need to probe for the last cache level
        MOV     r5, #Cache_Lx_MaxLevel-1
01
        MOV     r1, r5
        ARMop   Cache_Examine,,,r10
        CMP     r1, #0
        SUBEQ   r5, r5, #1
        BEQ     %BT01
        CMP     r3, r1
        MOVHI   r1, r3
      ]
        SUB     r1, r1, #1
        STR     r1, [sp, #DMAPrepW_CacheMask]
        ; Get initial input region
        BL      DMAPrep_CallInputFunc
        CMP     r0, r3
        BEQ     %FT90
05
        STMIA   lr, {r0, r2, r3}
10
        ; Get another input region, see if we can merge it with InChunk
        BL      DMAPrep_CallInputFunc
        CMP     r0, r3
        BEQ     %FT19
        LDMIB   lr, {r4, r5}
        CMP     r4, r2
        CMPEQ   r5, r0
        STREQ   r3, [lr, #8]
        BEQ     %BT10
19
        ; Can't merge this region, store it in InHold
        ASSERT  DMAPrepW_InHold = DMAPrepW_InChunk-12
        STMDB   lr, {r0, r2, r3}
20
        ; Perform address translation for the start of InChunk
        LDR     r4, [sp, #DMAPrepW_InChunk]
        BL      DMAPrep_Translate
        ; Store in PhyChunk
        ADD     lr, sp, #DMAPrepW_PhyChunk
        STMIA   lr, {r4-r6}
        ; Align start to cache boundary
        TST     r5, #DMAPrep_NonCacheable+(DMAPrep_UseBounceBuffer :SHL: DMAPrep_FlagOffset)
        BNE     %FT25
        LDR     lr, [sp, #DMAPrepW_Flags]
        LDR     r10, [sp, #DMAPrepW_CacheMask]
        TST     lr, #DMAPrep_Write
        TSTNE   r4, r10
        BEQ     %FT25
        ; Unaligned write to cacheable memory -> bounce required
        ADD     r1, r4, r10
        BIC     r1, r1, r10 ; End of current cache line
        ; Only round down to end of current cache line if the end of the chunk
        ; is at or beyond the end of the next cache line
        ADD     r2, r1, r10 ; Last byte we can accept without needing to truncate
        CMP     r6, r2
        MOVHI   r6, r1 ; Truncate!
        ORR     r5, r5, #DMAPrep_UseBounceBuffer :SHL: DMAPrep_FlagOffset
        B       %FT40
25
        ; Start doesn't need splitting, so translate + append more pages
        ADD     lr, sp, #DMAPrepW_InChunk
        ASSERT  DMAPrepW_PhyChunk = DMAPrepW_InChunk + 12
        LDMIA   lr, {r0-r2, r4-r6}
        SUB     r3, r6, r4 ; Length of translated region
        SUB     r2, r2, r0 ; Length of input region
        CMP     r3, r2
        BEQ     %FT30
        ADD     r4, r0, r3 ; Translate next address in input address space
        BL      DMAPrep_Translate
        ; Try and merge with PhyChunk
        ADD     lr, sp, #DMAPrepW_PhyChunk
        LDMIB   lr, {r0, r1}
        CMP     r0, r5
        CMPEQ   r1, r4
        STREQ   r6, [sp, #DMAPrepW_PhyChunk + 8]
        BEQ     %BT25
        LDMIA   lr, {r4-r6}
30
        ; Can't merge any more pages into this chunk {r4-r6}
        ; Truncate / bounce the end if necessary
        TST     r5, #DMAPrep_NonCacheable+(DMAPrep_UseBounceBuffer :SHL: DMAPrep_FlagOffset)
        BNE     %FT50
        LDR     lr, [sp, #DMAPrepW_Flags]
        LDR     r10, [sp, #DMAPrepW_CacheMask]
        TST     lr, #DMAPrep_Write
        TSTNE   r6, r10
        BEQ     %FT40
        ; Unaligned write to cacheable memory -> bounce required
        BIC     r3, r6, r10
        CMP     r3, r4
        ORREQ   r5, r5, #DMAPrep_UseBounceBuffer :SHL: DMAPrep_FlagOffset ; Bounce
        MOVNE   r6, r3 ; Truncate
40
        ; Perform cache maintenance if necessary
        ; For safety we always perform this before calling the output function, rather than caching and attempting to merge the regions (output function may alter cacheability of pages?)
        TST     r5, #DMAPrep_NonCacheable+(DMAPrep_UseBounceBuffer :SHL: DMAPrep_FlagOffset)
        BNE     %FT50
        ADD     r1, r6, r10
        BIC     r0, r4, r10
        BIC     r1, r1, r10
        MOV     lr, pc
        LDR     pc, [sp, #DMAPrepW_ARMop]
50
        ; Call the output function
        LDR     lr, [sp, #DMAPrepW_Flags]
        TST     lr, #DMAPrep_End
        BNE     %FT60 ; No output func for end-of-op
        MOV     r0, r4
        ADD     r1, r4, r5, LSL #12
        SUB     r2, r6, r4
        MOV     r3, r5, LSR #DMAPrep_FlagOffset
        LDR     r12, [sp, #DMAPrepW_R12]
        AND     r3, r3, #DMAPrep_UseBounceBuffer ; Mask out internal flags
        ADD     r9, sp, #DMAPrepW_OutR9
        CLRV    ; Ensure V is clear on entry so simple functions don't confuse us
        MOV     lr, pc
        ASSERT  DMAPrepW_OutFunc = DMAPrepW_OutR9 + 4
        LDMIA   r9, {r9, pc}            ; Call output function
        STR     r9, [sp, #DMAPrepW_OutR9] ; Always write back updated R9
        BVS     %FT90
60
        ; Advance InChunk by the length of {r4-r6}
        LDR     r0, [sp, #DMAPrepW_InChunk]
        ADD     r0, r0, r6
        LDR     r1, [sp, #DMAPrepW_InChunk+8]
        SUB     r0, r0, r4
        STR     r0, [sp, #DMAPrepW_InChunk]
        CMP     r0, r1
        BNE     %BT20
        ; InChunk depleted, copy InHold to InChunk and try for more input
        ADD     lr, sp, #DMAPrepW_InChunk
        ASSERT  DMAPrepW_InHold = 0
        LDMIA   sp, {r0,r2,r3}
        CMP     r0, r3
        BNE     %BT05
        ; InHold was empty, so no more regions to process
90
        FRAMSTR r0, VS
        EXIT

95
        ADRL    r0, ErrorBlock_BadAddress
        SETV
        B       %BT90

96
        PullEnv
        B       MemoryBadParameters

; Out: R0, R2, R3 = block
;      LR -> InChunk
;      R1, R4, R9, R12 corrupt
DMAPrep_CallInputFunc
        MOV     r4, lr ; Avoid pushing onto stack, to simplify workspace indexing and error handling
        LDR     r12, [sp, #DMAPrepW_R12]
        ADD     r9, sp, #DMAPrepW_InR9
        CLRV    ; Ensure V is clear on entry so simple functions don't confuse us
        MOV     lr, pc
        ASSERT  DMAPrepW_InFunc = DMAPrepW_InR9 + 4
        LDMIA   r9, {r9, pc}            ; Call the input function
        STR     r9, [sp, #DMAPrepW_InR9] ; Always write back updated R9
        BVS     %BT90
        CMP     r2, #DMAPrep_UseBounceBuffer
        BHI     %BT96
        ; Pack into InChunk
        MOV     r2, r2, LSL #DMAPrep_FlagOffset
        ADD     lr, sp, #DMAPrepW_InChunk
        ADD     r3, r0, r1
        MOV     pc, r4

; Translate the start of InChunk into a block
; In: r4 = Address to translate
;     r7 = MaxCamEntry
;     r8 -> L2PT
; Out: r4, r5, r6 = block
;      r1, r3, r9-r12 corrupt
DMAPrep_Translate
        MOV     r1, lr
        LDR     r12, [sp, #DMAPrepW_InChunk+8]
        SUB     r12, r12, r4            ; Length of input region
        LDR     lr, [sp, #DMAPrepW_Flags]
        LDR     r6, [sp, #DMAPrepW_CamEntriesPointer]
        TST     lr, #DMAPrep_PhysProvided
        BNE     %FT20
      [ AMB_LazyMapIn
        MOV     r9, r0
        MOV     r0, r4
        BL      AMB_MakeHonestLA
        MOV     r0, r9
      ]
        BL      logical_to_physical     ; r4, r8 -> r5
        BLCC    physical_to_ppn         ; r5, r7 -> r3
        BCS     %BT95
        ; r9-r11 corrupt
        ; Grab page flags
        ADD     lr, r6, r3, LSL #CAM_EntrySizeLog2
        LDR     lr, [lr, #CAM_PageFlags]
        B       %FT30
20
        MOV     r5, r4
        BL      physical_to_ppn         ; r5, r7 -> r3
        BCS     %BT95
        ; r9-r11 corrupt
        ; Manual ppn -> logical so we can get the page flags at the same time
        ; TODO this won't deal with mapped out pages in a sensible manner (will output them all individually)
      [ AMB_LazyMapIn
        MOV     r9, r0
        MOV     r0, r3
        BL      AMB_MakeHonestPN
        MOV     r0, r9
      ]
        ADD     lr, r6, r3, LSL #CAM_EntrySizeLog2
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
        LDMIA   lr, {r3, lr}
        ; Merge in the offset within the page
        MOV     r3, r3, LSR #12
        ORR     r4, r3, r4, LSL #20
        MOV     r4, r4, ROR #20 
30
        LDR     r3, [sp, #DMAPrepW_InChunk+4]
        ; Combine the cacheability + phys offset into r5
        SUB     r5, r5, r4
        TST     lr, #DynAreaFlags_NotCacheable
        ORR     r5, r3, r5, LSR #12
        ORRNE   r5, r5, #DMAPrep_NonCacheable
        ; Work out how much of r12 fits into this page
        ; This is done by comparing against the length of the input region,
        ; since the input could be logical or physical
        ADD     r6, r4, #4096
        MOV     r6, r6, LSR #12
        RSB     r6, r4, r6, LSL #12
        CMP     r6, r12
        MOVHI   r6, r12
        ADD     r6, r4, r6
        MOV     pc, r1        

;----------------------------------------------------------------------------------------
;
;       In:     r0 = flags
;                       bit     meaning
;                       0-7     20 (reason code)
;                       8-31    reserved (set to 0)
;               r1 = 0 to disable compatibility page
;                    1 to enable compatibility page
;                    -1 to read state
;
;       Out:    r1 = new/current state:
;                    0 if disabled
;                    1 if enabled
;                    -1 if not supported
;
;       Controls the page zero compatibility page located at &0
;
;       If the compatibility page isn't supported, attempts to enable it will
;       silently fail, with a result of r1 = -1
;
ChangeCompatibility ROUT
        CMP     r1, #-1
        CMPNE   r1, #1
        CMPLS   r0, #255
        BHI     MemoryBadParameters
 [ :LNOT: CompatibilityPage
        MOV     r1, #-1
        MOV     pc, lr
 |
        Entry   "r0-r11", DANode_NodeSize
        LDR     r12, =ZeroPage
        LDRB    r0, [r12, #CompatibilityPageEnabled]
        FRAMSTR r0,,r1 ; return pre-change state in r1 (will be updated later, as necessary)
        CMP     r1, #-1
        CMPNE   r0, r1
        EXIT    EQ
        ; If we're attempting to enable it, make sure nothing else has mapped itself in to page zero
        LDR     r8, =L2PT
        CMP     r1, #0
        LDRNE   r0, [r8]
        CMPNE   r0, #0
        MOVNE   r1, #-1
        FRAMSTR r1,NE
        EXIT    NE
        ; Set up temp DANode on the stack so we can use a Batcall to manage the mapping
        MOV     r2, sp
        MOV     r0, #DynAreaFlags_NotCacheable
        STR     r0, [r2, #DANode_Flags]
        MOV     r0, #0
        STR     r0, [r2, #DANode_Base]
        STR     r0, [r2, #DANode_Handler]
        CMP     r1, #1
        STREQ   r0, [r2, #DANode_Size]
        MOV     r0, #4096
        STRNE   r0, [r2, #DANode_Size]
        STR     r0, [r2, #DANode_MaxSize]
        MOV     r0, #ChangeDyn_Batcall
        MOV     r1, #4096
        RSBNE   r1, r1, #0
        SWI     XOS_ChangeDynamicArea
        FRAMSTR r0,VS
        EXIT    VS
        ; If we just enabled the page, fill it with the special value and then change it to read-only
        FRAMLDR r1
        RSBS    r1, r1, #1 ; invert returned state, to be correct for the above action
        STRB    r1, [r12, #CompatibilityPageEnabled] ; Also update our state flag
        FRAMSTR r1
        EXIT    EQ
        MOV     r0, #0
        ADR     r1, %FT20
10
        CMP     r0, #%FT30-%FT20
        LDRLO   r2, [r1, r0]
        STR     r2, [r0], #4
        CMP     r0, #4096
        BNE     %BT10
        LDR     r7, [r12, #MaxCamEntry]
        MOV     r4, #0
        BL      logical_to_physical
        BL      physical_to_ppn
        ; r9-r11 corrupt, r3 = page number, r5 = phys addr
        MOV     r0, #OSMemReason_FindAccessPrivilege
        MOV     r1, #2_100100
        MOV     r2, #2_100100
        SWI     XOS_Memory ; Get AP number for read-only access (will make area XN on ARMv6+)
        ORRVC   r11, r0, #DynAreaFlags_NotCacheable
        MOVVC   r2, r3
        MOVVC   r3, #0
        BLVC    BangCamUpdate
        EXIT

20
        ; Pattern to place in compatibility page
        DCD     &FDFDFDFD ; A few of words of invalid addresses, which should also be invalid instructions on ARMv5 (ARMv6+ will have this page non-executable, ARMv4 and lower can't have high processor vectors)
        DCD     &FDFDFDFD
        DCD     &FDFDFDFD
        DCD     &FDFDFDFD
        = "!!!!NULL.POINTER.DEREFERENCE!!!!", 0 ; Readable message if interpretered as a string. Also, all words are unaligned pointers.
        ALIGN
        DCD     0 ; Fill the rest with zero (typically, most of ZeroPage is zero)
30
 ]

;----------------------------------------------------------------------------------------
;
;        In:    r0 = flags
;                       bit     meaning
;                       0-7     24 (reason code)
;                       8-31    reserved (set to 0)
;               r1 = low address (inclusive)
;               r2 = high address (exclusive)
;
;       Out:    r1 = access flags:
;               bit 0: completely readable in user mode
;               bit 1: completely writable in user mode
;               bit 2: completely readable in privileged modes
;               bit 3: completely writable in privileged modes
;               bit 4: partially readable in user mode
;               bit 5: partially writable in user mode
;               bit 6: partially readable in privileged modes
;               bit 7: partially writable in privileged modes
;               bit 8: completely physically mapped (i.e. IO memory)
;               bit 9: completely abortable (i.e. custom data abort handler)
;               bit 10: completely non-executable in user mode
;               bit 11: completely non-executable in privileged modes
;               bit 12: partially physically mapped
;               bit 13: partially abortable
;               bit 14: partially non-executable in user mode
;               bit 15: partially non-executable in privileged modes
;               bits 16+: reserved
;
;       Return various attributes for the given memory region

; NOTE: To make the flags easier to calculate, this routine calculates executability rather than non-executability. This means that unmapped memory has flags of zero. On exit we invert the sense of the bits in order to get non-executability (so that the public values are backwards-compatible with OS versions that didn't return executability information)
CMA_Completely_Inverted * CMA_Completely_UserXN + CMA_Completely_PrivXN

CMA_CheckL2PT          * 1<<31 ; Pseudo flag used internally for checking sparse areas
CMA_DecodeAP           * 1<<30 ; Used with CheckL2PT to indicate AP flags should be decoded from L2PT

; AP_ equivalents

CheckMemoryAccess ROUT
        Entry   "r0,r2-r10"
        CMP     r0, #24
        BNE     %FT99
        LDR     r10, =ZeroPage
        ; Set all the 'completely' flags, we'll clear them as we go along
        LDR     r0, =&0F0F0F0F
        ; Make end address inclusive so we don't have to worry so much about
        ; wrap around at 4G
        TEQ     r1, r2
        SUBNE   r2, r2, #1
        ; Split memory up into five main regions:
        ; * scratchspace/zeropage
        ; * application space
        ; * dynamic areas
        ; * IO memory
        ; * special areas (stacks, ROM, HAL workspace, etc.)
        ; All ranges are checked in increasing address order, so the
        ; completeness flags are returned correctly if we happen to cross from
        ; one range into another
        ; Note that application space can't currently be checked in DA block as
        ; (a) it's not linked to DAList/DynArea_AddrLookup
        ; (b) we need to manually add the abortable flag
        CMP     r1, #32*1024
        BHS     %FT10
        ; Check zero page
        ASSERT  ProcVecs = ZeroPage
     [ ZeroPage = 0
        MOV     r3, #0
        MOV     r4, #16*1024
        LDR     r5, =CMA_ZeroPage
        BL      CMA_AddRange
     |
      [ CompatibilityPage
        ; Zero page compatibility page
        LDR     r3, =ZeroPage
        LDRB    r3, [r3, #CompatibilityPageEnabled]
        CMP     r3, #0
        BEQ     %FT05
        MOV     r3, #0
        MOV     r4, #4096
        ; This represents our ideal access flags; it may not correspond to reality
        LDR     r5, =CMA_Partially_UserR+CMA_Partially_PrivR
        BL      CMA_AddRange
05
      ]
        ; DebuggerSpace
        ASSERT  DebuggerSpace < ScratchSpace
        LDR     r3, =DebuggerSpace
        LDR     r4, =(DebuggerSpace_Size + &FFF) :AND: &FFFFF000
        LDR     r5, =CMA_DebuggerSpace
        BL      CMA_AddRange
     ]
        ; Scratch space
        LDR     r3, =ScratchSpace
        MOV     r4, #16*1024
        LDR     r5, =CMA_ScratchSpace
        BL      CMA_AddRange
10
        ; Application space
        ; Note - checking AplWorkSize as opposed to AplWorkMaxSize to cope with
        ; software which creates DAs within application space (e.g. Aemulor)
        LDR     r4, [r10, #AplWorkSize]
        CMP     r1, r4
        BHS     %FT20
        LDR     r3, [r10, #AMBControl_ws]
        LDR     r3, [r3, #:INDEX:AMBFlags]
        LDR     r5, =CMA_AppSpace
        TST     r3, #AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
        MOV     r3, #32*1024
        ORREQ   r5, r5, #CMA_Partially_Abort
        BL      CMA_AddRange2
20
        ; Dynamic areas
        LDR     r7, [r10, #IOAllocLimit]
        CMP     r1, r7
        BHS     %FT30
        ; Look through the quick lookup table until we find a valid DANode ptr
        LDR     r6, [r10, #DynArea_ws]
        MOV     r3, r1
        TEQ     r6, #0 ; We can get called during ROM init, before the workspace is allocated (pesky OS_Heap validating its pointers)
        ADD     r6, r6, #(:INDEX:DynArea_AddrLookup) :AND: &00FF
        LDREQ   r9, [r10, #DAList] ; So just start at the first DA
        ADD     r6, r6, #(:INDEX:DynArea_AddrLookup) :AND: &FF00
        BEQ     %FT22
21
        AND     r8, r3, #DynArea_AddrLookupMask
        LDR     r9, [r6, r8, LSR #30-DynArea_AddrLookupBits]
        TEQ     r9, #0
        BNE     %FT22
        ; Nothing here, skip ahead to next block
        ADD     r3, r8, #DynArea_AddrLookupSize
        CMP     r3, r2
        BHI     %FT90 ; Hit end of search area
        CMP     r3, r7
        BLO     %BT21
        ; Hit end of DA area and wandered into IO area
        B       %FT30
22
        ; Now that we've found a DA to start from, walk through and process all
        ; the entries until we hit the end of the list, or any DAs above
        ; IOAllocLimit
        LDR     r3, [r9, #DANode_Base]
        LDR     r6, [r9, #DANode_Flags]
        CMP     r3, r7
        BHS     %FT30
        ; Decode AP flags
        LDR     r5, [r10, #MMU_PPLAccess]
        AND     lr, r6, #DynAreaFlags_APBits
        LDR     r5, [r5, lr, LSL #2]
        TST     r6, #DynAreaFlags_PMP
        ORRNE   r5, r5, #CMA_DecodeAP
        TSTEQ   r6, #DynAreaFlags_SparseMap
        LDREQ   lr, [r9, #DANode_Size]
        LDRNE   r4, [r9, #DANode_SparseHWM] ; Use HWM as bounds when checking sparse/PMP areas
        ORRNE   r5, r5, #CMA_CheckL2PT ; ... and request L2PT check
        ADDEQ   r4, r3, lr
        TST     r6, #DynAreaFlags_DoublyMapped ; Currently impossible for Sparse/PMP areas - so use of lr safe
        SUBNE   r3, r3, lr
        BL      CMA_AddRange2
        LDR     r9, [r9, #DANode_Link]
        TEQ     r9, #0
        BNE     %BT22
        ; Hit the end of the list
30
        ; IO memory
        CMP     r1, #IO
        BHS     %FT40
        MOV     r3, r1, LSR #20
        LDR     r4, [r10, #IOAllocPtr]
        MOV     r3, r3, LSL #20 ; Get MB-aligned addr of first entry to check
        CMP     r3, r4
        LDR     r7, =L1PT
        MOVLO   r3, r4 ; Skip all the unallocated regions
31
        Push    "r0,r1"
        LDR     r0, [r7, r3, LSR #20-2]
        BL      DecodeL1Entry           ; TODO bit wasteful. We only care about access privileges, but this call gives us cache info too.
        LDR     r5, [r10, #MMU_PPLAccess]
        AND     lr, r1, #DynAreaFlags_APBits
        LDR     r5, [r5, lr, LSL #2]
        Pull    "r0,r1"
        ADD     r4, r3, #1<<20
        ORR     r5, r5, #CMA_Partially_Phys
        BL      CMA_AddRange2
        CMP     r4, #IO
        MOV     r3, r4
        BNE     %BT31
40
        ; Everything else!
        LDR     r3, =HALWorkspace
        LDR     r4, [r10, #HAL_WsSize]
        LDR     r5, =CMA_HALWorkspace
        BL      CMA_AddRange
        ASSERT  IRQStackAddress > HALWorkspace
        LDR     r3, =IRQStackAddress
        LDR     r4, =IRQStackSize
        LDR     r5, =CMA_IRQStack
        BL      CMA_AddRange
        ASSERT  SVCStackAddress > IRQStackAddress
        LDR     r3, =SVCStackAddress
        LDR     r4, =SVCStackSize
        LDR     r5, =CMA_SVCStack
        BL      CMA_AddRange
        ASSERT  ABTStackAddress > SVCStackAddress
        LDR     r3, =ABTStackAddress
        LDR     r4, =ABTStackSize
        LDR     r5, =CMA_ABTStack
        BL      CMA_AddRange
        ASSERT  UNDStackAddress > ABTStackAddress
        LDR     r3, =UNDStackAddress
        LDR     r4, =UNDStackSize
        LDR     r5, =CMA_UNDStack
        BL      CMA_AddRange
        ASSERT  PhysicalAccess > UNDStackAddress
        LDR     r3, =L1PT + (PhysicalAccess:SHR:18)
        LDR     r3, [r3]
        TEQ     r3, #0
        BEQ     %FT50
        LDR     r3, =PhysicalAccess
        LDR     r4, =&100000
        ; Assume IO memory mapped there
      [ MEMM_Type = "VMSAv6"
        LDR     r5, =CMA_Partially_PrivR+CMA_Partially_PrivW+CMA_Partially_Phys
      |
        LDR     r5, =CMA_Partially_PrivR+CMA_Partially_PrivW+CMA_Partially_PrivXN+CMA_Partially_Phys
      ]
        BL      CMA_AddRange
50
        ASSERT  DCacheCleanAddress > PhysicalAccess
        LDR     r4, =DCacheCleanAddress+DCacheCleanSize
        CMP     r1, r4
        BHS     %FT60
        ; Check that DCacheCleanAddress is actually used
        Push    "r0-r2,r9"
        AddressHAL r10
        MOV     a1, #-1
        CallHAL HAL_CleanerSpace
        CMP     a1, #-1
        Pull    "r0-r2,r9"
        BEQ     %FT60
        SUB     r3, r4, #DCacheCleanSize
        MOV     r4, #DCacheCleanSize
        ; Mark as IO, it may not be actual memory there
        LDR     r5, =CMA_DCacheClean+CMA_Partially_Phys
        BL      CMA_AddRange
60
        ASSERT  KbuffsBaseAddress > DCacheCleanAddress
        LDR     r3, =KbuffsBaseAddress
        LDR     r4, =(KbuffsSize + &FFF) :AND: &FFFFF000
        LDR     r5, =CMA_Kbuffs
        BL      CMA_AddRange
        ASSERT  HALWorkspaceNCNB > KbuffsBaseAddress
        LDR     r3, [r10, #HAL_Descriptor]
        LDR     r3, [r3, #HALDesc_Flags]
        TST     r3, #HALFlag_NCNBWorkspace
        BEQ     %FT70
        LDR     r3, =HALWorkspaceNCNB
        LDR     r4, =32*1024
        LDR     r5, =CMA_HALWorkspaceNCNB
        BL      CMA_AddRange
70
        ASSERT  L2PT > HALWorkspaceNCNB
        LDR     r3, =L2PT
        MOV     r4, #4*1024*1024
        LDR     r5, =CMA_PageTablesAccess+CMA_CheckL2PT ; L2PT contains gaps due to logical indexing
        BL      CMA_AddRange
        ASSERT  L1PT > L2PT
        LDR     r3, =L1PT
        MOV     r4, #16*1024
        LDR     r5, =CMA_PageTablesAccess
        BL      CMA_AddRange
        ASSERT  CursorChunkAddress > L1PT
        LDR     r3, =CursorChunkAddress
        MOV     r4, #32*1024
        LDR     r5, =CMA_CursorChunk
        BL      CMA_AddRange
        ASSERT  CAM > CursorChunkAddress
        LDR     r3, =CAM
        LDR     r4, [r10, #SoftCamMapSize]
        LDR     r5, =CMA_CAM
        BL      CMA_AddRange
        ASSERT  ROM > CAM
        LDR     r3, =ROM
        LDR     r4, =OSROM_ImageSize*1024
        LDR     r5, =CMA_ROM
        BL      CMA_AddRange
        ; Finally, high processor vectors/relocated zero page
        ASSERT  ProcVecs = ZeroPage
      [ ZeroPage > 0
        ASSERT  ZeroPage > ROM
        MOV     r3, r10
        LDR     r4, =16*1024
        LDR     r5, =CMA_ZeroPage
        BL      CMA_AddRange
      ]
90
        ; If there's anything else, we've wandered off into unallocated memory
        LDR     r3, =&0F0F0F0F
        BIC     r1, r0, r3
        B       CMA_Done

99
        PullEnv
        B       MemoryBadParameters

        ; Add range r3..r4 to attributes in r0
        ; Corrupts r8
CMA_AddRange ROUT ; r3 = start, r4 = length
        ADD     r4, r3, r4
CMA_AddRange2 ; r3 = start, r4 = end (excl.)
        LDR     r8, =&0F0F0F0F
        ; Increment r1 and exit if we hit r2
        ; Ignore any ranges which are entirely before us
        CMP     r1, r4
        MOVHS   pc, lr
        ; Check for any gap at the start, i.e. r3 > r1
        CMP     r3, r1
        BICHI   r0, r0, r8
        MOVHI   r1, r3 ; Update r1 for L2PT check code
        ; Exit if the range starts after our end point
        CMP     r3, r2
        BHI     %FT10
        ; Process the range
        TST     r5, #CMA_CheckL2PT
        BNE     %FT20
        CMP     r3, r4 ; Don't apply any flags for zero-length ranges
04      ; Note L2PT check code relies on NE condition here
        ORR     r8, r5, r8
        ORRNE   r0, r0, r5 ; Set new partial flags
        ANDNE   r0, r0, r8, ROR #4 ; Discard completion flags which aren't for this range
05
        CMP     r4, r2
        MOV     r1, r4 ; Continue search from the end of this range
        MOVLS   pc, lr
10
        ; We've ended inside this range
        MOV     r1, r0
CMA_Done
        ; Invert the sense of the executability flags
        ;               Completely_X Partially_X -> Completely_XN Partially_XN
        ; Completely X             1           1                0            0
        ; Partially X              0           1                0            1
        ; XN                       0           0                1            1
        ; I.e. swap the positions of the two bits and invert them
        EOR     r0, r1, r1, LSR #4      ; Completely EOR Partially
        MVN     r0, r0                  ; Invert as well as swap
        AND     r0, r0, #CMA_Completely_Inverted ; Only touch these bits
        EOR     r1, r1, r0              ; Swap + invert Completely flags
        EOR     r1, r1, r0, LSL #4      ; Swap + invert Partially flags
        CLRV
        EXIT

20
        ; Check L2PT for sparse region r1..min(r2+1,r4)
        ; r4 guaranteed page aligned
        CMP     r3, r4
        BIC     r5, r5, #CMA_CheckL2PT
        BEQ     %BT05
        Push    "r2,r4,r5,r8,r9,r10,lr"
        LDR     lr, =&FFF
        CMP     r4, r2
        ADDHS   r2, r2, #4096
        BICHS   r2, r2, lr
        MOVLO   r2, r4
        ; r2 is now page aligned min(r2+1,r4)
        LDR     r8, =L2PT
        TST     r5, #CMA_DecodeAP
        BIC     r4, r1, lr
        BNE     %FT35
        MOV     r10, #0
30
        BL      logical_to_physical
        ORRCC   r10, r10, #1
        ADD     r4, r4, #4096
        ORRCS   r10, r10, #2
        CMP     r4, r2
        BNE     %BT30
        CMP     r10, #2
        ; 01 -> entirely mapped
        ; 10 -> entirely unmapped
        ; 11 -> partially mapped
        Pull    "r2,r4,r5,r8,r9,r10,lr"
        BICHS   r0, r0, r8 ; Not fully mapped, clear completion flags
        BNE     %BT04 ; Partially/entirely mapped
        B       %BT05 ; Completely unmapped

35
        ; Check L2PT, with AP decoding on a per-page basis
40
        LDR     r10, =&0F0F0F0F
        BL      logical_to_physical
        BICCS   r0, r0, r10 ; Not fully mapped, clear completion flags
        BCS     %FT45
        ; Get the L2PT entry and decode the flags
        Push    "r0-r2"
        LDR     r0, [r8, r4, LSR #10]
        BL      DecodeL2Entry           ; TODO bit wasteful. We only care about access privileges, but this call gives us cache info too. Also, if we know the L2PT backing exists (it should do) we could skip the logical_to_physical call
        ; r1 = DA flags
        ; Extract and decode AP
        LDR     r0, =ZeroPage
        LDR     r5, [r0, #MMU_PPLAccess]
        AND     lr, r1, #DynAreaFlags_APBits
        LDR     r5, [r5, lr, LSL #2]
        Pull    "r0-r2"
        ORR     r10, r5, r10
        ORR     r0, r0, r5 ; Set new partial flags
        AND     r0, r0, r10, ROR #4 ; Discard completion flags which aren't for this range
45
        ADD     r4, r4, #4096
        CMP     r4, r2
        BNE     %BT40
        Pull    "r2,r4,r5,r8,r9,r10,lr"
        B       %BT05

        LTORG

        END
@


4.14
log
@Merge SMP branch to trunk
Detail:
  Since the current SMP changes are fairly minor, and the trunk is seeing most development, from a maintenance perspective it makes sense to merge the changes to trunk. This will also make sure they get some wider testing ready for when the next round of SMP development takes place.
  Changes:
  - Docs/SMP - New docs folder describing SMP-related changes to the HAL and interrupt handling. Some of the IRQ changes can also be taken advantage of by single-core devices, since it introduces a way to describe which interrupt sources can be routed to IRQ & FIQ
  - Makefile, hdr/DBellDevice, hdr/HALDevice - New HAL device for an inter-processor software-generated interrupt source ("doorbell")
  - hdr/HALEntries - Reuse the unused matrix keyboard & touchscreen HAL entry points for the new IRQ handling & SMP-related HAL calls
  - hdr/KernelWS - Bump up MaxInterrupts
  - hdr/OSMem, s/MemInfo - Introduce OS_Memory 19, to allow for DMA to/from cacheable memory without actually altering the cacheability of the pages (which can be even more tricky in SMP systems than it is in uniprocessor systems)
  - hdr/Options - Introduce SMP build switch. Currently this controls whether the ARMops will operate in "SMP-friendly" mode or not (when running on MP processors)
  - s/ARMops, s/MemMap2 - Introduce the ARMv7MP ARMop implementation. Simplify DCache_LineLen / ICache_LineLen handling for WB_CR7_Lx so that it's the plain value rather than log2(n)-2
  - s/ExtraSWIs - If ARMops are in SMP-friendly mode, global OS_SynchroniseCodeAreas now only syncs application space and the RMA. This is because there is no trivial MP-safe global IMB operation available. This will also make global OS_SynchroniseCodeAreas significantly slower, but the documentation has always warned against performing a global IMB for just that reason, so code that suffers performance penalties should really try and switch to a ranged IMB.
  - s/NewIRQs - Update some comments regarding IRQ handler entry/exit conditions
Admin:
  Untested


Version 6.09. Tagged as 'Kernel-6_09'
@
text
@d139 1
a139 1
        MSR     CPSR_c, r11
@


4.13
log
@Tweak handling of zero page compatibility page
Detail:
  s/MemInfo, hdr/KernelWS - Rather than peeking L2PT to determine if the compatibility page is enabled, use a workspace var to track its state. This ensures we won't get confused if other software decides to map something of its own to &0.
  s/NewReset - Ensure the CompatibilityPageEnabled flag is initialised correctly
Admin:
  Tested in Iyonix ROM softload


Version 5.90. Tagged as 'Kernel-5_90'
@
text
@d72 1
a72 1
        B       %BT20                           ; Reason code 19 reserved (for DMAPrep, on SMP branch)
d1012 2
d1342 347
@


4.12
log
@Add a compatibility page zero for high processor vectors / zero page relocation builds
Detail:
  When HiProcVecs is enabled, there will now be a read-only page located at &0 in order to ease compatibility with buggy software which reads from null pointers
  Although most of the page is zero-filled, the start of the page contains a few words which are invalid pointers, discouraging dereferencing them, and a warning message if the memory is interpreted as a string.
  On ARMv6+ the page is also made non-executable, to deal with branch-through-zero type situations
  OS_Memory 20 has been introduced as a way of determining whether the compatibility page is present, and also to enable/disable it
  File changes:
  - hdr/Options - Add CompatibilityPage option
  - hdr/OSMem - Declare OS_Memory reason code 20
  - hdr/KernelWS - When CompatibilityPage is enabled, make sure nothing else is located at &0
  - s/NewReset - Enable compatibility page just before Service_PostInit (try and keep zero-tolerance policy for null pointer dereferencing during ROM init)
  - s/MemInfo - OS_Memory 20 implementation. Add knowledge of the compatibility page to OS_Memory 16 and 24.
Admin:
  Tested on BB-xM


Version 5.87. Tagged as 'Kernel-5_87'
@
text
@d1185 2
a1186 2
        LDR     r0, =L2PT
        LDR     r3, [r0]
d1371 3
a1373 6
        ; Peek L2PT to see if anything's mapped to &0
        LDR     r8, =L2PT
        LDR     r0, [r8]
        CMP     r0, #0
        MOVNE   r0, #1
        FRAMSTR r0,,r1 ; return pre-change state in r1
d1377 8
d1406 1
a1416 1
        LDR     r12, =ZeroPage
d1517 2
a1518 2
        LDR     r3, =L2PT
        LDR     r3, [r3]
@


4.11
log
@Implement support for cacheable pagetables
Detail:
  Modern ARMs (ARMv6+) introduce the possibility for the page table walk hardware to make use of the data cache(s) when performing memory accesses. This can significantly reduce the cost of a TLB miss on the system, and since the accesses are cache-coherent with the CPU it allows us to make the page tables cacheable for CPU (program) accesses also, improving the performance of page table manipulation by the OS.
  Even on ARMs where the page table walk can't use the data cache, it's been measured that page table manipulation operations can still benefit from placing the page tables in write-through or bufferable memory.
  So with that in mind, this set of changes updates the OS to allow cacheable/bufferable page tables to be used by the OS + MMU, using a system-appropriate cache policy.
  File changes:
  - hdr/KernelWS - Allocate workspace for storing the page flags that are to be used by the page tables
  - hdr/OSMem - Re-specify CP_CB_AlternativeDCache as having a different behaviour on ARMv6+ (inner write-through, outer write-back)
  - hdr/Options - Add CacheablePageTables option to allow switching back to non-cacheable page tables if necessary. Add SyncPageTables var which will be set {TRUE} if either the OS or the architecture requires a DSB after writing to a faulting page table entry.
  - s/ARM600, s/VMSAv6 - Add new SetTTBR & GetPageFlagsForCacheablePageTables functions. Update VMSAv6 for wider XCBTable (now 2 bytes per element)
  - s/ARMops - Update pre-ARMv7 MMU_Changing ARMops to drain the write buffer on entry if cacheable pagetables are in use (ARMv7+ already has this behaviour due to architectural requirements). For VMSAv6 Normal memory, change the way that the OS encodes the cache policy in the page table entries so that it's more compatible with the encoding used in the TTBR.
  - s/ChangeDyn - Update page table page flag handling to use PageTable_PageFlags. Make use of new PageTableSync macro.
  - s/Exceptions, s/AMBControl/memmap - Make use of new PageTableSync macro.
  - s/HAL - Update MMU initialisation sequence to make use of PageTable_PageFlags + SetTTBR
  - s/Kernel - Add PageTableSync macro, to be used after any write to a faulting page table entry
  - s/MemInfo - Update OS_Memory 0 page flag conversion. Update OS_Memory 24 to use new symbol for page table access permissions.
  - s/MemMap2 - Use PageTableSync. Add routines to enable/disable cacheable pagetables
  - s/NewReset - Enable cacheable pagetables once we're fully clear of the MMU initialision sequence (doing earlier would be trickier due to potential double-mapping)
Admin:
  Tested on pretty much everything currently supported
  Delivers moderate performance benefits to page table ops on old systems (e.g. 10% faster), astronomical benefits on some new systems (up to 8x faster)
  Stats: https://www.riscosopen.org/forum/forums/3/topics/2728?page=2#posts-58015


Version 5.71. Tagged as 'Kernel-5_71'
@
text
@d72 2
a73 2
        B       %BT20                           ; 19 |
        B       %BT20                           ; 20 | Reserved for us
d75 1
a75 1
        B       %BT20                           ; 22 |
d1032 1
d1068 1
d1181 11
d1340 99
d1504 1
a1504 1
      [ ZeroPage = 0
d1509 14
a1522 1
      |
d1529 1
a1529 1
      ]
@


4.11.2.1
log
@Add OS_Memory 19, which is intended to replace the OS_Memory 0 "make uncacheable" feature, when used for DMA
Detail:
  Making pages uncacheable to allow them to be used with DMA can be troublesome for a number of reasons:
  * Many processors ignore cache hits for non-cacheable pages, so to avoid breaking any IRQ handlers the page table manipulation + cache maintenance must be performed with IRQs disabled, impacting the IRQ latency of the system
  * Some processors don't support LDREX/STREX to non-cacheable pages
  * In SMP setups it may be necessary to temporarily park the other cores somewhere safe, or perform some other explicit synchronisation to make sure they all have consistent views of the cache/TLB
  The above issues are most likely to cause problems when the page is shared by multiple programs; a DMA operation which targets one part of a page could impact the programs which are using the other parts.
  To combat these problems, OS_Memory 19 is being introduced, which allows DMA cache coherency/address translation to be performed without altering the attributes of the pages.
  Files changed:
  - hdr/OSMem - Add definitions for OS_Memory 19
  - s/MemInfo - Add OS_Memory 19 implementation
Admin:
  Tested on Raspberry Pi 3, iMx6


Version 5.86, 4.129.2.3. Tagged as 'Kernel-5_86-4_129_2_3'
@
text
@d72 1
a72 1
        B       DMAPrep                         ; 19
a1011 2
        LTORG

a1330 348
;                       0-7     19 (reason code)
;                       8       Input function provides physical addresses
;                       9       DMA is writing to RAM
;                       10      DMA is complete, perform any post-op cache maintenance
;                       11-31   reserved (set to 0)
;               r1 = R12 value to provide to called functions
;               r2 = Initial R9 value to provide to input function
;               r3 -> Input function:
;                      in:  r9 = r2 from SWI / value from previous call
;                           r12 = r1 from SWI
;                      out: r0 = start address of region
;                           r1 = length of region (0 if end of transfer)
;                           r2 = flags:
;                                bit 0: Bounce buffer will be used
;                           r9 = new r9 for next input call
;                           r12 corrupt
;               r4 = Initial R9 value to provide to output function
;               r5 -> Output function (if bit 10 of R0 clear):
;                      in: r0 = logical address of start of region
;                          r1 = physical address of start of region
;                          r2 = length of region
;                          r3 = flags:
;                               bit 0: Bounce buffer must be used
;                          r9 = r4 from SWI / value from previous call
;                          r12 = r1 from SWI
;                      out: r9 = new r9 value for next output call
;                           r0-r3, r12 corrupt
;
;       Out:    r2, r4 updated to match values returned by input/output calls
;               All other regs preserved
;
; Performs address translation and cache maintenance necessary to allow for DMA
; to be performed to/from cacheable memory.
;
; To allow Service_PagesUnsafe to be dealt with in a straightforward manner, we
; have to be careful not to cache the results of any address translations over
; calls to the input/output functions. E.g. if the output function tries to
; allocate from PCI RAM, that may trigger claiming of a specific page by the
; PCI DA, potentially invalidating any existing logical -> physical translation.
; This restriction hampers the routines ability to merge together input and
; output blocks, and to perform minimal cache maintenance. However for typical
; scatter lists of low to medium complexity it should still produce acceptable
; output.
;
; Note that if the input function provides physical addresses, the caller must
; take care to abort the entire operation if one of the physical pages involved
; in the request becomes claimed by someone else while the OS_Memory call is in
; progress. This is because we have no sensible way of dealing with this case
; ourselves (even if we didn't attempt to call the input function multiple times
; and merge together the blocks, we'd still have to buffer things internally to
; deal with when blocks need splitting for cache alignment)
;
; Internally, blocks are stored in the following format:
;
; Word 0 = Start logical address (incl.)
; Word 1 = Logical -> physical address offset (low bits) + flags (high bits)
; Word 2 = End logical address (excl.)
;
; This minimises the number of registers needed to hold a block, and simplifies
; the merge calculation (blocks can be merged if words 2 + 1 of first block
; match words 0 + 1 of second block)

; Workspace struct that's stored on the stack
                    ^ 0
DMAPrepW_InHold     # 12
DMAPrepW_InChunk    # 12
DMAPrepW_PhyChunk   # 12
DMAPrepW_CacheMask  # 4 ; Cache line length - 1
DMAPrepW_ARMop      # 4 ; Cache maintenenace ARMop to use
DMAPrepW_CamEntriesPointer # 4 ; CamEntriesPointer copy
DMAPrepW_Size       # 0
                        ; These next few correspond directly to the input registers in the stack frame
DMAPrepW_Flags      # 4
DMAPrepW_R12        # 4
DMAPrepW_InR9       # 4
DMAPrepW_InFunc     # 4
DMAPrepW_OutR9      # 4
DMAPrepW_OutFunc    # 4

DMAPrep_FlagOffset * 20
DMAPrep_NonCacheable * 1:SHL:21 ; Internal flag used for tracking non-cacheable pages

DMAPrep ROUT
        CMP     r0, #1<<11
        BHS     MemoryBadParameters
        ; The end of a read from RAM is a no-op (no cache maintenance required)
        AND     r11, r0, #DMAPrep_Write :OR: DMAPrep_End
        TEQ     r11, #DMAPrep_End
        MOVEQ   pc, lr
        Entry   "r0-r9", DMAPrepW_Size
        ; Determine the cache maintenance function we need to use
        CMP     r11, #DMAPrep_Write
        LDR     r10, =ZeroPage
        ASSERT  DMAPrep_End > DMAPrep_Write
        LDRLE   r11, [r10, #Proc_Cache_CleanRange] ; Start of DMA (read or write)
        LDRGT   r11, [r10, #Proc_Cache_InvalidateRange] ; End of DMA write
        STR     r11, [sp, #DMAPrepW_ARMop]
        ; Get the params needed for address translation
        LDR     r6, [r10, #CamEntriesPointer]
        LDR     r7, [r10, #MaxCamEntry]
        LDR     r8, =L2PT
        ; Init workspace
        STR     r6, [sp, #DMAPrepW_CamEntriesPointer]
        ; Get the cache line mask value
      [ MEMM_Type == "ARM600"
        LDRB    r1, [r10, #DCache_LineLen]
      |
        ; Yuck, need to probe for the last cache level
        MOV     r5, #Cache_Lx_MaxLevel-1
01
        MOV     r1, r5
        ARMop   Cache_Examine,,,r10
        CMP     r1, #0
        SUBEQ   r5, r5, #1
        BEQ     %BT01
        CMP     r3, r1
        MOVHI   r1, r3
      ]
        SUB     r1, r1, #1
        STR     r1, [sp, #DMAPrepW_CacheMask]
        ; Get initial input region
        BL      DMAPrep_CallInputFunc
        CMP     r0, r3
        BEQ     %FT90
05
        STMIA   lr, {r0, r2, r3}
10
        ; Get another input region, see if we can merge it with InChunk
        BL      DMAPrep_CallInputFunc
        CMP     r0, r3
        BEQ     %FT19
        LDMIB   lr, {r4, r5}
        CMP     r4, r2
        CMPEQ   r5, r0
        STREQ   r3, [lr, #8]
        BEQ     %BT10
19
        ; Can't merge this region, store it in InHold
        ASSERT  DMAPrepW_InHold = DMAPrepW_InChunk-12
        STMDB   lr, {r0, r2, r3}
20
        ; Perform address translation for the start of InChunk
        LDR     r4, [sp, #DMAPrepW_InChunk]
        BL      DMAPrep_Translate
        ; Store in PhyChunk
        ADD     lr, sp, #DMAPrepW_PhyChunk
        STMIA   lr, {r4-r6}
        ; Align start to cache boundary
        TST     r5, #DMAPrep_NonCacheable+(DMAPrep_UseBounceBuffer :SHL: DMAPrep_FlagOffset)
        BNE     %FT25
        LDR     lr, [sp, #DMAPrepW_Flags]
        LDR     r10, [sp, #DMAPrepW_CacheMask]
        TST     lr, #DMAPrep_Write
        TSTNE   r4, r10
        BEQ     %FT25
        ; Unaligned write to cacheable memory -> bounce required
        ADD     r1, r4, r10
        BIC     r1, r1, r10 ; End of current cache line
        ; Only round down to end of current cache line if the end of the chunk
        ; is at or beyond the end of the next cache line
        ADD     r2, r1, r10 ; Last byte we can accept without needing to truncate
        CMP     r6, r2
        MOVHI   r6, r1 ; Truncate!
        ORR     r5, r5, #DMAPrep_UseBounceBuffer :SHL: DMAPrep_FlagOffset
        B       %FT40
25
        ; Start doesn't need splitting, so translate + append more pages
        ADD     lr, sp, #DMAPrepW_InChunk
        ASSERT  DMAPrepW_PhyChunk = DMAPrepW_InChunk + 12
        LDMIA   lr, {r0-r2, r4-r6}
        SUB     r3, r6, r4 ; Length of translated region
        SUB     r2, r2, r0 ; Length of input region
        CMP     r3, r2
        BEQ     %FT30
        ADD     r4, r0, r3 ; Translate next address in input address space
        BL      DMAPrep_Translate
        ; Try and merge with PhyChunk
        ADD     lr, sp, #DMAPrepW_PhyChunk
        LDMIB   lr, {r0, r1}
        CMP     r0, r5
        CMPEQ   r1, r4
        STREQ   r6, [sp, #DMAPrepW_PhyChunk + 8]
        BEQ     %BT25
        LDMIA   lr, {r4-r6}
30
        ; Can't merge any more pages into this chunk {r4-r6}
        ; Truncate / bounce the end if necessary
        TST     r5, #DMAPrep_NonCacheable+(DMAPrep_UseBounceBuffer :SHL: DMAPrep_FlagOffset)
        BNE     %FT50
        LDR     lr, [sp, #DMAPrepW_Flags]
        LDR     r10, [sp, #DMAPrepW_CacheMask]
        TST     lr, #DMAPrep_Write
        TSTNE   r6, r10
        BEQ     %FT40
        ; Unaligned write to cacheable memory -> bounce required
        BIC     r3, r6, r10
        CMP     r3, r4
        ORREQ   r5, r5, #DMAPrep_UseBounceBuffer :SHL: DMAPrep_FlagOffset ; Bounce
        MOVNE   r6, r3 ; Truncate
40
        ; Perform cache maintenance if necessary
        ; For safety we always perform this before calling the output function, rather than caching and attempting to merge the regions (output function may alter cacheability of pages?)
        TST     r5, #DMAPrep_NonCacheable+(DMAPrep_UseBounceBuffer :SHL: DMAPrep_FlagOffset)
        BNE     %FT50
        ADD     r1, r6, r10
        BIC     r0, r4, r10
        BIC     r1, r1, r10
        MOV     lr, pc
        LDR     pc, [sp, #DMAPrepW_ARMop]
50
        ; Call the output function
        LDR     lr, [sp, #DMAPrepW_Flags]
        TST     lr, #DMAPrep_End
        BNE     %FT60 ; No output func for end-of-op
        MOV     r0, r4
        ADD     r1, r4, r5, LSL #12
        SUB     r2, r6, r4
        MOV     r3, r5, LSR #DMAPrep_FlagOffset
        LDR     r12, [sp, #DMAPrepW_R12]
        AND     r3, r3, #DMAPrep_UseBounceBuffer ; Mask out internal flags
        ADD     r9, sp, #DMAPrepW_OutR9
        CLRV    ; Ensure V is clear on entry so simple functions don't confuse us
        MOV     lr, pc
        ASSERT  DMAPrepW_OutFunc = DMAPrepW_OutR9 + 4
        LDMIA   r9, {r9, pc}            ; Call output function
        STR     r9, [sp, #DMAPrepW_OutR9] ; Always write back updated R9
        BVS     %FT90
60
        ; Advance InChunk by the length of {r4-r6}
        LDR     r0, [sp, #DMAPrepW_InChunk]
        ADD     r0, r0, r6
        LDR     r1, [sp, #DMAPrepW_InChunk+8]
        SUB     r0, r0, r4
        STR     r0, [sp, #DMAPrepW_InChunk]
        CMP     r0, r1
        BNE     %BT20
        ; InChunk depleted, copy InHold to InChunk and try for more input
        ADD     lr, sp, #DMAPrepW_InChunk
        ASSERT  DMAPrepW_InHold = 0
        LDMIA   sp, {r0,r2,r3}
        CMP     r0, r3
        BNE     %BT05
        ; InHold was empty, so no more regions to process
90
        FRAMSTR r0, VS
        EXIT

95
        ADRL    r0, ErrorBlock_BadAddress
        SETV
        B       %BT90

96
        PullEnv
        B       MemoryBadParameters

; Out: R0, R2, R3 = block
;      LR -> InChunk
;      R1, R4, R9, R12 corrupt
DMAPrep_CallInputFunc
        MOV     r4, lr ; Avoid pushing onto stack, to simplify workspace indexing and error handling
        LDR     r12, [sp, #DMAPrepW_R12]
        ADD     r9, sp, #DMAPrepW_InR9
        CLRV    ; Ensure V is clear on entry so simple functions don't confuse us
        MOV     lr, pc
        ASSERT  DMAPrepW_InFunc = DMAPrepW_InR9 + 4
        LDMIA   r9, {r9, pc}            ; Call the input function
        STR     r9, [sp, #DMAPrepW_InR9] ; Always write back updated R9
        BVS     %BT90
        CMP     r2, #DMAPrep_UseBounceBuffer
        BHI     %BT96
        ; Pack into InChunk
        MOV     r2, r2, LSL #DMAPrep_FlagOffset
        ADD     lr, sp, #DMAPrepW_InChunk
        ADD     r3, r0, r1
        MOV     pc, r4

; Translate the start of InChunk into a block
; In: r4 = Address to translate
;     r7 = MaxCamEntry
;     r8 -> L2PT
; Out: r4, r5, r6 = block
;      r1, r3, r9-r12 corrupt
DMAPrep_Translate
        MOV     r1, lr
        LDR     r12, [sp, #DMAPrepW_InChunk+8]
        SUB     r12, r12, r4            ; Length of input region
        LDR     lr, [sp, #DMAPrepW_Flags]
        LDR     r6, [sp, #DMAPrepW_CamEntriesPointer]
        TST     lr, #DMAPrep_PhysProvided
        BNE     %FT20
      [ AMB_LazyMapIn
        MOV     r9, r0
        MOV     r0, r4
        BL      AMB_MakeHonestLA
        MOV     r0, r9
      ]
        BL      logical_to_physical     ; r4, r8 -> r5
        BLCC    physical_to_ppn         ; r5, r7 -> r3
        BCS     %BT95
        ; r9-r11 corrupt
        ; Grab page flags
        ADD     lr, r6, r3, LSL #CAM_EntrySizeLog2
        LDR     lr, [lr, #CAM_PageFlags]
        B       %FT30
20
        MOV     r5, r4
        BL      physical_to_ppn         ; r5, r7 -> r3
        BCS     %BT95
        ; r9-r11 corrupt
        ; Manual ppn -> logical so we can get the page flags at the same time
        ; TODO this won't deal with mapped out pages in a sensible manner (will output them all individually)
      [ AMB_LazyMapIn
        MOV     r9, r0
        MOV     r0, r3
        BL      AMB_MakeHonestPN
        MOV     r0, r9
      ]
        ADD     lr, r6, r3, LSL #CAM_EntrySizeLog2
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
        LDMIA   lr, {r3, lr}
        ; Merge in the offset within the page
        MOV     r3, r3, LSR #12
        ORR     r4, r3, r4, LSL #20
        MOV     r4, r4, ROR #20 
30
        LDR     r3, [sp, #DMAPrepW_InChunk+4]
        ; Combine the cacheability + phys offset into r5
        SUB     r5, r5, r4
        TST     lr, #DynAreaFlags_NotCacheable
        ORR     r5, r3, r5, LSR #12
        ORRNE   r5, r5, #DMAPrep_NonCacheable
        ; Work out how much of r12 fits into this page
        ; This is done by comparing against the length of the input region,
        ; since the input could be logical or physical
        ADD     r6, r4, #4096
        MOV     r6, r6, LSR #12
        RSB     r6, r4, r6, LSL #12
        CMP     r6, r12
        MOVHI   r6, r12
        ADD     r6, r4, r6
        MOV     pc, r1        

;----------------------------------------------------------------------------------------
;
;        In:    r0 = flags
;                       bit     meaning
@


4.11.2.2
log
@Merge in latest changes from main branch


Version 5.88, 4.129.2.4. Tagged as 'Kernel-5_88-4_129_2_4'
@
text
@d73 1
a73 1
        B       ChangeCompatibility             ; 20
d75 1
a75 1
        B       %BT20                           ; 22 | Reserved for us
a1033 1
;                               16=Compatibility page
a1068 1
        B       MAI_CompatibilityPage
a1180 11
MAI_CompatibilityPage
      [ CompatibilityPage
        MOV     r1, #0
        MOV     r2, #4096
        LDR     r0, =L2PT
        LDR     r3, [r0]
        CMP     r3, #0
        MOVNE   r3, #4096
      ]
        EXIT

a1678 98
;       In:     r0 = flags
;                       bit     meaning
;                       0-7     20 (reason code)
;                       8-31    reserved (set to 0)
;               r1 = 0 to disable compatibility page
;                    1 to enable compatibility page
;                    -1 to read state
;
;       Out:    r1 = new/current state:
;                    0 if disabled
;                    1 if enabled
;                    -1 if not supported
;
;       Controls the page zero compatibility page located at &0
;
;       If the compatibility page isn't supported, attempts to enable it will
;       silently fail, with a result of r1 = -1
;
ChangeCompatibility ROUT
        CMP     r1, #-1
        CMPNE   r1, #1
        CMPLS   r0, #255
        BHI     MemoryBadParameters
 [ :LNOT: CompatibilityPage
        MOV     r1, #-1
        MOV     pc, lr
 |
        Entry   "r0-r11", DANode_NodeSize
        ; Peek L2PT to see if anything's mapped to &0
        LDR     r8, =L2PT
        LDR     r0, [r8]
        CMP     r0, #0
        MOVNE   r0, #1
        FRAMSTR r0,,r1 ; return pre-change state in r1
        CMP     r1, #-1
        CMPNE   r0, r1
        EXIT    EQ
        ; Set up temp DANode on the stack so we can use a Batcall to manage the mapping
        MOV     r2, sp
        MOV     r0, #DynAreaFlags_NotCacheable
        STR     r0, [r2, #DANode_Flags]
        MOV     r0, #0
        STR     r0, [r2, #DANode_Base]
        STR     r0, [r2, #DANode_Handler]
        CMP     r1, #1
        STREQ   r0, [r2, #DANode_Size]
        MOV     r0, #4096
        STRNE   r0, [r2, #DANode_Size]
        STR     r0, [r2, #DANode_MaxSize]
        MOV     r0, #ChangeDyn_Batcall
        MOV     r1, #4096
        RSBNE   r1, r1, #0
        SWI     XOS_ChangeDynamicArea
        FRAMSTR r0,VS
        EXIT    VS
        ; If we just enabled the page, fill it with the special value and then change it to read-only
        FRAMLDR r1
        RSBS    r1, r1, #1 ; invert returned state, to be correct for the above action
        FRAMSTR r1
        EXIT    EQ
        MOV     r0, #0
        ADR     r1, %FT20
10
        CMP     r0, #%FT30-%FT20
        LDRLO   r2, [r1, r0]
        STR     r2, [r0], #4
        CMP     r0, #4096
        BNE     %BT10
        LDR     r12, =ZeroPage
        LDR     r7, [r12, #MaxCamEntry]
        MOV     r4, #0
        BL      logical_to_physical
        BL      physical_to_ppn
        ; r9-r11 corrupt, r3 = page number, r5 = phys addr
        MOV     r0, #OSMemReason_FindAccessPrivilege
        MOV     r1, #2_100100
        MOV     r2, #2_100100
        SWI     XOS_Memory ; Get AP number for read-only access (will make area XN on ARMv6+)
        ORRVC   r11, r0, #DynAreaFlags_NotCacheable
        MOVVC   r2, r3
        MOVVC   r3, #0
        BLVC    BangCamUpdate
        EXIT

20
        ; Pattern to place in compatibility page
        DCD     &FDFDFDFD ; A few of words of invalid addresses, which should also be invalid instructions on ARMv5 (ARMv6+ will have this page non-executable, ARMv4 and lower can't have high processor vectors)
        DCD     &FDFDFDFD
        DCD     &FDFDFDFD
        DCD     &FDFDFDFD
        = "!!!!NULL.POINTER.DEREFERENCE!!!!", 0 ; Readable message if interpretered as a string. Also, all words are unaligned pointers.
        ALIGN
        DCD     0 ; Fill the rest with zero (typically, most of ZeroPage is zero)
30
 ]

;----------------------------------------------------------------------------------------
;
d1742 1
a1742 1
     [ ZeroPage = 0
d1747 1
a1747 14
     |
      [ CompatibilityPage
        ; Zero page compatibility page
        LDR     r3, =L2PT
        LDR     r3, [r3]
        CMP     r3, #0
        BEQ     %FT05
        MOV     r3, #0
        MOV     r4, #4096
        ; This represents our ideal access flags; it may not correspond to reality
        LDR     r5, =CMA_Partially_UserR+CMA_Partially_PrivR
        BL      CMA_AddRange
05
      ]
d1754 1
a1754 1
     ]
@


4.11.2.3
log
@Merge in latest changes from main branch

Version 5.97, 4.129.2.7. Tagged as 'Kernel-5_97-4_129_2_7'
@
text
@d1187 2
a1188 2
        LDR     r0, =ZeroPage
        LDRB    r3, [r0,#CompatibilityPageEnabled]
d1720 6
a1725 3
        LDR     r12, =ZeroPage
        LDRB    r0, [r12, #CompatibilityPageEnabled]
        FRAMSTR r0,,r1 ; return pre-change state in r1 (will be updated later, as necessary)
a1728 8
        ; If we're attempting to enable it, make sure nothing else has mapped itself in to page zero
        LDR     r8, =L2PT
        CMP     r1, #0
        LDRNE   r0, [r8]
        CMPNE   r0, #0
        MOVNE   r1, #-1
        FRAMSTR r1,NE
        EXIT    NE
a1749 1
        STRB    r1, [r12, #CompatibilityPageEnabled] ; Also update our state flag
d1760 1
d1861 2
a1862 2
        LDR     r3, =ZeroPage
        LDRB    r3, [r3, #CompatibilityPageEnabled]
@


4.10
log
@Make MMU_Changing ARMops perform the sub-operations in a sensible order
Detail:
  For a while we've known that the correct way of doing cache maintenance on ARMv6+ (e.g. when converting a page from cacheable to non-cacheable) is as follows:
  1. Write new page table entry
  2. Flush old entry from TLB
  3. Clean cache + drain write buffer
  The MMU_Changing ARMops (e.g. MMU_ChangingEntry) implement the last two items, but in the wrong order. This has caused the operations to fall out of favour and cease to be used, even in pre-ARMv6 code paths where the effects of improper cache/TLB management perhaps weren't as readily visible.
  This change re-specifies the relevant ARMops so that they perform their sub-operations in the correct order to make them useful on modern ARMs, updates the implementations, and updates the kernel to make use of the ops whereever relevant.
  File changes:
  - Docs/HAL/ARMop_API - Re-specify all the MMU_Changing ARMops to state that they are for use just after a page table entry has been changed (as opposed to before - e.g. 5.00 kernel behaviour). Re-specify the cacheable ones to state that the TLB invalidatation comes first.
  - s/ARM600, s/ChangeDyn, s/HAL, s/MemInfo, s/VMSAv6, s/AMBControl/memmap - Replace MMU_ChangingUncached + Cache_CleanInvalidate pairs with equivalent MMU_Changing op
  - s/ARMops - Update ARMop implementations to do everything in the correct order
  - s/MemMap2 - Update ARMop usage, and get rid of some lingering sledgehammer logic from ShuffleDoublyMappedRegionForGrow
Admin:
  Tested on pretty much everything currently supported


Version 5.70. Tagged as 'Kernel-5_70'
@
text
@d257 2
a258 1
        LDRB    lr, [r5, lr, LSR #4]    ; convert to X, C and B bits for this CPU
d260 1
a260 1
        BIC     r5, r5, #(L2_C+L2_B+L2_TEX) :AND: 255 ; Knock out existing attributes (n.b. assumed to not be large page!)
d1577 1
a1577 1
        LDR     r5, =CMA_L2PT+CMA_CheckL2PT ; L2PT contains gaps due to logical indexing
d1582 1
a1582 1
        LDR     r5, =CMA_L1PT
@


4.9
log
@Reimplement AMBControl ontop of the PMP system
Detail:
  With this set of changes, each AMB node is now the owner of a fake DANode which is linked to a PMP.
  From a user's perspective the behaviour of AMBControl is the same as before, but rewriting it to use PMPs internally offers the following (potential) benefits:
  * Reduction in the amount of code which messes with the CAM & page tables, simplifying future work/maintenance. Some of the AMB ops (grow, shrink) now just call through to OS_ChangeDynamicArea. However all of the old AMB routines were well-optimised, so to avoid a big performance hit for common operations not all of them have been removed (e.g. mapslot / mapsome). Maybe one day these optimal routines will be made available for use by regular PMP DAs.
  * Removal of the slow Service_MemoryMoved / Service_PagesSafe handlers that had to do page list fixup after the core kernel had reclaimed/moved pages. Since everything is a PMP, the kernel will now deal with this on behalf of AMB.
  * Removal of a couple of other slow code paths (e.g. Do_AMB_MakeUnsparse calls from OS_ChangeDynamicArea)
  * Potential for more flexible mapping of application space in future, e.g. sparse allocation of memory to the wimp slot
  * Simpler transition to an ASID-based task swapping scheme on ARMv6+?
  Other changes of note:
  * AMB_LazyMapIn switch has been fixed up to work correctly (i.e. turning it off now disables lazy task swapping and all associated code instead of producing a build error)
  * The DANode for the current app should be accessed via the GetAppSpaceDANode macro. This will either return the current AMB DANode, or AppSpaceDANode (if e.g. pre-Wimp). However be aware that AppSpaceDANode retains the legacy behaviour of having a base + size relative to &0, while the AMB DANodes (identifiable via the PMP flag) are sane and have their base + size relative to &8000.
  * Mostly-useless DebugAborts switch removed
  * AMBPhysBin (page number -> phys addr lookup table) removed. Didn't seem to give any tangible performance benefit, and was imposing hidden restrictions on memory usage (all phys RAM fragments in PhysRamTable must be multiple of 512k). And if it really was a good optimisation, surely it should have been applied to all areas of the kernel, not just AMB!
  Other potential future improvements:
  * Turn the fake DANodes into real dynamic areas, reducing the amount of special code needed in some places, but allow the DAs to be hidden from OS_DynamicArea 3 so that apps/users won't get too confused
  * Add a generic abort trapping system to PMPs/DAs (lazy task swapping abort handler is still a special case)
  File changes:
  - s/ARM600, s/VMSAv6, s/ExtraSWIs - Remove DebugAborts
  - s/ArthurSWIs - Remove AMB service call handler dispatch
  - s/ChangeDyn - AMB_LazyMapIn switch fixes. Add alternate internal entry points for some PMP ops to allow the DANode to be specified (used by AMB)
  - s/Exceptions - Remove DebugAborts, AMB_LazyMapIn switch fixes
  - s/Kernel - Define GetAppSpaceDANode macro, AMB_LazyMapIn switch fix
  - s/MemInfo - AMB_LazyMapIn switch fixes
  - s/AMBControl/AMB - Update GETs
  - s/AMBControl/Memory - Remove block size quantisation, AMB_BlockResize (page list blocks are now allocated by PMP code)
  - s/AMBControl/Options - Remove PhysBin definitions, AMBMIRegWords (moved to Workspace file), AMB_LimpidFreePool switch. Add AMB_Debug switch.
  - s/AMBControl/Workspace - Update AMBNode to contain an embedded DANode. Move AMBMIRegWords here from Options file.
  - s/AMBControl/allocate - Fake DA node initialisation
  - s/AMBControl/deallocate - Add debug output
  - s/AMBControl/growp, growshrink, mapslot, mapsome, shrinkp - Rewrite to use PMP ops where possible, add debug output
  - s/AMBControl/main - Remove PhysBin initialisation. Update the enumerate/mjs_info call.
  - s/AMBControl/memmap - Low-level memory mapping routines updated or rewritten as appropriate.
  - s/AMBControl/readinfo - Update to cope with DANode
  - s/AMBControl/service - Remove old service call handlers
  - s/AMBControl/handler - DA handler for responding to PMP calls from OS_ChangeDynamicArea; just calls through to growpages/shrinkpages as appropriate.
Admin:
  Tested on pretty much everything currently supported


Version 5.66. Tagged as 'Kernel-5_66'
@
text
@d295 1
a295 5
        ARMop   MMU_ChangingUncachedEntry,,,r3   ; Clean TLB
        MOV     r0, r4, LSL #10
        MOV     r10, r1
        ADD     r1, r0, #4096
        ARMop   Cache_CleanInvalidateRange,,,r3  ; Clean page from cache
a297 1
        MOV     r1, r10
@


4.8
log
@Add support for shareable pages and additional access privileges
Detail:
  This set of changes:
  * Refactors page table entry encoding/decoding so that it's (mostly) performed via functions in the MMU files (s.ARM600, s.VMSAv6) rather than on an ad-hoc basis as was the case previously
  * Page table entry encoding/decoding performed during ROM init is also handled via the MMU functions, which resolves some cases where the wrong cache policy was in use on ARMv6+
  * Adds basic support for shareable pages - on non-uniprocessor systems all pages will be marked as shareable (however, we are currently lacking ARMops which broadcast cache maintenance operations to other cores, so safe sharing of cacheable regions isn't possible yet)
  * Adds support for the VMSA XN flag and the "privileged ROM" access permission. These are exposed via RISC OS access privileges 4 and above, taking advantage of the fact that 4 bits have always been reserved for AP values but only 4 values were defined
  * Adds OS_Memory 17 and 18 to convert RWX-style access flags to and from RISC OS access privelege numbers; this allows us to make arbitrary changes to the mappings of AP values 4+ between different OS/hardware versions, and allows software to more easily cope with cases where the most precise AP isn't available (e.g. no XN on <=ARMv5)
  * Extends OS_Memory 24 (CheckMemoryAccess) to return executability information
  * Adds exported OSMem header containing definitions for OS_Memory and OS_DynamicArea
  File changes:
  - Makefile - export C and assembler versions of hdr/OSMem
  - Resources/UK/Messages - Add more text for OS_Memory errors
  - hdr/KernelWS - Correct comment regarding DCacheCleanAddress. Allocate workspace for MMU_PPLTrans and MMU_PPLAccess.
  - hdr/OSMem - New file containing exported OS_Memory and OS_DynamicArea constants, and public page flags
  - hdr/Options - Reduce scope of ARM6support to only cover builds which require ARMv3 support
  - s/AMBControl/Workspace - Clarify AMBNode_PPL usage
  - s/AMBControl/growp, mapslot, mapsome, memmap - Use AreaFlags_ instead of AP_
  - s/AMBControl/main, memmap - Use GetPTE instead of generating page table entry manually
  - s/ARM600 - Remove old coments relating to lack of stack. Update BangCam to use GetPTE. Update PPL tables, removing PPLTransL1 (L1 entries are now derived from L2 table on demand) and adding a separate table for ARM6. Implement the ARM600 versions of the Get*PTE ('get page table entry') and Decode*Entry functions
  - s/ARMops - Add Init_PCBTrans function to allow relevant MMU_PPLTrans/MMU_PCBTrans pointers to be set up during the pre-MMU stage of ROM init. Update ARM_Analyse to set up the pointers that are used post MMU init.
  - s/ChangeDyn - Move a bunch of flags to hdr/OSMem. Rename the AP_ dynamic area flags to AreaFlags_ to avoid name clashes and confusion with the page table AP_ values exported by Hdr:MEMM.ARM600/Hdr:MEMM.VMSAv6. Also generate the relevant flags for OS_Memory 24 so that it can refer to the fixed areas by their name instead of hardcoding the permissions.
  - s/GetAll - GET Hdr:OSMem
  - s/HAL - Change initial page table setup to use DA/page flags and GetPTE instead of building page table entries manually. Simplify AllocateL2PT by removing the requirement for the user to supply the access perimssions that will be used for the area; instead for ARM6 we just assume that cacheable memory is the norm and set L1_U for any L1 entry we create here.
  - s/Kernel - Add GetPTE macro (for easier integration of Get*PTE functions) and GenPPLAccess macro (for easy generation of OS_Memory 24 flags)
  - s/MemInfo - Fixup OS_Memory 0 to not fail on seeing non-executable pages. Implement OS_Memory 17 & 18. Tidy up some error generation. Make OS_Memory 13 use GetPTE. Extend OS_Memory 24 to return (non-) executability information, to use the named CMA_ constants generated by s/ChangeDyn, and to use the Decode*Entry functions when it's necessary to decode page table entries.
  - s/NewReset - Use AreaFlags_ instead of AP_
  - s/VMSAv6 - Remove old comments relating to lack of stack. Update BangCam to use GetPTE. Update PPL tables, removing PPLTransL1 (L1 entries are now derived from L2 table on demand) and adding a separate table for shareable pages. Implement the VMSAv6 versions of the Get*PTE and Decode*Entry functions.
Admin:
  Tested on Raspberry Pi 1, Raspberry Pi 3, Iyonix, RPCEmu (ARM6 & ARM7), comparing before and after CAM and page table dumps to check for any unexpected differences


Version 5.55. Tagged as 'Kernel-5_55'
@
text
@d174 1
a174 1
   [ ChocolateAMB
d339 1
a339 1
   [ ChocolateAMB
d370 1
a370 1
   ] ;ChocolateAMB
@


4.7
log
@Delete pre-HAL and 26bit code
Detail:
  This change gets rid of the following switches from the source (picking appropriate code paths for a 32bit HAL build):
  * HAL
  * HAL26
  * HAL32
  * No26bitCode
  * No32bitCode
  * IncludeTestSrc
  * FixR9CorruptionInExtensionSWI
  Various old files have also been removed (POST code, Arc/STB keyboard drivers, etc.)
Admin:
  Identical binary to previous revision for IOMD & Raspberry Pi builds


Version 5.49. Tagged as 'Kernel-5_49'
@
text
@d70 2
a71 2
        B       %BT20                           ; 17 |
        B       %BT20                           ; 18 |
d418 9
a426 3
        EOR     r5, r5, #2_10           ; Check for valid page.
        TST     r5, #3
        BNE     meminfo_returncs
d429 2
a430 3
        EOR     r5, r5, #2_10           ; Check for valid page.
        TST     r5, #3
        BNE     meminfo_returncs
d765 4
a768 4
        ADRL    r0, ErrorBlock_BadParameters
      [ International
        BL      TranslateError
      |
d770 1
a770 2
      ]
        EXIT
d805 3
a807 4
        ADREQL  r0, ErrorBlock_BadParameters
        STREQ   r0, [sp, #0]
        SETV    EQ
        EXIT
d909 1
a909 1
        ADR     r0,RP_error
d914 1
a914 4
RP_error
        DCD     0
        DCB     "No chunk available (OS_Memory 12)",0
        ALIGN
d936 25
a960 28
        Push    "r0-r2,r12,lr"
        MOV     lr, r0
        LDR     r12, =ZeroPage
        ASSERT  XCB_NB = 1:SHL:0
        ASSERT  XCB_NC = 1:SHL:1
        ASSERT  XCB_P = 1:SHL:2
        AND     r0, r0, #&1F00
        MOV     r0, r0, LSR #8
        LDR     r12, [r12, #MMU_PCBTrans]
        EOR     r0, r0, #XCB_NB+XCB_NC ; Invert C+B to match XCBTable
        LDRB    r0, [r12, r0]
        ; Convert from L2 attributes to L1
        ASSERT  L1_C = L2_C
        ASSERT  L1_B = L2_B
        ASSERT  L2_TEXShift < L1_TEXShift
        AND     r12, r0, #L2_TEX
        BIC     r0, r0, #L2_TEX
        ORR     r0, r0, r12, LSL #L1_TEXShift-L2_TEXShift
        ; Deal with other flags
        TST     lr, #1:SHL:16
        ORRNE   r0, r0, #MapInFlag_DoublyMapped
        TST     lr, #1:SHL:17
        ANDNE   lr, lr, #2_1111:SHL:24
        ADRNEL  r12, PPLTransL1
        LDRNE   r12, [r12, lr, LSR #22]
        ORRNE   r0, r0, #MapInFlag_APSpecified
        ORRNE   r0, r0, r12
        BL      RISCOS_MapInIO
d962 1
d964 3
a966 5
        Pull    "r0-r2,r12,pc",NE
        ADR     r0, MIp_error
        SETV
        STR     r0, [sp]
        Pull    "r0-r2,r12,pc"
d968 1
a968 4
MIp_error
        DCD     0
        DCB     "No room for IO space (OS_Memory 13)",0
        ALIGN
d1074 2
a1075 4
        ADRL    r0, ErrorBlock_BadParameters
        SETV
        STR     r0, [sp, #Proc_RegOffset+0]
        EXIT
d1187 148
d1351 2
a1352 1
;               bits 10,11: reserved
d1355 3
a1357 1
;               bits 14+: reserved
d1360 3
a1362 12
CMA_Completely_UserR   * 1<<0
CMA_Completely_UserW   * 1<<1
CMA_Completely_PrivR   * 1<<2
CMA_Completely_PrivW   * 1<<3
CMA_Partially_UserR    * 1<<4
CMA_Partially_UserW    * 1<<5
CMA_Partially_PrivR    * 1<<6
CMA_Partially_PrivW    * 1<<7
CMA_Completely_Phys    * 1<<8
CMA_Completely_Abort   * 1<<9
CMA_Partially_Phys     * 1<<12
CMA_Partially_Abort    * 1<<13
a1367 4
CMA_ROM  * CMA_Partially_UserR+CMA_Partially_PrivR
CMA_Read * CMA_ROM+CMA_Partially_PrivW
CMA_Full * CMA_Read+CMA_Partially_UserW
CMA_None * CMA_Partially_PrivR+CMA_Partially_PrivW
d1399 1
a1399 1
        MOV     r5, #CMA_Read
d1406 1
a1406 1
        MOV     r5, #CMA_Read
d1412 1
a1412 1
        MOV     r5, #CMA_Read
d1423 1
a1423 1
        MOV     r5, #CMA_Full
d1463 3
a1465 7
        ANDS    lr, r6, #3
        MOVEQ   r5, #CMA_Full
        TEQ     lr, #1
        MOVEQ   r5, #CMA_Read
        CMP     lr, #2
        MOVEQ   r5, #CMA_None
        MOVGT   r5, #CMA_ROM
d1491 7
a1497 10
        LDR     r4, [r7, r3, LSR #20-2]
        AND     r4, r4, #L1_AP
        ; Decode page AP flags
        MOV     r5, #CMA_ROM
        CMP     r4, #AP_None*L1_APMult
        MOVEQ   r5, #CMA_None
        CMP     r4, #AP_Read*L1_APMult
        MOVEQ   r5, #CMA_Read
        CMP     r4, #AP_Full*L1_APMult
        MOVEQ   r5, #CMA_Full
d1508 1
a1508 1
        MOV     r5, #CMA_Read
d1513 1
a1513 1
        MOV     r5, #CMA_None
d1518 1
a1518 1
        MOV     r5, #CMA_Read
d1523 1
a1523 1
        MOV     r5, #CMA_None
d1528 1
a1528 1
        MOV     r5, #CMA_None
d1537 6
a1542 1
        LDR     r5, =CMA_None+CMA_Partially_Phys ; Assume IO memory mapped there
d1559 2
a1560 1
        LDR     r5, =CMA_None+CMA_Partially_Phys ; Mark as IO, it may not be actual memory there
d1566 1
a1566 1
        MOV     r5, #CMA_Read
d1575 1
a1575 1
        MOV     r5, #CMA_None
a1579 1
        MOV     r5, #CMA_None
d1581 1
a1581 1
        ORR     r5, r5, #CMA_CheckL2PT ; L2PT contains gaps due to logical indexing
d1586 1
a1586 1
        MOV     r5, #CMA_None
d1591 1
a1591 1
        MOV     r5, #CMA_Read
d1596 1
a1596 1
        MOV     r5, #CMA_None
d1601 1
a1601 1
        MOV     r5, #CMA_ROM
d1609 1
a1609 1
        MOV     r5, #CMA_Read
d1616 1
a1616 2
        CLRV
        EXIT
d1620 1
a1620 3
        ADRL    r0, ErrorBlock_BadParameters
        SETV
        MOV     pc, lr
d1654 12
d1711 10
a1720 13
        LDR     r9, [r8, r4, LSR #10]
      [ MEMM_Type = "VMSAv6"
        AND     r9, r9, #L2_AP
      |
        AND     r9, r9, #L2X_AP
      ]
        MOV     r5, #CMA_ROM
        CMP     r9, #AP_None*L2X_APMult
        MOVEQ   r5, #CMA_None
        CMP     r9, #AP_Read*L2X_APMult
        MOVEQ   r5, #CMA_Read
        CMP     r9, #AP_Full*L2X_APMult
        MOVEQ   r5, #CMA_Full
@


4.6
log
@Merge HAL branch to trunk
Detail:
  This change merges the past 15+ years of HAL branch development back to the trunk.
  This is effectively the end for non-HAL builds of the kernel, as no attempt has been made to maintain it during this merge, and all non-HAL & non-32bit code will soon be removed anyway.
  Rather than list everything that's been added to the HAL branch, it's easier to describe the change in terms of the things that the HAL branch was lacking:
  * Trunk version of Docs/32bit contained updated comments for the SVC stack structure during ErrorV
  * Trunk version of s/HeapMan contained a tweak to try and reduce the number of small free blocks that are created
  * Trunk version of s/Kernel contained a change to only copy 248 bytes of the error string to the error buffer (down from 252 bytes), to take into account the extra 4 bytes needed by the PSR. However this goes against the decision that's been made in the HAL branch that the error buffer should be enlarged to 260 bytes instead (ref: https://www.riscosopen.org/tracker/tickets/201), so the HAL build will retain its current behaviour.
  * Trunk version of s/MsgCode had RMNot32bit error in the list of error messages to count when countmsgusage {TRUE}
  * Trunk version of s/PMF/i2cutils contained support for OS_Memory 5, "read/write value of NVRamWriteSize". Currently the HAL branch doesn't have a use for this (in particular, the correct NVRamWriteSize should be specified by the HAL, so there should be no need for software to change it at runtime), and so this code will remain switched out in the HAL build.
Admin:
  Tested on Raspberry Pi


Version 5.48. Tagged as 'Kernel-5_48'
@
text
@a388 1
 [ No26bitCode
a389 3
 |
        ORRCCS  pc, lr, #C_bit          ; Invalid so return C set.
 ]
a396 1
 [ No26bitCode
a398 3
 |
        BICS    pc, lr, #C_bit          ; Return C clear.
 ]
a419 1
 [ No26bitCode
a420 3
 |
        ORRNES  pc, lr, #C_bit
 ]
a424 1
 [ No26bitCode
a425 3
 |
        ORRNES  pc, lr, #C_bit
 ]
a430 1
 [ No26bitCode
a432 3
 |
        BICS    pc, lr, #C_bit
 ]
a530 1
 [ HAL
a538 5
 |
        MOV     r1, #PhysSpaceSize :SHR: ByteShift
        MOV     r2, #4*1024
        MOV     pc, lr
 ]
a697 1
 [ HAL
a766 47
 |
        BICS    lr, r0, #&FF            ; Get type of memory required (leave bits 12-31, non-zero => error).
        BEQ     %FT30                   ; Don't understand 0 (so the spec says).

        TEQ     lr, #5:SHL:8            ; Check for soft ROM
        BNE     %FT10
        LDR     r1, =L1PT
        MOV     r2, #ROM
        ADD     r1, r1, r2, LSR #(20-2)   ; L1PT address for ROM
        LDR     r2, [r1]
        MOV     r2, r2, LSR #12
        TEQ     r2, #PhysROM :SHR: 12     ; see if we have hard or soft ROM
        MOVEQ   r1, #0                    ; no soft ROM
        MOVNE   r1, #OSROM_ImageSize*1024 ; this much soft ROM
        B       %FT20

10
        TEQ     lr, #4:SHL:8            ; Check for IO space.
        LDREQ   r1, =136*1024*1024      ; Just return 136M (includes VIDC and EASI space).
        BEQ     %FT20

        TEQ     lr, #3:SHL:8            ; Check for ROM.
        LDREQ   r1, =OSROM_ImageSize*1024
        BEQ     %FT20

        TEQ     lr, #2:SHL:8            ; Check for VRAM.
        MOVEQ   r1, #0                  ; Return amount of VRAM.
        LDREQ   r1, [r1, #VRAMSize]
        BEQ     %FT20

        TEQ     lr, #1:SHL:8            ; Check for DRAM.
        BNE     %FT30
        MOV     r1, #0
        LDR     lr, [r1, #RAMLIMIT]
        LDR     r1, [r1, #VRAMSize]
        SUB     r1, lr, r1              ; Return amount of RAM - amount of VRAM.

20
        MOV     r1, r1, LSR #12         ; Return as number of pages.
        MOV     r2, #4*1024             ; Return page size.
        EXIT

30
        ADRL    r0, ErrorBlock_BadParameters
        SETV
        EXIT
 ]
a794 1
 [ HAL
a804 43
 |
MemoryIOSpace   ROUT
        Entry   "r2"

        AND     r2, r1, #&FF            ; Get sequence number.
        MOV     r1, r1, LSR #8          ; Get controller type.
        CMP     r1, #6                  ; Make sure it's in range.
        ADRCSL  r0, ErrorBlock_BadParameters
        SETV    CS
        EXIT    CS

        ADR     lr, controller_types
        LDR     r1, [lr, r1, LSL #2]    ; Get base address or offset to table.
        TEQ     r1, #0                  ; If not present or
        CMPNE   r1, #1024               ;    not offset to table then
        EXIT    GE                      ;    return.

        ADD     lr, lr, r1              ; Point to table indexed by sequence number.
        LDR     r1, [lr], #4            ; Get sequence number limit.
        CMP     r2, r1                  ; Make sure it's in range.
        BCS     %FT20
        LDR     r1, [lr, r2, LSL #2]    ; Get base address.
        EXIT

controller_types
        DCD     IOMD_Base + IOMD_ECTCR  ; Expansion card timing control.
        DCD     easi_space_table - controller_types
        DCD     0
        DCD     VIDC
        DCD     IOMD_Base
        DCD     0

easi_space_table
        DCD     8                       ; Maximum of 8 expansion cards.
        DCD     PhysSpace + IOMD_EASI_Base0
        DCD     PhysSpace + IOMD_EASI_Base1
        DCD     PhysSpace + IOMD_EASI_Base2
        DCD     PhysSpace + IOMD_EASI_Base3
        DCD     PhysSpace + IOMD_EASI_Base4
        DCD     PhysSpace + IOMD_EASI_Base5
        DCD     PhysSpace + IOMD_EASI_Base6
        DCD     PhysSpace + IOMD_EASI_Base7
 ] ; HAL
a1106 1
 [ No26bitCode
a1109 1
 ]
a1138 1
 [ HAL
a1142 1
 ]
a1145 1
 [ HAL
a1152 1
 ]
@


4.5
log
@  Imported OS_Memory 12 from Ursula.
  Imported EarlierReentrancyinDAShrink fix from Ursula.
Detail:
  OS_Memory 12 allows the kernel to suggest physical pages to callers
    who need that sort of information without wanting to have them
    grub around in the page tables.
  EarlierReentrancyInDAShrink should allow RAMFS and FileCore to delete
    their RAMFS related dynamic areas when the Switcher is used to drag
    the RAM disc bar to zero.
Admin:
  Required by Interlace 0.63 or later.

Version 5.36. Tagged as 'Kernel-5_36'
@
text
@d53 1
a53 1
        B       MemoryConvert
d59 5
a63 5
        B       MemoryPhysSize
        B       MemoryReadPhys
        B       MemoryAmounts
        B       MemoryIOSpace
        B       %BT20                           ; Reason code 10 reserved (for MemoryFreePoolLock).
d65 14
a78 1
        B       RecommendPage
a121 1
flush_cache     *       1:SHL:31        ; Internal flag.
d123 26
a148 2
MemoryConvert   ROUT
        ENTRY   "r0-r11"                ; Need lots of registers!!
d164 1
a164 1
        MOV     r6, #0
a167 2

        BIC     r0, r0, #flush_cache    ; Bit set if any page is really made uncacheable (we must flush the cache).
d174 4
d180 6
a185 3
        TST     r0, #logical,given      ; If LA not given (rotate clears C) then
        BLEQ    ppn_to_logical          ;   get it from PN (PA wanted (not given) & LA not given => PN given).
        BLCC    logical_to_physical     ; Get PA from LA.
d220 3
a222 1
        ADD     r3, r6, r3, LSL #3      ; Point to CAM entry for this page.
d240 1
a240 1
        STR     r5, [r3, #4]            ; Write back new PPL.
d244 1
d246 16
a264 1
        ORREQ   r0, r0, #flush_cache    ; If disable then we must flush cache later.
d266 41
d308 7
a314 2

        B       %BT10                   ; Do next entry.
d317 1
a317 3
        TST     r0, #flush_cache        ; If any page has been made uncacheable in L2 then flush!
        BLNE    meminfo_flushplease
75
d331 1
a331 1
        BLNE    MemoryConvert
d335 1
a335 1
        STR     r0, [sp]
d339 33
d395 2
a396 1
        LDR     r4, [r6, r3, LSL #3]    ; If valid then lookup logical address.
d454 2
d473 2
a474 1
        MOV     r9, #PhysRamTable
d476 1
d479 1
a479 5
 [ No26bitCode
        BCC     meminfo_returncs
 |
        ORRCCS  pc, lr, #C_bit          ;   (return with C set).
 ]
d482 2
a483 2
        SUB     r10, r5, r10            ; Determine if given address is in this block.
        CMP     r10, r11
d487 3
a489 2
        ADD     r3, r3, r10, LSR #12
 [ No26bitCode
d492 28
a519 3
 |
        BICS    pc, lr, #C_bit          ; Return with C clear.
 ]
d551 10
d564 1
a577 1
        ENTRY   "r1-r10"
d579 13
a591 41
        ; &00000000 to OSROM_ImageSize*1024 is ROM.
        MOV     r2, #(OSROM_ImageSize*1024-&00000000) :SHR: WordShift
        LDR     r3, =ROM_Pattern :OR: NotAvailable
        BL      fill_words

        ; OSROM_ImageSize*1024 to &02000000 is allocated to ROM but is not present.
        MOV     r2, #(&02000000-OSROM_ImageSize*1024) :SHR: WordShift
        LDR     r3, =NotPresent :OR: NotAvailable
        BL      fill_words

        ; &02000000 to &02400000 is VRAM or not present.
        MOV     r4, #0
        LDR     r4, [r4, #VRAMSize]     ; Get amount of VRAM (in bytes).
        TEQ     r4, #0
        MOVNE   r2, r4, LSR #WordShift  ; If there is some then fill part of table.
        LDRNE   r3, =VRAM_Pattern :OR: NotAvailable
        BLNE    fill_words

        ; End of VRAM to &03000000 is not present.
        RSB     r4, r4, #&03000000-&02000000
        MOV     r2, r4, LSR #WordShift
        LDR     r3, =NotPresent :OR: NotAvailable
        BL      fill_words

        ; &03000000 to &03800000 is I/O.
        MOV     r2, #(&03800000-&03000000) :SHR: WordShift
        LDR     r3, =IO_Pattern :OR: NotAvailable
        BL      fill_words

        ; &03800000 to &08000000 is not present.
        MOV     r2, #(&08000000-&03800000) :SHR: WordShift
        LDR     r3, =NotPresent :OR: NotAvailable
        BL      fill_words

        ; &08000000 to &10000000 is I/O (EASI space).
        MOV     r2, #(&10000000-&08000000) :SHR: WordShift
        LDR     r3, =IO_Pattern :OR: NotAvailable
        BL      fill_words

        ; &10000000 to &20000000 is DRAM or not present.
        MOV     r2, #&10000000          ; Current physical address.
d594 11
d606 5
a610 5
        MOV     r6, #PhysRamTable
        LDR     r7, [r3, #CamEntriesPointer]
        ADD     r7, r7, #4              ; Point to PPL entries.
        LDR     r8, [r3, #MaxCamEntry]
10
d612 2
d615 22
a636 5
        CMP     r9, #&10000000          ; If not DRAM then
        ADDCC   r5, r5, r10, LSR #12    ;   adjust current page number
        BCC     %BT10                   ;   and try next block.

        ADD     r10, r10, r9            ; Add amount of unused space between current and start of block.
d648 1
a648 1
        LDR     lr, [r7, r5, LSL #3]    ; Page is there so get PPL and determine if it's available or not.
d658 1
a658 2
        CMP     r8, r5                  ; Stop if we run out of pages.
        BCS     %BT10
d660 1
d669 22
a690 5
        ; End of last block of DRAM to &20000000 is not present.
        RSBS    r2, r2, #&20000000
        MOVNE   r2, r2, LSR #WordShift
        LDRNE   r3, =NotPresent :OR: NotAvailable
        BLNE    fill_words
d692 1
d722 1
a722 1
        ENTRY   "r3"
d724 71
d840 1
d852 1
a852 1
;                               1 = EASI space
d855 8
d868 13
d882 1
a882 1
        ENTRY   "r2"
d886 4
a889 2
        CMP     r1, #4                  ; Make sure it's in range.
        BCS     %FT20
d894 2
a895 2
        CMPNE   r1, #1024               ;     not offset to table then
        EXIT    GE                      ;   return.
a903 6
20
        ADRL    r0, ErrorBlock_BadParameters
        SETV
        EXIT


a904 1
 [ IO_Type = "IOMD"
a906 5
 |
        DCD     0
        DCD     0
 ]
 [ VIDC_Type = "VIDC20"
d909 1
a909 2
 |
        DCD     VIDC
a910 1
 ]
a911 1
 [ IO_Type = "IOMD"
d922 1
a922 1
 ]
d925 1
a925 3
; MemoryFreePoolLock
;
; See code on Ursula branch
d937 2
a938 1
;               r0 bits 8..31 = 0 (reserved flags)
d948 1
a948 1
        Push    "r0-r2,r4-r10,lr"
d950 3
a952 1
        BHI     RP_failed            ;refuse to look for alignments above 1G
d964 1
a964 1
        MOV     r0,#PhysRamTable
d966 1
a966 1
        LDR     r5,=CamEntriesPointer
d968 1
a968 1
        ADD     r5,r5,#4         ; [r5,<page no.>,LSL #3] addresses flags word in CAM
d976 2
d979 1
d981 1
d995 1
a995 1
        LDR     r6,[r5,r7,LSL #3]  ;page flags from CAM
d1010 1
a1010 1
        LDR     r6,[r5,r10,LSL #3] ;page flags from CAM
d1020 1
a1020 1
        Pull    "r0-r2,r4-r10,pc"
d1027 1
a1027 1
        Pull    "r0-r2,r4-r10,pc"
d1034 690
@


4.4
log
@  32-bit Kernel.

Details:
  The Kernel will now compile to produce a pure 32-bit system if No26bitCode is
  set to TRUE.
  If No26bitCode is FALSE, then the Kernel will be a standard 26-bit Kernel,
  although some internal changes have taken place to minimise compile
  switches between the two cases. See Docs.32bit for more technical info.

  The hardest part was the flood-fill...

Other changes:
  Pointer shape changes now take place on the next VSync, rather than actually
  WAITING for the VSync. Turning the Hourglass on shouldn't slow your machine
  down by 5% now :)

  Lots of really crusty pre-IOMD code removed.

Admin:
  Tested in 32 and 26-bit forms in a limited desktop build. Basically, this
  will need to see a lot of use to iron out difficulties. I'd like anyone who
  has a non-frozen project to at least attempt using this Kernel.

Version 5.23. Tagged as 'Kernel-5_23'
@
text
@d63 3
d416 1
a416 1
        ; &02000000 to &02200000 is VRAM or not present.
d516 1
a516 1
;                       8-11    1=return amount of DRAM
d520 1
d534 13
d650 105
@


4.4.2.1
log
@* Converted to building with ObjAsm (but still a single object file using ORG).
* Added ARM_IMB and ARM_IMBRange SWIs as recommended by ARMv5.
* Some early prototype HAL bits popped in - a lot of source restructuring still
  to come.
* New debug target creates an AIF image with debug information, and translates
  this into an ASCII object file for the 16702B logic analyser.

Version 5.35, 4.79.2.1. Tagged as 'Kernel-5_35-4_79_2_1'
@
text
@d109 1
a109 1
        Entry   "r0-r11"                ; Need lots of registers!!
d401 1
a401 1
        Entry   "r1-r10"
a402 3
 [ HAL
        ! 0, "Sort out MemoryReadPhys"
 |
a495 1
  ]
d525 1
a525 1
        Entry   "r3"
a526 3
 [ HAL
        ! 0, "Sort out MemoryAmounts"
 |
a557 1
 ]
d579 1
a579 1
        Entry   "r2"
@


4.4.2.2
log
@Wahey! This version gives you a display.

It says "Abort on data transfer".
@
text
@a611 3
 [ HAL
   ! 0, "Sort out OS_Memory 9"
 ]
a613 3
 [ HAL
        DCD     0,0,0,0
 |
a639 1
 ]
@


4.4.2.3
log
@More L7200 HAL work
@
text
@a585 9

 [ HAL
MemoryIOSpace   ROUT
        Entry   "r0,r2,r3,sb,ip"
        AddressHAL
        CallHAL HAL_ControllerAddress
        MOV     r1, r0
        EXIT
 |
d612 4
d617 3
d647 1
a647 1
 ] ; HAL
@


4.4.2.4
log
@Added OS_Memory 12 and EarlierReentrancyInDAShrink
Imported from Kernel 5.36.
@
text
@a62 3
        B       %BT20                           ; Reason code 10 reserved (for MemoryFreePoolLock).
        B       %BT20                           ; Reason code 11 reserved (for PCImapping).
        B       RecommendPage
d416 1
a416 1
        ; &02000000 to &02400000 is VRAM or not present.
d517 1
a517 1
;                       8-11    1=return amount of DRAM (excludes any soft ROM)
a520 1
;                               5=return amount of soft ROM (ROM loaded into hidden DRAM)
a536 13
        TEQ     lr, #5:SHL:8            ; Check for soft ROM
        BNE     %FT10
        LDR     r1, =L1PT
        MOV     r2, #ROM
        ADD     r1, r1, r2, LSR #(20-2)   ; L1PT address for ROM
        LDR     r2, [r1]
        MOV     r2, r2, LSR #12
        TEQ     r2, #PhysROM :SHR: 12     ; see if we have hard or soft ROM
        MOVEQ   r1, #0                    ; no soft ROM
        MOVNE   r1, #OSROM_ImageSize*1024 ; this much soft ROM
        B       %FT20

10
a649 105

;----------------------------------------------------------------------------------------
; MemoryFreePoolLock
;
; See code on Ursula branch

;----------------------------------------------------------------------------------------
;PCImapping - reserved for Acorn use (PCI manager)
;
; See code on Ursula branch


;----------------------------------------------------------------------------------------
;RecommendPage
;
;       In:     r0 bits 0..7  = 12 (reason code 12)
;               r0 bits 8..31 = 0 (reserved flags)
;               r1 = size of physically contiguous RAM region required (bytes)
;               r2 = log2 of required alignment of base of region (eg. 12 = 4k, 20 = 1M)
;
;       Out:    r3 = page number of first page of recommended region that could be
;                    grown as specific pages by dynamic area handler (only guaranteed
;                    if grow is next page claiming operation)
;        - or error if not possible (eg too big, pages unavailable)
;
RecommendPage ROUT
        Push    "r0-r2,r4-r10,lr"
        CMP     r2,#30
        BHI     RP_failed            ;refuse to look for alignments above 1G
;
        ADD     r1,r1,#&1000
        SUB     r1,r1,#1
        MOV     r1,r1,LSR #12
        MOVS    r1,r1,LSL #12     ;size rounded up to whole no. of pages
;
        CMP     r2,#12
        MOVLO   r2,#12            ;log2 alignment must be at least 12 (4k pages)
        MOV     r0,#1
        MOV     r4,r0,LSL r2      ;required alignment-1
;
        MOV     r0,#PhysRamTable
        MOV     r3,#0            ;page number, starts at 0
        LDR     r5,=CamEntriesPointer
        LDR     r5,[r5]
        ADD     r5,r5,#4         ; [r5,<page no.>,LSL #3] addresses flags word in CAM
        LDMIA   r0!,{r7,r8}      ;address,size of video chunk (skip this one)
;
RP_nextchunk
        ADD     r3,r3,r8,LSR #12 ;page no. of first page of next chunk
        LDMIA   r0!,{r7,r8}      ;address,size of next physical chunk
        CMP     r8,#0
        BEQ     RP_failed
;
        ADD     r6,r7,r4
        SUB     r6,r6,#1         ;round up
        MOV     r6,r6,LSR r2
        MOV     r6,r6,LSL r2
        SUB     r6,r6,r7         ;adjustment to first address of acceptable alignment
        CMP     r6,r8
        BHS     RP_nextchunk     ;negligible chunk
        ADD     r7,r3,r6,LSR #12 ;first page number of acceptable alignment
        SUB     r9,r8,r6         ;remaining size of chunk
;
;find first available page
RP_nextpage
        CMP     r9,r1
        BLO     RP_nextchunk
        LDR     r6,[r5,r7,LSL #3]  ;page flags from CAM
        ;must not be marked Unavailable or Required
        TST     r6,#PageFlags_Unavailable :OR: PageFlags_Required
        BEQ     RP_checkotherpages
RP_nextpagecontinue
        CMP     r9,r4
        BLS     RP_nextchunk
        ADD     r7,r7,r4,LSR #12   ;next page of suitable alignment
        SUB     r9,r9,r4
        B       RP_nextpage
;
RP_checkotherpages
        ADD     r10,r7,r1,LSR #12
        SUB     r10,r10,#1         ;last page required
RP_checkotherpagesloop
        LDR     r6,[r5,r10,LSL #3] ;page flags from CAM
        TST     r6,#PageFlags_Unavailable :OR: PageFlags_Required
        BNE     RP_nextpagecontinue
        SUB     r10,r10,#1
        CMP     r10,r7
        BHI     RP_checkotherpagesloop
;
;success!
;
        MOV     r3,r7
        Pull    "r0-r2,r4-r10,pc"

RP_failed
        MOV     r3,#0
        ADR     r0,RP_error
        SETV
        STR     r0,[sp]
        Pull    "r0-r2,r4-r10,pc"

RP_error
        DCD     0
        DCB     "No chunk available (OS_Memory 12)",0
        ALIGN
@


4.4.2.5
log
@First attempt at ARM9 support, and general clean-up of old ARM-specific
code, now using vectored ARMops.
Not tested.

Version 5.35, 4.79.2.14. Tagged as 'Kernel-5_35-4_79_2_14'
@
text
@d213 1
a213 3
        BEQ     %FT75
        MOV     r0, #0
        ARMop   Cache_CleanInvalidateAll,,,r0
@


4.4.2.6
log
@spectacular new OS_Memory reason codes
13 map permanent I/O space, return logical address
14 access temporary physical mapping
15 release temporary physical mapping
DA creation and I/O space creation now avoid collision if address
space fills

Version 5.35, 4.79.2.28. Tagged as 'Kernel-5_35-4_79_2_28'
@
text
@d53 1
a53 1
        B       MemoryConvert                   ; 0
d59 4
a62 4
        B       MemoryPhysSize                  ; 6
        B       MemoryReadPhys                  ; 7
        B       MemoryAmounts                   ; 8
        B       MemoryIOSpace                   ; 9
d65 1
a65 4
        B       RecommendPage                   ; 12
        B       MapIOpermanent                  ; 13
        B       AccessPhysAddr                  ; 14
        B       ReleasePhysAddr                 ; 15
a773 78

;----------------------------------------------------------------------------------------
;MapIOpermanent - map IO space (if not already mapped) and return logical address
;
;       In:     r0 bits 0..7  = 13 (reason code 13)
;               r0 bit  8     = 1 to map bufferable space (0 is normal, non-bufferable) 
;               r0 bits 9..31 = 0 (reserved flags)
;               r1 = physical address of base of IO space required
;               r2 = size of IO space required (bytes)
;
;       Out:    r3 = logical address of base of IO space
;        - or error if not possible (no room)
;
MapIOpermanent ROUT
        Push    "r0-r2,r12,lr"
        TST     r0, #&100           ;test bufferable bit
        MOVNE   r0, #L1_B
        MOVEQ   r0, #0
        BL      RISCOS_MapInIO
        MOV     r3, r0
        CMP     r3, #0              ;MOV,CMP rather than MOVS to be sure to clear V
        Pull    "r0-r2,r12,pc",NE
        ADR     r0, MIp_error
        SETV
        STR     r0, [sp]
        Pull    "r0-r2,r12,pc"

MIp_error
        DCD     0
        DCB     "No room for IO space (OS_Memory 13)",0
        ALIGN

;----------------------------------------------------------------------------------------
;AccessPhysAddr - claim temporary access to given physical address (in fact,
;                 controls access to the 1Mb aligned space containing the address)
;                 The access remains until the next AccessPhysAddr or until a
;                 ReleasePhysAddr (although interrupts or subroutines may temporarily
;                 make their own claims, but restore on Release before returning)
;
;       In:     r0 bits 0..7  = 14 (reason code 14)
;               r0 bit  8     = 1 to map bufferable space, 0 for unbufferable
;               r0 bits 9..31 = 0 (reserved flags)
;               r1 = physical address
;
;       Out:    r2 = logical address corresponding to phys address r1
;               r3 = old state (for ReleasePhysAddr)
;
; Use of multiple accesses: it is fine to make several Access calls, and
; clean up with a single Release at the end. In this case, it is the old state
; (r3) of the *first* Access call that should be passed to Release in order to
; restore the state before any of your accesses. (The r3 values of the other
; access calls can be ignored.)
;
AccessPhysAddr ROUT
        Push    "r0,r1,r12,lr"
        TST     r0, #&100           ;test bufferable bit
        MOVNE   r0, #L1_B
        MOVEQ   r0, #0
        SUB     sp, sp, #4          ; word for old state
        MOV     r2, sp              ; pointer to word
        BL      RISCOS_AccessPhysicalAddress
        MOV     r2, r0
        Pull    r3                  ; old state
        Pull    "r0,r1,r12,pc"

;----------------------------------------------------------------------------------------
;ReleasePhysAddr - release temporary access that was claimed by AccessPhysAddr
;
;       In:     r0 bits 0..7  = 15 (reason code 15)
;               r0 bits 8..31 = 0 (reserved flags)
;               r1 = old state to restore
;
ReleasePhysAddr
        Push    "r0-r3,r12,lr"
        MOV     r0, r1
        BL      RISCOS_ReleasePhysicalAddress
        Pull    "r0-r3,r12,pc"

@


4.4.2.7
log
@* Fixed the IIC code.
* Kernel puts sensible default FIQ handler in through the HAL.
* Fix to temporary page uncaching code.

Version 5.35, 4.79.2.30. Tagged as 'Kernel-5_35-4_79_2_30'
@
text
@a111 1
flush_tlb       *       1:SHL:30        ; Internal flag
a112 1
flush_all       *       flush_tlb :OR: flush_cache
a116 5
;        MRS     lr, CPSR
;        Push    "lr"
;        ORR     lr, lr, #I32_bit+F32_bit
;        MSR     CPSR_c, lr

d136 1
a136 1
        BIC     r0, r0, #flush_all      ; Bit set if any page is really made uncacheable (we must flush the cache).
a205 1
        ORR     r0, r0, #flush_tlb
d215 2
a216 10
        ; *** KJB - this assumes that uncacheable pages still allow cache hits (true on all
        ; ARMs so far).
        ; *** Also, fix it up to do it page by page.
        TST     r0, #flush_tlb
        EXIT    EQ                      ; If not flush_tlb, can't have flush_cache
        MOV     r1, r0
        MOV     r0, #0
        ARMop   TLB_InvalidateAll,,,r0
        TST     r1, #flush_cache        ; If any page has been made uncacheable in L2 then flush!
        EXIT    EQ
d219 1
a219 3
;75
;        Pull    "lr"
;        MSR     CPSR_c, lr
@


4.4.2.8
log
@Reimplement Lazy task swapping, an amusing idea from Ursula,
would have done it sooner but couldn't be bothered (humour).
Currently activates for all ARMs flagged as base-restored
abort model. No handling of eg. StrongARM pre-revT bug, but
then the kernel no longer runs on StrongARM (progress).
Still some details to fix: all aborts in current app space
assumed to be missing pages, but this must be fixed to
handle abort code in app space, things like debuggers
marking code read only.

Plus, small fixes:
  OS_Memory 8 returns vaguely useful info for RAM,VRAM
  in HAL build (temporary partial implementation)
  Broken handling of old BBC commands with (fx,tv etc)
  with no spaces fixed (fudgeulike code from Ursula,
  now 32-bit).

Version 5.35, 4.79.2.31. Tagged as 'Kernel-5_35-4_79_2_31'
@
text
@a559 23
        ;
        ;temp bodge for DRAM,VRAM
        ;
        BICS    lr, r0, #&FF            ; Get type of memory required (leave bits 12-31, non-zero => error).
        BEQ     %FT30                   ; Don't understand 0 (so the spec says).
        TEQ     lr, #1:SHL:8
        BNE     %FT10
        MOV     r1, #0
        LDR     r1, [r1, #RAMLIMIT]     ; assume 0 VRAM
        B       %FT20
10
        TEQ     lr, #2:SHL:8
        BNE     %FT40                   ; not implementing anything else in this bodge
        MOV     r1, #0                  ; assume 0 VRAM
20
        MOV     r1, r1, LSR #12         ; Return as number of pages.
        MOV     r2, #4*1024             ; Return page size.
        EXIT
30
        ADRL    r0, ErrorBlock_BadParameters
        SETV
40
        EXIT
d605 1
a606 1
 ]
@


4.4.2.9
log
@Further work on Lazy task swapping:
  hooks to give correct mapping info for OS_Memory 0
  same for OS_ReadMemMapEntries
  same for OS_FindMemMapEntries
  Lazy fixup routine no longer assumes an abort in current
  app space must be a truant page. However, work in this
  area not complete (no support yet for abort handler code
  in app space itself, eg. for C trampoline)
Good to know this will be a big performance boost when
our products use one monolithic application (sarcasm).
Ta

Version 5.35, 4.79.2.32. Tagged as 'Kernel-5_35-4_79_2_32'
@
text
@a149 4
   [ ChocolateAMB
        BL      handle_AMBHonesty       ; may need to make page honest (as if not lazily mapped)
   ]

a257 33

   [ ChocolateAMB
;
;  entry: r3,r4,r5 = provided PN,LA,PA triple for entry to make honest (at least one given)
;         r0 bits flag which of PN,LA,PA are given
;  exit:  mapping made honest (as if not lazily mapped) if necessary
handle_AMBHonesty  ROUT
        Push    "r0, r3-r5, lr"
        TST     r0, #logical,given
        BEQ     %FT10
        MOV     r0, r4
        BL      AMB_MakeHonestLA
        B       %FT90
10
        TST     r0, #ppn,given
        BEQ     %FT20
15
        MOV     r0, r3
        BL      AMB_MakeHonestPN
        B       %FT90
20
        TST     r0, #physical,given
        BEQ     %FT90
        Push    "r7, r9-r11"
        MOV     r14, #0
        LDR     r7, [r14, #MaxCamEntry]
        BL      physical_to_ppn
        Pull    "r7, r9-r11"
        BCC     %BT15
90
        Pull    "r0, r3-r5, pc"

   ] ;ChocolateAMB
@


4.4.2.10
log
@Reimplement enhancements to kernel Dynamic Area support from
Ursula. Quite a hairy code merge really, so let's hope it is
worth it to someone. What you get (back after 2 or 3 years):
- much more efficient for largish numbers of DAs (relevance
  to current build = approx 0)
- fancy reason codes to support fast update of
  Switcher bar display (relevance = 0)
- support for clamped maximum area sizes, to avoid address
  space exhaustion with big memory (relevance = 0)
- better implementation of shrinkable DAs, performance
  wise (if lots of DAs, relevance = approx 0)
- support for 'Sparse' DAs. Holey dynamic areas, Batman!
  (relevance, go on someone use the darned things)
Moderately development tested on HAL/32bit ARM9 desktop.
Note the Switcher should be compiled to use the new
reason codes 6&7, for fabled desktop builds.

Also, during this work, so I could see the wood for the
trees, redid some source code clean up, removing pre-Medusa
stuff (like I did about 3 years ago on Ursula, sigh). That's
why loads of source files have changed. The new DA stuff
is confined pretty much to hdr.KernelWS and s.ChangeDyn.

Ta.

Version 5.35, 4.79.2.38. Tagged as 'Kernel-5_35-4_79_2_38'
@
text
@d722 1
d725 5
d732 4
d737 1
d748 1
@


4.4.2.11
log
@Lots of Tungsten work.

Version 5.35, 4.79.2.48. Tagged as 'Kernel-5_35-4_79_2_48'
@
text
@d602 1
a602 6

        TEQ     lr, #5:SHL:8            ; Check for soft ROM
        MOV     r1, #OSROM_ImageSize*1024 ; this much soft ROM
        B       %FT20

05      TEQ     lr, #1:SHL:8
d678 1
a678 1
;                               1 = EASI space(s)
a680 2
;                               4 = S space (IOMD,podules,NICs,blah blah)
;                               5 = Extension ROM(s)
d692 1
a692 5
        CMP     r0, #-1
        MOVNE   r1, r0
        ADREQL  r0, ErrorBlock_BadParameters
        STREQ   r0, [sp, #0]
        SETV    EQ
d700 2
a701 4
        CMP     r1, #6                  ; Make sure it's in range.
        ADRCSL  r0, ErrorBlock_BadParameters
        SETV    CS
        EXIT    CS
d706 2
a707 2
        CMPNE   r1, #1024               ;    not offset to table then
        EXIT    GE                      ;    return.
d716 5
a725 2
        DCD     IOMD_Base
        DCD     0
d848 2
a849 4
;               r0 bit  8     = 1 to map bufferable space (0 is normal, non-bufferable)
;               r0 bits 9..15 = 0 (reserved flags)
;               r0 bit  16    = 1 to doubly map
;               r0 bits 17..31 = 0 (reserved flags)
d858 1
a858 2
        MOV     lr, r0
        TST     lr, #&100           ;test bufferable bit
a860 2
        TST     lr, #1:SHL:16
        ORRNE   r0, r0, #MapInFlag_DoublyMapped
@


4.4.2.12
log
@  Commit of kernel as featured in release 5.00.
Detail:
  Lots of changes since last version, at least the following:
  * Updated OS timestamp, removed alpha status
  * Negative INKEY OS version changed to &AA
  * GraphicsV is now alocated vector number &2A
  * ROM moved up to &FC000000
  * Max application slot increased to 512 Mbytes (for now)
  * Max size of RMA increased to 256 Mbytes
  * RMA is now first-created dynamic area (so it gets lowest address after
    top of application slot)
  * OS_Memory 10 reimplemeted
  * New OS_ReadSysInfo 6 values 18-22 added
  * OS_ReadSysInfo 8 gains flag bit to indicate soft power-off
  * Misc internal top-bit-set-address fixes
  * *ChangeDynamicArea can take sizes in megabytes or gigabytes
  * Magic word "&off" in R0 passed to OS_Reset powers down if possible
  * Added acceleration: block copy; CLS; text window scroll up; rectangle
    fill
  * Disabled LED flashing in page mode (liable to crash)
  * Masked sprite plot and VDU 5 text avoids reading the screen if possible
  * Framestore made USR mode accessible
  * Fix for VDU 5,127 bug - now relies on font definitions being in extreme
    quarters of memory, rather than bottom half
  * Allocated 64-bit OS_Convert... SWIs
  * IIC errors use allocated error numbers
  * Looks for Dallas RTC before Philips RTC because we're using a Philips
    NVRAM device with the same ID
  * Fix to bug that meant the oscillator in the Dallas RTC wasn't enabled
  * Default mouse type (USB) changed to allocated number
  * Ram disc max size increased to 128 Mbytes (Ursula merge) and made
    cacheable for StrongARMs (not XScale)
  * Branch through zero handler now works in USR mode, by use of a
    trampoline in the system stack to allow PC-relative register storage
  * Address exception handler changed to not use 0 as workspace
  * OS_Memory 13 extended to allow specification of cacheability and access
    privileges
  * Added OS_Memory 16 to return important memory addresses
  * RISCOS_MapInIO() takes cacheable flag in bit 3, access permissions in
    bits 10 and 11, doubly-mapped flag in bit 20, and access permissions
    specified flag in bit 21
  * Bug fix in last version for application abort handlers didn't quite
    work; register shuffle required
  * "Module is not 32-bit compatible" error now reports the module name
  * Default configured language changed from 10 to 11 (now Desktop again)

Version 5.35, 4.79.2.51. Tagged as 'Kernel-5_35-4_79_2_51'
@
text
@d63 1
a63 1
        B       MemoryFreePoolLock              ; 10
a68 1
        B       MemoryAreaInfo                  ; 16
d752 1
a752 37
;
;       In:     r0 bits 0..7  = 10 (reason code 10)
;               r0 bit  8     = 1 if call is by Wimp (reserved for Acorn use), 0 otherwise
;               r0 bits 9..31 reserved (must be 0)
;               r1 = 0  to resume any background actions that may manipulate FreePool mapping
;                       (currently, this is just VRAMRescue)
;                  = 1  to suspend any background actions that may manipulate FreePool mapping
;                  all other values reserved (undefined behaviour)
;
;       Out:    r1 = previous state (1=suspended, 0=not suspended)
;
; A suspend call from the Wimp will be taken to mean that the whole FreePool is locked, and
; memory cannot be moved (supports Wimp_ClaimFreeMemory)
;
; VRAM rescue doesn't happen in this Kernel, but we still support the call for the Wimp.
;
MemoryFreePoolLock ROUT
        Push    "r2,r3,lr"
        MOV     r3,#0
        LDRB    lr,[r3,#VRAMRescue_control]
        TEQ     r1,#0
        BNE     %FT10
;flush TLB(s) for resume - needed for Wimp resume, precaution for other - no cache flush
;since Free Pool is uncached
        ARM_flush_TLB r2
10
        TST     r0,#&100                    ;is it Wimp?
        MOVEQ   r2,#VRRc_suspend
        MOVNE   r2,#VRRc_wimp_lock
        TEQ     r1,#0
        AND     r1,lr,r2                    ;previous state
        BICEQ   lr,lr,r2
        ORRNE   lr,lr,r2
        STRB    lr,[r3,#VRAMRescue_control] ;set/clear relevant bit
        TEQ     r1,#0
        MOVNE   r1,#1
        Pull    "r2,r3,pc"
d859 1
a859 3
;               r0 bit  9     = 1 to map cacheable space (0 is normal, non-cacheable)
;               r0 bits 10..12 = 0 (reserved for cache policy)
;               r0 bits 13..15 = 0 (reserved flags)
d861 1
a861 4
;               r0 bit  17    = 1 if access privileges specified
;               r0 bits 18..23 = 0 (reserved flags)
;               r0 bits 24..27 = access privileges (if bit 17 set)
;               r0 bits 28..31 = 0 (reserved flags)
d871 1
a871 1
        TST     lr, #1:SHL:8        ;test bufferable bit
a873 2
        TST     lr, #1:SHL:9        ;test bufferable bit
        ORRNE   r0, r0, #L1_C
a875 6
        TST     lr, #1:SHL:17
        ANDNE   lr, lr, #2_1111:SHL:24
        ADRNEL  r12, PPLTransL1
        LDRNE   r12, [r12, lr, LSR #22]
        ORRNE   r0, r0, #MapInFlag_APSpecified
        ORRNE   r0, r0, r12
a935 121
;----------------------------------------------------------------------------------------
;
;        In:    r0 = flags
;                       bit     meaning
;                       0-7     16 (reason code)
;                       8-15    1=cursor/system/sound
;                               2=IRQ stack
;                               3=SVC stack
;                               4=ABT stack
;                               5=UND stack
;                               6=Soft CAM
;                               7=Level 1 page tables
;                               8=Level 2 page tables
;                               9=HAL workspace
;                               10=Kernel buffers
;                       16-31   reserved (set to 0)
;
;       Out:    r1 = base of area
;               r2 = address space allocated for area (whole number of pages)
;               r3 = actual memory used by area (whole number of pages)
;               all values 0 if not present, or incorporated into another area
;
;       Return size of various low-level memory regions
MemoryAreaInfo ROUT
        Entry   "r0"
        MOV     r1, #0
        MOV     r2, #0
        MOV     r3, #0
        MOV     lr, r0, LSR #8
        AND     lr, lr, #&FF
        CMP     lr, #(MAI_TableEnd - MAI_TableStart)/4
        ADDLO   pc, pc, lr, LSL #2
        B       %FT70
MAI_TableStart
        B       %FT70
        B       MAI_CursSysSound
        B       MAI_IRQStk
        B       MAI_SVCStk
        B       MAI_ABTStk
        B       MAI_UNDStk
        B       MAI_SoftCAM
        B       MAI_L1PT
        B       MAI_L2PT
        B       MAI_HALWs
        B       MAI_Kbuffs
MAI_TableEnd

70
        ADRL    r0, ErrorBlock_BadParameters
        SETV
        STR     r0, [sp, #Proc_RegOffset+0]
        EXIT

MAI_CursSysSound
        LDR     r1, =CursorChunkAddress
        MOV     r2, #32*1024
        MOV     r3, r2
        EXIT

MAI_IRQStk
 [ IRQSTK < CursorChunkAddress :LOR: IRQSTK > CursorChunkAddress+32*1024
        LDR     r1, =IRQStackAddress
        MOV     r2, #IRQSTK-IRQStackAddress
        MOV     r3, r2
 ]
        EXIT

MAI_SVCStk
        LDR     r1, =SVCStackAddress
        MOV     r2, #SVCSTK-SVCStackAddress
        MOV     r3, r2
        EXIT

MAI_ABTStk
 [ No26bitCode
        LDR     r1, =ABTStackAddress
        MOV     r2, #ABTSTK-ABTStackAddress
        MOV     r3, r2
 ]
        EXIT

MAI_UNDStk
        LDR     r1, =UNDSTK :AND: &FFF00000
        LDR     r2, =UNDSTK :AND: &000FFFFF
        MOV     r3, r2
        EXIT

MAI_SoftCAM
        MOV     r0, #ZeroPage
        LDR     r1, [r0, #CamEntriesPointer]
        LDR     r2, =CAMspace
        LDR     r3, [r0, #SoftCamMapSize]
        EXIT

MAI_L1PT
        LDR     r1, =L1PT
        MOV     r2, #16*1024
        MOV     r3, r2
        EXIT

MAI_L2PT
        MOV     r0, #ZeroPage
        LDR     r1, =L2PT
        MOV     r2, #4*1024*1024
        LDR     r3, [r0, #L2PTUsed]
        EXIT

 [ HAL
MAI_HALWs
        MOV     r0, #ZeroPage
        LDR     r1, =HALWorkspace
        MOV     r2, #HALWorkspaceSpace
        LDR     r3, [r0, #HAL_WsSize]
 ]
        EXIT

MAI_Kbuffs
        LDR     r1, =KbuffsBaseAddress
        MOV     r2, #KbuffsMaxSize
        LDR     r3, =(KbuffsSize + &FFF) :AND: :NOT: &FFF
        EXIT
@


4.4.2.13
log
@Support for keys held down in the HAL at power on.
*Configure ANYTHINGsize was broken due to not setting R0 to ReadUnsigned
IIC ack message uninternationalised
OS_Memory was saying we only had 4M of RAM
VDU4 scrolling when output was switched to sprite was causing corruption
on use of CTRL-J and CTRL-K
Default SystemSize CMOS set to 32k

Version 5.35, 4.79.2.55. Tagged as 'Kernel-5_35-4_79_2_55'
@
text
@d605 2
a606 2
        MOVEQ   r1, #OSROM_ImageSize*1024 ; this much soft ROM
        BEQ     %FT20
d609 4
a612 3
        MOVEQ   r1, #0
        LDREQ   r1, [r1, #RAMLIMIT]     ; assume 0 VRAM
        BEQ     %FT20
a1000 1
;                               11=HAL uncacheable workspace
a1030 1
        B       MAI_HALWsNCNB
d1093 1
a1094 1
 [ HAL
a1098 12
 ]
        EXIT

MAI_HALWsNCNB
 [ HAL
        MOV     r0, #ZeroPage
        LDR     r1, =HALWorkspaceNCNB
        MOV     r2, #32*1024
        LDR     r3, [r0, #HAL_Descriptor]
        LDR     r3, [r3, #HALDesc_Flags]
        ANDS    r3, r3, #HALFlag_NCNBWorkspace
        MOVNE   r3, r2
@


4.4.2.14
log
@Huge update to L7200 HAL for Customer M 2 demo - now runs with 5.02 Kernel
used in Tungsten.
Added "fast" flash tool for Customer L, allowing ROMs to be sent serially at
115200 baud not 9600 baud.
Fix to VDU despatch for ARMv4 and later.
Fixes to power on delete keyboard and keyboard timeout
Implemented MemoryReadPhys and MemoryAmounts with the HAL.

Version 5.35, 4.79.2.59. Tagged as 'Kernel-5_35-4_79_2_59'
@
text
@a448 9
 [ HAL
        Entry   "r0,r3,sb,ip"
        AddressHAL
        MOV     r0, #0
        CallHAL HAL_PhysInfo
        MOV     r1, r0
        MOV     r2, #4*1024
        EXIT
 |
a451 1
 ]
d465 1
d468 1
a468 82
        Entry   "r0-r12"
        AddressHAL
        MOV     r0, r1
        SUB     sp, sp, #4
        MOV     r1, sp
        CallHAL HAL_PhysInfo            ; fills in everything except DRAM
        LDR     r11, [sp], #4

        ; r0 to r11 is DRAM or not present.
        LDR     r1, [sp, #4]            ; Get table address back
        ADD     r1, r1, r0, LSR #ByteShift
        MOV     r2, r0                  ; Current physical address.
        MOV     r3, #0                  ; Next word to store in table.
        MOV     r4, #32                 ; How much more we have to shift r3 before storing it.
        MOV     r5, #0                  ; Current page number.
        MOV     r6, #PhysRamTable
        LDR     r7, [r3, #CamEntriesPointer]
        ADD     r7, r7, #4              ; Point to PPL entries.
        LDR     r8, [r3, #MaxCamEntry]
10
        LDMIA   r6!, {r9,r10}           ; Get physical address and size of next block.

        CMP     r9, r0                  ; If not DRAM then
        CMPHS   r11, r9
        ADDLO   r5, r5, r10, LSR #12    ;   adjust current page number
        BLO     %BT10                   ;   and try next block.

        ADD     r10, r10, r9            ; Add amount of unused space between current and start of block.
        SUB     r10, r10, r2            ; size = size + (physaddr - current)
20
        SUBS    r4, r4, #4              ; Reduce shift.
        MOVCS   r3, r3, LSR #4          ; If more space in current word then shift it.
        STRCC   r3, [r1], #4            ; Otherwise, store current word
        MOVCC   r3, #0                  ;   and start a new one.
        MOVCC   r4, #28

        CMP     r2, r9                  ; If not reached start of block then page is not present.
        ORRCC   r3, r3, #(NotPresent :OR: NotAvailable) :SHL: 28
        BCC     %FT30
        LDR     lr, [r7, r5, LSL #3]    ; Page is there so get PPL and determine if it's available or not.
        TST     lr, #PageFlags_Unavailable
        ORREQ   r3, r3, #DRAM_Pattern :SHL: 28
        ORRNE   r3, r3, #(DRAM_Pattern :OR: NotAvailable) :SHL: 28
        ADD     r5, r5, #1              ; Increment page count.
30
        ADD     r2, r2, #&1000          ; Increase current address.
        SUBS    r10, r10, #&1000        ; Decrease size of block.
        BGT     %BT20                   ; Stop if no more block left.

        CMP     r8, r5                  ; Stop if we run out of pages.
        BCS     %BT10

        TEQ     r3, #0                          ; If not stored last word then
        MOVNE   r3, r3, LSR r4                  ;   put bits in correct position
        ADDNE   r2, r2, r4, LSL #BitShift       ;   adjust current address
        RSBNE   r4, r4, #32                     ;   rest of word is not present
        LDRNE   lr, =NotPresent :OR: NotAvailable
        ORRNE   r3, r3, lr, LSL r4
        STRNE   r3, [r1], #4                    ;   and store word.

        ; End of last block of DRAM to r11 is not present.
        MOV     r6, r0
        ADD     lr, r11, #1
        RSBS    r2, r2, lr
        MOVNE   r0, r1
        LDRNE   r1, =NotPresent :OR: NotAvailable
        MOVNE   r2, r2, LSR #ByteShift
        BLNE    memset

        ; If softloaded (ie ROM image is wholely within DRAM area returned
        ; by HAL_PhysInfo), mark that as unavailable DRAM.
        MOV     r0, #0
        LDR     r0, [r0, #ROMPhysAddr]
        LDR     r1, [sp, #4]
        CMP     r0, r6
        ADDHS   lr, r0, #OSROM_ImageSize*1024
        SUBHS   lr, lr, #1
        CMPHS   r11, lr
        ADDHS   r0, r1, r0, LSR #ByteShift
        LDRHS   r1, =DRAM_Pattern :OR: NotAvailable
        MOVHS   r2, #(OSROM_ImageSize*1024) :SHR: ByteShift
        BLHS    memset
a469 1
        Entry   "r1-r10"
d605 2
a606 10
        BNE     %FT05
        Push    "r0"
        MOV     r0, #8
        SWI     XOS_ReadSysInfo         ; are we softloaded?
        MOVVS   r1, #0
        Pull    "r0"
        AND     r1, r1, r2
        ANDS    r1, r1, #1:SHL:4
        MOVNE   r1, #OSROM_ImageSize*1024 ; this much soft ROM
        B       %FT20
@


4.4.2.15
log
@  Added comment
Detail:
  Noted reservation of IO controller type  passed to OS_Memory 9 used when
  system is running as a coprocessor. Not used by current code but we need to
  make sure that any future reservations use different numbers.
Admin:
  No functional change. Brought to our attention by Rob Sprowson.

Version 5.35, 4.79.2.103. Tagged as 'Kernel-5_35-4_79_2_103'
@
text
@a786 1
;                               6 = Tube ULA
@


4.4.2.16
log
@Update list of OS_Memory 9 controllers
Detail:
  s/MemInfo - List of OS_Memory 9 controllers now updated to include details of the ones that ROL are using, along with which numbers should/shouldn't be safe for us to expand into in the future.
Admin:
  Tested in ROM softload on RiscPC


Version 5.35, 4.79.2.114. Tagged as 'Kernel-5_35-4_79_2_114'
@
text
@a787 5
;                               7-31 = Reserved (for us)
;                               32 = Primary ROM
;                               33 = IOMD
;                               34 = FDC37C665/SMC37C665/82C710/SuperIO/whatever
;                               35+ = Reserved (for ROL)
@


4.4.2.17
log
@Merge Cortex kernel into HAL branch
Detail:
  This is a full merge of the Cortex kernel back into the HAL branch. Since the Cortex kernel is/was just a superset of the HAL branch, at this point in time both branches are identical.
  Main features the HAL branch gains from this merge:
  - ARMv6/ARMv7 support
  - High processor vectors/zero page relocation support
  - objasm 4 warning fixes
  - Improved HAL related functionality:
    - Support for HAL-driven RTCs instead of kernel-driven IIC based ones
    - Support for arbitrary size machine IDs
    - Support for multiple IIC busses
    - Support for any HAL size, instead of hardcoded 64k size
    - Probably some other stuff I've forgotten
  - Probably a few bug fixes here and there
Admin:
  Tested on BB-xM & Iyonix.
  Was successfully flashed to ROM on an Iyonix to test the Cortex branch implementation of the 2010 RTC bug fix.
  IOMD build untested - but has been known to work in the past.


Version 5.35, 4.79.2.123. Tagged as 'Kernel-5_35-4_79_2_123'
@
text
@d139 1
a139 1
        LDR     r6, =ZeroPage
d234 1
a234 1
        LDR     r0, =ZeroPage
d238 1
a238 1
        LDR     r0, =ZeroPage
d287 1
a287 1
        LDR     r14, =ZeroPage
d395 1
a395 1
        LDR     r9, =ZeroPage+PhysRamTable
d492 2
a493 2
        LDR     r6, =ZeroPage+PhysRamTable
        LDR     r7, [r6, #CamEntriesPointer-PhysRamTable]
d495 1
a495 1
        LDR     r8, [r6, #MaxCamEntry-PhysRamTable]
d548 1
a548 1
        LDR     r0, =ZeroPage
d875 1
a875 1
        LDR     r3,=ZeroPage
a880 5
 [ HAL
        Push    "r0"
        ARMop   TLB_InvalidateAll,,,r3
        Pull    "r0"
 |
a881 1
 ]
d929 1
a929 1
        LDR     r0,=ZeroPage+PhysRamTable
d931 1
a931 1
        LDR     r5,=ZeroPage+CamEntriesPointer
d1180 1
a1180 1
        LDR     r0, =ZeroPage
d1193 1
a1193 1
        LDR     r0, =ZeroPage
d1201 1
a1201 1
        LDR     r0, =ZeroPage
d1210 1
a1210 1
        LDR     r0, =ZeroPage
@


4.4.2.17.2.1
log
@  Merged OS_Memory 8 changes across from HAL branch to RPi branch
Detail:
  Revisions Kernel-5_35-4_79_2_153 and Kernel-5_35-4_79_2_161 merged with one
  trivial conflict.
Admin:
  Confirmed that this builds, but not tested on hardware.

Version 5.35, 4.79.2.147.2.15. Tagged as 'Kernel-5_35-4_79_2_147_2_15'
@
text
@d450 1
a450 1
        Entry   "r0-r1,r3,sb,ip"
d452 1
a452 2
        MOV     r0, #PhysInfo_GetTableSize
        ADD     r1, sp, #4
d454 1
d479 3
a481 3
        MOV     r0, #PhysInfo_WriteTable
        SUB     sp, sp, #8
        MOV     r2, sp
a482 1
        LDR     r0, [sp], #4
d688 4
d693 1
a693 9
        CMP     lr, #6:SHL:8
        ADDCC   pc, pc, lr, LSR #8-2
        NOP
        B       %FT99                   ; Don't understand 0 (so the spec says).
        B       %FT10                   ; DRAM
        B       %FT20                   ; VRAM
        B       %FT30                   ; ROM
        B       %FT40                   ; I/O
        B       %FT50                   ; Soft ROM
d695 2
a696 36
10
        LDR     r1, =ZeroPage
        LDR     r3, [r1, #VRAMFlags]
        TST     r3, #1
        MOVNE   r3, r3, LSR #12         ; Extract size from flags when genuine VRAM
        MOVNE   r3, r3, LSL #12
        MOVEQ   r3, #0
        LDR     r1, [r1, #RAMLIMIT]
        SUB     r1, r1, r3              ; DRAM = RAMLIMIT - VRAMSize
        B       %FT97
20
        LDR     r1, =ZeroPage
        LDR     r1, [r1, #VRAMFlags]
        TST     r1, #1
        MOVNE   r1, r1, LSR #12
        MOVNE   r1, r1, LSL #12         ; VRAM = VRAMSize
        MOVEQ   r1, #0
        B       %FT97
30
        Push    "r0, sb, ip"
        AddressHAL
        MOV     r0, #PhysInfo_HardROM
        SUB     sp, sp, #8
        MOV     r2, sp
        CallHAL HAL_PhysInfo
        LDMIA   sp!, {r0-r1}
        SUB     r1, r1, r0              ; ROM = ROMPhysTop - ROMPhysBot
        Pull    "r0, sb, ip"
        B       %FT97
40
        LDR     r1, =ZeroPage
        LDR     r1, [r1, #IOAllocLimit]
        LDR     r3, =IO
        SUB     r1, r3, r1              ; IO = IO ceiling - IO floor
        B       %FT97
50
d699 2
a700 1
        SWI     XOS_ReadSysInfo         ; Are we softloaded?
d703 13
a715 4
        ANDS    r1, r1, #1:SHL:4        ; Test OS-runs-from-RAM flag
        MOVNE   r1, #OSROM_ImageSize*1024
        B       %FT97
97
d719 1
a719 1
99
a720 3
      [ International
        BL      TranslateError
      |
d722 1
a722 1
      ]
d1209 1
a1209 1
        MOV     r2, #HALWorkspaceSize
@


4.4.2.18
log
@Make OS_Memory 8 return more correct values
The only fake result now is the hard ROM amount, which is hardwired to 4MB and might not be correct.
Unrelated changes
 hdr.HALDevice: Assign a device for VIDC20.
 hdr.KernelWS: Reorder into ascending order, remove legacy addresses.
 s.ARM600: Move PhysSpaceSize inside :LNOT:HAL switch.
 s.Kernel: Move PhysSpaceSize inside :LNOT:HAL switch.

Version 5.35, 4.79.2.153. Tagged as 'Kernel-5_35-4_79_2_153'
@
text
@d688 4
d693 1
a693 9
        CMP     lr, #6:SHL:8
        ADDCC   pc, pc, lr, LSR #8-2
        NOP
        B       %FT99                   ; Don't understand 0 (so the spec says).
        B       %FT10                   ; DRAM
        B       %FT20                   ; VRAM
        B       %FT30                   ; ROM
        B       %FT40                   ; I/O
        B       %FT50                   ; Soft ROM
d695 2
a696 29
10
        LDR     r1, =ZeroPage
        LDR     r3, [r1, #VRAMFlags]
        TST     r3, #1
        MOVNE   r3, r3, LSR #12         ; Extract size from flags when genuine VRAM
        MOVNE   r3, r3, LSL #12
        MOVEQ   r3, #0
        LDR     r1, [r1, #RAMLIMIT]
        SUB     r1, r1, r3              ; DRAM = RAMLIMIT - VRAMSize
        B       %FT97
20
        LDR     r1, =ZeroPage
        LDR     r1, [r1, #VRAMFlags]
        TST     r1, #1
        MOVNE   r1, r1, LSR #12
        MOVNE   r1, r1, LSL #12         ; VRAM = VRAMSize
        MOVEQ   r1, #0
        B       %FT97
30
        ! 0, "Sort out hard ROM size for HALs with no hard ROM"
        MOV     r1, #4*1024*1024        ; Bodge for IOMD and Tungsten
        B       %FT97
40
        LDR     r1, =ZeroPage
        LDR     r1, [r1, #IOAllocLimit]
        LDR     r3, =IO
        SUB     r1, r3, r1              ; IO = IO ceiling - IO floor
        B       %FT97
50
d699 2
a700 1
        SWI     XOS_ReadSysInfo         ; Are we softloaded?
d703 13
a715 4
        ANDS    r1, r1, #1:SHL:4        ; Test OS-runs-from-RAM flag
        MOVNE   r1, #OSROM_ImageSize*1024
        B       %FT97
97
d719 1
a719 1
99
a720 3
      [ International
        BL      TranslateError
      |
d722 1
a722 1
      ]
d1209 1
a1209 1
        MOV     r2, #HALWorkspaceSize
@


4.4.2.19
log
@Sort out hard ROM size for HALs with no hard ROM
Previously HAL_PhysInfo took one of two routes to get the arrangment table of arrangement table size.
Reorganised to accept a subreason to allow other physical info to be requested.


Version 5.35, 4.79.2.161. Tagged as 'Kernel-5_35-4_79_2_161'
@
text
@d450 1
a450 1
        Entry   "r0-r1,r3,sb,ip"
d452 1
a452 2
        MOV     r0, #PhysInfo_GetTableSize
        ADD     r1, sp, #4
d454 1
d479 3
a481 3
        MOV     r0, #PhysInfo_WriteTable
        SUB     sp, sp, #8
        MOV     r2, sp
a482 1
        LDR     r0, [sp], #4
d718 2
a719 9
        Push    "r0, sb, ip"
        AddressHAL
        MOV     r0, #PhysInfo_HardROM
        SUB     sp, sp, #8
        MOV     r2, sp
        CallHAL HAL_PhysInfo
        LDMIA   sp!, {r0-r1}
        SUB     r1, r1, r0              ; ROM = ROMPhysTop - ROMPhysBot
        Pull    "r0, sb, ip"
@


4.4.2.20
log
@Teach the kernel about different memory attributes
Detail:
  Briefly, this set of changes:
  * Adjusts PhysRamTable so that it retains the flags passed in by the HAL from OS_AddRAM (by storing them in the lower 12 bits of the size field)
  * Sorts the non-VRAM entries of PhysRamTable by speed and DMA capability, to ensure optimal memory allocation during OS startup.
  * Adjust the initial memory allocation logic to allow the cursor/sound chunk and HAL noncacheable workspace to come from DMA capable memory
  * Extends OS_Memory 12 to accept a 'must be DMA capable' flag in bit 8 of R0. This is the same as available in ROL's OS.
  * Extends OS_DynamicArea 0 to allow the creation of dynamic areas that automatically allocate from DMA capable memory. In ROL's OS this was done by setting bit 12 of R4, but we're using bits 12-14 for specifying the cache policy, so instead bit 15 is used.
  * Fixes OS_ReadSysInfo 6 to return the correct DevicesEnd value now that the IRQ/device limit is computed at runtime
  File changes:
  * hdr/OSEntries - Add definitions of the various flags passed to OS_AddRAM by the HAL. Add a new flag, NoDMA, for memory which can't be used for DMA.
  * hdr/KernelWS - Tidy PhysRamTable definition a bit by removing all the DRAM bank definitions except the first - this makes it easier to search for code which is interacting with the table. Remove VRAMFlags, it's redundant now that the flags are kept in the table. Add DMA allocation info to InitWs.
  * s/AMBControl/memmap - Updated to mask out the flags from PhysRamTable when reading RAM block sizes.
  * s/ARM600 - Strip out a lot of IOMD specific pre-HAL code.
  * s/ChangeDyn - Updated to cope with the flags stored in PhysRamTable. Implement support for DMA-capable dynamic areas. Rewrite InitDynamicAreas to insert pages into the free pool in the right order so that the fastest memory will be taken from it first.
  * s/GetAll, s/Middle - Fix OS_ReadSysInfo 6 to return the correct HAL-specific DevicesEnd value
  * s/HAL - Significant rework of initial RAM allocation code to allow the kernel workspace to come from the fastest DMA incapable RAM, while also allowing allocation of DMA capable memory for HAL NCNB workspace & kernel cursor/sound chunks. ClearPhysRAM rewritten as part of this.
  * s/MemInfo - Updated to cope with the flags stored in PhysRamTable. Add support for the new OS_Memory 12 flag. Update OS_Memory 7 to not assume PhysRamTable entries are sorted in address order, and rip out the old pre-HAL IOMD implementation.
  * s/NewReset - Remove GetPagesFromFreePool option, assume TRUE (as this has been the case for the past 10+ years). Revise a few comments and strip dead code. Update to cope with PhysRamTable flags.
  * s/VMSAv6 - Remove a couple of unused definitions
  * s/vdu/vdudriver - Update to cope with PhysRamTable flags
Admin:
  Tested in Kinetic RiscPC ROM softload, Iyonix softload, & OMAP3


Version 5.35, 4.79.2.186. Tagged as 'Kernel-5_35-4_79_2_186'
@
text
@a396 1
        MOV     r5, r5, LSR #12
d406 2
a407 2
        SUB     r10, r5, r10, LSR #12   ; Determine if given address is in this block.
        CMP     r10, r11, LSR #12
d411 1
a411 1
        ADD     r3, r3, r10
d476 1
d492 3
a494 2
        LDR     r6, =ZeroPage+CamEntriesPointer
        LDR     r7, [r6]
d496 1
a496 5
        LDR     r8, [r6, #MaxCamEntry-CamEntriesPointer]
        MOV     r5, #0                  ; last block address processed + 1
        Push    "r5"

        ; Ugly logic to process PhysRamTable entries in address order instead of physical page order
a497 8
        Pull    "r12"
        MVN     lr, #0
        MOV     r5, #0                  ; Current page number.
        Push    "r5,lr"
        LDR     r6, =ZeroPage+PhysRamTable
        MOV     r10, #0
11
        ADD     r5, r5, r10, LSR #12
a498 2
        CMP     r10, #0
        BEQ     %FT12
d502 2
a503 1
        BLO     %BT11                   ; try next block.
d505 1
a505 18
        CMP     r9, r12                 ; have we processed this entry?
        CMPHS   lr, r9                  ; is it the lowest one we've seen?
        BLO     %BT11                   ; yes, try the next
        ; This is the best match so far
        STMIA   sp, {r5,r6}             ; Remember page number & details ptr
        MOV     lr, r9                  ; Remember base address
        B       %BT11
12
        Pull    "r5,r6"
        CMP     r6, #-1                 ; did we find anything?
        BEQ     %FT40
        LDMDB   r6,{r9,r10}
        ADD     r12, r9, #1        
        Push    "r12"                   ; Remember that we've processed up to here

        ; Now process this entry
        MOV     r10, r10, LSR #12
        ADD     r10, r9, r10, LSL #12   ; Add amount of unused space between current and start of block.
d527 2
a528 1
        B       %BT10
a529 1
40
d560 96
d702 2
a703 2
        LDR     r3, [r1, #VideoSizeFlags]
        TST     r3, #OSAddRAM_IsVRAM
d712 2
a713 2
        LDR     r1, [r1, #VideoSizeFlags]
        TST     r1, #OSAddRAM_IsVRAM
d943 1
a943 2
;               r0 bit 8 = 1 if region must be DMAable
;               r0 bits 9..31 = 0 (reserved flags)
d953 1
a953 1
        Push    "r0-r2,r4-r11,lr"
d955 1
a955 3
        BHI     RP_failed         ;refuse to look for alignments above 1G
        ANDS    r11,r0,#1:SHL:8   ;convert flag into something usable in the loop
        MOVNE   r11,#OSAddRAM_NoDMA
a978 2
        TST     r8,r11           ;ignore non-DMA regions if bit 8 of R0 was set
        BNE     RP_nextchunk
a979 1
        MOV     r8,r8,LSR #12
a980 1
        MOV     r8,r8,LSL #12
d1019 1
a1019 1
        Pull    "r0-r2,r4-r11,pc"
d1026 1
a1026 1
        Pull    "r0-r2,r4-r11,pc"
@


4.4.2.21
log
@Comment corrected
@
text
@d998 1
a998 1
        TST     lr, #1:SHL:9        ;test cacheable bit
@


4.4.2.22
log
@Miscellaneous fixes
Detail:
  s/MemInfo - Fix OS_Memory 0 physical-to-logical conversion returning bad addresses for most cases due to R5 being modified by physical_to_ppn (bug introduced in revision 4.4.2.20)
  s/vdu/vdudriver - Fix some VDU driver variables not being initialised correctly when switching GraphicsV driver
  s/vdu/vdugrafv - Fix handling of VSync events from GraphicsV drivers other than driver zero
  s/vdu/vduswis - Fix abort when OS_ScreenMode 11 is passed a bad driver number
Admin:
  Tested on Raspberry Pi
  There still seems to be a bug lurking somewhere when switching to a GraphicsV driver that uses DA2; the system will crash horribly unless DA2 is already a suitable size for the initial mode change


Version 5.35, 4.79.2.206. Tagged as 'Kernel-5_35-4_79_2_206'
@
text
@a377 2
meminfo_returncs_pullr5
        Pull    "r5"
a394 1
        Push    "r5"
d400 5
a404 1
        BCC     meminfo_returncs_pullr5
a411 2
        Pull    "r5"

d413 1
d416 3
@


4.4.2.23
log
@Add OS_Memory 24 implementation. Change OS_ValidateAddress to use it. Fix kernel leaving the physical access MB in a messy state. Try and protect against infinite abort loops caused by bad environment handlers.
Detail:
  s/MemInfo - Added an implementation of ROL's OS_Memory 24 call. Unlike the old OS_ValidateAddress call, this call should successfully report the presence of all memory areas known to the kernel. It should also correctly indicate which parts of a sparse DA are mapped in, unlike the old OS_ValidateAddress implementation.
  s/ChangeDyn - Update dynamic area handling to construct a lookup table for mapping logical addresses to dynamic areas; this is used by OS_Memory 24 to quickly locate which DA(s) hit a given region
  s/AMBControl/main - Make sure lazy task swapping is marked as disabled when AMB_LazyMapIn is {FALSE} - required so that OS_Memory 24 will give application space the correct flags
  s/ArthurSWIs - Switch OS_ValidateAddress over to using OS_Memory 24, as per ROL. For compatibility, Service_ValidateAddress is still issued for any areas which the kernel doesn't recognise (currently, OS_Memory 24 doesn't issue any service calls itself)
  s/Convrsions - ADR -> ADRL to keep things happy
  s/HAL - Fix L2PT page allocation and RAM clear to release the physical access region once they're done with it
  s/Kernel - Make the error dispatcher validate the error handler code ptr & error buffer using OS_Memory 24 before attempting to use them. If they look bad, reset to default. Should prevent getting stuck in an infinite abort loop in some situations (e.g. as was the case with ticket 279). The system might not fully recover, but it's better than a hard crash.
  s/Middle - Rework data/prefetch/etc. abort handlers so that DumpyTheRegisters can validate the exception dump area via OS_Memory 24 before anything gets written to it. Should also help to prevent some infinite abort loops. Strip 26bit/pre-HAL code to make things a bit more readable.
  hdr/KernelWS - Update comment
Admin:
  Tested on BB-xM, Raspberry Pi


Version 5.35, 4.79.2.222. Tagged as 'Kernel-5_35-4_79_2_222'
@
text
@a69 9
        B       %BT20                           ; 17 |
        B       %BT20                           ; 18 |
        B       %BT20                           ; 19 |
        B       %BT20                           ; 20 | Reserved for us
        B       %BT20                           ; 21 |
        B       %BT20                           ; 22 |
        B       %BT20                           ; 23 |
        B       CheckMemoryAccess               ; 24
                                                ; 25+ reserved for ROL
a1200 372
;----------------------------------------------------------------------------------------
;
;        In:    r0 = flags
;                       bit     meaning
;                       0-7     24 (reason code)
;                       8-31    reserved (set to 0)
;               r1 = low address (inclusive)
;               r2 = high address (exclusive)
;
;       Out:    r1 = access flags:
;               bit 0: completely readable in user mode
;               bit 1: completely writable in user mode
;               bit 2: completely readable in privileged modes
;               bit 3: completely writable in privileged modes
;               bit 4: partially readable in user mode
;               bit 5: partially writable in user mode
;               bit 6: partially readable in privileged modes
;               bit 7: partially writable in privileged modes
;               bit 8: completely physically mapped (i.e. IO memory)
;               bit 9: completely abortable (i.e. custom data abort handler)
;               bits 10,11: reserved
;               bit 12: partially physically mapped
;               bit 13: partially abortable
;               bits 14+: reserved
;
;       Return various attributes for the given memory region
CMA_Completely_UserR   * 1<<0
CMA_Completely_UserW   * 1<<1
CMA_Completely_PrivR   * 1<<2
CMA_Completely_PrivW   * 1<<3
CMA_Partially_UserR    * 1<<4
CMA_Partially_UserW    * 1<<5
CMA_Partially_PrivR    * 1<<6
CMA_Partially_PrivW    * 1<<7
CMA_Completely_Phys    * 1<<8
CMA_Completely_Abort   * 1<<9
CMA_Partially_Phys     * 1<<12
CMA_Partially_Abort    * 1<<13

CMA_CheckL2PT          * 1<<31 ; Pseudo flag used internally for checking sparse areas

; AP_ equivalents
CMA_ROM  * CMA_Partially_UserR+CMA_Partially_PrivR
CMA_Read * CMA_ROM+CMA_Partially_PrivW
CMA_Full * CMA_Read+CMA_Partially_UserW
CMA_None * CMA_Partially_PrivR+CMA_Partially_PrivW

CheckMemoryAccess ROUT
        Entry   "r0,r2-r10"
        CMP     r0, #24
        BNE     %FT99
        LDR     r10, =ZeroPage
        ; Set all the 'completely' flags, we'll clear them as we go along
        LDR     r0, =&0F0F0F0F
        ; Make end address inclusive so we don't have to worry so much about
        ; wrap around at 4G
        TEQ     r1, r2
        SUBNE   r2, r2, #1
        ; Split memory up into five main regions:
        ; * scratchspace/zeropage
        ; * application space
        ; * dynamic areas
        ; * IO memory
        ; * special areas (stacks, ROM, HAL workspace, system heap, etc.)
        ; All ranges are checked in increasing address order, so the
        ; completeness flags are returned correctly if we happen to cross from
        ; one range into another
        ; Note that application space can't currently be checked in DA block as
        ; (a) it's not linked to DAList/DynArea_AddrLookup
        ; (b) we need to manually add the abortable flag
        CMP     r1, #32*1024
        BHS     %FT10
        ; Check zero page
      [ ZeroPage = 0
        MOV     r3, #0
        MOV     r4, #16*1024
        MOV     r5, #CMA_Read
        BL      CMA_AddRange
      |
        ; DebuggerSpace
        ASSERT  DebuggerSpace < ScratchSpace
        LDR     r3, =DebuggerSpace
        LDR     r4, =(DebuggerSpace_Size + &FFF) :AND: &FFFFF000
        MOV     r5, #CMA_Read
        BL      CMA_AddRange
      ]
        ; Scratch space
        LDR     r3, =ScratchSpace
        MOV     r4, #16*1024
        MOV     r5, #CMA_Read
        BL      CMA_AddRange
10
        ; Application space
        ; Note - checking AplWorkSize as opposed to AplWorkMaxSize to cope with
        ; software which creates DAs within application space (e.g. Aemulor)
        LDR     r4, [r10, #AplWorkSize]
        CMP     r1, r4
        BHS     %FT20
        LDR     r3, [r10, #AMBControl_ws]
        LDR     r3, [r3, #:INDEX:AMBFlags]
        MOV     r5, #CMA_Full
        TST     r3, #AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
        MOV     r3, #32*1024
        ORREQ   r5, r5, #CMA_Partially_Abort
        BL      CMA_AddRange2
20
        ; Dynamic areas
        LDR     r7, [r10, #IOAllocLimit]
        CMP     r1, r7
        BHS     %FT30
        ; Look through the quick lookup table until we find a valid DANode ptr
        LDR     r6, [r10, #DynArea_ws]
        MOV     r3, r1
        ADD     r6, r6, #(:INDEX:DynArea_AddrLookup) :AND: &00FF
        ADD     r6, r6, #(:INDEX:DynArea_AddrLookup) :AND: &FF00
21
        AND     r8, r3, #DynArea_AddrLookupMask
        LDR     r9, [r6, r8, LSR #30-DynArea_AddrLookupBits]
        TEQ     r9, #0
        BNE     %FT22
        ; Nothing here, skip ahead to next block
        ADD     r3, r8, #DynArea_AddrLookupSize
        CMP     r3, r2
        BHI     %FT90 ; Hit end of search area
        CMP     r3, r7
        BLO     %BT21
        ; Hit end of DA area and wandered into IO area
        B       %FT30
22
        ; Now that we've found a DA to start from, walk through and process all
        ; the entries until we hit the system heap (or any other DAs above
        ; IOAllocLimit)
        LDR     r3, [r9, #DANode_Base]
        LDR     r6, [r9, #DANode_Flags]
        CMP     r3, r7
        BHS     %FT30
        ; Decode AP flags
        ANDS    lr, r6, #3
        MOVEQ   r5, #CMA_Full
        TEQ     lr, #1
        MOVEQ   r5, #CMA_Read
        CMP     lr, #2
        MOVEQ   r5, #CMA_None
        MOVGT   r5, #CMA_ROM
        TST     r6, #DynAreaFlags_SparseMap
        LDREQ   lr, [r9, #DANode_Size]
        LDRNE   lr, [r9, #DANode_SparseHWM] ; Use HWM as bounds when checking sparse areas
        ORRNE   r5, r5, #CMA_CheckL2PT ; ... and request L2PT check
        TST     r6, #DynAreaFlags_DoublyMapped
        ADD     r4, r3, lr
        SUBNE   r3, r3, lr
        BL      CMA_AddRange2
        LDR     r9, [r9, #DANode_Link]
        TEQ     r9, #0
        BNE     %BT22
        ; Hit the end of the list. This shouldn't happen with the current heap setup!
30
        ; IO memory
        CMP     r1, #IO
        BHS     %FT40
        MOV     r3, r1, LSR #20
        LDR     r4, [r10, #IOAllocPtr]
        MOV     r3, r3, LSL #20 ; Get MB-aligned addr of first entry to check
        CMP     r3, r4
        LDR     r7, =L1PT
        MOVLO   r3, r4 ; Skip all the unallocated regions
31
        LDR     r4, [r7, r3, LSR #20-2]
        AND     r4, r4, #L1_AP
        ; Decode page AP flags
        MOV     r5, #CMA_ROM
        CMP     r4, #AP_None*L1_APMult
        MOVEQ   r5, #CMA_None
        CMP     r4, #AP_Read*L1_APMult
        MOVEQ   r5, #CMA_Read
        CMP     r4, #AP_Full*L1_APMult
        MOVEQ   r5, #CMA_Full
        ADD     r4, r3, #1<<20
        ORR     r5, r5, #CMA_Partially_Phys
        BL      CMA_AddRange2
        CMP     r4, #IO
        MOV     r3, r4
        BNE     %BT31
40
        ; Everything else!
        LDR     r3, =HALWorkspace
        LDR     r4, [r10, #HAL_WsSize]
        MOV     r5, #CMA_Read
        BL      CMA_AddRange
        ASSERT  IRQStackAddress > HALWorkspace
        LDR     r3, =IRQStackAddress
        LDR     r4, =IRQStackSize
        MOV     r5, #CMA_None
        BL      CMA_AddRange
        ASSERT  SVCStackAddress > IRQStackAddress
        LDR     r3, =SVCStackAddress
        LDR     r4, =SVCStackSize
        MOV     r5, #CMA_Read
        BL      CMA_AddRange
        ASSERT  ABTStackAddress > SVCStackAddress
        LDR     r3, =ABTStackAddress
        LDR     r4, =ABTStackSize
        MOV     r5, #CMA_None
        BL      CMA_AddRange
        ASSERT  UNDStackAddress > ABTStackAddress
        LDR     r3, =UNDStackAddress
        LDR     r4, =UNDStackSize
        MOV     r5, #CMA_None
        BL      CMA_AddRange
        ASSERT  PhysicalAccess > UNDStackAddress
        LDR     r3, =L1PT + (PhysicalAccess:SHR:18)
        LDR     r3, [r3]
        TEQ     r3, #0
        BEQ     %FT50
        LDR     r3, =PhysicalAccess
        LDR     r4, =&100000
        LDR     r5, =CMA_None+CMA_Partially_Phys ; Assume IO memory mapped there
        BL      CMA_AddRange
50
        ASSERT  DCacheCleanAddress > PhysicalAccess
        LDR     r4, =DCacheCleanAddress+DCacheCleanSize
        CMP     r1, r4
        BHS     %FT60
        ; Check that DCacheCleanAddress is actually used
        Push    "r0-r2,r9"
        AddressHAL r10
        MOV     a1, #-1
        CallHAL HAL_CleanerSpace
        CMP     a1, #-1
        Pull    "r0-r2,r9"
        BEQ     %FT60
        SUB     r3, r4, #DCacheCleanSize
        MOV     r4, #DCacheCleanSize
        LDR     r5, =CMA_None+CMA_Partially_Phys ; Mark as IO, it may not be actual memory there
        BL      CMA_AddRange
60
        ASSERT  KbuffsBaseAddress > DCacheCleanAddress
        LDR     r3, =KbuffsBaseAddress
        LDR     r4, =(KbuffsSize + &FFF) :AND: &FFFFF000
        MOV     r5, #CMA_Read
        BL      CMA_AddRange
        ASSERT  HALWorkspaceNCNB > KbuffsBaseAddress
        LDR     r3, [r10, #HAL_Descriptor]
        LDR     r3, [r3, #HALDesc_Flags]
        TST     r3, #HALFlag_NCNBWorkspace
        BEQ     %FT70
        LDR     r3, =HALWorkspaceNCNB
        LDR     r4, =32*1024
        MOV     r5, #CMA_None
        BL      CMA_AddRange
70
        ASSERT  CursorChunkAddress > HALWorkspaceNCNB
        LDR     r3, =CursorChunkAddress
        MOV     r4, #32*1024
        MOV     r5, #CMA_Read
        BL      CMA_AddRange
        ASSERT  L2PT > CursorChunkAddress
        LDR     r3, =L2PT
        MOV     r5, #CMA_None
        MOV     r4, #4*1024*1024
        ORR     r5, r5, #CMA_CheckL2PT ; L2PT contains gaps due to logical indexing
        BL      CMA_AddRange
        ASSERT  L1PT > L2PT
        LDR     r3, =L1PT
        MOV     r4, #16*1024
        MOV     r5, #CMA_None
        BL      CMA_AddRange
        ; Note that system heap needs to be checked manually due to being
        ; outside main DA address range
        ASSERT  SysHeapChunkAddress > L1PT
        LDR     r3, =SysHeapChunkAddress
        LDR     r4, [r10, #SysHeapDANode + DANode_Size]
        MOV     r5, #CMA_Full
        BL      CMA_AddRange
        ASSERT  CAM > SysHeapChunkAddress
        LDR     r3, =CAM
        LDR     r4, [r10, #SoftCamMapSize]
        MOV     r5, #CMA_None
        BL      CMA_AddRange
        ASSERT  ROM > CAM
        LDR     r3, =ROM
        LDR     r4, =OSROM_ImageSize*1024
        MOV     r5, #CMA_ROM
        BL      CMA_AddRange
        ; Finally, high processor vectors/relocated zero page
      [ ZeroPage > 0
        ASSERT  ZeroPage > ROM
        MOV     r3, r10
        LDR     r4, =16*1024
        MOV     r5, =CMA_Read
        BL      CMA_AddRange
      ]
90
        ; If there's anything else, we've wandered off into unallocated memory
        LDR     r3, =&0F0F0F0F
        BIC     r1, r0, r3
        CLRV
        EXIT

99
        PullEnv
        ADRL    r0, ErrorBlock_BadParameters
        SETV
        MOV     pc, lr

        ; Add range r3..r4 to attributes in r0
        ; Corrupts r8
CMA_AddRange ROUT ; r3 = start, r4 = length
        ADD     r4, r3, r4
CMA_AddRange2 ; r3 = start, r4 = end (excl.)
        LDR     r8, =&0F0F0F0F
        ; Increment r1 and exit if we hit r2
        ; Ignore any ranges which are entirely before us
        CMP     r1, r4
        MOVHS   pc, lr
        ; Check for any gap at the start, i.e. r3 > r1
        CMP     r3, r1
        BICHI   r0, r0, r8
        MOVHI   r1, r3 ; Update r1 for L2PT check code
        ; Exit if the range starts after our end point
        CMP     r3, r2
        BHI     %FT10
        ; Process the range
        TST     r5, #CMA_CheckL2PT
        BNE     %FT20
        CMP     r3, r4 ; Don't apply any flags for zero-length ranges
04      ; Note L2PT check code relies on NE condition here
        ORR     r8, r5, r8
        ORRNE   r0, r0, r5 ; Set new partial flags
        ANDNE   r0, r0, r8, ROR #4 ; Discard completion flags which aren't for this range
05
        CMP     r4, r2
        MOV     r1, r4 ; Continue search from the end of this range
        MOVLS   pc, lr
10
        ; We've ended inside this range
        MOV     r1, r0
        CLRV
        EXIT

20
        ; Check L2PT for sparse region r1..min(r2+1,r4)
        ; r4 guaranteed page aligned
        CMP     r3, r4
        BIC     r5, r5, #CMA_CheckL2PT
        BEQ     %BT05
        Push    "r2,r4,r5,r8,r9,r10,lr"
        LDR     lr, =&FFF
        CMP     r4, r2
        ADDHS   r2, r2, #4096
        BICHS   r2, r2, lr
        MOVLO   r2, r4
        ; r2 is now page aligned min(r2+1,r4)
        BIC     r4, r1, lr
        LDR     r8, =L2PT
        MOV     r10, #0
30
        BL      logical_to_physical
        ORRCC   r10, r10, #1
        ADD     r4, r4, #4096
        ORRCS   r10, r10, #2
        CMP     r4, r2
        BNE     %BT30
        CMP     r10, #2
        ; 01 -> entirely mapped
        ; 10 -> entirely unmapped
        ; 11 -> partially mapped
        Pull    "r2,r4,r5,r8,r9,r10,lr"
        BICHS   r0, r0, r8 ; Not fully mapped, clear completion flags
        BNE     %BT04 ; Partially/entirely mapped
        B       %BT05 ; Completely unmapped

@


4.4.2.24
log
@Fix GraphicsV_StartupMode call. Fix HiProcVecs build.
Detail:
  s/MemInfo - Fixed typo causing build error with HiProcVecs/zero page relocated kernel
  s/PMF/osinit - Fix the call to GraphicsV_StartupMode to work correctly with non-zero driver numbers
Admin:
  Tested on BB-xM with high processor vectors


Version 5.35, 4.79.2.226. Tagged as 'Kernel-5_35-4_79_2_226'
@
text
@d1499 1
a1499 1
        MOV     r5, #CMA_Read
@


4.4.2.25
log
@Fixes to HAL memory info calls and docs
MemInfo.s:
 Several places in this code called the HAL or other ATPCS defined functions like memset() and hoped that the overall result was V clear. If any of them accidentally set V (for example a CMP that straddles 0x80000000) you ended up trying to look up an international error at the address of the reason code to OS_Memory.
 Now, explicitly clear V in the non error cases where an ATPCS function was called.
 Change the HAL_PhysInfo call to expect a physical ROM size back as an inclusive range, to match the RAM range subreason code. Add 1 to correct for this. A value of 0 & 0 is taken to mean "no physical ROM" as before.
Middle.s:
 Document that 255 means "no IOMD" or "no VIDC", that's what the HALs have been using since year dot.

Version 5.35, 4.79.2.248. Tagged as 'Kernel-5_35-4_79_2_248'
@
text
@d241 1
a241 1
        BEQ     %FT75                   ; If not flush_tlb, can't have flush_cache
d246 1
a246 1
        BEQ     %FT75
d249 3
a251 2
75
        CLRV
d269 1
a269 1
        STR     r0, [sp, #Proc_RegOffset+0]
a462 1
        CLRV
a595 1
        CLRV
d665 1
a665 2
        SUBS    r1, r1, r0
        ADDNE   r1, r1, #1              ; ROM = ROMPhysTop + 1 - ROMPhysBot
a685 1
        CLRV
@


4.4.2.26
log
@Perform extra TLB maintenance on ARMv6+. Other cache/TLB maintenance tweaks.
Detail:
  s/ARMops - Implement Cache_RangeThreshold for PL310 (helps AMBControl to decide what type of TLB maintenance is best). Fix MMU_ChangingEntry_PL310 doing more work than is necessary; was attempting to flush all ways for a given address tag, when really it should have only been flushing all the lines within a page and letting the cache worry about the tags/indices they correspond to.
  s/ChangeDyn, s/VMSAv6, s/AMBControl/memmap - Do extra TLB maintenance following writes to the page tables, as mandated by the ARMv6+ memory order model. Fixes frequent crashes on Cortex-A9 when running with lazy task swapping disabled (and presumably fixes other crashes too)
  s/MemInfo - Fix OS_Memory cache/uncache so that it does cache/TLB maintenance on a per-page basis instead of a global basis. Vastly improves performance when you have a large cache, but may need tweaking again in future to do a global op if large numbers of pages are being modified.
Admin:
  Tested on Pandaboard


Version 5.35, 4.79.2.255. Tagged as 'Kernel-5_35-4_79_2_255'
@
text
@d122 3
d152 2
d227 1
d230 1
d233 4
a236 3
        MOV     r5, r0
        ASSERT  (L2PT :SHL: 12) = 0     ; Ensure we can convert r4 back to the page log addr
        MOV     r0, r4, LSL #10
d239 11
a249 9
        LDR     r3, =ZeroPage
        ADR     lr, %FT65
        ARMop   MMU_ChangingEntry,EQ,tailcall,r3         ; Clean cache & TLB
        ARMop   MMU_ChangingUncachedEntry,NE,tailcall,r3 ; Clean TLB
65
        MOV     r0, r5
        B       %BT10

70
@


4.4.2.27
log
@Expose more areas via OS_ReadSysInfo 6 & OS_Memory 16. Expose processor vectors base + size via OS_PlatformFeatures.
Detail:
  hdr/KernelWS - Define processor vectors address. Currently same as ZeroPage, but in the future will differ for some machines.
  hdr/OSRSI6, s/Middle - Expose VecPtrTab & NVECTORS via OS_ReadSysInfo items 85 & 86
  s/Kernel - Add OS_PlatformFeatures 32, for returning the base + size of the processor vectors
  s/MemInfo - Add areas 12 thru 15 to OS_Memory 16, for reporting ZeroPage, ProcVecs, DebuggerSpace and ScratchSpace. The task manager can now use these for calculating memory usage instead of assuming 32K workspace from &0-&8000.
Admin:
  Tested on Raspberry Pi


Version 5.35, 4.79.2.271. Tagged as 'Kernel-5_35-4_79_2_271'
@
text
@a1082 4
;                               12=Kernel 'ZeroPage' workspace
;                               13=Processor vectors
;                               14=DebuggerSpace
;                               15=Scratch space
a1113 4
        B       MAI_ZeroPage
        B       MAI_ProcVecs
        B       MAI_DebuggerSpace
        B       MAI_ScratchSpace
a1202 31
MAI_ZeroPage
        LDR     r1, =ZeroPage
        MOV     r2, #16*1024
        MOV     r3, #16*1024
        EXIT

MAI_ProcVecs
      [ ZeroPage != ProcVecs
        LDR     r1, =ProcVecs
        MOV     r2, #4096
        MOV     r3, #4096
      ]
        EXIT

MAI_DebuggerSpace
        ; Only report if DebuggerSpace is a standalone page. The debugger module
        ; finds DebuggerSpace via OS_ReadSysInfo 6, this call is only for the
        ; benefit of the task manager.
      [ DebuggerSpace_Size >= &1000
        LDR     r1, =DebuggerSpace
        MOV     r2, #DebuggerSpace_Size
        MOV     r3, #DebuggerSpace_Size
      ]
        EXIT

MAI_ScratchSpace
        LDR     r1, =ScratchSpace
        MOV     r2, #16*1024
        MOV     r3, #16*1024
        EXIT

a1275 1
        ASSERT  ProcVecs = ZeroPage
a1487 1
        ASSERT  ProcVecs = ZeroPage
@


4.4.2.28
log
@Improve support for VMSAv6 cache policies & memory types. Expose raw ARMops via OS_MMUControl & cache information via OS_PlatformFeatures.
Detail:
  Docs/HAL/ARMop_API - Document two new ARMops: Cache_Examine and IMB_List
  hdr/KernelWS - Shuffle workspace round a bit to allow space for the two new ARMops. IOSystemType now deleted (has been deprecated and fixed at 0 for some time)
  s/ARM600 - Cosmetic changes to BangCam to make it clearer what's going on. Add OS_MMUControl 2 (get ARMop) implementation.
  s/ARMops - Switch out different ARMop implementations and XCB tables depending on MMU model - helps reduce assembler warnings and make it clearer what code paths are and aren't possible. Add implementations of the two new ARMops. Simplify ARM_Analyse_Fancy by removing some tests which we know will have certain results. Use CCSIDR constants in ARMv7 ARMops instead of magic numbers. Update XCB table comments, and add a new table for VMSAv6
  s/ChangeDyn - Define constant for the new NCB 'idempotent' cache policy (VMSAv6 normal, non-cacheable memory)
  s/HAL - Use CCSIDR constants instead of magic numbers. Extend RISCOS_MapInIO to allow the TEX bits to be specified.
  s/Kernel - OS_PlatformFeatures 33 (read cache information) implementation (actually, just calls through to an ARMop)
  s/MemInfo - Modify VMSAv6 OS_Memory 0 cache/uncache implementation to use the XCB table instead of modifying L2_C directly. This allows the cacheability to be changed without affecting the memory type - important for e.g. unaligned accesses to work correctly. Implement cache policy support for OS_Memory 13.
  s/Middle - Remove IOSystemType from OS_ReadSysInfo 6.
  s/VMSAv6 - Make sure BangCam uses the XCB table for working out the attributes of temp-uncacheable pages instead of manipulating L2_C directly. Add OS_MMUControl 2 implementation.
  s/AMBControl/memmap - Update VMSAv6 page table pokeing to use XCB table
  s/PMF/osinit - Remove IOSystemType reference, and switch out some pre-HAL code that was trying to use IOSystemType.
Admin:
  Tested on Iyonix, ARM11, Cortex-A7, -A8, -A9, -A15
  Note that contrary to the comments in the source the default NCB policy currently maps to VMSAv6 Device memory type (as per previous kernel versions). This is just a temporary measure, and it will be switched over to Normal, non-cacheable once appropriate memory barriers have been added to the affected IO code.


Version 5.35, 4.79.2.273. Tagged as 'Kernel-5_35-4_79_2_273'
@
text
@a219 1
        LDR     r3, =ZeroPage
a220 17
 [ MEMM_Type = "VMSAv6"
        ; VMSAv6 is hard, use XCBTable/PCBTrans
        ASSERT  DynAreaFlags_CPBits = 7*XCB_P :SHL: 10
        ASSERT  DynAreaFlags_NotCacheable = XCB_NC :SHL: 4
        ASSERT  DynAreaFlags_NotBufferable = XCB_NB :SHL: 4
        TST     r0, #cacheable_bit      ; n.b. must match EQ/NE used by ARMop calls
        AND     lr, r5, #DynAreaFlags_NotCacheable + DynAreaFlags_NotBufferable
        AND     r5, r5, #DynAreaFlags_CPBits
        ORR     lr, lr, r5, LSR #10-4
        LDR     r5, [r3, #MMU_PCBTrans]
        ORREQ   lr, lr, #XCB_TU<<4      ; if temp uncache, set TU bit
        LDRB    lr, [r5, lr, LSR #4]    ; convert to X, C and B bits for this CPU
        LDR     r5, [r4]                ; Get L2 entry (safe as we know address is valid).
        BIC     r5, r5, #(L2_C+L2_B+L2_TEX) :AND: 255 ; Knock out existing attributes (n.b. assumed to not be large page!)
        ORR     r5, r5, lr              ; Set new attributes
        STR     r5, [r4]                ; Write back new L2 entry.
 |
a225 1
 ]
d227 1
a227 1
        ASSERT  (L2PT :SHL: 10) = 0     ; Ensure we can convert r4 back to the page log addr
d231 1
d978 1
a978 1
;               r0 bits 10..12 = cache policy
d994 5
a998 17
        LDR     r12, =ZeroPage
        ASSERT  XCB_NB = 1:SHL:0
        ASSERT  XCB_NC = 1:SHL:1
        ASSERT  XCB_P = 1:SHL:2
        AND     r0, r0, #&1F00
        MOV     r0, r0, LSR #8
        LDR     r12, [r12, #MMU_PCBTrans]
        EOR     r0, r0, #XCB_NB+XCB_NC ; Invert C+B to match XCBTable
        LDRB    r0, [r12, r0]
        ; Convert from L2 attributes to L1
        ASSERT  L1_C = L2_C
        ASSERT  L1_B = L2_B
        ASSERT  L2_TEXShift < L1_TEXShift
        AND     r12, r0, #L2_TEX
        BIC     r0, r0, #L2_TEX
        ORR     r0, r0, r12, LSL #L1_TEXShift-L2_TEXShift
        ; Deal with other flags
@


4.4.2.29
log
@Add initial support for "physical memory pools"
Detail:
  This set of changes adds support for "physical memory pools" (aka PMPs), a new type of dynamic area which allow physical pages to be claimed/allocated without mapping them in to the logical address space. PMPs have full control over which physical pages they use (similar to DAs which request specific physical pages), and also have full control over the logical mapping of their pages (which pages go where, and per-page access/cacheability control).
  Currently the OS makes use of two PMPs: one for the free pool (which now has a logical size of zero - freeing up gigabytes of logical space), and one for the RAM disc (logical size of 1MB, allowing for a physical size limited only by the amount of free memory)
  Implementing these changes has required a number of other changes to be made:
  * The CAM has been expanded from 8 bytes per entry to 16 bytes per entry, in order to allow each RAM page to store information about its PMP association
  * The system heap has been expanded to 32MB in size (from just under 4MB), in order to allow it to be used to store PMP page lists (1 word needed per page, but PMP pages may not always have physical pages assigned to them - so to allow multiple large PMPs to exist we need more than just 1 word per RAM page)
  * The &FA000000-&FBFFFFFF area of fixed kernel workspace has been shuffled around to accomodate the larger CAM, and the system heap is now located just above the RMA.
  * SoftResets code stripped out (unlikely we'll ever want to fix and re-enable it)
  * A couple of FastCDA options are now permanently on
  * Internal page flags shuffled around a bit. PageFlags_Unavailable now publicly exposed so that PMP clients can lock/unlock pages at will.
  * When OS_ChangeDynamicArea is asked to grow or shrink the free pool, it now implicitly converts it into a shrink or grow of application space (which is what would happen anyway). This simplifies the implementation; during a grow, pages (or replacement pages) are always sourced from the free pool, and during a shrink pages are always sent to the free pool.
  File changes:
  - hdr/KernelWS - Extend DANode structure. Describe CAM format. Adjust kernel workspace.
  - hdr/OSRSI6, s/Middle - Add new item to expose the CAM format
  - hdr/Options - Remove SoftResets switch. Add some PMP switches.
  - s/ARM600, s/VMSAv6 - Updated for new CAM format. Note that although the CAM stores PMP information, BangCamUpdate currently doesn't deal with updating that data - it's the caller's responsibility to do so where appropriate.
  - s/ChangeDyn - Lots of changes to implement PMP support, and to cope with the new CAM format.
  - s/HAL - Updated to cope with new CAM format, and lack of logical mapping of free pool.
  - s/MemInfo - Updated to cope with new CAM format. OS_Memory 0 updated to cope with converting PPN to PA for pages which are mapped out. OS_Memory 24 updated to decode the access permissions on a per-page basis for PMPs, and fixed its HWM usage for sparse DAs.
  - s/NewReset - Soft reset code and unused AddCamEntries function removed. Updated to cope with new CAM format, PMP free pool, PMP RAMFS
  - s/AMBControl/allocate - Update comment (RMA hasn't been used for AMBControl nodes for a long time)
  - s/AMBControl/growp, s/AMBControl/memmap, s/AMBControl/shrinkp - Update for new CAM format + PMP free pool
  - s/vdu/vdudriver - Strip out soft reset code.
Admin:
  Tested on Pandaboard
  This is just a first iteration of the PMP feature, with any luck future changes will improve functionality. This means APIs are subject to change as well.


Version 5.35, 4.79.2.284. Tagged as 'Kernel-5_35-4_79_2_284'
@
text
@d161 3
a163 6
        TST     r0, #logical,given      ; If LA given (rotate clears C) then
        ADR     lr, %FT15
        BNE     logical_to_physical     ; Get PA from LA
        BL      ppn_to_logical          ; Else get LA from PN (PA wanted (not given) & LA not given => PN given).
        BLCC    ppn_to_physical         ; And get PA from PN (more accurate than getting PA from LA - page may be mapped out)
15
d198 1
a198 3
        ADD     r3, r6, r3, LSL #CAM_EntrySizeLog2 ; Point to CAM entry for this page.
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
d216 1
a216 1
        STR     r5, [r3, #CAM_PageFlags] ; Write back new PPL.
d336 1
a336 2
        ASSERT  CAM_LogAddr=0
        LDR     r4, [r6, r3, LSL #CAM_EntrySizeLog2] ; If valid then lookup logical address.
a432 28
;----------------------------------------------------------------------------------------
; ppn_to_physical
;
;       In:     r3 = page number
;
;       Out:    r9 corrupted
;               CC => r5 = physical address
;               CS => invalid page number, r5 corrupted
;
;       Convert physical page number to physical address.
;
ppn_to_physical ROUT
        Push    "r3,lr"
        LDR     r9, =ZeroPage+PhysRamTable
10
        LDMIA   r9!, {r5,lr}            ; Get start address and size of next block.
        MOVS    lr, lr, LSR #12
        BEQ     %FT20
        CMP     r3, lr
        SUBHS   r3, r3, lr
        BHS     %BT10

        ADD     r5, r5, r3, LSL #12
        Pull    "r3,pc"
20
        SEC
        Pull    "r3,pc"

d508 1
a508 1
        ADD     r7, r7, #CAM_PageFlags  ; Point to PPL entries.
d560 1
a560 1
        LDR     lr, [r7, r5, LSL #CAM_EntrySizeLog2] ; Page is there so get PPL and determine if it's available or not.
d924 1
a924 1
        ADD     r5,r5,#CAM_PageFlags ; [r5,<page no.>,LSL #3] addresses flags word in CAM
d951 1
a951 1
        LDR     r6,[r5,r7,LSL #CAM_EntrySizeLog2] ;page flags from CAM
d966 1
a966 1
        LDR     r6,[r5,r10,LSL #CAM_EntrySizeLog2] ;page flags from CAM
a1311 1
CMA_DecodeAP           * 1<<30 ; Used with CheckL2PT to indicate AP flags should be decoded from L2PT
d1335 1
a1335 1
        ; * special areas (stacks, ROM, HAL workspace, etc.)
a1385 1
        TEQ     r6, #0 ; We can get called during ROM init, before the workspace is allocated (pesky OS_Heap validating its pointers)
a1386 1
        LDREQ   r9, [r10, #DAList] ; So just start at the first DA
a1387 1
        BEQ     %FT22
d1403 2
a1404 2
        ; the entries until we hit the end of the list, or any DAs above
        ; IOAllocLimit
d1417 1
a1417 3
        TST     r6, #DynAreaFlags_PMP
        ORRNE   r5, r5, #CMA_DecodeAP
        TSTEQ   r6, #DynAreaFlags_SparseMap
d1419 1
a1419 1
        LDRNE   r4, [r9, #DANode_SparseHWM] ; Use HWM as bounds when checking sparse/PMP areas
d1421 2
a1422 2
        ADDEQ   r4, r3, lr
        TST     r6, #DynAreaFlags_DoublyMapped ; Currently impossible for Sparse/PMP areas - so use of lr safe
d1428 1
a1428 1
        ; Hit the end of the list
d1524 6
a1529 1
        ASSERT  L2PT > HALWorkspaceNCNB
d1540 6
a1545 4
        ASSERT  CursorChunkAddress > L1PT
        LDR     r3, =CursorChunkAddress
        MOV     r4, #32*1024
        MOV     r5, #CMA_Read
d1547 1
a1547 1
        ASSERT  CAM > CursorChunkAddress
d1627 1
a1628 3
        TST     r5, #CMA_DecodeAP
        BIC     r4, r1, lr
        BNE     %FT35
a1645 31
35
        ; Check L2PT, with AP decoding on a per-page basis
40
        LDR     r10, =&0F0F0F0F
        BL      logical_to_physical
        BICCS   r0, r0, r10 ; Not fully mapped, clear completion flags
        BCS     %FT45
        ; Get the L2PT entry and decode the flags
        LDR     r9, [r8, r4, LSR #10]
      [ MEMM_Type = "VMSAv6"
        AND     r9, r9, #L2_AP
      |
        AND     r9, r9, #L2X_AP
      ]
        MOV     r5, #CMA_ROM
        CMP     r9, #AP_None*L2X_APMult
        MOVEQ   r5, #CMA_None
        CMP     r9, #AP_Read*L2X_APMult
        MOVEQ   r5, #CMA_Read
        CMP     r9, #AP_Full*L2X_APMult
        MOVEQ   r5, #CMA_Full
        ORR     r10, r5, r10
        ORR     r0, r0, r5 ; Set new partial flags
        AND     r0, r0, r10, ROR #4 ; Discard completion flags which aren't for this range
45
        ADD     r4, r4, #4096
        CMP     r4, r2
        BNE     %BT40
        Pull    "r2,r4,r5,r8,r9,r10,lr"
        B       %BT05

@


4.4.2.30
log
@Remove OS_Memory 10 and associated code
Detail:
  s/MemInfo - Remove OS_Memory 10 (free pool locking). Locking the free pool has never been a very nice thing to do, so now that there's no logical mapping of the free pool it seems like it's a good time to outlaw the behaviour altogether.
  s/ChangeDyn - No free pool locking means one less thing to check when claiming the OS_ChangeDynamicArea mutex.
  hdr/KernelWS - VRAMRescue_control workspace variable is no longer needed
Admin:
  Tested on Pandaboard


Version 5.35, 4.79.2.285. Tagged as 'Kernel-5_35-4_79_2_285'
@
text
@d63 1
a63 1
        B       %BT20                           ; Reason code 10 reserved (for free pool locking)
d871 45
a915 1
; MemoryFreePoolLock - removed now that free pool is a PMP
@


4.4.2.31
log
@Cache maintenance fixes
Detail:
  This set of changes tackles two main issues:
  * Before mapping out a cacheable page or making it uncacheable, the OS performs a cache clean+invalidate op. However this leaves a small window where data may be fetched back into the cache, either accidentally (dodgy interrupt handler) or via agressive prefetch (as allowed for by the architecture). This rogue data can then result in coherency issues once the pages are mapped out or made uncacheable a short time later.
    The fix for this is to make the page uncacheable before performing the cache maintenance (although this isn't ideal, as prior to ARMv7 it's implementation defined whether address-based cache maintenance ops affect uncacheable pages or not - and on ARM11 it seems that they don't, so for that CPU we currently force a full cache clean instead)
  * Modern ARMs generally ignore unexpected cache hits, so there's an interrupt hole in the current OS_Memory 0 "make temporarily uncacheable" implementation where the cache is being flushed after the page has been made uncacheable (consider the case of a page that's being used by an interrupt handler, but the page is being made uncacheable so it can also be used by DMA). As well as affecting ARMv7+ devices this was found to affect XScale (and ARM11, although untested for this issue, would have presumably suffered from the "can't clean uncacheable pages" limitation)
    The fix for this is to disable IRQs around the uncache sequence - however FIQs are currently not being dealt with, so there's still a potential issue there.
  File changes:
  - Docs/HAL/ARMop_API, hdr/KernelWS, hdr/OSMisc - Add new Cache_CleanInvalidateRange ARMop
  - s/ARM600, s/VMSAv6 - BangCam updated to make the page uncacheable prior to flushing the cache. Add GetTempUncache macro to help with calculating the page flags required for making pages uncacheable. Fix abort in OS_MMUControl on Raspberry Pi - MCR-based ISB was resetting ZeroPage pointer to 0
  - s/ARMops - Cache_CleanInvalidateRange implementations. PL310 MMU_ChangingEntry/MMU_ChangingEntries refactored to rely on Cache_CleanInvalidateRange_PL310, which should be a more optimal implementation of the cache cleaning code that was previously in MMU_ChangingEntry_PL310.
  - s/ChangeDyn - Rename FastCDA_UpFront to FastCDA_Bulk, since the cache maintenance is no longer performed upfront. CheckCacheabilityR0ByMinusR2 now becomes RemoveCacheabilityR0ByMinusR2. PMP LogOp implementation refactored quite a bit to perform cache/TLB maintenance after making page table changes instead of before. One flaw with this new implementation is that mapping out large areas of cacheable pages will result in multiple full cache cleans while the old implementation would have (generally) only performed one - a two-pass approach over the page list would be needed to solve this.
  - s/GetAll - Change file ordering so GetTempUncache macro is available earlier
  - s/HAL - ROM decompression changed to do full MMU_Changing instead of MMU_ChangingEntries, to make sure earlier cached data is truly gone from the cache. ClearPhysRAM changed to make page uncacheable before flushing cache.
  - s/MemInfo - OS_Memory 0 interrupt hole fix
  - s/AMBControl/memmap - AMB_movepagesout_L2PT now split into cacheable+non-cacheable variants. Sparse map out operation now does two passes through the page list so that they can all be made uncacheable prior to the cache flush + map out.
Admin:
  Tested on StrongARM, XScale, ARM11, Cortex-A7, Cortex-A9, Cortex-A15, Cortex-A53
  Appears to fix the major issues plaguing SATA on IGEPv5


Version 5.35, 4.79.2.306. Tagged as 'Kernel-5_35-4_79_2_306'
@
text
@d242 1
d248 1
a249 29
        BNE     %FT63
        ; Making page non-cacheable
        ; There's a potential interrupt hole here - many ARMs ignore cache hits
        ; for pages which are marked as non-cacheable (seen on XScale,
        ; Cortex-A53, Cortex-A15 to name but a few, and documented in many TRMs)
        ; We can't be certain that this page isn't being used by an interrupt
        ; handler, so if we're making it non-cacheable we have to take the safe
        ; route of disabling interrupts around the operation.
        ; Note - currently no consideration is given to FIQ handlers.
        ; Note - we clean the cache as the last step (as opposed to doing it at
        ; the start) to make sure prefetching doesn't pull data back into the
        ; cache.
        PHPSEI  r11                     ; IRQs off
        STR     r5, [r4]                ; Write back new L2 entry.
        MOV     r5, r0
        ASSERT  (L2PT :SHL: 10) = 0     ; Ensure we can convert r4 back to the page log addr
        MOV     r0, r4, LSL #10
        ARMop   MMU_ChangingUncachedEntry,,,r3   ; Clean TLB
        MOV     r0, r4, LSL #10
        MOV     r10, r1
        ADD     r1, r0, #4096
        ARMop   Cache_CleanInvalidateRange,,,r3  ; Clean page from cache
        PLP     r11                     ; IRQs back on again
        MOV     r1, r10
        B       %FT65
63
        ; Making page cacheable again
        ; Shouldn't be any cache maintenance worries
        STR     r5, [r4]                ; Write back new L2 entry.
d253 5
a257 1
        ARMop   MMU_ChangingUncachedEntry,,,r3   ; Clean TLB
@


4.4.2.32
log
@Fix crash when making SVC stack uncacheable. Fix poor Pi 3 memory benchmark performance
Detail:
  s/MemInfo - To avoid cache coherency issues when the current SVC stack page is being made uncacheable, shift SP somewhere else by temporarily dropping into IRQ mode
  s/ARMops - Change default VMSAv6 cache policy to writeback, write allocate. Unlike other CPUs we've supported so far, Cortex-A53 suffers very badly from writes to read-allocate pages, with performance being roughly equivalent to writes to non-cacheable memory. Using a write (+read) allocate policy seems to be needed to get the expected performance, and may help boost other CPUs too.
Admin:
  Tested on IGEPv5, Pi 3


Version 5.35, 4.79.2.307. Tagged as 'Kernel-5_35-4_79_2_307'
@
text
@d260 1
a260 13
        MRS     r11, CPSR
        ORR     lr, r11, #I32_bit       ; IRQs off
        ; Yuck, we also need to deal with the case where we're making the
        ; current SVC stack page uncacheable (coherency issue when calling the
        ; ARMops if cache hits to uncacheable pages are ignored). Deal with this
        ; by temporarily dropping into IRQ mode (and thus a different stack) if
        ; we think this is going to happen.
        MOV     r10, r4, LSL #10
        SUB     r10, sp, r10
        CMP     r10, #8192              ; Be extra cautious
        EORLO   lr, lr, #SVC32_mode :EOR: IRQ32_mode
        MSR     CPSR_c, lr              ; Switch mode
        Push    "r0, lr"                ; Preserve OS_Memory flags and (potential) IRQ lr
d262 1
d270 1
a270 2
        Pull    "r5, lr"                ; Restore OS_Memory flags + IRQ lr
        MSR     CPSR_c, r11             ; Back to original mode + IRQ state
@


4.4.2.33
log
@Improve safety of OS_Memory 0 "make temporarily uncacheable" and *Cache off
Detail:
  s/MemInfo - Wrap OS_Memory 0 in some code which will temporarily claim the FIQ vector when making pages temporarily uncacheable, to avoid any issues caused by modern ARMs ignoring unexpected cache hits
  s/VMSAv6 - Claim FIQs when OS_MMUControl is asked to make a change to the SCTLR, to avoid similar issues on modern ARMs. Also make the stack temporarily uncacheable before disabling the cache, so that we don't run into any problems using the stack inbetween disabling the cache and completing the clean+invalidate.
Admin:
  Tested on Pi 2B, 3B
  *Cache off now works reliably on Pi 2B, although there is sometimes a pause of a few seconds while things sort themselves out (USB?)
  *Cache off "works" on Pi 3B but everything will fall over soon afterwards due to the Cortex-A53 not supporting LDREX/STREX to non-cacheable pages (or when the page is effectively non-cacheable, i.e. cacheable page with cache disabled)


Version 5.35, 4.79.2.311. Tagged as 'Kernel-5_35-4_79_2_311'
@
text
@d53 1
a53 1
        B       MemoryConvertFIQCheck           ; 0
d123 1
a123 20
; Small wrapper to make sure FIQs are disabled if we're making pages uncacheable
; (Modern ARMs ignore unexpected cache hits, so big coherency issues if we make
; a page uncacheable which is being used by FIQ).
MemoryConvertFIQCheck ROUT
        AND     r11, r0, #3:SHL:14
        TEQ     r11, #2:SHL:14
        BNE     MemoryConvertNoFIQCheck
        Entry   "r0-r1"
        MOV     r1, #Service_ClaimFIQ
        SWI     XOS_ServiceCall
        LDMIA   sp, {r0-r1}
        BL      MemoryConvertNoFIQCheck
        FRAMSTR r0
        MRS     r11, CPSR
        MOV     r1, #Service_ReleaseFIQ
        SWI     XOS_ServiceCall
        MSR     CPSR_c, r11
        EXIT

MemoryConvertNoFIQCheck   ROUT
d312 1
a312 1
        BLNE    MemoryConvertNoFIQCheck
@


4.4.2.34
log
@Add new OS_PlatformFeatures reason code for reading CPU features (inspired by ARMv6+ CPUID scheme). Add OS_ReadSysInfo 8 flags for indicating the alignment mode the ROM was built with. Fix long-standing bug with OS_PlatformFeatures when an unknown reason code is used.
Detail:
  s/CPUFeatures, hdr/OSMisc, hdr/KernelWS - Code and definitions for reading CPU features and reporting them via OS_PlatformFeatures 34. All the instruction set features which are exposed by the CPUID scheme and which are relevant to RISC OS are exposed, along with a few extra flags which we derive ourselves (e.g. things relating to < ARMv4, and some register usage restrictions in instructions). s/CPUFeatures is designed to be easily copyable into a future version of CallASWI without requiring any changes.
  s/ARMops - Read and cache CPU features during ARMop initialisation
  s/GetAll - GET new file
  s/Kernel - Hook up the CPU features code to OS_PlatformFeatures. Fix a long standing stack imbalance bug (fixed in RISC OS 3.8, but never merged back to our main branch) which meant that calling OS_PlatformFeatures with an invalid reason code would raise an error, even if it was the X form of the SWI that was called. Similar fix also applied to the unused service call code, along with a fix for the user's R1-R9 being corrupt (shuffled up one place) should an error have been generated.
  s/MemInfo - Extra LTORG needed to keep things happy
  s/Middle - Extend OS_ReadSysInfo 8 to include flags for indicating what memory alignment mode (if any) the OS relies upon. Together with OS_PlatformFeatures 34 this could e.g. be used by !CPUSetup to determine which options should be offered to the user.
Admin:
  Tested on Raspberry Pi 1, 2, 3


Version 5.35, 4.79.2.319. Tagged as 'Kernel-5_35-4_79_2_319'
@
text
@a1721 2
        LTORG

@


4.4.2.14.2.1
log
@  Add support for Cortex cache type. Extend ARM_Analyse to, where appropriate, use CPU feature registers to identify CPU capabilities.
Detail:
  s/ARMops - Support for Cortex multi-level cache (CT_ctype_WB_CR7_Lx). New ARM_Analyse_Fancy to identify CPU capabilities using feature registers.
  s/HAL - Modify pre-ARMop cache code to handle Cortex-syle caches.
  s/MemInfo - Replace ARM_flush_TLB macro call with appropriate ARMop to provide Cortex compatability
  hdr/ARMops - Update list of ARM architectures
  hdr/CoPro15ops - Deprecate ARM_flush_* macros for HAL kernels, as they are no longer capable of flushing all cache types. ARMops should be used instead.
  hdr/KernelWS - Add storage space for multi-level cache properties required for new cache cleaning code.
Admin:
  Tested under qemu-omap3. Still unable to verify on real hardware due to lack of appropriate MMU code. However new OMAP3 HAL code that uses similar cache management functions appears to work fine on real hardware.


Version 5.35, 4.79.2.98.2.2. Tagged as 'Kernel-5_35-4_79_2_98_2_2'
@
text
@a874 6
 [ HAL
        Push    "r0"
        MOV     r0,#ZeroPage
        ARMop   TLB_InvalidateAll,,,r0
        Pull    "r0"
 |
a875 1
 ]
@


4.4.2.14.2.2
log
@Bring Cortex kernel branch in line with HAL branch
Detail:
  Makefile - Now uses ${PERL} for running perl
  s/Kernel - Now uses correct "Bad OS_PlatformFeatures reason code" error number
  s/MemInfo - Updated list of OS_Memory 9 controllers
Admin:
  OMAP3 ROM compiles OK; untested at runtime


Version 5.35, 4.79.2.98.2.43. Tagged as 'Kernel-5_35-4_79_2_98_2_43'
@
text
@a786 6
;                               6 = Tube ULA
;                               7-31 = Reserved (for us)
;                               32 = Primary ROM
;                               33 = IOMD
;                               34 = FDC37C665/SMC37C665/82C710/SuperIO/whatever
;                               35+ = Reserved (for ROL)
@


4.4.2.14.2.3
log
@Add zero page relocation support
Detail:
  A whole mass of changes to add high processor vectors + zero page relocation support to the Cortex branch of the kernel
  At the moment the code can only cope with two ZeroPage locations, &0 and &FFFF0000. But with a bit more tweaking those restrictions can probably be lifted, allowing ZeroPage to be hidden at almost any address (assuming it's fixed at compile time). If I've done my job right, these restrictions should all be enforced by asserts.
  There's a new option, HiProcVecs, in hdr/Options to control whether high processor vectors are used. When enabling it and building a ROM, remember:
  * FPEmulator needs to be built with the FPEAnchor=High option specified in the components file (not FPEAnchorType=High as my FPEmulator commit comments suggested)
  * ShareFS needs unplugging/removing since it can't cope with it yet
  * Iyonix users will need to use the latest ROOL boot sequence, to ensure the softloaded modules are compatible (OMAP, etc. don't really softload much so they're OK with older sequences)
  * However VProtect also needs patching to fix a nasty bug there - http://www.riscosopen.org/tracker/tickets/294
  The only other notable thing I can think of is that the ProcessTransfer code in s/ARM600 & s/VMSAv6 is disabled if high processor vectors are in use (it's fairly safe to say that code is obsolete in HAL builds anyway?)
  Fun challenge for my successor: Try setting ZeroPage to &FFFF00FF (or similar) so its value can be loaded with MVN instead of LDR. Then use positive/negative address offsets to access the contents.
  File changes:
  - hdr/ARMops - Modified ARMop macro to take the ZeroPage pointer as a parameter instead of 'zero'
  - hdr/Copro15ops - Corrected $quick handling in myISB macro
  - hdr/Options - Added ideal setting for us to use for HiProcVecs
  - s/AMBControl/allocate, s/AMBControl/growp, s/AMBControl/mapslot, s/AMBControl/memmap, s/AMBControl/service, s/AMBControl/shrinkp, s/Arthur2, s/Arthur3, s/ArthurSWIs, s/ChangeDyn, s/ExtraSWIs, s/HAL, s/HeapMan, s/Kernel, s/MemInfo, s/Middle, s/ModHand, s/MoreSWIs, s/MsgCode, s/NewIRQs, s/NewReset, s/Oscli, s/PMF/buffer, s/PMF/IIC, s/PMF/i2cutils, s/PMF/key, s/PMF/mouse, s/PMF/osbyte, s/PMF/oseven, s/PMF/osinit, s/PMF/osword, s/PMF/oswrch, s/SWINaming, s/Super1, s/SysComms, s/TickEvents, s/Utility, s/vdu/vdu23, s/vdu/vdudriver, s/vdu/vdugrafl, s/vdu/vdugrafv, s/vdu/vdupalxx, s/vdu/vdupointer, s/vdu/vduswis, s/vdu/vduwrch - Lots of updates to deal with zero page relocation
  - s/ARM600 - UseProcessTransfer option. Zero page relocation support. Deleted pre-HAL ClearPhysRAM code to tidy the file up a bit.
  - s/ARMops - Zero page relocation support. Set CPUFlag_HiProcVecs when high vectors are in use.
  - s/KbdResPC - Disable compilation of dead code
  - s/VMSAv6 - UseProcessTransfer option. Zero page relocation support.
Admin:
  Tested with OMAP & Iyonix ROM softloads, both with high & low zero page.
  High zero page hasn't had extensive testing, but boot sequence + ROM apps seem to work.


Version 5.35, 4.79.2.98.2.48. Tagged as 'Kernel-5_35-4_79_2_98_2_48'
@
text
@d139 1
a139 1
        LDR     r6, =ZeroPage
d234 1
a234 1
        LDR     r0, =ZeroPage
d238 1
a238 1
        LDR     r0, =ZeroPage
d287 1
a287 1
        LDR     r14, =ZeroPage
d395 1
a395 1
        LDR     r9, =ZeroPage+PhysRamTable
d492 2
a493 2
        LDR     r6, =ZeroPage+PhysRamTable
        LDR     r7, [r6, #CamEntriesPointer-PhysRamTable]
d495 1
a495 1
        LDR     r8, [r6, #MaxCamEntry-PhysRamTable]
d548 1
a548 1
        LDR     r0, =ZeroPage
d875 1
a875 1
        LDR     r3,=ZeroPage
d883 2
a884 1
        ARMop   TLB_InvalidateAll,,,r3
d936 1
a936 1
        LDR     r0,=ZeroPage+PhysRamTable
d938 1
a938 1
        LDR     r5,=ZeroPage+CamEntriesPointer
d1187 1
a1187 1
        LDR     r0, =ZeroPage
d1200 1
a1200 1
        LDR     r0, =ZeroPage
d1208 1
a1208 1
        LDR     r0, =ZeroPage
d1217 1
a1217 1
        LDR     r0, =ZeroPage
@


4.3
log
@Up to 16M of ROM now mapped in from 03800000-04800000.
Video memory now limited to 8M (instead of 16M) to make room.
OS_Memory 7 now reports ROM correctly when the image is >4M.

Version 4.76. Tagged as 'Kernel-4_76'
@
text
@d250 3
d254 1
d261 4
d266 1
d288 3
d292 1
d297 3
d301 1
d307 4
d312 1
d314 3
d335 3
d339 1
d348 4
d353 1
@


4.2
log
@Kernel merged
@
text
@d122 1
a122 1
        EOR     lr, lr, #all,wanted   
d369 2
a370 3
 [ OSROM_ImageSize = 4096
        ; &00000000 to &00400000 is ROM.
        MOV     r2, #(&00400000-&00000000) :SHR: WordShift
d374 2
a375 2
        ; &00400000 to &02000000 is allocated to ROM but is not present.
        MOV     r2, #(&02000000-&00400000) :SHR: WordShift
a377 11
 |
        ; &00000000 to &00200000 is ROM.
        MOV     r2, #(&00200000-&00000000) :SHR: WordShift
        LDR     r3, =ROM_Pattern :OR: NotAvailable
        BL      fill_words

        ; &00200000 to &02000000 is allocated to ROM but is not present.
        MOV     r2, #(&02000000-&00200000) :SHR: WordShift
        LDR     r3, =NotPresent :OR: NotAvailable
        BL      fill_words
 ]
@


4.2.2.1
log
@ 1 Simplify source by removing various long-standing compile flags
   and pre-Medusa h/w support

 2 Fix bug with Pages_Unsafe/Pages_Safe page moving for StrongARM
   (interrupt hole) - also better performance for StrongARM

 3 Improve perfromance of physical memory clear for StrongARM
   (make sure it uses burst write for STM)

 4 Suspend Chocolate task switching for StrongARM if SALDMIBbroken
   is TRUE
@
text
@d45 1
d47 1
d588 2
d595 2
@


4.2.2.2
log
@1) Fixes and tidy ups:
   - mapping of Cur/Sys/Sound area done more elegantly, and soft CAM info
     is now consistent with it
   - cached screen cleaning on VSync performed *after* VSync events
   - comments at top of ARM600 modernised
   - Pages_Unsafe/Safe code fixed to work properly on StrongARM with
     pages that are involved in interrupts (there is no fix for ARM8,
     since that is unlikely to be needed - an ASSERT checks use of ARM8
   - OS_DynamicArea code souped up, to be much more efficient for large
     numbers of dynamic areas (see comments near top of ChangeDyn)
   - cached screen is now suspended on h/w scroll (avoids possible cache
     incoherency)
2) API changes:
   - new OS_Memory reason code (10) allows Wimp to inform kernel of
     Wimp_ClaimFreeMemory, and can control VRAM rescue (see below)
   - new OS_ReadSysInfo reason code (6) allows reading of kernel values
     (reserved for Acorn use, eg. for SoftLoad, ROMPatch)
   - new OS_DynamicArea reason codes (6 and 7) allow for more efficient
     monitoring of dynamic areas by TaskManager (reserved for Acorn use)
3) Changes for Phoebe:
   - kernel runs a VRAM rescue process, which ensures that any VRAM not
     used for the screen is reclaimed if necessary and sinks to the bottom
     of the Free Pool. This is important for Phoebe, where VRAM is slower
     than SDRAM, but does no harm on other platforms.
   - logical copy of physical RAM is removed from memory map. This frees
     up 256M of address space that will later be used for PCI on Phoebe,
     but should do no harm on other platforms (this space is marked
     private in PRMs, so 3rd parties should not use it).
@
text
@a60 1
        B       MemoryFreePoolLock
d349 1
a349 1
        MOV     r1, #PhysSpaceSize_Full :SHR: ByteShift
a604 38

;----------------------------------------------------------------------------------------
; MemoryFreePoolLock
;
;       In:     r0 bits 0..7  = 10 (reason code 10)
;               r0 bit  8     = 1 if call is by Wimp (reserved for Acorn use), 0 otherwise
;               r0 bits 9..31 reserved (must be 0)
;               r1 = 0  to resume any background actions that may manipulate FreePool mapping
;                       (currently, this is just VRAMRescue)
;                  = 1  to suspend any background actions that may manipulate FreePool mapping
;                  all other values reserved (undefined behaviour)
;
;       Out:    r1 = previous state (1=suspended, 0=not suspended)
;
; A suspend call from the Wimp will be taken to mean that the whole FreePool is locked, and
; memory cannot be moved (supports Wimp_ClaimFreeMemory)
;
MemoryFreePoolLock ROUT
        Push    "r2,r3,lr"
        MOV     r3,#0
        LDRB    lr,[r3,#VRAMRescue_control]
        TEQ     r1,#0
        BNE     %FT10
;flush TLB(s) for resume - needed for Wimp resume, precaution for other - no cache flush
;since Free Pool is uncached
        ARM_flush_TLB r2                    
10
        TST     r0,#&100                    ;is it Wimp?
        MOVEQ   r2,#VRRc_suspend
        MOVNE   r2,#VRRc_wimp_lock
        TEQ     r1,#0
        AND     r1,lr,r2                    ;previous state
        BICEQ   lr,lr,r2
        ORRNE   lr,lr,r2
        STRB    lr,[r3,#VRAMRescue_control] ;set/clear relevant bit
        TEQ     r1,#0
        MOVNE   r1,#1
        Pull    "r2,r3,pc"
@


4.2.2.2.2.1
log
@Changed compile switches, to build Ursula kernel for RPC and A7000(+),
switches now set as follows:
  ARM67Support      TRUE  (for 610,710,7500,7500FE)
  ARMSASupport      TRUE  (for StrongARM)
  ARMSASupport_RevS FALSE (for StrongARMs before rev S)
  IOMD1Support      TRUE  (for old machines)
  IOMD2Support      FALSE (They killed Phoebe!)
Version set to 4.00 (RISC OS 4)
This is the same as my last commit to the Ursula branch
@
text
@a61 2
        B       PCImapping
        B       RecommendPage
d338 1
d350 1
a350 13
        MOV     r1, #IOMD_Base
        LDRB    r1, [r1,#IOMD_ID0]
        TEQ     r1, #IOMD_Original :AND: &FF
        TEQNE   r1, #IOMD_7500     :AND: &FF
        TEQNE   r1, #IOMD_7500FE   :AND: &FF
        MOVEQ   r1, #PhysSpaceSize_Full  :SHR: ByteShift  ;IOMD1
        BEQ     MPS_return
        TEQ     r1, #IOMD_IOMD2    :AND: &FF
        MOVEQ   r1, #PhysSpaceSize_IOMD2 :SHR: ByteShift  ;IOMD2
        BEQ     MPS_return
        B       MemoryPhysSize     ;deliberate panic hang up
;
MPS_return
d390 1
a390 1
        ; &02000000 to &02400000 is VRAM or not present.
d461 6
a466 7
        BEQ     %FT90
        MOV     r3, r3, LSR r4                  ;   put bits in correct position
        ADD     r2, r2, r4, LSL #BitShift       ;   adjust current address
        RSB     r4, r4, #32                     ;   rest of word is not present
        LDR     lr, =NotPresent :OR: NotAvailable
        ORR     r3, r3, lr, LSL r4
        STR     r3, [r1], #4                    ;   and store word.
d468 2
a469 10
        ; End of last block of DRAM to &20000000 (IOMD1) is not present.
        ; End of last block of DRAM to &30000000 (IOMD2) is not present.
90
        Push    "r0"
        MOV     r0, #IOMD_Base
        LDRB    r0, [r0, #IOMD_ID0]
        TEQ     r0, #IOMD_IOMD2 :AND: &FF       ;we just want to distinguish IOMD2 here
        RSBNES  r2, r2, #&20000000
        RSBEQS  r2, r2, #&30000000
        Pull    "r0"
d490 1
a490 1
;                       8-11    1=return amount of DRAM (excludes any soft ROM)
a493 1
;                               5=return amount of soft ROM (ROM loaded into hidden DRAM)
a506 13
        TEQ     lr, #5:SHL:8            ; Check for soft ROM
        BNE     %FT10
        LDR     r1, =L1PT
        MOV     r2, #ROM
        ADD     r1, r1, r2, LSR #(20-2)   ; L1PT address for ROM
        LDR     r2, [r1]
        MOV     r2, r2, LSR #12
        TEQ     r2, #PhysROM :SHR: 12     ; see if we have hard or soft ROM
        MOVEQ   r1, #0                    ; no soft ROM
        MOVNE   r1, #OSROM_ImageSize*1024 ; this much soft ROM
        B       %FT20

10
d583 1
a583 1

d586 3
d591 2
d594 1
d605 1
a643 220

;----------------------------------------------------------------------------------------
;PCImapping - reserved for Acorn use (PCI manager)
;
;       In:     r0 bits 0..7  = 11 (reason code 11)
;               r0 bit 8      = 1 to read info, 0 to map PCI
;               r0 bits 9..31 reserved (must be 0)
;
;       If r0 bit 8 = 1:
;         Out:  r0 = base address of logical PCI space   (&90000000)
;               r1 = base address of physical PCI space  (&60000000)
;               r2 = size of logical PCI space           (&10000000)
;               r3 = size of physical PCI space          (&20000000)
;
;       If r0 bit 8 = 0:
;         In:   r1 = logical base address of region to map
;               r2 = physical base address of region to map
;               r3 = size of region to map (bytes)
;
;         - size must be power of 2, and >= 4k
;         - region must be within physical and logical space returned by above
;         - physical and logical bases must be aligned to granularity of size
;           (eg. 4k region must be 4k aligned, 2M region 2M aligned etc).
;         - client is expected to map regions in order of decreasing size, to
;           allow kernel simple method of mapping efficiently
;         - client is responsible for avoiding inconsistent mappings
;
PCImapping ROUT
        Push    "r0-r5,lr"
        LDR     r4,=PCI_status
        LDR     r4,[r4]
        CMP     r4,#0
        BEQ     %FT80
        TST     r0,#&100            ;read info or map?
        BEQ     %FT20
;return info
        ADR     r0,PCImapinfo
        LDMIA   r0,{r0-r3}
        STMIA   sp,{r0-r3}
        B       %FT70
20
;map - with rudimentary checks (largely trust PCI manager)
;
        CMP     r1,#PCISpace            ;insist on region wholly within PCI logical and physical space
        BLO     %FT80
        ADD     r4,r1,r3
        CMP     r4,#PCISpace+&10000000
        BHI     %FT80
        CMP     r2,#&60000000
        BLO     %FT80
        ADD     r4,r2,r3
        CMP     r4,#&60000000+&20000000
        BHI     %FT80
;
; - we section map regions >=4M (4M rather than 1M to allow simple use of allocation of pages
;   for L2 mapping, given that one 4k page of L2 covers 4M)
; - we page map regions of < 4M
;
        CMP     r3,#&400000            ;threshold for page/section map
        BLO     %FT40
;we can section map
        MOV     r4,#&400000
        SUB     r4,r4,#1
        TST     r1,r4
        TSTEQ   r2,r4
        TSTEQ   r3,r4
        BNE     %FT80                  ;insist on 4M alignment and granularity
        LDR     r0,=L1PT
        LDR     r4,=&412               ;svc r/w, usr -/-, ~B, ~C, Section
        ADD     r0,r0,r1,LSR #(20-2)   ;L1PT address for start of region
        ORR     r2,r2,r4               ;L1PT entry for first 1M of region
25
        STR     r2,[r0],#4
        ADD     r2,r2,#&100000
        SUBS    r3,r3,#&100000        ;next 1M
        BNE     %BT25
        B       %FT70
;
40
;we will page map
        MOV     r4,#&1000
        SUB     r4,r4,#1
        TST     r1,r4
        TSTEQ   r2,r4
        TSTEQ   r3,r4
        BNE     %FT80                  ;insist on 4k alignment and granularity
        MOV     r5,r3                  ;size
        MOV     r3,r1                  ;base logical address
        MOV     r4,#0                  ;not doubly mapped
        BL      AllocateBackingLevel2  ;gets pages, fills in L1, sets L2 to abort
        BVS     %FT90                  ;oh dear
        MOV     r0,#L2PT
        LDR     r4,=&552               ;svc r/w, usr -/-, ~B, ~C, Small Page
        ADD     r0,r0,r3,LSR #(12-2)   ;L2PT address for start of region
        ORR     r2,r2,r4               ;L2PT entry for first 4k of region
45
        STR     r2,[r0],#4
        ADD     r2,r2,#&1000
        SUBS    r5,r5,#&1000           ;next 4k
        BNE     %BT45
;
70
        CLRV
        Pull    "r0-r5,pc"

80
        ADR     r0,PCImaperror
        SETV
        STR     r0,[sp]
90
        Pull    "r0-r5,pc"

PCImapinfo
        DCD     PCISpace
        DCD     &60000000
        DCD     &10000000   ;256M
        DCD     &20000000   ;512M

PCImaperror
        DCD     0
        DCB     "Bad PCI op (OS_Memory 11)",0
        ALIGN

;----------------------------------------------------------------------------------------
;RecommendPage
;
;       In:     r0 bits 0..7  = 12 (reason code 12)
;               r0 bits 8..31 = 0 (reserved flags)
;               r1 = size of physically contiguous RAM region required (bytes)
;               r2 = log2 of required alignment of base of region (eg. 12 = 4k, 20 = 1M)
;
;       Out:    r3 = page number of first page of recommended region that could be
;                    grown as specific pages by dynamic area handler (only guaranteed
;                    if grow is next page claiming operation)
;        - or error if not possible (eg too big, pages unavailable)
;
RecommendPage ROUT
        Push    "r0-r2,r4-r10,lr"
        CMP     r2,#30
        BHI     RP_failed            ;refuse to look for alignments above 1G
;
        ADD     r1,r1,#&1000
        SUB     r1,r1,#1
        MOV     r1,r1,LSR #12
        MOV     r1,r1,LSL #12     ;size rounded up to whole no. of pages
;
        CMP     r2,#12
        MOVLO   r2,#12            ;log2 alignment must be at least 12 (4k pages)
        MOV     r0,#1
        MOV     r4,r0,LSL r2      ;required alignment-1
;
        MOV     r0,#PhysRamTable
        MOV     r3,#0            ;page number, starts at 0
        LDR     r5,=CamEntriesPointer
        LDR     r5,[r5]
        ADD     r5,r5,#4         ; [r5,<page no.>,LSL #3] addresses flags word in CAM
        LDMIA   r0!,{r7,r8}      ;address,size of video chunk (skip this one)
;
RP_nextchunk
        ADD     r3,r3,r8,LSR #12 ;page no. of first page of next chunk
        LDMIA   r0!,{r7,r8}      ;address,size of next physical chunk
        CMP     r8,#0
        BEQ     RP_failed
;
        ADD     r6,r7,r4
        SUB     r6,r6,#1         ;round up
        MOV     r6,r6,LSR r2
        MOV     r6,r6,LSL r2
        SUB     r6,r6,r7         ;adjustment to first address of acceptable alignment
        CMP     r6,r8
        BHS     RP_nextchunk     ;negligible chunk
        ADD     r7,r3,r6,LSR #12 ;first page number of acceptable alignment
        SUB     r9,r8,r6         ;remaining size of chunk
;
;find first available page
RP_nextpage
        CMP     r9,r1
        BLO     RP_nextchunk
        LDR     r6,[r5,r7,LSL #3]  ;page flags from CAM
        ;must not be marked Unavailable or Required
        TST     r6,#PageFlags_Unavailable :OR: PageFlags_Required
        BEQ     RP_checkotherpages
RP_nextpagecontinue
        CMP     r9,r4
        BLS     RP_nextchunk
        ADD     r7,r7,r4,LSR #12   ;next page of suitable alignment
        SUB     r9,r9,r4
        B       RP_nextpage
;
RP_checkotherpages
        ADD     r10,r7,r1,LSR #12
        SUB     r10,r10,#1         ;last page required
RP_checkotherpagesloop
        LDR     r6,[r5,r10,LSL #3] ;page flags from CAM
        TST     r6,#PageFlags_Unavailable :OR: PageFlags_Required
        BNE     RP_nextpagecontinue
        SUB     r10,r10,#1
        CMP     r10,r7
        BHI     RP_checkotherpagesloop
;
;success!
;
        MOV     r3,r7
        Pull    "r0-r2,r4-r10,pc"

RP_failed
        MOV     r3,#0
        ADR     r0,RP_error
        SETV
        STR     r0,[sp]
        Pull    "r0-r2,r4-r10,pc"

RP_error
        DCD     0
        DCB     "No chunk available (OS_Memory 12)",0
        ALIGN

END


@


4.2.2.3
log
@Phoebe aware version of kernel
Source currently builds for Phoebe only. Flipping source switches will
build for Risc PC and/or A7000(+) as well (or instead). Not tested
much on older platforms.
Known issues remaining:
 - on Phoebe, kernel does not always set up the video (new VCO)
   properly. It appears that anything via the display manager is ok,
   old modes are ok before a monitor definition is seen, but mode
   changes via applications in the desktop always/often (?) aren't.
   Most likely area for investigation is whether kernel catches all
   mode change routes for ensuring it programs the new VCO.
 - on Phoebe, kernel does not yet have the hooks to support multiple
   CPU(s) (to park the slaves and allow them to be used later). I
   have a technical note on this, which should be archived as part of
   the Ursula burial work.
 - on older platforms, the areas that need checking most are CMOS
   power on reset (when in ROM) and mode changes by all routes (since
   these areas are bent by Phoebe support)
Note that kernel currently builds for rev S or better StrongARM. The
switch ARMSASupport_RevS should be set false if building for Risc PC.
@
text
@a61 2
        B       PCImapping
        B       RecommendPage
d338 1
d350 1
a350 13
        MOV     r1, #IOMD_Base
        LDRB    r1, [r1,#IOMD_ID0]
        TEQ     r1, #IOMD_Original :AND: &FF
        TEQNE   r1, #IOMD_7500     :AND: &FF
        TEQNE   r1, #IOMD_7500FE   :AND: &FF
        MOVEQ   r1, #PhysSpaceSize_Full  :SHR: ByteShift  ;IOMD1
        BEQ     MPS_return
        TEQ     r1, #IOMD_IOMD2    :AND: &FF
        MOVEQ   r1, #PhysSpaceSize_IOMD2 :SHR: ByteShift  ;IOMD2
        BEQ     MPS_return
        B       MemoryPhysSize     ;deliberate panic hang up
;
MPS_return
d390 1
a390 1
        ; &02000000 to &02400000 is VRAM or not present.
d461 6
a466 7
        BEQ     %FT90
        MOV     r3, r3, LSR r4                  ;   put bits in correct position
        ADD     r2, r2, r4, LSL #BitShift       ;   adjust current address
        RSB     r4, r4, #32                     ;   rest of word is not present
        LDR     lr, =NotPresent :OR: NotAvailable
        ORR     r3, r3, lr, LSL r4
        STR     r3, [r1], #4                    ;   and store word.
d468 2
a469 10
        ; End of last block of DRAM to &20000000 (IOMD1) is not present.
        ; End of last block of DRAM to &30000000 (IOMD2) is not present.
90
        Push    "r0"
        MOV     r0, #IOMD_Base
        LDRB    r0, [r0, #IOMD_ID0]
        TEQ     r0, #IOMD_IOMD2 :AND: &FF       ;we just want to distinguish IOMD2 here
        RSBNES  r2, r2, #&20000000
        RSBEQS  r2, r2, #&30000000
        Pull    "r0"
d490 1
a490 1
;                       8-11    1=return amount of DRAM (excludes any soft ROM)
a493 1
;                               5=return amount of soft ROM (ROM loaded into hidden DRAM)
a506 13
        TEQ     lr, #5:SHL:8            ; Check for soft ROM
        BNE     %FT10
        LDR     r1, =L1PT
        MOV     r2, #ROM
        ADD     r1, r1, r2, LSR #(20-2)   ; L1PT address for ROM
        LDR     r2, [r1]
        MOV     r2, r2, LSR #12
        TEQ     r2, #PhysROM :SHR: 12     ; see if we have hard or soft ROM
        MOVEQ   r1, #0                    ; no soft ROM
        MOVNE   r1, #OSROM_ImageSize*1024 ; this much soft ROM
        B       %FT20

10
d583 1
a583 1

d586 3
d591 2
d594 1
d605 1
a643 220

;----------------------------------------------------------------------------------------
;PCImapping - reserved for Acorn use (PCI manager)
;
;       In:     r0 bits 0..7  = 11 (reason code 11)
;               r0 bit 8      = 1 to read info, 0 to map PCI
;               r0 bits 9..31 reserved (must be 0)
;
;       If r0 bit 8 = 1:
;         Out:  r0 = base address of logical PCI space   (&90000000)
;               r1 = base address of physical PCI space  (&60000000)
;               r2 = size of logical PCI space           (&10000000)
;               r3 = size of physical PCI space          (&20000000)
;
;       If r0 bit 8 = 0:
;         In:   r1 = logical base address of region to map
;               r2 = physical base address of region to map
;               r3 = size of region to map (bytes)
;
;         - size must be power of 2, and >= 4k
;         - region must be within physical and logical space returned by above
;         - physical and logical bases must be aligned to granularity of size
;           (eg. 4k region must be 4k aligned, 2M region 2M aligned etc).
;         - client is expected to map regions in order of decreasing size, to
;           allow kernel simple method of mapping efficiently
;         - client is responsible for avoiding inconsistent mappings
;
PCImapping ROUT
        Push    "r0-r5,lr"
        LDR     r4,=PCI_status
        LDR     r4,[r4]
        CMP     r4,#0
        BEQ     %FT80
        TST     r0,#&100            ;read info or map?
        BEQ     %FT20
;return info
        ADR     r0,PCImapinfo
        LDMIA   r0,{r0-r3}
        STMIA   sp,{r0-r3}
        B       %FT70
20
;map - with rudimentary checks (largely trust PCI manager)
;
        CMP     r1,#PCISpace            ;insist on region wholly within PCI logical and physical space
        BLO     %FT80
        ADD     r4,r1,r3
        CMP     r4,#PCISpace+&10000000
        BHI     %FT80
        CMP     r2,#&60000000
        BLO     %FT80
        ADD     r4,r2,r3
        CMP     r4,#&60000000+&20000000
        BHI     %FT80
;
; - we section map regions >=4M (4M rather than 1M to allow simple use of allocation of pages
;   for L2 mapping, given that one 4k page of L2 covers 4M)
; - we page map regions of < 4M
;
        CMP     r3,#&400000            ;threshold for page/section map
        BLO     %FT40
;we can section map
        MOV     r4,#&400000
        SUB     r4,r4,#1
        TST     r1,r4
        TSTEQ   r2,r4
        TSTEQ   r3,r4
        BNE     %FT80                  ;insist on 4M alignment and granularity
        LDR     r0,=L1PT
        LDR     r4,=&412               ;svc r/w, usr -/-, ~B, ~C, Section
        ADD     r0,r0,r1,LSR #(20-2)   ;L1PT address for start of region
        ORR     r2,r2,r4               ;L1PT entry for first 1M of region
25
        STR     r2,[r0],#4
        ADD     r2,r2,#&100000
        SUBS    r3,r3,#&100000        ;next 1M
        BNE     %BT25
        B       %FT70
;
40
;we will page map
        MOV     r4,#&1000
        SUB     r4,r4,#1
        TST     r1,r4
        TSTEQ   r2,r4
        TSTEQ   r3,r4
        BNE     %FT80                  ;insist on 4k alignment and granularity
        MOV     r5,r3                  ;size
        MOV     r3,r1                  ;base logical address
        MOV     r4,#0                  ;not doubly mapped
        BL      AllocateBackingLevel2  ;gets pages, fills in L1, sets L2 to abort
        BVS     %FT90                  ;oh dear
        MOV     r0,#L2PT
        LDR     r4,=&552               ;svc r/w, usr -/-, ~B, ~C, Small Page
        ADD     r0,r0,r3,LSR #(12-2)   ;L2PT address for start of region
        ORR     r2,r2,r4               ;L2PT entry for first 4k of region
45
        STR     r2,[r0],#4
        ADD     r2,r2,#&1000
        SUBS    r5,r5,#&1000           ;next 4k
        BNE     %BT45
;
70
        CLRV
        Pull    "r0-r5,pc"

80
        ADR     r0,PCImaperror
        SETV
        STR     r0,[sp]
90
        Pull    "r0-r5,pc"

PCImapinfo
        DCD     PCISpace
        DCD     &60000000
        DCD     &10000000   ;256M
        DCD     &20000000   ;512M

PCImaperror
        DCD     0
        DCB     "Bad PCI op (OS_Memory 11)",0
        ALIGN

;----------------------------------------------------------------------------------------
;RecommendPage
;
;       In:     r0 bits 0..7  = 12 (reason code 12)
;               r0 bits 8..31 = 0 (reserved flags)
;               r1 = size of physically contiguous RAM region required (bytes)
;               r2 = log2 of required alignment of base of region (eg. 12 = 4k, 20 = 1M)
;
;       Out:    r3 = page number of first page of recommended region that could be
;                    grown as specific pages by dynamic area handler (only guaranteed
;                    if grow is next page claiming operation)
;        - or error if not possible (eg too big, pages unavailable)
;
RecommendPage ROUT
        Push    "r0-r2,r4-r10,lr"
        CMP     r2,#30
        BHI     RP_failed            ;refuse to look for alignments above 1G
;
        ADD     r1,r1,#&1000
        SUB     r1,r1,#1
        MOV     r1,r1,LSR #12
        MOV     r1,r1,LSL #12     ;size rounded up to whole no. of pages
;
        CMP     r2,#12
        MOVLO   r2,#12            ;log2 alignment must be at least 12 (4k pages)
        MOV     r0,#1
        MOV     r4,r0,LSL r2      ;required alignment-1
;
        MOV     r0,#PhysRamTable
        MOV     r3,#0            ;page number, starts at 0
        LDR     r5,=CamEntriesPointer
        LDR     r5,[r5]
        ADD     r5,r5,#4         ; [r5,<page no.>,LSL #3] addresses flags word in CAM
        LDMIA   r0!,{r7,r8}      ;address,size of video chunk (skip this one)
;
RP_nextchunk
        ADD     r3,r3,r8,LSR #12 ;page no. of first page of next chunk
        LDMIA   r0!,{r7,r8}      ;address,size of next physical chunk
        CMP     r8,#0
        BEQ     RP_failed
;
        ADD     r6,r7,r4
        SUB     r6,r6,#1         ;round up
        MOV     r6,r6,LSR r2
        MOV     r6,r6,LSL r2
        SUB     r6,r6,r7         ;adjustment to first address of acceptable alignment
        CMP     r6,r8
        BHS     RP_nextchunk     ;negligible chunk
        ADD     r7,r3,r6,LSR #12 ;first page number of acceptable alignment
        SUB     r9,r8,r6         ;remaining size of chunk
;
;find first available page
RP_nextpage
        CMP     r9,r1
        BLO     RP_nextchunk
        LDR     r6,[r5,r7,LSL #3]  ;page flags from CAM
        ;must not be marked Unavailable or Required
        TST     r6,#PageFlags_Unavailable :OR: PageFlags_Required
        BEQ     RP_checkotherpages
RP_nextpagecontinue
        CMP     r9,r4
        BLS     RP_nextchunk
        ADD     r7,r7,r4,LSR #12   ;next page of suitable alignment
        SUB     r9,r9,r4
        B       RP_nextpage
;
RP_checkotherpages
        ADD     r10,r7,r1,LSR #12
        SUB     r10,r10,#1         ;last page required
RP_checkotherpagesloop
        LDR     r6,[r5,r10,LSL #3] ;page flags from CAM
        TST     r6,#PageFlags_Unavailable :OR: PageFlags_Required
        BNE     RP_nextpagecontinue
        SUB     r10,r10,#1
        CMP     r10,r7
        BHI     RP_checkotherpagesloop
;
;success!
;
        MOV     r3,r7
        Pull    "r0-r2,r4-r10,pc"

RP_failed
        MOV     r3,#0
        ADR     r0,RP_error
        SETV
        STR     r0,[sp]
        Pull    "r0-r2,r4-r10,pc"

RP_error
        DCD     0
        DCB     "No chunk available (OS_Memory 12)",0
        ALIGN

END


@


4.1
log
@Initial revision
@
text
@d209 3
a211 2
        TST     r0, #flush_cache        ; If any page has been made uncacheable in L2 then
        SetCop  r9, CR_IDCFlush, NE     ;   flush the cache.
@


4.1.7.1
log
@NCOS 1.06 Imported from Zip drive
@
text
@@


4.1.5.1
log
@Import from SrcFiler
@
text
@@


4.1.3.1
log
@Import from cleaned 370 CD
@
text
@d209 2
a210 3
        TST     r0, #flush_cache        ; If any page has been made uncacheable in L2 then flush!
        BLNE    meminfo_flushplease
75
@


4.1.1.1
log
@Import from cleaned 360 CD
@
text
@@
