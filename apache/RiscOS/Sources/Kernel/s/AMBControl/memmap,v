head	4.7;
access;
symbols
	Kernel-6_14:4.7
	Kernel-6_01-3:4.7
	Kernel-6_13:4.7
	Kernel-6_12:4.7
	Kernel-6_11:4.7
	Kernel-6_10:4.7
	Kernel-6_09:4.7
	Kernel-6_08-4_129_2_10:4.7
	Kernel-6_08-4_129_2_9:4.7
	Kernel-6_08:4.7
	Kernel-6_07:4.7
	Kernel-6_06:4.7
	Kernel-6_05-4_129_2_8:4.7
	Kernel-6_05:4.7
	Kernel-6_04:4.7
	Kernel-6_03:4.7
	Kernel-6_01-2:4.7
	Kernel-6_01-4_146_2_1:4.7
	Kernel-6_02:4.7
	Kernel-6_01-1:4.7
	Kernel-6_01:4.7
	Kernel-6_00:4.7
	Kernel-5_99:4.7
	Kernel-5_98:4.7
	Kernel-5_97-4_129_2_7:4.7
	Kernel-5_97:4.7
	Kernel-5_96:4.7
	Kernel-5_95:4.7
	Kernel-5_94:4.7
	Kernel-5_93:4.7
	Kernel-5_92:4.7
	Kernel-5_91:4.7
	Kernel-5_90:4.7
	Kernel-5_89-4_129_2_6:4.7
	Kernel-5_89:4.7
	Kernel-5_88-4_129_2_5:4.7
	Kernel-5_88-4_129_2_4:4.7
	Kernel-5_88:4.7
	Kernel-5_87:4.7
	Kernel-5_86-4_129_2_3:4.7
	Kernel-5_86-4_129_2_2:4.7
	Kernel-5_86-4_129_2_1:4.7
	Kernel-5_86:4.7
	SMP:4.7.0.2
	SMP_bp:4.7
	Kernel-5_85:4.7
	Kernel-5_54-1:4.3
	Kernel-5_84:4.7
	Kernel-5_83:4.7
	Kernel-5_82:4.7
	Kernel-5_81:4.7
	Kernel-5_80:4.7
	Kernel-5_79:4.7
	Kernel-5_78:4.7
	Kernel-5_77:4.7
	Kernel-5_76:4.7
	Kernel-5_75:4.7
	Kernel-5_74:4.7
	Kernel-5_73:4.7
	Kernel-5_72:4.7
	Kernel-5_71:4.7
	Kernel-5_70:4.6
	Kernel-5_69:4.5
	Kernel-5_68:4.5
	Kernel-5_67:4.5
	Kernel-5_66:4.5
	Kernel-5_65:4.4
	Kernel-5_64:4.4
	Kernel-5_63:4.4
	Kernel-5_62:4.4
	Kernel-5_61:4.4
	Kernel-5_60:4.4
	Kernel-5_59:4.4
	Kernel-5_58:4.4
	Kernel-5_57:4.4
	Kernel-5_56:4.4
	Kernel-5_55:4.4
	Kernel-5_54:4.3
	Kernel-5_53:4.3
	Kernel-5_52:4.3
	Kernel-5_51:4.3
	Kernel-5_50:4.3
	Kernel-5_49:4.3
	HAL_merge:4.1.3.3.6.18
	Kernel-5_48:4.2
	Kernel-5_35-4_79_2_327:4.1.3.3.6.18
	Kernel-5_35-4_79_2_326:4.1.3.3.6.18
	Kernel-5_35-4_79_2_325:4.1.3.3.6.18
	Kernel-5_35-4_79_2_324:4.1.3.3.6.18
	Kernel-5_35-4_79_2_323:4.1.3.3.6.18
	Kernel-5_35-4_79_2_322:4.1.3.3.6.18
	Kernel-5_35-4_79_2_321:4.1.3.3.6.18
	Kernel-5_35-4_79_2_320:4.1.3.3.6.18
	Kernel-5_35-4_79_2_319:4.1.3.3.6.18
	Kernel-5_35-4_79_2_318:4.1.3.3.6.18
	Kernel-5_35-4_79_2_317:4.1.3.3.6.18
	Kernel-5_35-4_79_2_316:4.1.3.3.6.18
	Kernel-5_35-4_79_2_315:4.1.3.3.6.18
	Kernel-5_35-4_79_2_314:4.1.3.3.6.18
	Kernel-5_35-4_79_2_313:4.1.3.3.6.18
	Kernel-5_35-4_79_2_312:4.1.3.3.6.18
	Kernel-5_35-4_79_2_311:4.1.3.3.6.18
	Kernel-5_35-4_79_2_310:4.1.3.3.6.18
	Kernel-5_35-4_79_2_309:4.1.3.3.6.18
	Kernel-5_35-4_79_2_308:4.1.3.3.6.18
	Kernel-5_35-4_79_2_307:4.1.3.3.6.18
	Kernel-5_35-4_79_2_306:4.1.3.3.6.18
	Kernel-5_35-4_79_2_305:4.1.3.3.6.17
	Kernel-5_35-4_79_2_304:4.1.3.3.6.17
	Kernel-5_35-4_79_2_303:4.1.3.3.6.17
	Kernel-5_35-4_79_2_302:4.1.3.3.6.17
	Kernel-5_35-4_79_2_301:4.1.3.3.6.17
	Kernel-5_35-4_79_2_300:4.1.3.3.6.17
	Kernel-5_35-4_79_2_299:4.1.3.3.6.17
	Kernel-5_35-4_79_2_298:4.1.3.3.6.17
	Kernel-5_35-4_79_2_297:4.1.3.3.6.17
	Kernel-5_35-4_79_2_296:4.1.3.3.6.17
	Kernel-5_35-4_79_2_295:4.1.3.3.6.17
	Kernel-5_35-4_79_2_294:4.1.3.3.6.17
	Kernel-5_35-4_79_2_293:4.1.3.3.6.17
	Kernel-5_35-4_79_2_292:4.1.3.3.6.17
	Kernel-5_35-4_79_2_291:4.1.3.3.6.17
	Kernel-5_35-4_79_2_290:4.1.3.3.6.17
	Kernel-5_35-4_79_2_289:4.1.3.3.6.17
	Kernel-5_35-4_79_2_288:4.1.3.3.6.17
	Kernel-5_35-4_79_2_287:4.1.3.3.6.17
	Kernel-5_35-4_79_2_286:4.1.3.3.6.16
	Kernel-5_35-4_79_2_285:4.1.3.3.6.16
	Kernel-5_35-4_79_2_284:4.1.3.3.6.16
	Kernel-5_35-4_79_2_283:4.1.3.3.6.15
	Kernel-5_35-4_79_2_282:4.1.3.3.6.15
	Kernel-5_35-4_79_2_281:4.1.3.3.6.15
	Kernel-5_35-4_79_2_280:4.1.3.3.6.15
	Kernel-5_35-4_79_2_279:4.1.3.3.6.15
	Kernel-5_35-4_79_2_278:4.1.3.3.6.15
	Kernel-5_35-4_79_2_277:4.1.3.3.6.15
	Kernel-5_35-4_79_2_276:4.1.3.3.6.15
	Kernel-5_35-4_79_2_275:4.1.3.3.6.15
	Kernel-5_35-4_79_2_274:4.1.3.3.6.15
	Kernel-5_35-4_79_2_273:4.1.3.3.6.15
	Kernel-5_35-4_79_2_272:4.1.3.3.6.14
	Kernel-5_35-4_79_2_271:4.1.3.3.6.14
	Kernel-5_35-4_79_2_270:4.1.3.3.6.14
	Kernel-5_35-4_79_2_269:4.1.3.3.6.14
	Kernel-5_35-4_79_2_268:4.1.3.3.6.14
	Kernel-5_35-4_79_2_267:4.1.3.3.6.14
	Kernel-5_35-4_79_2_266:4.1.3.3.6.14
	Kernel-5_35-4_79_2_265:4.1.3.3.6.14
	Kernel-5_35-4_79_2_264:4.1.3.3.6.14
	Kernel-5_35-4_79_2_263:4.1.3.3.6.14
	Kernel-5_35-4_79_2_262:4.1.3.3.6.14
	Kernel-5_35-4_79_2_261:4.1.3.3.6.14
	Kernel-5_35-4_79_2_260:4.1.3.3.6.14
	Kernel-5_35-4_79_2_259:4.1.3.3.6.14
	Kernel-5_35-4_79_2_258:4.1.3.3.6.14
	Kernel-5_35-4_79_2_257:4.1.3.3.6.14
	Kernel-5_35-4_79_2_256:4.1.3.3.6.14
	Kernel-5_35-4_79_2_255:4.1.3.3.6.14
	Kernel-5_35-4_79_2_254:4.1.3.3.6.13
	Kernel-5_35-4_79_2_253:4.1.3.3.6.13
	Kernel-5_35-4_79_2_252:4.1.3.3.6.13
	Kernel-5_35-4_79_2_251:4.1.3.3.6.13
	Kernel-5_35-4_79_2_250:4.1.3.3.6.13
	Kernel-5_35-4_79_2_249:4.1.3.3.6.13
	Kernel-5_35-4_79_2_248:4.1.3.3.6.13
	Kernel-5_35-4_79_2_247:4.1.3.3.6.13
	Kernel-5_35-4_79_2_246:4.1.3.3.6.13
	Kernel-5_35-4_79_2_245:4.1.3.3.6.13
	Kernel-5_35-4_79_2_244:4.1.3.3.6.13
	Kernel-5_35-4_79_2_243:4.1.3.3.6.13
	Kernel-5_35-4_79_2_242:4.1.3.3.6.13
	Kernel-5_35-4_79_2_241:4.1.3.3.6.13
	Kernel-5_35-4_79_2_240:4.1.3.3.6.13
	Kernel-5_35-4_79_2_239:4.1.3.3.6.12
	Kernel-5_35-4_79_2_238:4.1.3.3.6.12
	Kernel-5_35-4_79_2_237:4.1.3.3.6.12
	Kernel-5_35-4_79_2_236:4.1.3.3.6.12
	Kernel-5_35-4_79_2_235:4.1.3.3.6.12
	Kernel-5_35-4_79_2_234:4.1.3.3.6.12
	Kernel-5_35-4_79_2_233:4.1.3.3.6.12
	Kernel-5_35-4_79_2_232:4.1.3.3.6.12
	Kernel-5_35-4_79_2_231:4.1.3.3.6.12
	Kernel-5_35-4_79_2_230:4.1.3.3.6.12
	Kernel-5_35-4_79_2_229:4.1.3.3.6.12
	Kernel-5_35-4_79_2_228:4.1.3.3.6.12
	Kernel-5_35-4_79_2_227:4.1.3.3.6.12
	Kernel-5_35-4_79_2_226:4.1.3.3.6.12
	Kernel-5_35-4_79_2_225:4.1.3.3.6.12
	Kernel-5_35-4_79_2_224:4.1.3.3.6.12
	Kernel-5_35-4_79_2_223:4.1.3.3.6.12
	Kernel-5_35-4_79_2_222:4.1.3.3.6.12
	Kernel-5_35-4_79_2_221:4.1.3.3.6.12
	Kernel-5_35-4_79_2_220:4.1.3.3.6.12
	Kernel-5_35-4_79_2_219:4.1.3.3.6.12
	Kernel-5_35-4_79_2_218:4.1.3.3.6.12
	Kernel-5_35-4_79_2_217:4.1.3.3.6.12
	Kernel-5_35-4_79_2_216:4.1.3.3.6.12
	Kernel-5_35-4_79_2_215:4.1.3.3.6.12
	Kernel-5_35-4_79_2_214:4.1.3.3.6.12
	Kernel-5_35-4_79_2_213:4.1.3.3.6.12
	Kernel-5_35-4_79_2_212:4.1.3.3.6.12
	Kernel-5_35-4_79_2_211:4.1.3.3.6.12
	Kernel-5_35-4_79_2_210:4.1.3.3.6.12
	Kernel-5_35-4_79_2_209:4.1.3.3.6.12
	Kernel-5_35-4_79_2_208:4.1.3.3.6.12
	Kernel-5_35-4_79_2_207:4.1.3.3.6.12
	Kernel-5_35-4_79_2_206:4.1.3.3.6.12
	Kernel-5_35-4_79_2_205:4.1.3.3.6.12
	Kernel-5_35-4_79_2_204:4.1.3.3.6.12
	Kernel-5_35-4_79_2_203:4.1.3.3.6.12
	Kernel-5_35-4_79_2_202:4.1.3.3.6.12
	Kernel-5_35-4_79_2_201:4.1.3.3.6.12
	Kernel-5_35-4_79_2_200:4.1.3.3.6.12
	Kernel-5_35-4_79_2_199:4.1.3.3.6.12
	Kernel-5_35-4_79_2_198:4.1.3.3.6.12
	Kernel-5_35-4_79_2_197:4.1.3.3.6.12
	Kernel-5_35-4_79_2_196:4.1.3.3.6.12
	Kernel-5_35-4_79_2_195:4.1.3.3.6.12
	Kernel-5_35-4_79_2_194:4.1.3.3.6.12
	Kernel-5_35-4_79_2_193:4.1.3.3.6.12
	Kernel-5_35-4_79_2_192:4.1.3.3.6.12
	Kernel-5_35-4_79_2_191:4.1.3.3.6.12
	Kernel-5_35-4_79_2_190:4.1.3.3.6.12
	Kernel-5_35-4_79_2_189:4.1.3.3.6.12
	Kernel-5_35-4_79_2_188:4.1.3.3.6.12
	Kernel-5_35-4_79_2_187:4.1.3.3.6.12
	Kernel-5_35-4_79_2_186:4.1.3.3.6.12
	Kernel-5_35-4_79_2_185:4.1.3.3.6.11
	Kernel-5_35-4_79_2_184:4.1.3.3.6.11
	Kernel-5_35-4_79_2_183:4.1.3.3.6.11
	Kernel-5_35-4_79_2_182:4.1.3.3.6.11
	Kernel-5_35-4_79_2_181:4.1.3.3.6.11
	Kernel-5_35-4_79_2_180:4.1.3.3.6.11
	Kernel-5_35-4_79_2_179:4.1.3.3.6.11
	Kernel-5_35-4_79_2_178:4.1.3.3.6.11
	Kernel-5_35-4_79_2_177:4.1.3.3.6.11
	Kernel-5_35-4_79_2_176:4.1.3.3.6.11
	Kernel-5_35-4_79_2_175:4.1.3.3.6.11
	Kernel-5_35-4_79_2_174:4.1.3.3.6.11
	Kernel-5_35-4_79_2_173:4.1.3.3.6.11
	Kernel-5_35-4_79_2_172:4.1.3.3.6.11
	Kernel-5_35-4_79_2_171:4.1.3.3.6.11
	Kernel-5_35-4_79_2_170:4.1.3.3.6.11
	Kernel-5_35-4_79_2_169:4.1.3.3.6.11
	Kernel-5_35-4_79_2_168:4.1.3.3.6.11
	Kernel-5_35-4_79_2_167:4.1.3.3.6.11
	Kernel-5_35-4_79_2_166:4.1.3.3.6.11
	Kernel-5_35-4_79_2_165:4.1.3.3.6.11
	RPi_merge:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_23:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_22:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_21:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_20:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_19:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_18:4.1.3.3.6.11
	Kernel-5_35-4_79_2_164:4.1.3.3.6.11
	Kernel-5_35-4_79_2_163:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_17:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_16:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_15:4.1.3.3.6.11
	Kernel-5_35-4_79_2_162:4.1.3.3.6.11
	Kernel-5_35-4_79_2_161:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_14:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_13:4.1.3.3.6.11
	Kernel-5_35-4_79_2_160:4.1.3.3.6.11
	Kernel-5_35-4_79_2_159:4.1.3.3.6.11
	Kernel-5_35-4_79_2_158:4.1.3.3.6.11
	Kernel-5_35-4_79_2_157:4.1.3.3.6.11
	Kernel-5_35-4_79_2_156:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_12:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_11:4.1.3.3.6.11
	Kernel-5_35-4_79_2_155:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_10:4.1.3.3.6.11
	Kernel-5_35-4_79_2_154:4.1.3.3.6.11
	Kernel-5_35-4_79_2_153:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_9:4.1.3.3.6.11
	Kernel-5_35-4_79_2_152:4.1.3.3.6.11
	Kernel-5_35-4_79_2_151:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_8:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_7:4.1.3.3.6.11
	Kernel-5_35-4_79_2_150:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_6:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_5:4.1.3.3.6.11
	Kernel-5_35-4_79_2_149:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_4:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_3:4.1.3.3.6.11
	Kernel-5_35-4_79_2_148:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_2:4.1.3.3.6.11
	Kernel-5_35-4_79_2_147_2_1:4.1.3.3.6.11
	RPi:4.1.3.3.6.11.0.2
	RPi_bp:4.1.3.3.6.11
	Kernel-5_35-4_79_2_98_2_52_2_1:4.1.3.3.6.10.2.5
	alees_Kernel_dev:4.1.3.3.6.10.2.5.0.2
	alees_Kernel_dev_bp:4.1.3.3.6.10.2.5
	Kernel-5_35-4_79_2_147:4.1.3.3.6.11
	Kernel-5_35-4_79_2_146:4.1.3.3.6.11
	Kernel-5_35-4_79_2_145:4.1.3.3.6.11
	Kernel-5_35-4_79_2_144:4.1.3.3.6.11
	Kernel-5_35-4_79_2_143:4.1.3.3.6.11
	Kernel-5_35-4_79_2_142:4.1.3.3.6.11
	Kernel-5_35-4_79_2_141:4.1.3.3.6.11
	Kernel-5_35-4_79_2_140:4.1.3.3.6.11
	Kernel-5_35-4_79_2_139:4.1.3.3.6.11
	Kernel-5_35-4_79_2_138:4.1.3.3.6.11
	Kernel-5_35-4_79_2_137:4.1.3.3.6.11
	Kernel-5_35-4_79_2_136:4.1.3.3.6.11
	Kernel-5_35-4_79_2_135:4.1.3.3.6.11
	Kernel-5_35-4_79_2_134:4.1.3.3.6.11
	Kernel-5_35-4_79_2_133:4.1.3.3.6.11
	Kernel-5_35-4_79_2_132:4.1.3.3.6.11
	Kernel-5_35-4_79_2_131:4.1.3.3.6.11
	Kernel-5_35-4_79_2_130:4.1.3.3.6.11
	Kernel-5_35-4_79_2_129:4.1.3.3.6.11
	Kernel-5_35-4_79_2_128:4.1.3.3.6.11
	Kernel-5_35-4_79_2_127:4.1.3.3.6.11
	Kernel-5_35-4_79_2_126:4.1.3.3.6.11
	Kernel-5_35-4_79_2_125:4.1.3.3.6.11
	Kernel-5_35-4_79_2_124:4.1.3.3.6.11
	Kernel-5_35-4_79_2_123:4.1.3.3.6.11
	Cortex_merge:4.1.3.3.6.10.2.5
	Kernel-5_35-4_79_2_122:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_54:4.1.3.3.6.10.2.5
	Kernel-5_35-4_79_2_98_2_53:4.1.3.3.6.10.2.5
	Kernel-5_35-4_79_2_98_2_52:4.1.3.3.6.10.2.5
	Kernel-5_35-4_79_2_98_2_51:4.1.3.3.6.10.2.5
	Kernel-5_35-4_79_2_98_2_50:4.1.3.3.6.10.2.5
	Kernel-5_35-4_79_2_98_2_49:4.1.3.3.6.10.2.5
	Kernel-5_35-4_79_2_98_2_48:4.1.3.3.6.10.2.5
	Kernel-5_35-4_79_2_121:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_47:4.1.3.3.6.10.2.4
	Kernel-5_35-4_79_2_120:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_46:4.1.3.3.6.10.2.4
	Kernel-5_35-4_79_2_119:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_45:4.1.3.3.6.10.2.4
	Kernel-5_35-4_79_2_98_2_44:4.1.3.3.6.10.2.4
	Kernel-5_35-4_79_2_118:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_43:4.1.3.3.6.10.2.4
	Kernel-5_35-4_79_2_117:4.1.3.3.6.10
	Kernel-5_35-4_79_2_116:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_42:4.1.3.3.6.10.2.4
	Kernel-5_35-4_79_2_115:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_41:4.1.3.3.6.10.2.4
	Kernel-5_35-4_79_2_98_2_40:4.1.3.3.6.10.2.4
	Kernel-5_35-4_79_2_114:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_39:4.1.3.3.6.10.2.4
	Kernel-5_35-4_79_2_98_2_38:4.1.3.3.6.10.2.4
	Kernel-5_35-4_79_2_113:4.1.3.3.6.10
	Kernel-5_35-4_79_2_112:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_37:4.1.3.3.6.10.2.3
	Kernel-5_35-4_79_2_98_2_36:4.1.3.3.6.10.2.3
	Kernel-5_35-4_79_2_98_2_35:4.1.3.3.6.10.2.3
	Kernel-5_35-4_79_2_98_2_34:4.1.3.3.6.10.2.3
	Kernel-5_35-4_79_2_98_2_33:4.1.3.3.6.10.2.3
	Kernel-5_35-4_79_2_98_2_32:4.1.3.3.6.10.2.3
	Kernel-5_35-4_79_2_98_2_31:4.1.3.3.6.10.2.2
	Kernel-5_35-4_79_2_98_2_30:4.1.3.3.6.10.2.2
	Kernel-5_35-4_79_2_98_2_29:4.1.3.3.6.10.2.2
	Kernel-5_35-4_79_2_98_2_28:4.1.3.3.6.10.2.2
	Kernel-5_35-4_79_2_98_2_27:4.1.3.3.6.10.2.2
	Kernel-5_35-4_79_2_98_2_26:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_111:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_25:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_24:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_23:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_110:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_22:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_109:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_21:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_20:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_108:4.1.3.3.6.10
	Kernel-5_35-4_79_2_107:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_19:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_18:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_17:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_16:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_15:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_106:4.1.3.3.6.10
	Kernel-5_35-4_79_2_105:4.1.3.3.6.10
	Kernel-5_35-4_79_2_104:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_14:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_13:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_12:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_11:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_10:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_9:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_103:4.1.3.3.6.10
	Kernel-5_35-4_79_2_102:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_8:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_7:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_6:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_5:4.1.3.3.6.10.2.1
	Kernel-5_35-4_79_2_98_2_4:4.1.3.3.6.10
	Kernel-5_35-4_79_2_101:4.1.3.3.6.10
	Kernel-5_35-4_79_2_100:4.1.3.3.6.10
	Kernel-5_35-4_79_2_99:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_3:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_2:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98_2_1:4.1.3.3.6.10
	Cortex:4.1.3.3.6.10.0.2
	Cortex_bp:4.1.3.3.6.10
	Kernel-5_35-4_79_2_98:4.1.3.3.6.10
	Kernel-5_35-4_79_2_97:4.1.3.3.6.10
	Kernel-5_35-4_79_2_96:4.1.3.3.6.10
	Kernel-5_35-4_79_2_95:4.1.3.3.6.10
	Kernel-5_35-4_79_2_94:4.1.3.3.6.10
	Kernel-5_35-4_79_2_93:4.1.3.3.6.10
	Kernel-5_35-4_79_2_92:4.1.3.3.6.10
	Kernel-5_35-4_79_2_91:4.1.3.3.6.10
	Kernel-5_35-4_79_2_90:4.1.3.3.6.10
	Kernel-5_35-4_79_2_89:4.1.3.3.6.10
	Kernel-5_35-4_79_2_88:4.1.3.3.6.10
	Kernel-5_35-4_79_2_87:4.1.3.3.6.10
	Kernel-5_35-4_79_2_86:4.1.3.3.6.10
	Kernel-5_35-4_79_2_85:4.1.3.3.6.10
	Kernel-5_35-4_79_2_84:4.1.3.3.6.10
	Kernel-5_35-4_79_2_83:4.1.3.3.6.10
	Kernel-5_35-4_79_2_82:4.1.3.3.6.10
	Kernel-5_35-4_79_2_81:4.1.3.3.6.10
	Kernel-5_35-4_79_2_80:4.1.3.3.6.10
	Kernel-5_35-4_79_2_79:4.1.3.3.6.10
	Kernel-5_35-4_79_2_78:4.1.3.3.6.10
	Kernel-5_35-4_79_2_77:4.1.3.3.6.10
	RO_5_07:4.1.3.3.6.10
	Kernel-5_35-4_79_2_76:4.1.3.3.6.10
	Kernel-5_35-4_79_2_75:4.1.3.3.6.10
	Kernel-5_35-4_79_2_74:4.1.3.3.6.10
	Kernel-5_35-4_79_2_73:4.1.3.3.6.10
	Kernel-5_35-4_79_2_72:4.1.3.3.6.10
	Kernel-5_35-4_79_2_71:4.1.3.3.6.10
	Kernel-5_35-4_79_2_70:4.1.3.3.6.10
	Kernel-5_35-4_79_2_69:4.1.3.3.6.10
	Kernel-5_35-4_79_2_68:4.1.3.3.6.10
	Kernel-5_35-4_79_2_67:4.1.3.3.6.10
	Kernel-5_35-4_79_2_66:4.1.3.3.6.10
	Kernel-5_35-4_79_2_65:4.1.3.3.6.10
	Kernel-5_35-4_79_2_64:4.1.3.3.6.10
	Kernel-5_35-4_79_2_63:4.1.3.3.6.10
	Kernel-5_35-4_79_2_62:4.1.3.3.6.10
	Kernel-5_35-4_79_2_61:4.1.3.3.6.10
	Kernel-5_35-4_79_2_59:4.1.3.3.6.10
	Kernel-5_35-4_79_2_58:4.1.3.3.6.10
	Kernel-5_35-4_79_2_57:4.1.3.3.6.10
	Kernel-5_35-4_79_2_56:4.1.3.3.6.10
	Kernel-5_35-4_79_2_55:4.1.3.3.6.10
	Kernel-5_35-4_79_2_54:4.1.3.3.6.10
	Kernel-5_35-4_79_2_53:4.1.3.3.6.10
	Kernel-5_35-4_79_2_52:4.1.3.3.6.10
	Kernel-5_35-4_79_2_51:4.1.3.3.6.10
	Kernel-5_35-4_79_2_50:4.1.3.3.6.9
	Kernel-5_35-4_79_2_49:4.1.3.3.6.8
	Kernel-5_35-4_79_2_48:4.1.3.3.6.8
	Kernel-5_47:4.1.3.3
	Kernel-5_46-4_90_2_1:4.1.3.3
	nbingham_Kernel_FastNC_dev_bp:4.1.3.3
	nbingham_Kernel_FastNC_dev:4.1.3.3.0.10
	Kernel-5_46:4.1.3.3
	Kernel-5_45:4.1.3.3
	Kernel-5_35-4_79_2_47:4.1.3.3.6.8
	Kernel-5_35-4_79_2_46:4.1.3.3.6.8
	Kernel-5_35-4_79_2_45:4.1.3.3.6.8
	Kernel-5_35-4_79_2_44:4.1.3.3.6.8
	Kernel-5_35-4_79_2_25_2_2:4.1.3.3.6.4
	Kernel-5_35-4_79_2_43:4.1.3.3.6.8
	Kernel-5_35-4_79_2_42:4.1.3.3.6.8
	Kernel-5_35-4_79_2_41:4.1.3.3.6.8
	Kernel-5_35-4_79_2_40:4.1.3.3.6.8
	Kernel-5_35-4_79_2_39:4.1.3.3.6.8
	Kernel-5_35-4_79_2_38:4.1.3.3.6.8
	Kernel-5_35-4_79_2_37:4.1.3.3.6.7
	Kernel-5_35-4_79_2_36:4.1.3.3.6.7
	Kernel-5_35-4_79_2_35:4.1.3.3.6.6
	Kernel-5_35-4_79_2_34:4.1.3.3.6.6
	Kernel-5_35-4_79_2_33:4.1.3.3.6.6
	Kernel-5_35-4_79_2_32:4.1.3.3.6.6
	Kernel-5_44:4.1.3.3
	Kernel-5_35-4_79_2_25_2_1:4.1.3.3.6.4
	Kernel-5_43:4.1.3.3
	Kernel-5_35-4_79_2_31:4.1.3.3.6.5
	Kernel-5_35-4_79_2_30:4.1.3.3.6.4
	Kernel-5_35-4_79_2_29:4.1.3.3.6.4
	Kernel-5_35-4_79_2_28:4.1.3.3.6.4
	Kernel-5_35-4_79_2_27:4.1.3.3.6.4
	Kernel-5_35-4_79_2_26:4.1.3.3.6.4
	Kernel-5_42:4.1.3.3
	Kernel-5_41:4.1.3.3
	Kernel-5_40:4.1.3.3
	Kernel-5_35-4_79_2_25:4.1.3.3.6.4
	Kernel-5_35-4_79_2_24:4.1.3.3.6.4
	Kernel-5_35-4_79_2_23:4.1.3.3.6.4
	Kernel-5_35-4_79_2_22:4.1.3.3.6.4
	Kernel-5_35-4_79_2_21:4.1.3.3.6.4
	Kernel-5_35-4_79_2_20:4.1.3.3.6.4
	Kernel-5_35-4_79_2_19:4.1.3.3.6.4
	Kernel-5_35-4_79_2_18:4.1.3.3.6.4
	Kernel-5_35-4_79_2_17:4.1.3.3.6.4
	Kernel-5_35-4_79_2_16:4.1.3.3.6.4
	Kernel-5_35-4_79_2_15:4.1.3.3.6.3
	Kernel-5_35-4_79_2_14:4.1.3.3.6.3
	Kernel-5_39:4.1.3.3
	Kernel-5_13-4_52_2_1:4.1.3.3
	Bethany:4.1.3.3.0.8
	Kernel-5_38:4.1.3.3
	Kernel-5_35-4_79_2_13:4.1.3.3.6.3
	Kernel-5_35-4_79_2_12:4.1.3.3.6.3
	Kernel-5_35-4_79_2_11:4.1.3.3.6.2
	Kernel-5_37:4.1.3.3
	Kernel-5_35-4_79_2_10:4.1.3.3.6.1
	Kernel-5_35-4_79_2_9:4.1.3.3
	Kernel-5_36:4.1.3.3
	Kernel-5_35-4_79_2_8:4.1.3.3
	Kernel-5_35-4_79_2_7:4.1.3.3
	Kernel-5_35-4_79_2_6:4.1.3.3
	Kernel-5_35-4_79_2_5:4.1.3.3
	Kernel-5_35-4_79_2_4:4.1.3.3
	Kernel-5_35-4_79_2_3:4.1.3.3
	Kernel-5_35-4_79_2_2:4.1.3.3
	dellis_autobuild_BaseSW:4.1.3.3
	Kernel-5_35-4_79_2_1:4.1.3.3
	HAL:4.1.3.3.0.6
	Kernel-5_35:4.1.3.3
	Kernel-5_34:4.1.3.3
	Kernel-5_33:4.1.3.3
	Kernel-5_32:4.1.3.3
	Kernel-5_31:4.1.3.3
	Kernel-5_30:4.1.3.3
	Kernel-5_29:4.1.3.3
	Kernel-5_28:4.1.3.3
	Kernel-5_27:4.1.3.3
	Kernel-5_26:4.1.3.3
	Kernel-5_25:4.1.3.3
	Kernel-5_24:4.1.3.3
	Kernel-5_23:4.1.3.3
	Kernel-5_22:4.1.3.3
	sbrodie_sedwards_16Mar2000:4.1.3.3
	Kernel-5_21:4.1.3.3
	Kernel-5_20:4.1.3.3
	Kernel-5_19:4.1.3.3
	Kernel-5_18:4.1.3.3
	Kernel-5_17:4.1.3.3
	Kernel-5_16:4.1.3.3
	Kernel-5_15:4.1.3.3
	Kernel-5_14:4.1.3.3
	Kernel-5_13:4.1.3.3
	Kernel-5_12:4.1.3.3
	Kernel-5_11:4.1.3.3
	Kernel-5_10:4.1.3.3
	Kernel-5_09:4.1.3.3
	Kernel-5_08:4.1.3.3
	Kernel-5_07:4.1.3.3
	Kernel-5_06:4.1.3.3
	Kernel-5_05:4.1.3.3
	Kernel-5_04:4.1.3.3
	Kernel-5_03:4.1.3.3
	Kernel-5_02:4.1.3.3
	Kernel-5_01:4.1.3.3
	Kernel-5_00:4.1.3.3
	Kernel-4_99:4.1.3.3
	Kernel-4_98:4.1.3.3
	Kernel-4_97:4.1.3.3
	Kernel-4_96:4.1.3.3
	Kernel-4_95:4.1.3.3
	Kernel-4_94:4.1.3.3
	Kernel-4_93:4.1.3.3
	Kernel-4_92:4.1.3.3
	Kernel-4_91:4.1.3.3
	Kernel-4_90:4.1.3.3
	dcotton_autobuild_BaseSW:4.1.3.3
	Kernel-4_89:4.1.3.3
	Kernel-4_88:4.1.3.3
	Kernel-4_87:4.1.3.3
	Kernel-4_86:4.1.3.3
	Kernel-4_85:4.1.3.3
	sbrodie_UrsulaRiscPC_Kernel_19Aug99:4.1.3.3.2.3.2.1
	Kernel-4_84:4.1.3.3
	sbrodie_UrsulaRiscPC_Kernel_18Aug99:4.1.3.3.2.3.2.1
	Ursula_RiscPC_bp:4.1.3.3.2.3
	Kernel-4_83:4.1.3.3
	Kernel-4_82:4.1.3.3
	Kernel-4_81:4.1.3.3
	Kernel-4_80:4.1.3.3
	Kernel-4_79:4.1.3.3
	Kernel-4_78:4.1.3.3
	Kernel-4_77:4.1.3.3
	Kernel-4_76:4.1.3.3
	Kernel-4_75:4.1.3.3
	Kernel-4_74:4.1.3.3
	Kernel-4_73:4.1.3.3
	Kernel-4_72:4.1.3.3
	Kernel-4_71:4.1.3.3
	Kernel-4_70:4.1.3.3
	Kernel-4_69:4.1.3.3
	Kernel-4_68:4.1.3.3
	mstphens_UrsulaRiscPCBuild_20Nov98:4.1.3.3.2.3.2.1
	Ursula_RiscPC:4.1.3.3.2.3.0.2
	Kernel-4_67:4.1.3.3
	Kernel-4_66:4.1.3.3
	Kernel-4_65:4.1.3.3
	Ursula_merge:4.1.3.3
	Kernel-4_64:4.1.3.3
	mstphens_Kernel-3_81:4.1.3.3.2.4
	rthornb_UrsulaBuild-19Aug1998:4.1.3.3.2.3
	UrsulaBuild_FinalSoftload:4.1.3.3.2.3
	rthornb_UrsulaBuild-12Aug1998:4.1.3.3.2.3
	aglover_UrsulaBuild-05Aug1998:4.1.3.3.2.3
	rthornb_UrsulaBuild-29Jul1998:4.1.3.3.2.3
	rthornb_UrsulaBuild-22Jul1998:4.1.3.3.2.3
	rthornb_UrsulaBuild-15Jul1998:4.1.3.3.2.3
	rthornb_UrsulaBuild-07Jul1998:4.1.3.3.2.3
	rthornb_UrsulaBuild-17Jun1998:4.1.3.3.2.3
	rthornb_UrsulaBuild-03Jun1998:4.1.3.3.2.3
	rthornb_UrsulaBuild-27May1998:4.1.3.3.2.3
	mstphens_Kernel-3_80:4.1.3.3.2.3
	rthornb_UrsulaBuild-21May1998:4.1.3.3.2.3
	rthornb_UrsulaBuild_01May1998:4.1.3.3.2.3
	afrost_NC2_Generic:4.1.3.3
	Daytona:4.1.3.3.0.4
	Daytona_bp:4.1.3.3
	Ursula_bp:4.1.3.3
	Ursula:4.1.3.3.0.2
	RO_3_71:4.1.3.3
	MergeFiles:4.1.3.1
	RO_3_70:4.1.3.1
	StrongARM:4.1.3;
locks; strict;
comment	@# @;


4.7
date	2016.12.13.19.41.15;	author jlee;	state Exp;
branches;
next	4.6;
commitid	XeVhUEC50BLVkRxz;

4.6
date	2016.12.13.19.03.37;	author jlee;	state Exp;
branches;
next	4.5;
commitid	dvbJa4TQHit18Rxz;

4.5
date	2016.12.13.16.42.53;	author jlee;	state Exp;
branches;
next	4.4;
commitid	aGog9bB8f4QKlQxz;

4.4
date	2016.08.02.22.10.47;	author jlee;	state Exp;
branches;
next	4.3;
commitid	CnQYuUGzojQfrMgz;

4.3
date	2016.06.30.20.28.59;	author jlee;	state Exp;
branches;
next	4.2;
commitid	lMnWzoE9eJz3Wwcz;

4.2
date	2016.06.30.20.08.12;	author jlee;	state Exp;
branches;
next	4.1;
commitid	IWoXxARWeuLDOwcz;

4.1
date	96.11.06.02.01.36;	author nturton;	state Exp;
branches
	4.1.3.1;
next	;

4.1.3.1
date	96.11.06.02.01.36;	author nturton;	state Exp;
branches
	4.1.3.1.2.1;
next	4.1.3.2;

4.1.3.2
date	97.05.01.08.09.31;	author kbracey;	state Exp;
branches;
next	4.1.3.3;

4.1.3.3
date	97.05.07.06.30.31;	author kbracey;	state Exp;
branches
	4.1.3.3.2.1
	4.1.3.3.6.1;
next	;

4.1.3.1.2.1
date	97.04.30.16.46.09;	author kbracey;	state Exp;
branches;
next	;

4.1.3.3.2.1
date	97.05.21.09.30.34;	author mstphens;	state Exp;
branches;
next	4.1.3.3.2.2;

4.1.3.3.2.2
date	97.09.09.13.33.54;	author mstphens;	state Exp;
branches;
next	4.1.3.3.2.3;

4.1.3.3.2.3
date	97.10.21.15.31.32;	author mstphens;	state Exp;
branches
	4.1.3.3.2.3.2.1;
next	4.1.3.3.2.4;

4.1.3.3.2.4
date	98.09.24.13.17.26;	author mstphens;	state Exp;
branches;
next	;

4.1.3.3.2.3.2.1
date	98.11.23.14.59.24;	author mstphens;	state Exp;
branches;
next	;

4.1.3.3.6.1
date	2000.10.16.11.55.38;	author kbracey;	state Exp;
branches;
next	4.1.3.3.6.2;

4.1.3.3.6.2
date	2000.10.20.15.48.04;	author mstephen;	state Exp;
branches;
next	4.1.3.3.6.3;

4.1.3.3.6.3
date	2000.11.10.14.41.16;	author mstephen;	state Exp;
branches;
next	4.1.3.3.6.4;

4.1.3.3.6.4
date	2001.01.23.16.32.41;	author mstephen;	state Exp;
branches;
next	4.1.3.3.6.5;

4.1.3.3.6.5
date	2001.05.22.15.27.55;	author mstephen;	state Exp;
branches;
next	4.1.3.3.6.6;

4.1.3.3.6.6
date	2001.06.06.14.24.01;	author mstephen;	state Exp;
branches;
next	4.1.3.3.6.7;

4.1.3.3.6.7
date	2001.06.13.16.37.53;	author mstephen;	state Exp;
branches;
next	4.1.3.3.6.8;

4.1.3.3.6.8
date	2001.06.18.14.49.46;	author mstephen;	state Exp;
branches;
next	4.1.3.3.6.9;

4.1.3.3.6.9
date	2002.10.28.16.13.47;	author bavison;	state Exp;
branches;
next	4.1.3.3.6.10;

4.1.3.3.6.10
date	2002.11.30.00.31.10;	author bavison;	state Exp;
branches
	4.1.3.3.6.10.2.1;
next	4.1.3.3.6.11;

4.1.3.3.6.11
date	2011.11.26.21.11.20;	author jlee;	state Exp;
branches;
next	4.1.3.3.6.12;
commitid	cI3W0zbtALQG6TIv;

4.1.3.3.6.12
date	2013.03.28.21.36.29;	author jlee;	state Exp;
branches;
next	4.1.3.3.6.13;
commitid	UN0GP6eB0LlNyBJw;

4.1.3.3.6.13
date	2014.10.15.01.57.59;	author jballance;	state Exp;
branches;
next	4.1.3.3.6.14;
commitid	JZXM55LgKjwAQeUx;

4.1.3.3.6.14
date	2015.01.20.20.21.29;	author jlee;	state Exp;
branches;
next	4.1.3.3.6.15;
commitid	XTFnnthpWkiPPN6y;

4.1.3.3.6.15
date	2015.08.05.21.51.34;	author jlee;	state Exp;
branches;
next	4.1.3.3.6.16;
commitid	SpZpzVH47zb408wy;

4.1.3.3.6.16
date	2015.08.31.19.28.42;	author jlee;	state Exp;
branches;
next	4.1.3.3.6.17;
commitid	Ni3KL17bG70fnszy;

4.1.3.3.6.17
date	2015.09.06.18.45.17;	author jlee;	state Exp;
branches;
next	4.1.3.3.6.18;
commitid	9JoJW3FhqXIqWdAy;

4.1.3.3.6.18
date	2016.03.10.22.57.45;	author jlee;	state Exp;
branches;
next	;
commitid	DAXUqMY2ucjim9Yy;

4.1.3.3.6.10.2.1
date	2009.05.10.18.49.17;	author jlee;	state Exp;
branches;
next	4.1.3.3.6.10.2.2;

4.1.3.3.6.10.2.2
date	2010.06.23.22.34.28;	author jlee;	state Exp;
branches;
next	4.1.3.3.6.10.2.3;

4.1.3.3.6.10.2.3
date	2010.10.04.22.22.17;	author jlee;	state Exp;
branches;
next	4.1.3.3.6.10.2.4;

4.1.3.3.6.10.2.4
date	2011.06.04.15.54.35;	author jlee;	state Exp;
branches;
next	4.1.3.3.6.10.2.5;
commitid	xmzeXYEfZlUPYmmv;

4.1.3.3.6.10.2.5
date	2011.08.08.23.28.30;	author jlee;	state Exp;
branches;
next	;
commitid	D7rzILnwRRSXoLuv;


desc
@@


4.7
log
@Implement support for cacheable pagetables
Detail:
  Modern ARMs (ARMv6+) introduce the possibility for the page table walk hardware to make use of the data cache(s) when performing memory accesses. This can significantly reduce the cost of a TLB miss on the system, and since the accesses are cache-coherent with the CPU it allows us to make the page tables cacheable for CPU (program) accesses also, improving the performance of page table manipulation by the OS.
  Even on ARMs where the page table walk can't use the data cache, it's been measured that page table manipulation operations can still benefit from placing the page tables in write-through or bufferable memory.
  So with that in mind, this set of changes updates the OS to allow cacheable/bufferable page tables to be used by the OS + MMU, using a system-appropriate cache policy.
  File changes:
  - hdr/KernelWS - Allocate workspace for storing the page flags that are to be used by the page tables
  - hdr/OSMem - Re-specify CP_CB_AlternativeDCache as having a different behaviour on ARMv6+ (inner write-through, outer write-back)
  - hdr/Options - Add CacheablePageTables option to allow switching back to non-cacheable page tables if necessary. Add SyncPageTables var which will be set {TRUE} if either the OS or the architecture requires a DSB after writing to a faulting page table entry.
  - s/ARM600, s/VMSAv6 - Add new SetTTBR & GetPageFlagsForCacheablePageTables functions. Update VMSAv6 for wider XCBTable (now 2 bytes per element)
  - s/ARMops - Update pre-ARMv7 MMU_Changing ARMops to drain the write buffer on entry if cacheable pagetables are in use (ARMv7+ already has this behaviour due to architectural requirements). For VMSAv6 Normal memory, change the way that the OS encodes the cache policy in the page table entries so that it's more compatible with the encoding used in the TTBR.
  - s/ChangeDyn - Update page table page flag handling to use PageTable_PageFlags. Make use of new PageTableSync macro.
  - s/Exceptions, s/AMBControl/memmap - Make use of new PageTableSync macro.
  - s/HAL - Update MMU initialisation sequence to make use of PageTable_PageFlags + SetTTBR
  - s/Kernel - Add PageTableSync macro, to be used after any write to a faulting page table entry
  - s/MemInfo - Update OS_Memory 0 page flag conversion. Update OS_Memory 24 to use new symbol for page table access permissions.
  - s/MemMap2 - Use PageTableSync. Add routines to enable/disable cacheable pagetables
  - s/NewReset - Enable cacheable pagetables once we're fully clear of the MMU initialision sequence (doing earlier would be trickier due to potential double-mapping)
Admin:
  Tested on pretty much everything currently supported
  Delivers moderate performance benefits to page table ops on old systems (e.g. 10% faster), astronomical benefits on some new systems (up to 8x faster)
  Stats: https://www.riscosopen.org/forum/forums/3/topics/2728?page=2#posts-58015


Version 5.71. Tagged as 'Kernel-5_71'
@
text
@; Copyright 1996 Acorn Computers Ltd
;
; Licensed under the Apache License, Version 2.0 (the "License");
; you may not use this file except in compliance with the License.
; You may obtain a copy of the License at
;
;     http://www.apache.org/licenses/LICENSE-2.0
;
; Unless required by applicable law or agreed to in writing, software
; distributed under the License is distributed on an "AS IS" BASIS,
; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
; See the License for the specific language governing permissions and
; limitations under the License.
;
; > s.memmap

; low level memory mapping


; ----------------------------------------------------------------------------------
;
; AMB_SetMemMapEntries_MapIn_Lazy:
;
; entry:
;   R1 =  first page index in PMP/AMBNode
;   R5 =  no. of pages (must be >0)
;   R7 =  logical offset in pages - must be zero for this variant of routine
;   R10 = DANode
;
; Lazy task swapping variant of SetMemMapEntries_MapIn
; Performs a lazy map in if lazy task swapping is enabled
;
AMB_SetMemMapEntries_MapIn_Lazy ROUT
  [ AMB_LazyMapIn
        LDR     r7, AMBFlags
        ANDS    r7, r7, #AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
        MOVEQ   pc, lr ; Just exit if laziness enabled
        MOV     r7, #0
  ]
        ; Fall through...

; ----------------------------------------------------------------------------------
;
; AMB_SetMemMapEntries_MapIn:
;
; entry:
;   R1 =  first page index in PMP/AMBNode
;   R5 =  no. of pages (must be >0)
;   R7 =  logical offset in pages (zero for standard map in)
;   R10 = DANode
;
; Must not be used if current slot is mapped lazily
;
AMB_SetMemMapEntries_MapIn ROUT
        Entry   "r3,r8,r9,r10"
      [ AMB_Debug
        DebugReg r1, "MapIn phys start "
        DebugReg r5, "count "
        DebugReg r7, "log offset "
        DebugReg r10, "node "
      ]
        ; Update DA logical size prior to R10 being clobbered
        LDR     r3,[r10,#DANode_Size]
        ADD     r3,r3,r5,LSL #Log2PageSize
        STR     r3,[r10,#DANode_Size]

        ; Get correct parameters for the low-level calls
        ADD     r3,r1,r7
        MOV     r3,r3,LSL #Log2PageSize
        ADD     r3,r3,#ApplicationStart

        MOV     r8, r5

        LDR     r9,[r10,#DANode_Flags]
        LDR     lr,=DynAreaFlags_AccessMask
        AND     r9,r9,lr

        LDR     r10,[r10,#DANode_PMP]
        ADD     r10,r10,r1,LSL #2

        ; Map in the pages (assuming area currently unmapped)
        BL      AMB_movepagesin_L2PT
        BL      AMB_movepagesin_CAM
      [ PMPParanoid
        BL      ValidatePMPs
      ]
      [ AMB_Debug
        DebugTX "<MapIn"
      ]
        EXIT


; ----------------------------------------------------------------------------------
;
; AMB_SetMemMapEntries_MapOut:
;
; entry:
;   R1 =  first page index in PMP/AMBNode
;   R5 =  no. of pages (must be >0)
;   R7 =  logical offset in pages (zero for standard map out)
;   R10 = DANode
;
; Must not be used if current slot is mapped lazily
;
AMB_SetMemMapEntries_MapOut ROUT
        Entry   "r3,r4,r8,r9,r10"
      [ AMB_Debug
        DebugReg r1, "MapOut phys start "
        DebugReg r5, "count "
        DebugReg r7, "log offset "
        DebugReg r10, "node "
      ]
        ; Update DA logical size prior to R10 being clobbered
        LDR     r3,[r10,#DANode_Size]
        SUB     r3,r3,r5,LSL #Log2PageSize
        STR     r3,[r10,#DANode_Size]
        
        ; Get correct parameters for the low-level calls
        LDR     r3,[r10,#DANode_Flags]
        LDR     lr,=DynAreaFlags_AccessMask
        AND     r3,r3,lr

        ADD     r4,r1,r7
        MOV     r4,r4,LSL #Log2PageSize
        ADD     r4,r4,#ApplicationStart

        MOV     r8, r5

        MOV     r9, r3

        LDR     r10,[r10,#DANode_PMP]
        ADD     r10,r10,r1,LSL #2

        ; Map out the pages (assuming area currently fully mapped)
        BL      AMB_movecacheablepagesout_L2PT
        BL      AMB_movepagesout_CAM
      [ PMPParanoid
        BL      ValidatePMPs
      ]
      [ AMB_Debug
        DebugTX "<MapOut"
      ]
        EXIT

  [ AMB_LazyMapIn
; ----------------------------------------------------------------------------------
;
; AMB_SetMemMapEntries_MapOut_Lazy:
;
; entry:
;   R1 =  first page index in PMP/AMBNode
;   R5 =  no. of pages (must be >0)
;   R7 =  logical offset in pages - must be zero for this variant of routine
;   R10 = DANode
;
; Lazy task swapping variant of SetMemMapEntries_MapOut
; Performs a sparse map out if lazy task swapping is enabled
;
AMB_SetMemMapEntries_MapOut_Lazy ROUT
        LDR     r7, AMBFlags
        TST     r7, #AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
        MOV     r7, #0
        BNE     AMB_SetMemMapEntries_MapOut
        ; Perform a sparse map out via MappedInRegister
        ; Keep track of count of mapped in pages so we can early-exit if we unmap them all
        LDR     r7,[r10,#DANode_Size]
        CMP     r7,#0
        MOVEQ   pc,lr ; Hey, there's already nothing there!
        Entry   "r0-r6,r8-r12"
      [ AMB_Debug
        DebugReg r1, "MapOut_Lazy start "
        DebugReg r5, "count "
        DebugReg r10, "node "
        DebugReg r7, "current log size "
      ]

;to do this safely we need to do it in two passes
;first pass makes pages uncacheable
;second pass unmaps them
;n.b. like most of the AMB code, this assumes nothing will trigger an abort-based lazy map in operation while we're in the middle of this processing!

;
;pass one: make pages uncacheable (preserve bitmap, CAM)
;
        LDR     r2,=ZeroPage

;decide if we want to do TLB coherency as we go
        ARMop   Cache_RangeThreshold,,,r2         ;returns threshold (bytes) in r0
        CMP     r5,r7,LSR #Log2PageSize
        MOVLO   r6,r5
        MOVHS   r6,r7,LSR #Log2PageSize           ;r6 = max number of pages we'll be unmapping
        SUB     r6,r6,r0,LSR #Log2PageSize        ;r6 < 0 if doing coherency as we go

        ADD     r5,r5,r1                          ;r5 = end of region to unmap

        MOV     r3,r7                             ;r3 = logical size

        ; Get MappedInRegister ptr
        AND     r8,r1,#31
        MOV     r9,#1
        ADR     r7,AMBMappedInRegister
        MOV     r9,r9,LSL r8                      ;r9 = current bit in word
        BIC     r8,r1,#31
        LDR     r8,[r7,r8,LSR #5-2]!              ;r8 = current word, r7 = ptr
        Push    "r7-r9"

        LDR     r0,[r10,#DANode_Flags]
        LDR     r11,[r2,#MMU_PCBTrans]
        GetTempUncache r12, r0, r11, lr           ;r12 = temp uncache L2PT flags

        MOV     r4,r1                             ;r4 = current index
        LDR     r11,=L2PT+(ApplicationStart:SHR:(Log2PageSize-2));r11 -> L2PT, offset by appspace start
        CMP     r9,#1
        BNE     %FT32
        B       %FT31
30
        ; Advance to next word
        ADD     r4,r4,#32
        LDR     r8,[r7,#4]!
        CMP     r4,r5
        BHS     %FT39
31
        ; Check register word
        TEQ     r8,#0
        BEQ     %BT30
32
        ; Check register bit
        TST     r8,r9
        BEQ     %FT34
        ; Mapped in page found, make uncacheable
        LDR     r0,[r11,r4,LSL #2]                ;Get current L2PT entry
        CMP     r6,#0
        BIC     r0,r0,#TempUncache_L2PTMask
        ORR     r0,r0,r12
        STR     r0,[r11,r4,LSL #2]                ;Update L2PT
        BGE     %FT33
        ; Do cache/TLB maintenance
        MOV     r1,r4,LSL #Log2PageSize
        ADD     r0,r1,#ApplicationStart
        ARMop   MMU_ChangingEntry,,,r2
33
        SUBS    r3,r3,#PageSize
        ADDEQ   r5,r4,#1                          ;mapped out all pages in slot; set end to current+1
34
        ; Exit if we've just processed the last page
        ADD     r4,r4,#1
        CMP     r4,r5
        BEQ     %FT40
        ; Advance to next bit of current word
        MOVS    r9,r9,LSL #1
        BCC     %BT32
        ; Finished the word, go back to per-word loop if necessary
        LDR     r8,[r7,#4]!
        MOV     r9,#1
        CMP     r8,#0
        BEQ     %BT30                             ;Next word empty, skip it
        B       %BT32                             ;Not empty, process it

39
        ; Reached end of region
        LDR     r0,[r10,#DANode_Size]
        CMP     r0,r3
        ADDEQ   sp,sp,#12                         ;Junk stacked r7-r9
        BEQ     %FT95                             ;Nothing in region to map out

40
        ; Do global maintenance if required
        CMP     r6,#0
        BLT     %FT41
        ARMop   MMU_Changing,,,r2
41

;
;pass two: unmap pages (+ clear bitmap + update CAM)
;
        Pull    "r7-r9"
        FRAMLDR r4,,r1
        STR     r3,[r10,#DANode_Size]             ;Write back new logical size

        LDR     r10,[r10,#DANode_PMP]             ;r10 -> page list
        LDR     r3,[r2,#CamEntriesPointer]
        LDR     r12,=DuffEntry

        CMP     r9,#1
        BNE     %FT52
        B       %FT51
50
        ; Advance to next word
        ADD     r4,r4,#32
        LDR     r8,[r7,#4]!
        CMP     r4,r5
        BHS     %FT59
51
        ; Check register word
        TEQ     r8,#0
        BEQ     %BT50
52
        ; Check register bit
        TST     r8,r9
        BEQ     %FT54
        ; Mapped in page found, unmap it
        MOV     lr,#0
        LDR     r0,[r10,r4,LSL #2]                ;Get page number
        STR     lr,[r11,r4,LSL #2]                ;Zero L2PT
        BIC     r8,r8,r9
        ASSERT  CAM_LogAddr=0
        STR     r12,[r3,r0,LSL #CAM_EntrySizeLog2] ;Update CAM
        CMP     r6,#0
        BGE     %FT53
        ; Do TLB maintenance
        MOV     r1,r4,LSL #Log2PageSize
        ADD     r0,r1,#ApplicationStart
        ARMop   MMU_ChangingUncachedEntry,,,r2    ;flush TLB
53
54
        ; Exit if we've just processed the last page
        ADD     r4,r4,#1
        CMP     r4,r5
        BEQ     %FT55
        ; Advance to next bit of current word
        MOVS    r9,r9,LSL #1
        BCC     %BT52
        ; Finished the word, go back to per-word loop if necessary
        STR     r8,[r7]                           ;Writeback new value
        LDR     r8,[r7,#4]!
        MOV     r9,#1
        CMP     r8,#0
        BEQ     %BT50                             ;Next word empty, skip it
        B       %BT52                             ;Not empty, process it

55
        ; Writeback last bitmap word
        STR     r8,[r7]

59
        ; Reached end of region

        ; Do global maintenance if required
        CMP     r6,#0
        BLT     %FT95
        ARMop   MMU_ChangingUncached,,,r2

95
      [ AMB_Debug
        DebugTX "<MapOut"
        FRAMLDR r10
        LDR     r11,[r10,#DANode_Size]
        DebugReg r11, "new log size "
      ]
      [ PMPParanoid
        BL      ValidatePMPs
      ]
        MOV     r7,#0
        EXIT
  |
AMB_SetMemMapEntries_MapOut_Lazy * AMB_SetMemMapEntries_MapOut
  ]

;
; ----------------------------------------------------------------------------------
;
;convert page number in $pnum to L2PT entry (physical address+protection bits),
;using cached PhysRamTable entries for speed
;
;entry: $ptable -> PhysRamTable, $pbits = protection bits
;       $cache0, $cache1, $cache2 = PhysRamTable cache
;exit:  $temp corrupted
;       $cache0, $cache1, $cache2 updated
;

        MACRO
        PageNumToL2PT $pnum,$ptable,$cache0,$cache1,$cache2,$pbits,$temp
        SUB     $temp,$pnum,$cache0 ; no. pages into block
        CMP     $temp,$cache2
        BLHS    PageNumToL2PTCache_$ptable._$cache0._$cache1._$cache2._$temp
        ADD     $pnum,$cache1,$temp,LSL #Log2PageSize ; physical address of page
        ORR     $pnum,$pbits,$pnum ; munge in protection bits
        MEND

        MACRO
        PageNumToL2PTInit $ptable,$cache0,$cache1,$cache2
        ASSERT  $cache2 > $cache1
        LDR     $ptable,=ZeroPage+PhysRamTable
        MOV     $cache0,#0
        LDMIA   $ptable,{$cache1,$cache2}
        MOV     $cache2,$cache2,LSR #12
        MEND

PageNumToL2PTCache_r4_r5_r6_r7_r12 ROUT
        Entry   "r4"
        ADD     r12,r12,r5 ; Restore page number
        MOV     r5,#0
10
        LDMIA   r4!,{r6,r7} ; Get PhysRamTable entry
        MOV     r7,r7,LSR #12
        CMP     r12,r7
        SUBHS   r12,r12,r7
        ADDHS   r5,r5,r7
        BHS     %BT10
        EXIT    ; r5-r7 = cache entry, r12 = offset into entry


  [ AMB_LazyMapIn

; ----------------------------------------------------------------------------------
;
;AMB_LazyFixUp
;
; *Only* for ARMs where the abort handler can restart instructions
;
; Routine to be used in abort handlers (in abort32 mode), that checks to see if abort
; is expected, and fixes things up if so, ready to restart instruction.
;
; Fix up consists of mapping in affected page, and updating AMBMappedInRegister. This
; may seem like a lot of work, but remember that the L2PT and CAM updates for each page are
; needed anyway in non-lazy scheme, so there is really only a housekeeping overhead.
;
; There is no cache clean/flush consideration here, since the map is a map in from Nowhere.
; TLB flush consideration is left to main abort handler code - in fact there may not
; be a TLB flush consideration at all, if ARM TLB can be assumed not to cache an
; entry which is a translation fault, as seems rational.
;
; entry: r0 = aborting address (data address for data abort, instruction address
;        for prefetch abort), r1-r7 trashable, no stack
;        r2 = 1 for prefetch abort, 0 for data abort
;        FSR valid for data aborts, unpredictable for prefetch aborts
; exit:  r0 = non-zero (NE status) if abort was expected and fixed up, zero (EQ status) if not
;        FAR,FSR,SPSR_abt,lr_abt preserved
;
AMB_LazyFixUp ROUT
        MOV     r7,r12
        LDR     r12,=ZeroPage+AMBControl_ws
        LDR     r12,[r12]
        CMP     r12,#0
        BEQ     %FT90                                    ;not initialised!
        LDR     r1,AMBFlags
        TST     r1,#AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
        BNE     %FT90                                    ;not active
        LDR     r1,AMBMappedInNode
        CMP     r1,#0
        BEQ     %FT90                                    ;no current node
        ARM_read_FSR r6                                  ;hang onto FSR in case we have to preserve it
        TEQ     r2,#1                                    ;if data abort
        ANDNE   r3,r6,#&F
        TEQNE   r3,#7                                    ; and not a page translation fault
        BNE     %FT20                                    ; then not a lazy abort (and FAR may be invalid anyway)
        LDR     r2,[r1,#AMBNode_DANode+DANode_PMPSize]
        SUBS    r0,r0,#ApplicationStart
        BLO     %FT20                                    ;abort not in current app space
        MOV     r0,r0,LSR #Log2PageSize                  ;address now in terms of pages from ApplicationStart
        CMP     r0,r2
        BHS     %FT20                                    ;abort not in current app space
      [ AMB_Debug
        Push    "lr"
        DebugReg r0, "Lazy "
        Pull    "lr"
      ]
;
; check/update the MappedIn bitmap
;
        ADR     r2,AMBMappedInRegister
        MOV     r5,#1
        ADD     r2,r2,r0,LSR #5-2
        BIC     r2,r2,#3                                 ;r2 -> bitmap word affected
        AND     r3,r0,#31
        MOV     r5,r5,LSL r3                             ;mask for bit affected in bitmap word
        LDR     r3,[r2]
        LDR     r4,[r1,#AMBNode_DANode+DANode_Size]      ;count it
        TST     r3,r5                                    ;if page already mapped in, not a lazy abort
        BNE     %FT20
        ORR     r3,r3,r5                                 ;ok, mark that we are going to map this page in
        STR     r3,[r2]
        ADD     r4,r4,#PageSize
        STR     r4,[r1,#AMBNode_DANode+DANode_Size]
        ; Update sparse HWM
        MOV     r3,r0,LSL #Log2PageSize
        LDR     r4,[r1,#AMBNode_DANode+DANode_SparseHWM]
        ADD     r3,r3,#ApplicationStart+PageSize
        CMP     r3,r4
        STRHI   r3,[r1,#AMBNode_DANode+DANode_SparseHWM]
;
; now map in the the page that went pop
;
        LDR     r1,[r1,#AMBNode_DANode+DANode_PMP]
        LDR     r2,=ZeroPage+PhysRamTable
        LDR     r3,[r1,r0,LSL #2]                        ;r3 = page involved
        MOV     r6,r3
10
        LDMIA   r2!,{r4,r5}
        MOV     r5,r5,LSR #12
        CMP     r6,r5
        SUBHS   r6,r6,r5
        BHS     %BT10
     [ {FALSE}
        LDR     r1,AMBPageFlags
        ADD     r4,r4,r6,LSL #12
        ORR     r4,r4,r1
        MOV     r1,#0                                    ;0 = AP for ordinary page
     |
        ADD     r4,r4,r6,LSL #12
        MOV     r1,#DynAreaFlags_PMP
        GetPTE  r4,4K,r4,r1
     ]
;
;here, r0 = page index into appslot, r1 = PPL, r3 = page number of page involved, r4 = new L2PT entry value to map in page
;
        ADD     r0,r0,#ApplicationStart:SHR:Log2PageSize ;address now in terms of pages from 0
        LDR     r5,=L2PT
        STR     r4,[r5,r0,LSL #2]                        ;update L2PT
;
        LDR     r5,=ZeroPage
        LDR     r5,[r5,#CamEntriesPointer]
        ADD     r5,r5,r3,LSL #CAM_EntrySizeLog2          ;r5 -> CAM entry affected
        MOVS    r0,r0,LSL #Log2PageSize                  ;address is now ordinary again, and must be non-zero
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
        STMIA   r5,{r0,r1}                               ;update CAM entry
        MOV     r12,r7
        MOV     pc,lr                                    ;r0 is non-zero, NE status
;
; not our abort, but is possible that client abort handler is in app space, so force all
; app space pages in now (so that client abort handler does not cause lazy abort, scribbling over original abort details)
;
20
        MOV     r1,#ApplicationStart                     ;good old page walk to provoke lazy fixups
        LDR     r2,AMBMappedInNode
        LDR     r2,[r2,#AMBNode_DANode+DANode_PMPSize]
        CMP     r2,#0
        BEQ     %FT90
        MRS     r0,SPSR                                  ;preserve SPSR_abort for original abort details
        MOV     r4,lr                                    ;preserve lr_abort so we can return properly (!)
        ARM_read_FAR r5                                  ;preserve FAR in case client abort handler wants to read it
                                                         ;preserve FSR (already in r6) similarly
30
        LDR     r3,[r1]                                  ;bring that page in by the magic of aborts
        SUBS    r2,r2,#1
        ADD     r1,r1,#PageSize
        BNE     %BT30
        MSR     SPSR_cxsf,r0                             ;SPSR for original abort
        MOV     lr,r4                                    ;restore return address
        ARM_write_FAR r5                                 ;restore FAR
        ARM_write_FSR r6                                 ;restore FSR
      [ MEMM_Type = "VMSAv6"
        myISB   ,r0 ; Not sure if this is necessary or not; do it just in case
      ]
;
90
        MOVS    r0,#0
        MOV     r12,r7
        MOV     pc,lr                                    ;r0 is zero, EQ status

  ] ;AMB_LazyMapIn

; ----------------------------------------------------------------------------------

  [ AMB_LazyMapIn

;
; If page of given logical address (r0) is in current app space, make sure page is
; 'honest' ie. properly mapped in. This is for things like FindMemMapEntries
; that must return sensible info (and presumably their client needs a consistent
; view of app space mapping, so that laziness is transparent)
;
AMB_MakeHonestLA  ROUT
        CMP     r0,#AbsMaxAppSize                        ;quick dismiss if definitely not app address
        MOVHS   pc,lr
        Push    "r1,r12,lr"
        LDR     r12,=ZeroPage+AMBControl_ws
        LDR     r12,[r12]
        CMP     r12,#0
        BEQ     %FT90                                    ;we're dormant!
        SUBS    r14,r0,#ApplicationStart
        BMI     %FT90                                    ;below app space
        MOV     r14,r14,LSR #Log2PageSize                ;pages from ApplicationStart
        LDR     r1,AMBMappedInNode
        CMP     r1,#0
        BEQ     %FT90                                    ;no node mapped in
        LDR     r1,[r1,#AMBNode_DANode+DANode_PMPSize]
        CMP     r1,r14                                   ;HI if log addr is in current app space
        LDRHIB  r1, [r0,#0]                              ;make honest if necessary (magic of abort fixups!)
90
        Pull    "r1,r12,pc"


; similar to AMB_MakeHonestLA, but for page of given page number (r0)
;
AMB_MakeHonestPN  ROUT
        Push    "r1-r3,r12,lr"
        LDR     r14,=ZeroPage
        LDR     r12,[r14,#AMBControl_ws]
        CMP     r12,#0
        BEQ     %FT90                                    ;we're dormant!
        LDR     r1,[r14,#MaxCamEntry]
        CMP     r0,r1
        BHI     %FT90                                    ;invalid page number
        LDR     r1,[r14,#CamEntriesPointer]
        ADD     r1,r1,r0,LSL #CAM_EntrySizeLog2
        ASSERT  CAM_LogAddr = 0
        ASSERT  CAM_PageFlags = 4
        ASSERT  CAM_PMP = 8
        ASSERT  CAM_PMPIndex = 12
        LDMIA   r1,{r1-r3,r14}
        ASSERT  CAM_LogAddr = 0
        TST     r2,#DynAreaFlags_PMP                     ;can't be one of ours if not owned by a PMP
        BEQ     %FT90
        LDR     r2,=Nowhere
        TEQ     r1,r2
        BNE     %FT90                                    ;only a page at Nowhere might be dishonest
        LDR     r1,AMBMappedInNode                       ;let's check the current node
        ADD     r1,r1,#AMBNode_DANode
        CMP     r3,r1
        BNE     %FT90                                    ;doesn't belong to current node, or there is no current node
        ; r14 is the index within the PMP, and the flat logical<->physical indexing scheme used by AMB means it will also be the logical index into application space 
        MOV     r2,#ApplicationStart
        LDRB    r2,[r2,r14,LSL #Log2PageSize]            ;make honest if necessary (magic of abort fixups!)
90
        Pull    "r1-r3,r12,pc"

  ] ;AMB_LazyMapIn

; ----------------------------------------------------------------------------------
;
;AMB_movepagesin_L2PT
;
;updates L2PT for new logical page positions, does not update CAM
;
; entry:
;       r3  =  new logical address of 1st page
;       r8  =  number of pages
;       r9  =  page flags
;       r10 -> page list
;
AMB_movepagesin_L2PT ROUT
        Entry   "r0-r12"

        MOV     r0, #0
        GetPTE  r11, 4K, r0, r9

        PageNumToL2PTInit r4,r5,r6,r7

        LDR     r9,=L2PT
        ADD     r9,r9,r3,LSR #(Log2PageSize-2) ;r9 -> L2PT for 1st new logical page

        CMP     r8,#4
        BLT     %FT20
10
        LDMIA   r10!,{r0-r3}         ;next 4 page numbers
        PageNumToL2PT r0,r4,r5,r6,r7,r11,r12
        PageNumToL2PT r1,r4,r5,r6,r7,r11,r12
        PageNumToL2PT r2,r4,r5,r6,r7,r11,r12
        PageNumToL2PT r3,r4,r5,r6,r7,r11,r12
        STMIA   r9!,{r0-r3}          ;write 4 L2PT entries
        SUB     r8,r8,#4
        CMP     r8,#4
        BGE     %BT10
20
        CMP     r8,#0
        BEQ     %FT35
30
        LDR     r0,[r10],#4
        PageNumToL2PT r0,r4,r5,r6,r7,r11,r12
        STR     r0,[r9],#4
        SUBS    r8,r8,#1
        BNE     %BT30
35
        PageTableSync
        EXIT

; ----------------------------------------------------------------------------------
;
;update CAM entry for page number in $reg
;
;entry: r11 -> CAM, r9 = logical addr of page, lr = PPL of page
;exit: $reg = addr of CAM entry
;
        MACRO
        UpdateCAM $reg
        ADD     $reg,r11,$reg,LSL #CAM_EntrySizeLog2 ;r0 -> CAM entry for 1st page
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
        STMIA   $reg,{r9,lr}            ;store logical addr,PPL
        MEND

; ----------------------------------------------------------------------------------
;
;AMB_movepagesin_CAM
;
;updates CAM, does not update L2PT
;
; entry:
;       r3  =  new logical address of 1st page
;       r8  =  number of pages
;       r9  =  PPL for CAM
;       r10 -> page list
;
AMB_movepagesin_CAM ROUT
        Entry   "r0-r11"


        MOV     lr,r9
        MOV     r9,r3
        LDR     r11,=ZeroPage
        LDR     r11,[r11,#CamEntriesPointer]   ;r11 -> CAM

        CMP     r8,#8
        BLT     %FT20
10
        LDMIA   r10!,{r0-r7}                   ;next 8 page numbers
        UpdateCAM r0
        ADD     r9,r9,#PageSize                ;next logical addr
        UpdateCAM r1
        ADD     r9,r9,#PageSize
        UpdateCAM r2
        ADD     r9,r9,#PageSize
        UpdateCAM r3
        ADD     r9,r9,#PageSize
        UpdateCAM r4
        ADD     r9,r9,#PageSize
        UpdateCAM r5
        ADD     r9,r9,#PageSize
        UpdateCAM r6
        ADD     r9,r9,#PageSize
        UpdateCAM r7
        ADD     r9,r9,#PageSize
        SUB     r8,r8,#8
        CMP     r8,#8
        BGE     %BT10
20
        CMP     r8,#0
        EXIT    EQ
30
        LDR     r0,[r10],#4
        UpdateCAM r0
        ADD     r9,r9,#PageSize
        SUBS    r8,r8,#1
        BNE     %BT30
        EXIT

; ----------------------------------------------------------------------------------
;
;AMB_movepagesout_CAM
;
;updates CAM, does not update L2PT
;
; entry:
;       r8  =  number of pages
;       r9  =  PPL for CAM
;       r10 -> page list
;
AMB_movepagesout_CAM ROUT
        Entry   "r0-r11"

        MOV     lr,r9
        LDR     r9,=DuffEntry
        LDR     r11,=ZeroPage
        LDR     r11,[r11,#CamEntriesPointer]   ;r11 -> CAM

        CMP     r8,#8
        BLT     %FT20
10
        LDMIA   r10!,{r0-r7}                   ;next 8 page numbers
        UpdateCAM r0
        UpdateCAM r1
        UpdateCAM r2
        UpdateCAM r3
        UpdateCAM r4
        UpdateCAM r5
        UpdateCAM r6
        UpdateCAM r7
        SUB     r8,r8,#8
        CMP     r8,#8
        BGE     %BT10
20
        CMP     r8,#0
        EXIT    EQ
30
        LDR     r0,[r10],#4
        UpdateCAM r0
        SUBS    r8,r8,#1
        BNE     %BT30
        EXIT

; ----------------------------------------------------------------------------------
;
;AMB_movecacheablepagesout_L2PT
;
;updates L2PT for old logical page positions, does not update CAM
;
; entry:
;       r3  =  old page flags
;       r4  =  old logical address of 1st page
;       r8  =  number of pages
;
AMB_movecacheablepagesout_L2PT
        Entry   "r0-r8"

        ; Calculate L2PT flags needed to make the pages uncacheable
        ; Assume all pages will have identical flags (or at least close enough)
        LDR     lr,=ZeroPage
        LDR     lr,[lr, #MMU_PCBTrans]
        GetTempUncache r0, r3, lr, r1
        LDR     r1, =TempUncache_L2PTMask

        LDR     lr,=L2PT
        ADD     lr,lr,r4,LSR #(Log2PageSize-2)    ;lr -> L2PT 1st entry

        CMP     r8,#4
        BLT     %FT20
10
        LDMIA   lr,{r2-r5}
        BIC     r2,r2,r1
        BIC     r3,r3,r1
        BIC     r4,r4,r1
        BIC     r5,r5,r1
        ORR     r2,r2,r0
        ORR     r3,r3,r0
        ORR     r4,r4,r0
        ORR     r5,r5,r0
        STMIA   lr!,{r2-r5}
        SUB     r8,r8,#4
        CMP     r8,#4
        BGE     %BT10
20
        CMP     r8,#0
        BEQ     %FT35
30
        LDR     r2,[lr]
        BIC     r2,r2,r1
        ORR     r2,r2,r0
        STR     r2,[lr],#4
        SUBS    r8,r8,#1
        BNE     %BT30
35
        FRAMLDR r0,,r4                           ;address of 1st page
        FRAMLDR r1,,r8                           ;number of pages
        LDR     r3,=ZeroPage
        ARMop   MMU_ChangingEntries,,,r3
        FRAMLDR r4
        FRAMLDR r8
        B       %FT55 ; -> moveuncacheablepagesout_L2PT (avoid pop+push of large stack frame)

; ----------------------------------------------------------------------------------
;
;AMB_moveuncacheablepagesout_L2PT
;
;updates L2PT for old logical page positions, does not update CAM
;
; entry:
;       r4  =  old logical address of 1st page
;       r8  =  number of pages
;
AMB_moveuncacheablepagesout_L2PT
        ALTENTRY
55      ; Enter here from moveuncacheablepagesout
        LDR     lr,=L2PT
        ADD     lr,lr,r4,LSR #(Log2PageSize-2)    ;lr -> L2PT 1st entry

        MOV     r0,#0                             ;0 means translation fault
        MOV     r1,#0
        MOV     r2,#0
        MOV     r3,#0
        MOV     r4,#0
        MOV     r5,#0
        MOV     r6,#0
        MOV     r7,#0

        CMP     r8,#8
        BLT     %FT70
60
        STMIA   lr!,{r0-r7}                       ;blam! (8 entries)
        SUB     r8,r8,#8
        CMP     r8,#8
        BGE     %BT60
70
        CMP     r8,#0
        BEQ     %FT85
80
        STR     r0,[lr],#4
        SUBS    r8,r8,#1
        BNE     %BT80
85
        FRAMLDR r0,,r4                           ;address of 1st page
        FRAMLDR r1,,r8                           ;number of pages
        LDR     r3,=ZeroPage
        ARMop   MMU_ChangingUncachedEntries,,,r3 ;no cache worries, hoorah
        EXIT


        LTORG

    END
@


4.6
log
@Make MMU_Changing ARMops perform the sub-operations in a sensible order
Detail:
  For a while we've known that the correct way of doing cache maintenance on ARMv6+ (e.g. when converting a page from cacheable to non-cacheable) is as follows:
  1. Write new page table entry
  2. Flush old entry from TLB
  3. Clean cache + drain write buffer
  The MMU_Changing ARMops (e.g. MMU_ChangingEntry) implement the last two items, but in the wrong order. This has caused the operations to fall out of favour and cease to be used, even in pre-ARMv6 code paths where the effects of improper cache/TLB management perhaps weren't as readily visible.
  This change re-specifies the relevant ARMops so that they perform their sub-operations in the correct order to make them useful on modern ARMs, updates the implementations, and updates the kernel to make use of the ops whereever relevant.
  File changes:
  - Docs/HAL/ARMop_API - Re-specify all the MMU_Changing ARMops to state that they are for use just after a page table entry has been changed (as opposed to before - e.g. 5.00 kernel behaviour). Re-specify the cacheable ones to state that the TLB invalidatation comes first.
  - s/ARM600, s/ChangeDyn, s/HAL, s/MemInfo, s/VMSAv6, s/AMBControl/memmap - Replace MMU_ChangingUncached + Cache_CleanInvalidate pairs with equivalent MMU_Changing op
  - s/ARMops - Update ARMop implementations to do everything in the correct order
  - s/MemMap2 - Update ARMop usage, and get rid of some lingering sledgehammer logic from ShuffleDoublyMappedRegionForGrow
Admin:
  Tested on pretty much everything currently supported


Version 5.70. Tagged as 'Kernel-5_70'
@
text
@d666 1
a666 6
      [ MEMM_Type = "VMSAv6"
        ; DSB + ISB required to ensure effect of page table write is fully
        ; visible (after overwriting a faulting entry)
        myDSB   ,r0
        myISB   ,r0,,y
      ]
@


4.5
log
@Reimplement AMBControl ontop of the PMP system
Detail:
  With this set of changes, each AMB node is now the owner of a fake DANode which is linked to a PMP.
  From a user's perspective the behaviour of AMBControl is the same as before, but rewriting it to use PMPs internally offers the following (potential) benefits:
  * Reduction in the amount of code which messes with the CAM & page tables, simplifying future work/maintenance. Some of the AMB ops (grow, shrink) now just call through to OS_ChangeDynamicArea. However all of the old AMB routines were well-optimised, so to avoid a big performance hit for common operations not all of them have been removed (e.g. mapslot / mapsome). Maybe one day these optimal routines will be made available for use by regular PMP DAs.
  * Removal of the slow Service_MemoryMoved / Service_PagesSafe handlers that had to do page list fixup after the core kernel had reclaimed/moved pages. Since everything is a PMP, the kernel will now deal with this on behalf of AMB.
  * Removal of a couple of other slow code paths (e.g. Do_AMB_MakeUnsparse calls from OS_ChangeDynamicArea)
  * Potential for more flexible mapping of application space in future, e.g. sparse allocation of memory to the wimp slot
  * Simpler transition to an ASID-based task swapping scheme on ARMv6+?
  Other changes of note:
  * AMB_LazyMapIn switch has been fixed up to work correctly (i.e. turning it off now disables lazy task swapping and all associated code instead of producing a build error)
  * The DANode for the current app should be accessed via the GetAppSpaceDANode macro. This will either return the current AMB DANode, or AppSpaceDANode (if e.g. pre-Wimp). However be aware that AppSpaceDANode retains the legacy behaviour of having a base + size relative to &0, while the AMB DANodes (identifiable via the PMP flag) are sane and have their base + size relative to &8000.
  * Mostly-useless DebugAborts switch removed
  * AMBPhysBin (page number -> phys addr lookup table) removed. Didn't seem to give any tangible performance benefit, and was imposing hidden restrictions on memory usage (all phys RAM fragments in PhysRamTable must be multiple of 512k). And if it really was a good optimisation, surely it should have been applied to all areas of the kernel, not just AMB!
  Other potential future improvements:
  * Turn the fake DANodes into real dynamic areas, reducing the amount of special code needed in some places, but allow the DAs to be hidden from OS_DynamicArea 3 so that apps/users won't get too confused
  * Add a generic abort trapping system to PMPs/DAs (lazy task swapping abort handler is still a special case)
  File changes:
  - s/ARM600, s/VMSAv6, s/ExtraSWIs - Remove DebugAborts
  - s/ArthurSWIs - Remove AMB service call handler dispatch
  - s/ChangeDyn - AMB_LazyMapIn switch fixes. Add alternate internal entry points for some PMP ops to allow the DANode to be specified (used by AMB)
  - s/Exceptions - Remove DebugAborts, AMB_LazyMapIn switch fixes
  - s/Kernel - Define GetAppSpaceDANode macro, AMB_LazyMapIn switch fix
  - s/MemInfo - AMB_LazyMapIn switch fixes
  - s/AMBControl/AMB - Update GETs
  - s/AMBControl/Memory - Remove block size quantisation, AMB_BlockResize (page list blocks are now allocated by PMP code)
  - s/AMBControl/Options - Remove PhysBin definitions, AMBMIRegWords (moved to Workspace file), AMB_LimpidFreePool switch. Add AMB_Debug switch.
  - s/AMBControl/Workspace - Update AMBNode to contain an embedded DANode. Move AMBMIRegWords here from Options file.
  - s/AMBControl/allocate - Fake DA node initialisation
  - s/AMBControl/deallocate - Add debug output
  - s/AMBControl/growp, growshrink, mapslot, mapsome, shrinkp - Rewrite to use PMP ops where possible, add debug output
  - s/AMBControl/main - Remove PhysBin initialisation. Update the enumerate/mjs_info call.
  - s/AMBControl/memmap - Low-level memory mapping routines updated or rewritten as appropriate.
  - s/AMBControl/readinfo - Update to cope with DANode
  - s/AMBControl/service - Remove old service call handlers
  - s/AMBControl/handler - DA handler for responding to PMP calls from OS_ChangeDynamicArea; just calls through to growpages/shrinkpages as appropriate.
Admin:
  Tested on pretty much everything currently supported


Version 5.66. Tagged as 'Kernel-5_66'
@
text
@d240 1
a240 4
        ARMop   MMU_ChangingUncachedEntry,,,r2    ;flush TLB
        ADD     r0,r1,#ApplicationStart
        ADD     r1,r1,#ApplicationStart+PageSize
        ARMop   Cache_CleanInvalidateRange,,,r2   ;flush from cache
d270 1
a270 2
        ARMop   MMU_ChangingUncached,,,r2
        ARMop   Cache_CleanInvalidateAll,,,r2
d842 1
a842 5
        ARMop   MMU_ChangingUncachedEntries,,,r3 ;flush TLB
        FRAMLDR r0,,r4
        FRAMLDR r1,,r8
        ADD     r1,r0,r1,LSL #Log2PageSize
        ARMop   Cache_CleanInvalidateRange,,,r3  ;flush from cache
@


4.4
log
@Add support for shareable pages and additional access privileges
Detail:
  This set of changes:
  * Refactors page table entry encoding/decoding so that it's (mostly) performed via functions in the MMU files (s.ARM600, s.VMSAv6) rather than on an ad-hoc basis as was the case previously
  * Page table entry encoding/decoding performed during ROM init is also handled via the MMU functions, which resolves some cases where the wrong cache policy was in use on ARMv6+
  * Adds basic support for shareable pages - on non-uniprocessor systems all pages will be marked as shareable (however, we are currently lacking ARMops which broadcast cache maintenance operations to other cores, so safe sharing of cacheable regions isn't possible yet)
  * Adds support for the VMSA XN flag and the "privileged ROM" access permission. These are exposed via RISC OS access privileges 4 and above, taking advantage of the fact that 4 bits have always been reserved for AP values but only 4 values were defined
  * Adds OS_Memory 17 and 18 to convert RWX-style access flags to and from RISC OS access privelege numbers; this allows us to make arbitrary changes to the mappings of AP values 4+ between different OS/hardware versions, and allows software to more easily cope with cases where the most precise AP isn't available (e.g. no XN on <=ARMv5)
  * Extends OS_Memory 24 (CheckMemoryAccess) to return executability information
  * Adds exported OSMem header containing definitions for OS_Memory and OS_DynamicArea
  File changes:
  - Makefile - export C and assembler versions of hdr/OSMem
  - Resources/UK/Messages - Add more text for OS_Memory errors
  - hdr/KernelWS - Correct comment regarding DCacheCleanAddress. Allocate workspace for MMU_PPLTrans and MMU_PPLAccess.
  - hdr/OSMem - New file containing exported OS_Memory and OS_DynamicArea constants, and public page flags
  - hdr/Options - Reduce scope of ARM6support to only cover builds which require ARMv3 support
  - s/AMBControl/Workspace - Clarify AMBNode_PPL usage
  - s/AMBControl/growp, mapslot, mapsome, memmap - Use AreaFlags_ instead of AP_
  - s/AMBControl/main, memmap - Use GetPTE instead of generating page table entry manually
  - s/ARM600 - Remove old coments relating to lack of stack. Update BangCam to use GetPTE. Update PPL tables, removing PPLTransL1 (L1 entries are now derived from L2 table on demand) and adding a separate table for ARM6. Implement the ARM600 versions of the Get*PTE ('get page table entry') and Decode*Entry functions
  - s/ARMops - Add Init_PCBTrans function to allow relevant MMU_PPLTrans/MMU_PCBTrans pointers to be set up during the pre-MMU stage of ROM init. Update ARM_Analyse to set up the pointers that are used post MMU init.
  - s/ChangeDyn - Move a bunch of flags to hdr/OSMem. Rename the AP_ dynamic area flags to AreaFlags_ to avoid name clashes and confusion with the page table AP_ values exported by Hdr:MEMM.ARM600/Hdr:MEMM.VMSAv6. Also generate the relevant flags for OS_Memory 24 so that it can refer to the fixed areas by their name instead of hardcoding the permissions.
  - s/GetAll - GET Hdr:OSMem
  - s/HAL - Change initial page table setup to use DA/page flags and GetPTE instead of building page table entries manually. Simplify AllocateL2PT by removing the requirement for the user to supply the access perimssions that will be used for the area; instead for ARM6 we just assume that cacheable memory is the norm and set L1_U for any L1 entry we create here.
  - s/Kernel - Add GetPTE macro (for easier integration of Get*PTE functions) and GenPPLAccess macro (for easy generation of OS_Memory 24 flags)
  - s/MemInfo - Fixup OS_Memory 0 to not fail on seeing non-executable pages. Implement OS_Memory 17 & 18. Tidy up some error generation. Make OS_Memory 13 use GetPTE. Extend OS_Memory 24 to return (non-) executability information, to use the named CMA_ constants generated by s/ChangeDyn, and to use the Decode*Entry functions when it's necessary to decode page table entries.
  - s/NewReset - Use AreaFlags_ instead of AP_
  - s/VMSAv6 - Remove old comments relating to lack of stack. Update BangCam to use GetPTE. Update PPL tables, removing PPLTransL1 (L1 entries are now derived from L2 table on demand) and adding a separate table for shareable pages. Implement the VMSAv6 versions of the Get*PTE and Decode*Entry functions.
Admin:
  Tested on Raspberry Pi 1, Raspberry Pi 3, Iyonix, RPCEmu (ARM6 & ARM7), comparing before and after CAM and page table dumps to check for any unexpected differences


Version 5.55. Tagged as 'Kernel-5_55'
@
text
@d18 345
d367 1
a367 1
;using PhysBin table for speed
d369 2
a370 1
;entry: $ptable -> PhysBin table, $pbits = protection bits
d372 1
d374 10
d385 6
a390 6
        PageNumToL2PT $pnum,$ptable,$pbits,$temp
        BIC     $temp,$pnum,#(3:SHL:(AMBPhysBinShift-2))       ;word alignment for PhysBin lookup
        LDR     $temp,[$ptable,$temp,LSR #(AMBPhysBinShift-2)] ;start physical address of bin
        AND     $pnum,$pnum,#AMBPhysBinMask                    ;no. pages into bin
        ADD     $pnum,$temp,$pnum,LSL #Log2PageSize            ;physical address of page
        ORR     $pnum,$pnum,$pbits                             ;munge in protection bits
d393 13
d451 1
a451 1
        LDR     r2,[r1,#AMBNode_Npages]
d457 5
d472 1
a472 1
        LDR     r4,AMBMappedInNpages                     ;count it
d477 8
a484 2
        ADD     r4,r4,#1
        STR     r4,AMBMappedInNpages
d488 20
a507 9
        ADD     r1,r1,#AMBNode_pages
        ADD     r1,r1,r0,LSL #2                          ;r1 -> page involved, in node page list
        LDR     r2,AMBPhysBin

; Get the correct default page flags
        LDR     r3,AMBPageFlags 
        LDR     r4,[r1]
        MOV     r6,r4
        PageNumToL2PT r4,r2,r3,r5
d509 1
a509 1
;here, r6 = page number of page involved, r4 = new L2PT entry value to map in page
d517 1
a517 1
        ADD     r5,r5,r6,LSL #CAM_EntrySizeLog2          ;r5 -> CAM entry affected
a518 1
        MOV     r1,#0                                    ;0 = AP for ordinary page
d531 1
a531 1
        LDR     r2,[r2,#AMBNode_Npages]
d582 1
a582 1
        LDR     r1,[r1,#AMBNode_Npages]
d584 1
a584 2
        bichi   lr, r0, #3                               ; ensure word aligned
        ldrhi   r1, [lr,#0]                              ;make honest if necessary (magic of abort fixups!)
d593 2
a594 2
        LDR     r12,=ZeroPage+AMBControl_ws
        LDR     r12,[r12]
a596 1
        LDR     r14,=ZeroPage
d601 1
d603 9
a611 3
        LDR     r1,[r1,r0,LSL #CAM_EntrySizeLog2]       ;logical address from CAM
        LDR     r14,=Nowhere
        TEQ     r1,r14
d614 6
a619 16
        CMP     r1,#0
        BEQ     %FT90                                    ;no node mapped in
        LDR     r14,[r1,#AMBNode_Npages]
        MOV     r14,r14,LSL #Log2PageSize
        ADD     r14,r14,#ApplicationStart                ;top of current app space
        ADD     r1,r1,#AMBNode_pages                     ;[r1] is page number
        MOV     r2,#ApplicationStart                     ;r2 is logical address for page
10
        CMP     r2,r14
        BHS     %FT90
        LDR     r3,[r1],#4                               ;next page number in node
        TEQ     r3,r0                                    ;see if its the one that wants to be honest
        ADDNE   r2,r2,#PageSize                          ;next logical address
        BNE     %BT10
        bic     lr, r2, #3                               ;ensure word aligned
        ldr     r1,[lr,#0]                               ;make honest if necessary (magic of abort fixups!)
d634 1
a635 1
;       r11 =  protection/control bits for L2PT
d638 6
a643 1
        Push    "r0-r10,r12,lr"
a644 1
        LDR     lr,AMBPhysBin                  ;lr -> PhysBin
d648 1
a648 1
        CMP     r8,#8
d651 8
a658 12
        LDMIA   r10!,{r0-r7}         ;next 8 page numbers
        PageNumToL2PT r0,lr,r11,r12
        PageNumToL2PT r1,lr,r11,r12
        PageNumToL2PT r2,lr,r11,r12
        PageNumToL2PT r3,lr,r11,r12
        PageNumToL2PT r4,lr,r11,r12
        PageNumToL2PT r5,lr,r11,r12
        PageNumToL2PT r6,lr,r11,r12
        PageNumToL2PT r7,lr,r11,r12
        STMIA   r9!,{r0-r7}          ;write 8 L2PT entries
        SUB     r8,r8,#8
        CMP     r8,#8
d665 1
a665 1
        PageNumToL2PT r0,lr,r11,r12
d676 1
a676 1
        Pull    "r0-r10,r12,pc"
d706 1
a706 1
        Push    "r0-r11,lr"
d739 1
a739 1
        Pull    "r0-r11,pc",EQ
d746 1
a746 1
        Pull    "r0-r11,pc"
d760 1
a760 1
        Push    "r0-r11,lr"
d784 1
a784 1
        Pull    "r0-r11,pc",EQ
d790 1
a790 1
        Pull    "r0-r11,pc"
a900 423
; ----------------------------------------------------------------------------------
;
; AMB_SetMemMapEntries:
;
; entry:
;   R3 =  no. of pages
;   R4 -> list of page entries (1 word per entry, giving page no.)
;   R5 =  start logical address of mapping (-1 means 'out of the way')
;   R6 =  PPL ('page protection level') for mapping
;
AMB_SetMemMapEntries ROUT

        Push    "r0-r4,r7-r11,lr"

        MOVS    r8,r3
        BEQ     AMB_smme_exit

        CMP     r5,#-1
        MOVEQ   r9,#AreaFlags_Duff ;PPL for mapped out pages
        MOVNE   r9,r6        ;PPL for mapped in pages

01
;get L2PT protection etc. bits, appropriate to PPL in R9, into R11
        MOV     r0, #0
        GetPTE  r11, 4K, r0, r9

        LDR     r7,=ZeroPage
        MOV     r10,r4                      ;ptr to next page number
        LDR     r2,[r10]                    ;page number of 1st page
        LDR     r7,[r7,#CamEntriesPointer]  ;r7 -> CAM
        ADD     r1,r7,r2,LSL #CAM_EntrySizeLog2 ;r1 -> CAM entry for 1st page
        LDR     r4,[r1,#CAM_LogAddr]        ;fetch old logical addr. of 1st page from CAM
        LDR     r3,[r1,#CAM_PageFlags]      ;fetch old PPL of 1st page from CAM

        CMP     r5,#-1
        BEQ     AMB_smme_mapout
;map or mapin
        LDR     r1,=DuffEntry
        CMP     r4,r1
        BEQ     AMB_smme_mapin

;map from somewhere to somewhere (should be App Space <-> Free Pool)
;
  [ AMB_LimpidFreePool
    ;can avoid cache clean/flush for moving pages out from FreePool, since FreePool pages are uncacheable
    ;
        TST     r3, #DynAreaFlags_NotCacheable  ;test PPL of 1st page for not cacheable bit set
        BEQ     AMB_smme_mapnotlimpid           ;if clear, must do full map somewhere with cache clean/flush
    ;
    ;this should be map FreePool -> App Space then
    ;
        MOV     r3,r5
        BL      AMB_moveuncacheablepagesout_L2PT ;unmap 'em from where they are
        BL      AMB_movepagesin_L2PT             ;map 'em to where they now be
        BL      AMB_movepagesin_CAM              ;keep the bloomin' soft CAM up to date
        Pull    "r0-r4,r7-r11, pc"
AMB_smme_mapnotlimpid
  ]
;
        BL      AMB_movecacheablepagesout_L2PT
        MOV     r3,r5
        BL      AMB_movepagesin_L2PT
        BL      AMB_movepagesin_CAM
        Pull    "r0-r4,r7-r11, pc"

;all pages sourced from same old logical page Nowhere, ie. pages currently mapped out, no cache worries
;
AMB_smme_mapin
        MOV     r3,r5
        BL      AMB_movepagesin_L2PT
        BL      AMB_movepagesin_CAM
        Pull    "r0-r4,r7-r11, pc"

;all pages destined for same new logical page Nowhere, ie. mapping them out
;
AMB_smme_mapout
        LDR     lr,=DuffEntry
        CMP     r4,lr
        BEQ     %FT50                            ;pages already mapped out - just update CAM for new ownership
        BL      AMB_movecacheablepagesout_L2PT
50
        BL      AMB_movepagesout_CAM

AMB_smme_exit
        Pull    "r0-r4,r7-r11, pc"

; ----------------------------------------------------------------------------------
;
; AMB_SetMemMapEntries_MyPPL:
;
; As above, but uses provided PPL when mapping out instead of forcing to AreaFlags_Duff
;
; entry:
;   R3 =  no. of pages
;   R4 -> list of page entries (1 word per entry, giving page no.)
;   R5 =  start logical address of mapping (-1 means 'out of the way')
;   R6 =  PPL ('page protection level') for mapping
;
AMB_SetMemMapEntries_MyPPL
        Push    "r0-r4,r7-r11,lr"
        MOVS    r8,r3
        MOV     r9,r6
        BNE     %BT01
        B       AMB_smme_exit


  [ AMB_LazyMapIn

; ----------------------------------------------------------------------------------
;
; AMB_SetMemMapEntries_SparseMapOut:
;
;
; entry:
;   R3  =  no. of pages currently mapped in (0=none)
;   R4  -> list of page entries (1 word per entry, giving page no.)
;   R5  -> bitmap of pages mapped in (1 bit per page in whole page list)
;   R6  =  total no. of pages in slot
;
AMB_SetMemMapEntries_SparseMapOut ROUT

        CMP     r3,#0
        MOVEQ   pc,lr
        Entry   "r0-r11"

;to do this safely we need to do it in two passes
;first pass makes pages uncacheable
;second pass unmaps them
;n.b. like most of the AMB code, this assumes nothing will trigger an abort-based lazy map in operation while we're in the middle of this processing!

;
;pass one: make pages uncacheable (preserve bitmap, CAM)
;

        MOV     r10,r4                            ;ptr to page list
        LDR     r2,=ZeroPage
        LDR     r7,[r2,#CamEntriesPointer]        ;r7 -> CAM
        MOV     r4,#ApplicationStart              ;log. address of first page
        MOV     r9,#-1                            ;initialised to correct page flags once we find a mapped in page

;decide if we want to do TLB coherency as we go
        ARMop   Cache_RangeThreshold,,,r2         ;returns threshold (bytes) in r0
        CMP     r3,r0,LSR #Log2PageSize
        MOVLO   r6,#0                             ;r6 := 0 if we are to do coherency as we go

        B       %FT10

;skip next 32 pages then continue
06
        ADD     r10,r10,#32*4
        ADD     r4,r4,#32*PageSize

;find the sparsely mapped pages, make them uncacheable, doing coherency as we go if enabled
10
        MOV     r8,#1                             ;initial bitmap mask for new bitmap word
        LDR     r11,[r5],#4                       ;next word of bitmap
        CMP     r11,#0                            ;if next 32 bits of bitmap clear, skip
        BEQ     %BT06                             ;skip loop must terminate if r3 > 0
12
        TST     r11,r8                            ;page is currently mapped in if bit set
        BEQ     %FT16

        CMP     r9,#-1                            ;have page flags yet?
        BNE     %FT14
        LDR     r0,[r10]                          ;page no.
        ADD     r0,r7,r0,LSL #CAM_EntrySizeLog2   ;r0 -> CAM entry for page
        LDR     r0,[r0,#CAM_PageFlags]
        LDR     r2,=ZeroPage
        LDR     r2,[r2,#MMU_PCBTrans]
        GetTempUncache r9,r0,r2,lr
14
        LDR     lr,=L2PT                          ;lr -> L2PT
        LDR     r2,[lr,r4,LSR #(Log2PageSize-2)]  ;L2PT entry for page
        LDR     r0,=TempUncache_L2PTMask
        BIC     r2,r2,r0
        ORR     r2,r2,r9
        STR     r2,[lr,r4,LSR #(Log2PageSize-2)]  ;make uncacheable

        TEQ     r6, #0
        BNE     %FT15
        LDR     r2,=ZeroPage
        MOV     r0,r4
        ARMop   MMU_ChangingUncachedEntry,,,r2    ;flush TLB
        MOV     r0,r4
        ADD     r1,r0,#PageSize
        ARMop   Cache_CleanInvalidateRange,,,r2   ;flush from cache
15
        SUBS    r3,r3,#1
        BEQ     %FT40                             ;done
16
        ADD     r10,r10,#4                        ;next page no.
        ADD     r4,r4,#PageSize                   ;next logical address
        MOVS    r8,r8,LSL #1                      ;if 32 bits processed...
        BNE     %BT12
        B       %BT10

40
        TEQ     r6, #0
        BEQ     %FT45
        LDR     r2,=ZeroPage
        ARMop   MMU_ChangingUncached,,,r2
        ARMop   Cache_CleanInvalidateAll,,,r2
45

;
;pass two: unmap pages (+ clear bitmap + update CAM)
;

        FRAMLDR r3
        FRAMLDR r5
        FRAMLDR r10,,r4                           ;ptr to page list
        LDR     r2,=ZeroPage
        MOV     r9,#AreaFlags_Duff                ;permissions for DuffEntry
        LDR     r7,[r2,#CamEntriesPointer]        ;r7 -> CAM
        MOV     r4,#ApplicationStart              ;log. address of first page
        LDR     r1,=DuffEntry                     ;means Nowhere, in CAM

;decide if we want to do TLB coherency as we go
        MOV     r6, r3, LSR #5 ; r6!=0 if doing global coherency (32 entry TLB)

        B       %FT60

;skip next 32 pages then continue
56
        ADD     r10,r10,#32*4
        ADD     r4,r4,#32*PageSize

;find the sparsely mapped pages, map them out, doing coherency as we go if enabled
60
        MOV     r8,#1                             ;initial bitmap mask for new bitmap word
        LDR     r11,[r5],#4                       ;next word of bitmap
        CMP     r11,#0                            ;if next 32 bits of bitmap clear, skip
        BEQ     %BT56                             ;skip loop must terminate if r3 > 0
62
        TST     r11,r8                            ;page is currently mapped in if bit set
        BEQ     %FT66

        LDR     r0,[r10]                          ;page no.
        ADD     r0,r7,r0,LSL #CAM_EntrySizeLog2   ;r0 -> CAM entry for page
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
        STMIA   r0,{r1,r9}                        ;CAM entry for page set to DuffEntry,AreaFlags_Duff
        LDR     lr,=L2PT                          ;lr -> L2PT
        MOV     r2, #0
        STR     r2,[lr,r4,LSR #(Log2PageSize-2)]  ;L2PT entry for page set to 0 (means translation fault)
        TEQ     r6, #0
        BNE     %FT65
      [ ZeroPage != 0
        LDR     r2,=ZeroPage
      ]
        MOV     r0,r4
        ARMop   MMU_ChangingUncachedEntry,,,r2
      [ ZeroPage != 0
        MOV     r2,#0
      ]        
65
        SUBS    r3,r3,#1
        STREQ   r2,[r5,#-4]                       ;make sure we clear last word of bitmap, and...
        BEQ     %FT90                             ;done
66
        ADD     r10,r10,#4                        ;next page no.
        ADD     r4,r4,#PageSize                   ;next logical address
        MOVS    r8,r8,LSL #1                      ;if 32 bits processed...
        BNE     %BT62
        MOV     r2, #0
        STR     r2,[r5,#-4]                       ;zero word of bitmap we've just traversed
        B       %BT60

90
        TEQ     r6, #0
        BEQ     %FT95
        LDR     r2,=ZeroPage
        ARMop   MMU_ChangingUncached,,,r2
95
        EXIT


; ----------------------------------------------------------------------------------
;
; AMB_MakeUnsparse
;
; entry: r0 = size of area (at top of current slot) to ensure is not sparsely mapped
;
; action: walk over space involved, to force abort handler fix up to map in any
;         pages not already there
;
AMB_MakeUnsparse ROUT
        Push    "r0-r2,r12,lr"
;  Debug AMB,"AMB_MakeUnsparse r0",r0
        ADD     r0,r0,#PageSize
        SUB     r0,r0,#1
        MOVS    r0,r0,LSR #Log2PageSize
        BEQ     %FT20
        LDR     r12,=ZeroPage+AMBControl_ws
        LDR     r12,[r12]
        CMP     r12,#0
        BEQ     %FT20
        LDR     r1,AMBMappedInNode
        CMP     r1,#0
        BEQ     %FT20
        LDR     r2,AMBFlags
        TST     r2,#AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
        BNE     %FT20
  [ AMB_ChocTrace
        LDR     r2,AMBNmakeunsparse
        ADD     r2,r2,#1
        STR     r2,AMBNmakeunsparse
  ]
        LDR     r2,[r1,#AMBNode_Npages]
; Debug AMB,"AMB_MakeUnsparse pages Npages ",r0,r2
        CMP     r0,r2
        MOVHI   r0,r2
        SUB     lr,r2,r0
        MOV     lr,lr,LSL #Log2PageSize
        ADD     lr,lr,#ApplicationStart
;  Debug AMB,"AMB_MakeUnsparse MappedInNode addr pages ",r1,lr,r0
10
        LDR     r2,[lr]                ;tends to wash data cache a bit, but this should be called rarely
        ADD     lr,lr,#PageSize
        SUBS    r0,r0,#1
        BNE     %BT10
20
        Pull    "r0-r2,r12,pc"


  ] ;AMB_LazyMapIn

; ----------------------------------------------------------------------------------
;
; AMB_FindMemMapEntries
;
; finds page numbers for pages currently at given logical start address,
; and fills in buffer; pages must exist
;
; (does not have any page number guesses)
;
; entry:
;   R3 =  no. of pages
;   R4 -> buffer for page entries
;   R5 =  logical address of 1st page
; exit:
;   buffer at R4 filled in with page numbers
;
AMB_FindMemMapEntries ROUT

        Push    "r0-r11,lr"

;initialise r0,r1,r2 as physical RAM chunk cache for AMB_r11topagenum routine
        LDR     r9,=ZeroPage+PhysRamTable
        LDMIA   r9,{r0,r1}        ;r0,r1 := phys addr,size of chunk
        MOV     r1,r1,LSR #12
        ADD     r1,r0,r1,LSL #12  ;r0,r1 := lowest addr,highest addr + 1 of chunk
        MOV     r2,#0             ;r2    := first page number of chunk

        LDR     r10,=L2PT
        ADD     r10,r10,r5,LSR #(Log2PageSize-2) ;r10 -> L2 entry for 1st page
        CMP     r3,#4                            ;handle pages in chunks of 4
        BLT     %FT20
10
        LDMIA   r10!,{r5-r8}                     ;next 4 L2PT entries
        MOV     r11,r5,LSR #Log2PageSize         ;r11 := phys_addr/page_size
        BL      AMB_r11topagenum
        MOV     r5,r11
        MOV     r11,r6,LSR #Log2PageSize
        BL      AMB_r11topagenum
        MOV     r6,r11
        MOV     r11,r7,LSR #Log2PageSize
        BL      AMB_r11topagenum
        MOV     r7,r11
        MOV     r11,r8,LSR #Log2PageSize
        BL      AMB_r11topagenum
        STMIA   r4!,{r5-r7,r11}                  ;fill in next 4 page numbers
        SUB     r3,r3,#4
        CMP     r3,#4
        BGE     %BT10
20
        CMP     r3,#0
        Pull    "r0-r11,pc",EQ
30
        LDR     r11,[r10],#4
        MOV     r11,r11,LSR #Log2PageSize
        BL      AMB_r11topagenum
        STR     r11,[r4],#4
        SUBS    r3,r3,#1
        BNE     %BT30
        Pull    "r0-r11,pc"

; ----------------------------------------------------------------------------------
;
;AMB_r11topagenum
;entry:
;     r0,r1,r2 = lowest addr,highest addr +1,first page no.
;                (cached physical RAM chunk)
;     r11      = physical_addr/page_size for page
;
;exit:
;     r11      = page number of page
;     r0,r1,r2 cache updated if necessary
;     r9       corrupted
;
AMB_r11topagenum ROUT
        CMP     r11,r0,LSR #Log2PageSize
        BLO     %FT10
        CMP     r11,r1,LSR #Log2PageSize
        BHS     %FT10
;cache hit (phys address in range of cached chunk)
        SUB     r11,r11,r0,LSR #Log2PageSize    ;pages into chunk
        ADD     r11,r11,r2                      ;page number
        MOV     pc,lr
10
        LDR     r9,=ZeroPage+PhysRamTable
        MOV     r2,#0                        ;start at page number 0
20
        LDMIA   r9!,{r0,r1}                  ;r0,r1 := phys addr,size of chunk
        SUB     r11,r11,r0,LSR #Log2PageSize
        CMP     r11,r1,LSR #Log2PageSize
        ADDHS   r11,r11,r0,LSR #Log2PageSize
        ADDHS   r2,r2,r1,LSR #Log2PageSize
        BHS     %BT20
        MOV     r1,r1,LSR #12
        ADD     r11,r11,r2
        ADD     r1,r0,r1,LSL #12
        MOV     pc,lr
@


4.3
log
@Delete pre-HAL and 26bit code
Detail:
  This change gets rid of the following switches from the source (picking appropriate code paths for a 32bit HAL build):
  * HAL
  * HAL26
  * HAL32
  * No26bitCode
  * No32bitCode
  * IncludeTestSrc
  * FixR9CorruptionInExtensionSWI
  Various old files have also been removed (POST code, Arc/STB keyboard drivers, etc.)
Admin:
  Identical binary to previous revision for IOMD & Raspberry Pi builds


Version 5.49. Tagged as 'Kernel-5_49'
@
text
@d533 1
a533 1
        MOVEQ   r9,#AP_Duff  ;PPL for mapped out pages
d538 3
a540 2
        ADRL    r1,PPLTrans
        AND     lr,r9,#3
a541 22
        LDR     r11,[r1,lr,LSL #2]
  [ MEMM_Type = "VMSAv6"
        ; VMSAv6 is tricky, use XCBTable/PCBTrans
        ASSERT  DynAreaFlags_CPBits = 7*XCB_P :SHL: 10
        ASSERT  DynAreaFlags_NotCacheable = XCB_NC :SHL: 4
        ASSERT  DynAreaFlags_NotBufferable = XCB_NB :SHL: 4
        LDR     r2,[r7,#MMU_PCBTrans]
        TST     r9,#PageFlags_TempUncacheableBits
        AND     r1,r9,#DynAreaFlags_NotCacheable + DynAreaFlags_NotBufferable
        AND     r0,r9,#DynAreaFlags_CPBits
        ORRNE   r1,r1,#XCB_TU<<4            ; if temp uncache, set TU bit
        ORR     r1,r1,r0,LSR #10-4
        LDRB    r1,[r2,r1,LSR #4]           ; convert to X, C and B bits for this CPU
        ORR     r11,r11,r1
  |
        TST     r9,#DynAreaFlags_NotCacheable
        TSTEQ   r9,#PageFlags_TempUncacheableBits
        ORREQ   r11,r11,#L2_C         ;if cacheable (area bit CLEAR + temp count zero), then OR in C bit
        TST     r9,#DynAreaFlags_NotBufferable
        ORREQ   r11,r11,#L2_B         ;if bufferable (area bit CLEAR), then OR in B bit
  ]

a542 1

d605 1
a605 1
; As above, but uses provided PPL when mapping out instead of forcing to AP_Duff
d727 1
a727 1
        MOV     r9,#AP_Duff                       ;permissions for DuffEntry
d756 1
a756 1
        STMIA   r0,{r1,r9}                        ;CAM entry for page set to DuffEntry,AP_Duff
@


4.2
log
@Merge HAL branch to trunk
Detail:
  This change merges the past 15+ years of HAL branch development back to the trunk.
  This is effectively the end for non-HAL builds of the kernel, as no attempt has been made to maintain it during this merge, and all non-HAL & non-32bit code will soon be removed anyway.
  Rather than list everything that's been added to the HAL branch, it's easier to describe the change in terms of the things that the HAL branch was lacking:
  * Trunk version of Docs/32bit contained updated comments for the SVC stack structure during ErrorV
  * Trunk version of s/HeapMan contained a tweak to try and reduce the number of small free blocks that are created
  * Trunk version of s/Kernel contained a change to only copy 248 bytes of the error string to the error buffer (down from 252 bytes), to take into account the extra 4 bytes needed by the PSR. However this goes against the decision that's been made in the HAL branch that the error buffer should be enlarged to 260 bytes instead (ref: https://www.riscosopen.org/tracker/tickets/201), so the HAL build will retain its current behaviour.
  * Trunk version of s/MsgCode had RMNot32bit error in the list of error messages to count when countmsgusage {TRUE}
  * Trunk version of s/PMF/i2cutils contained support for OS_Memory 5, "read/write value of NVRamWriteSize". Currently the HAL branch doesn't have a use for this (in particular, the correct NVRamWriteSize should be specified by the HAL, so there should be no need for software to change it at runtime), and so this code will remain switched out in the HAL build.
Admin:
  Tested on Raspberry Pi


Version 5.48. Tagged as 'Kernel-5_48'
@
text
@a136 1
        ASSERT  No26bitCode                              ;assumes we have an abort stack! (recursive lazy fixup aborts may occur)
@


4.1
log
@Initial revision
@
text
@a17 2

; **************************************************************************************
d19 3
a21 2

;convert page number in $reg to L2PT entry (physical address+protection bits),
d24 2
a25 2
;entry: lr -> PhysBin table, r11 = protection bits
;exit:  r12 corrupted
d28 6
a33 6
        PageNumToL2PT $reg
        BIC     r12,$reg,#(3:SHL:(AMBPhysBinShift-2)) ;word alignment for PhysBin lookup
        LDR     r12,[lr,r12,LSR #(AMBPhysBinShift-2)] ;start physical address of bin
        AND     $reg,$reg,#AMBPhysBinMask             ;no. pages into bin
        ADD     $reg,r12,$reg,LSL #Log2PageSize    ;physical address of page
        ORR     $reg,$reg,r11                      ;munge in protection bits
d36 206
d262 10
a271 10
        LDMIA   r10!,{r0-r7}   ;next 8 page numbers
        PageNumToL2PT r0
        PageNumToL2PT r1
        PageNumToL2PT r2
        PageNumToL2PT r3
        PageNumToL2PT r4
        PageNumToL2PT r5
        PageNumToL2PT r6
        PageNumToL2PT r7
        STMIA   r9!,{r0-r7}    ;write 8 L2PT entries
d280 1
a280 1
        PageNumToL2PT r0
d285 6
a290 4
        ARM_read_ID r0
        AND     r0,r0,#&F000
        CMP     r0,#&A000
        ARMA_drain_WB EQ         ;because L2PT area for AppSpace will be bufferable
d293 2
d302 3
a304 1
        ADD     $reg,r11,$reg,LSL #3    ;r0 -> CAM entry for 1st page
d308 2
d326 1
a326 1
        MOV     r11,#0
d363 2
a364 1

d379 1
a379 1
        MOV     r11,#0
d407 13
d421 52
a472 1
;AMB_movepagesout_L2PT
d480 3
a482 3
AMB_movepagesout_L2PT ROUT
        Push    "r0-r8,lr"

d496 2
a497 2
        BLT     %FT20
10
d501 2
a502 2
        BGE     %BT10
20
d504 2
a505 2
        BEQ     %FT35
30
d508 7
a514 7
        BNE     %BT30
35
        ARM_read_ID r0
        AND     r0,r0,#&F000
        CMP     r0,#&A000
        ARMA_drain_WB EQ         ;because L2PT area for AppSpace will be bufferable
        Pull    "r0-r8,pc"
d516 3
a518 2
;**************************************************************************
; AMB_SetMemMapEntries: 
d526 1
a526 1
; +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
a527 1
AMB_SetMemMapEntries ROUT
d537 1
d541 1
d543 14
d562 1
a566 1
        MOV     r7,#0
d568 3
a570 2
        ADD     r1,r7,r2,LSL #3
        LDR     r4,[r1]                     ;fetch old logical addr. of 1st page from CAM
d580 18
a597 3
;could be an optimise here if source is FreePool and we know that FreePool
;has not been used - ie. no need to clean/flush cache(s) - not done yet (requires
;sorting of Wimp_ClaimFreeMemory)
a598 1
        BL      AMB_movepagesout_L2PT
d601 1
a601 1
        B       AMB_smme_cachecleanflush ;needed because of the map out from source
d603 2
a604 1
;all pages sourced from same old logical page 'nowhere'
d609 1
a609 4
;don't need to flush cache at end of mapin (already coherent, since
;nothing mapped in before), but do need to flush TLB (eg. TLB will cache
;access denial for app space after mapout)
        B       AMB_smme_TLBflush
d611 2
a612 1
;all pages destined for same new logical page 'nowhere'
d614 5
a618 2
        LDR     r3,=DuffEntry
        BL      AMB_movepagesout_L2PT
d621 1
a621 39
;(clean and) flush cache(s) appropriately
AMB_smme_cachecleanflush
        ARM_read_ID r0
        AND     r0,r0,#&F000
        CMP     r0,#&A000
        ARM67_flush_cache NE
        ARM67_flush_TLB NE
        Pull    "r0-r4,r7-r11, pc",NE

;we have a StrongARM then
;
;here, r4 = old logical addr. of 1st page, r8 = no. of pages
;
;StrongARM lets us clean data cache (DC) after remapping, because it writes back by
;physical address.
;
        MOV     r0,r4                      ;r0 := start address for clean/flush
        ADD     r1,r0,r8,LSL #Log2PageSize ;r1 := end address for clean/flush (exclusive)

;Cleaning a sufficiently small space by range will be quicker, because of the fixed
;memory reading cost for a full DC clean. A sufficiently large space will be better handled
;by full clean, because of the huge number of clean/flush line instructions for the range
;case. We use a threshold to switch between the two schemes. The value of the threshold
;depends on memory speed, core speed etc. but is not particularly critical.

        SUB     r2,r1,r0
        CMP     r2,#AMB_ARMA_CleanRange_thresh
        BLO     AMB_smme_StrongARM_flushrange

        MOV     r2,#ARMA_Cleaner_flipflop
        LDR     r1,[r2]
        EOR     r1,r1,#16*1024
        STR     r1,[r2]
        ARMA_clean_DC r1,r2,r3     ;effectively, fully clean/flush wrt non-interrupt stuff
        ARMA_drain_WB
        ARMA_flush_IC WithoutNOPs  ;do *not* flush DC - may be interrupt stuff in it
        MOV     r0,r0              ;NOPs to ensure 4 instructions after IC flush before return
        MOV     r0,r0
        ARMA_flush_TLBs
d624 19
a642 1
AMB_smme_StrongARM_flushrange
d644 40
a683 29
  [ SAcleanflushbroken                  ; ARMA_cleanflush_DCentry instruction seems to be ineffective.
01
        ARMA_clean_DCentry r0
        ARMA_flush_DCentry r0
        ADD     r0,r0,#32
        ARMA_clean_DCentry r0
        ARMA_flush_DCentry r0
        ADD     r0,r0,#32
        ARMA_clean_DCentry r0
        ARMA_flush_DCentry r0
        ADD     r0,r0,#32
        ARMA_clean_DCentry r0
        ARMA_flush_DCentry r0
        ADD     r0,r0,#32
        CMP     r0,r1
        BLO     %BT01                   ;loop to clean DC over logical range
  |
01
        ARMA_cleanflush_DCentry r0
        ADD     r0,r0,#32
        ARMA_cleanflush_DCentry r0
        ADD     r0,r0,#32
        ARMA_cleanflush_DCentry r0
        ADD     r0,r0,#32
        ARMA_cleanflush_DCentry r0
        ADD     r0,r0,#32
        CMP     r0,r1
        BLO     %BT01                   ;loop to clean DC over logical range
  ]
d685 4
a688 5
        ARMA_drain_WB
        ARMA_flush_IC WithoutNOPs
        MOV     r0,r0                   ;NOPs to ensure 4 instructions after IC flush before return
        MOV     r0,r0
        ARMA_flush_TLBs
d690 171
a860 1
        Pull    "r0-r4,r7-r11, pc"
a861 8
AMB_smme_TLBflush
        ARM_read_ID r0
        AND     r0,r0,#&F000
        CMP     r0,#&A000
        ARM67_flush_TLB NE
        ARMA_flush_TLBs EQ
AMB_smme_exit
        Pull    "r0-r4,r7-r11, pc"
d863 1
d865 2
a866 1
; ************************************************************************
a880 2
; ************************************************************************

d886 1
a886 1
        MOV     r9,#PhysRamTable
d888 2
a889 1
        ADD     r1,r1,r0          ;r0,r1 := lowest addr,highest addr + 1 of chunk
d925 2
d948 1
a948 1
        MOV     r9,#PhysRamTable
d957 1
a957 1
        ADD     r1,r1,r0
d959 1
@


4.1.3.1
log
@Import from cleaned 370 CD
@
text
@@


4.1.3.2
log
@RISC OS 3.71 version taken
@
text
@a237 27
  [ ARM810support
    ;Previously supported ARMs all tolerate cache (clean and) flush _after_
    ;remapping - ARMs 6,7 because there is no clean, StrongARM because the cache
    ;writebacks use physical address.
    ;ARM810 does not support clean of writeback cache after remapping, since
    ;writebacks use virtual address. Rather than completely restructure code,
    ;this routine is called before remapping where necessary, and cleans/flushes
    ;if it finds we are running on ARM 810.
    ;
    ;corrupts r3
    ;
AMB_cachecleanflush_ifARM810
        ARM_read_ID r3
        AND     r3,r3,#&F000
        CMP     r3,#&8000
        MOVNE   pc,lr           ;not ARM8
    [ ARM810cleanflushbroken
        Push    "lr"
        ARM8_cleanflush_IDC r3,lr
        Pull    "pc"
    |
        ARM8_cleanflush_IDC r3
        MOV     pc,lr
    ]

  ] ;ARM810support

d239 1
a239 1
; AMB_SetMemMapEntries:
a287 3
  [ ARM810support
        BL      AMB_cachecleanflush_ifARM810
  ]
a295 3
  [ ARM810support
        BL      AMB_cachecleanflush_ifARM810
  ]
a305 3
  [ ARM810support
        BL      AMB_cachecleanflush_ifARM810
  ]
d310 1
a310 1
;(clean and) flush cache(s) appropriately, then flush TLB(s)
a313 5
  [ ARM810support
        CMP     r0,#&8000      ;cache clean/flush done before remapping if ARM810
        ARM8_flush_TLB EQ
        Pull    "r0-r4,r7-r11, pc",EQ
  ]
a391 4
  [ ARM810support
    ;there is a general macro, should have used this before anyway
        ARM_flush_TLB r0
  |
a396 1
  ]
@


4.1.3.3
log
@Correct RISC OS 3.71 import
@
text
@d326 3
@


4.1.3.3.6.1
log
@More stuff. Up to the desktop now; cache on, working keyboard. Some source
restructuring to start to make splitting it up into several object files more
feasible.
@
text
@d353 3
a355 6
        BEQ     AMB_smme_cachecleanflush_strongarm

        ARMop   MMU_Changing
        Pull    "r0-r4,r7-r11, pc"

AMB_smme_cachecleanflush_strongarm
d430 10
a439 1
        ARMop   TLB_InvalidateAll
@


4.1.3.3.6.2
log
@more use of ARMops in page manipulation, change register usage of ARmops
tested by kernel boot to star prompt only

Version 5.35, 4.79.2.11. Tagged as 'Kernel-5_35-4_79_2_11'
@
text
@d355 1
a355 2
        MOV     r11, #0
        ARMop   MMU_Changing,,,r11
d433 1
a433 2
        MOV     r11,#0
        ARMop   TLB_InvalidateAll,,,r11
@


4.1.3.3.6.3
log
@reintroduce Ursula AMBControl, recoded with generic ARMop style, not debugged yet

Version 5.35, 4.79.2.12. Tagged as 'Kernel-5_35-4_79_2_12'
@
text
@d18 2
d21 2
a22 3
; ----------------------------------------------------------------------------------
;
;convert page number in $pnum to L2PT entry (physical address+protection bits),
d25 2
a26 2
;entry: $ptable -> PhysBin table, $pbits = protection bits
;exit:  $temp corrupted
d29 6
a34 6
        PageNumToL2PT $pnum,$ptable,$pbits,$temp
        BIC     $temp,$pnum,#(3:SHL:(AMBPhysBinShift-2))       ;word alignment for PhysBin lookup
        LDR     $temp,[$ptable,$temp,LSR #(AMBPhysBinShift-2)] ;start physical address of bin
        AND     $pnum,$pnum,#AMBPhysBinMask                    ;no. pages into bin
        ADD     $pnum,$temp,$pnum,LSL #Log2PageSize            ;physical address of page
        ORR     $pnum,$pnum,$pbits                             ;munge in protection bits
a36 93

  [ AMB_LazyMapIn

; ----------------------------------------------------------------------------------
;
;AMB_LazyFixUp
;
; *Only* for ARMs where the abort handler can restart instructions
;
; Routine to be used in abort handlers (in abort32 mode), that checks to see if abort
; is expected, and fixes things up if so, ready to restart instruction.
;
; Fix up consists of mapping in affected page, and updating AMBMappedInRegister. This
; may seem like a lot of work, but remember that the L2PT and CAM updates for each page are
; needed anyway in non-lazy scheme, so there is really only a housekeeping overhead.
;
; There is no cache clean/flush consideration here, since the map is a map in from Nowhere.
; TLB flush consideration is left to main abort handler code - in fact there may not
; be a TLB flush consideration at all, if ARM TLB can be assumed not to cache an
; entry which is a translation fault, as seems rational.
;
; entry: r0 = aborting address (data address for data abort, instruction address
;        for prefetch abort), r1-r7 trashable, no stack
; exit:  r0 = non-zero if abort was expected and fixed up, zero if not
;
AMB_LazyFixUp ROUT
        ; not hooked in yet!
        ! 0, "AMB_LazyFixUp needs hooking into abort handlers"
        MOV     r7,r12
        MOV     r12,#AMBControl_ws
        LDR     r12,[r12]
        CMP     r12,#0
        BEQ     %FT20
        SUBS    r0,r0,#ApplicationStart
        BMI     %FT20
        MOV     r0,r0,LSR #Log2PageSize                  ;address now in terms of pages from ApplicationStart
        LDR     r1,AMBMappedInNode 
        CMP     r1,#0
        BEQ     %FT20
        LDR     r2,[r1,#AMBNode_Npages]
        CMP     r2,r0
        BLS     %FT20
;
;need not check MappedInRegister first, since if abort has happened in range of current
;AppSpace, then the page can be assumed to be mapped out
;
        ADD     r1,r1,#AMBNode_pages
        ADD     r1,r1,r0,LSL #2                          ;r1 -> page involved, in node page list
        LDR     r2,AMBPhysBin
        ! 0, "AMB_LazyFixup a symbol for L2PT protection bits would be handy"
        MOV     r3,#&FF0
        ORR     r3,r3,#&E                                ;&FFE = L2PT protection bits for ordinary page
        LDR     r4,[r1]
        MOV     r6,r4
        PageNumToL2PT r4,r2,r3,r5
;
;here, r6 = page number of page involved, r4 = new L2PT entry value to map in page
;
        LDR     r2,AMBMappedInNpages                     ;for convenience, update bitmap etc. here...
        ADD     r2,r2,#1
        STR     r2,AMBMappedInNpages
        ADR     r2,AMBMappedInRegister
        ADD     r2,r2,r0,LSR #5-2                        ;r2 -> bitmap word affected
        BIC     r2,r2,#3
        AND     r3,r0,#31
        MOV     r5,#1
        MOV     r5,r5,LSL r3                             ;mask for bit affected in bitmap word
        LDR     r3,[r2]
        ORR     r3,r3,r5
        STR     r3,[r2]

        ADD     r0,r0,#ApplicationStart:SHR:Log2PageSize ;address now in terms of pages from 0
        MOV     r5,#L2PT
        STR     r4,[r5,r0,LSL #2]                        ;update L2PT

        MOV     r5,#0
        LDR     r5,[r5,#CamEntriesPointer]
        ADD     r5,r5,r6,LSL #3                          ;r5 -> CAM entry affected
        MOV     r0,r0,LSL #Log2PageSize                  ;address is now ordinary again, and must be non-zero
        MOV     r1,#0                                    ;0 = AP for ordinary page
        STMIA   r5,{r0,r1}                               ;update CAM entry
        MOV     r12,r7
        MOV     pc,lr                                    ;r0 is non-zero
20
        MOV     r0,#0
        MOV     r12,r7
        MOV     pc,lr

  ] ;AMB_LazyMapIn


; ----------------------------------------------------------------------------------
;
d57 10
a66 10
        LDMIA   r10!,{r0-r7}         ;next 8 page numbers
        PageNumToL2PT r0,lr,r11,r12
        PageNumToL2PT r1,lr,r11,r12
        PageNumToL2PT r2,lr,r11,r12
        PageNumToL2PT r3,lr,r11,r12
        PageNumToL2PT r4,lr,r11,r12
        PageNumToL2PT r5,lr,r11,r12
        PageNumToL2PT r6,lr,r11,r12
        PageNumToL2PT r7,lr,r11,r12
        STMIA   r9!,{r0-r7}          ;write 8 L2PT entries
d75 1
a75 1
        PageNumToL2PT r0,lr,r11,r12
d80 4
a85 2
; ----------------------------------------------------------------------------------
;
a96 2
; ----------------------------------------------------------------------------------
;
d150 1
a150 2
; ----------------------------------------------------------------------------------
;
d193 1
a193 2
; ----------------------------------------------------------------------------------
;
d232 4
d238 29
a266 3
; ----------------------------------------------------------------------------------
;
; AMB_SetMemMapEntries: 
d274 2
d277 1
a277 2

        Push    "r0-r4,r7-r11,lr"  
d301 1
a301 2
        ADD     r1,r7,r2,LSL #3             ;r1 -> CAM entry for 1st page
  [ AMB_LimpidFreePool
a302 4
        LDR     r3,[r1,#4]                  ;fetch old PPL of 1st page from CAM
  |
        LDR     r4,[r1]                     ;fetch old logical addr. of 1st page from CAM
  ]
d312 5
a316 19
;
  [ AMB_LimpidFreePool
    ;can avoid cache clean/flush for moving pages out from FreePool, since FreePool pages are uncacheable
    ;
        TST     r3, #DynAreaFlags_NotCacheable  ;test PPL of 1st page for not cacheable bit set
        BEQ     AMB_smme_mapnotlimpid           ;if clear, must do full map somewhere with cache clean/flush
    ;
    ;this should be map FreePool -> App Space then
    ;
        MOV     r0,r4                            ;address of 1st page
        MOV     r1,r8                            ;number of pages 
        MOV     r3,#0
        ARMop   MMU_ChangingUncachedEntries,,,r3 ;no cache worries, hoorah
        MOV     r3,r5
        BL      AMB_movepagesout_L2PT            ;unmap 'em from where they are
        BL      AMB_movepagesin_L2PT             ;map 'em to where they now be
        BL      AMB_movepagesin_CAM              ;keep the bloomin' soft CAM up to date
        Pull    "r0-r4,r7-r11, pc"
AMB_smme_mapnotlimpid
a317 5
;
        MOV     r0,r4                            ;address of 1st page
        MOV     r1,r8                            ;number of pages
        MOV     r3,#0
        ARMop   MMU_ChangingEntries,,,r3         ;
d322 1
a322 1
        Pull    "r0-r4,r7-r11, pc"
d324 1
a324 2
;all pages sourced from same old logical page Nowhere, ie. pages currently mapped out, no cache worries
;
a325 4
        MOV     r0,r4                            ;address of 1st page
        MOV     r1,r8                            ;number of pages
        MOV     r3,#0
        ARMop   MMU_ChangingUncachedEntries,,,r3 ;TLB coherency, possibly not needed (TLBs shouldn't cache 0 entries)
d329 4
a332 1
        Pull    "r0-r4,r7-r11, pc"
d334 1
a334 2
;all pages destined for same new logical page Nowhere, ie. mapping them out
;
d336 3
a338 4
        MOV     r0,r4                            ;address of 1st page
        MOV     r1,r8                            ;number of pages 
        MOV     r3,#0
        ARMop   MMU_ChangingEntries,,,r3         ;
d343 14
a356 1
AMB_smme_exit
d359 1
d361 1
a361 4

  [ AMB_LazyMapIn

; ----------------------------------------------------------------------------------
d363 1
a363 1
; AMB_SetMemMapEntries_SparseMapOut:
d365 2
d368 24
a391 7
; entry:
;   R3  =  no. of pages currently mapped in (0=none)
;   R4  -> list of page entries (1 word per entry, giving page no.)
;   R5  -> bitmap of pages mapped in (1 bit per page in whole page list)
;   R6  =  total no. of pages in slot
;
AMB_SetMemMapEntries_SparseMapOut ROUT
d393 1
a393 3
        CMP     r3,#0
        MOVEQ   pc,lr
        Push    "r0-r11,lr"
d395 29
a423 26
        MOV     r10,r4                            ;ptr to page list
        MOV     r7,#0
        LDR     r7,[r7,#CamEntriesPointer]        ;r7 -> CAM
        LDR     lr,=L2PT                          ;lr -> L2PT
        MOV     r9,#AP_Duff                       ;permissions for DuffEntry
        LDR     r1,=DuffEntry                     ;means Nowhere, in CAM
        MOV     r4,#ApplicationStart              ;log. address of first page
        MOV     r2,#0                             ;r2 is zero during sparse map out

        ;if the number of pages mapped in is small enough, we'll do cache/TLB coherency on
        ;just those pages, else global (performance decision, threshold probably not critical)

        ARMop   Cache_RangeThreshold,,,r2         ;returns threshold (bytes) in r0
        CMP     r3,r0,LSR #Log2PageSize
        MOVLO   r6,#0                             ;r6 := 0 if we are to do coherency as we go
        BLO     %FT10                             ;let's do it

        Push    "lr"                              ;global coherency
        ARMop   MMU_Changing,,,r2
        Pull    "lr"
        B       %FT10

;skip next 32 pages then continue
06
        ADD     r10,r10,#32*4                     
        ADD     r4,r4,#32*PageSize
d425 5
a429 30
;find the sparsely mapped pages, map them out, doing coherency as we go if enabled
10
        MOV     r8,#1                             ;initial bitmap mask for new bitmap word
        LDR     r11,[r5],#4                       ;next word of bitmap
        CMP     r11,#0                            ;if next 32 bits of bitmap clear, skip
        BEQ     %BT06                             ;skip loop must terminate if r3 > 0
12
        TST     r11,r8                            ;page is currently mapped in if bit set
        BEQ     %FT16
        TEQ     r6, #0
        BNE     %FT14                             ;check for coherency as we go
        Push    "lr"
        MOV     r0,r4                             ;address of page
        ARMop   MMU_ChangingEntry,,,r2
        Pull    "lr"
14
        LDR     r0,[r10]                          ;page no.
        ADD     r0,r7,r0,LSL #3                   ;r0 -> CAM entry for page
        STMIA   r0,{r1,r9}                        ;CAM entry for page set to DuffEntry,AP_Duff
        STR     r2,[lr,r4,LSR #(Log2PageSize-2)]  ;L2PT entry for page set to 0 (means translation fault)
        SUBS    r3,r3,#1
        STREQ   r2,[r5,#-4]                       ;make sure we clear last word of bitmap, and...
        BEQ     %FT20                             ;done
16
        ADD     r10,r10,#4                        ;next page no.
        ADD     r4,r4,#PageSize                   ;next logical address
        MOVS    r8,r8,LSL #1                      ;if 32 bits processed...
        BNE     %BT12
        STR     r2,[r5,#-4]                       ;zero word of bitmap we've just traversed (r2 is 0)
        B       %BT10
d431 1
a431 2
20
        Pull    "r0-r11,pc"
d433 5
d440 1
a440 52
; ----------------------------------------------------------------------------------
;
; AMB_MakeUnsparse
;
; entry: r0 = size of area (at top of current slot) to ensure is not sparsely mapped
;
; action: walk over space involved, to force abort handler fix up to map in any
;         pages not already there
;
AMB_MakeUnsparse ROUT
        Push    "r0-r2,r12,lr"
;  Debug AMB,"AMB_MakeUnsparse r0",r0
        ADD     r0,r0,#PageSize
        SUB     r0,r0,#1
        MOVS    r0,r0,LSR #Log2PageSize
        BEQ     %FT20
        MOV     r12,#AMBControl_ws
        LDR     r12,[r12]
        CMP     r12,#0
        BEQ     %FT20
        LDR     r1,AMBMappedInNode
        CMP     r1,#0
        BEQ     %FT20
        LDR     r2,AMBFlags
        TST     r2,#AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
        BNE     %FT20
  [ AMB_ChocTrace
        LDR     r2,AMBNmakeunsparse
        ADD     r2,r2,#1
        STR     r2,AMBNmakeunsparse
  ] 
        LDR     r2,[r1,#AMBNode_Npages]
; Debug AMB,"AMB_MakeUnsparse pages Npages ",r0,r2
        CMP     r0,r2
        MOVHI   r0,r2
        SUB     lr,r2,r0
        MOV     lr,lr,LSL #Log2PageSize
        ADD     lr,lr,#ApplicationStart
;  Debug AMB,"AMB_MakeUnsparse MappedInNode addr pages ",r1,lr,r0
10
        LDR     r2,[lr]                ;tends to wash data cache a bit, but this should be called rarely
        ADD     lr,lr,#PageSize
        SUBS    r0,r0,#1
        BNE     %BT10
20
        Pull    "r0-r2,r12,pc"


  ] ;AMB_LazyMapIn

; ----------------------------------------------------------------------------------
;
d455 2
a499 2
; ----------------------------------------------------------------------------------
;
@


4.1.3.3.6.4
log
@  fix for sparse map out
Detail:

Admin:
  not tested
@
text
@d461 1
d475 3
a477 1
        ARMop   MMU_Changing,,,r2                 ;global coherency
d496 1
d499 1
a503 1
        LDR     lr,=L2PT                          ;lr -> L2PT
@


4.1.3.3.6.5
log
@Reimplement Lazy task swapping, an amusing idea from Ursula,
would have done it sooner but couldn't be bothered (humour).
Currently activates for all ARMs flagged as base-restored
abort model. No handling of eg. StrongARM pre-revT bug, but
then the kernel no longer runs on StrongARM (progress).
Still some details to fix: all aborts in current app space
assumed to be missing pages, but this must be fixed to
handle abort code in app space, things like debuggers
marking code read only.

Plus, small fixes:
  OS_Memory 8 returns vaguely useful info for RAM,VRAM
  in HAL build (temporary partial implementation)
  Broken handling of old BBC commands with (fx,tv etc)
  with no spaces fixed (fudgeulike code from Ursula,
  now 32-bit).

Version 5.35, 4.79.2.31. Tagged as 'Kernel-5_35-4_79_2_31'
@
text
@d59 1
a59 1
; exit:  r0 = non-zero (NE status) if abort was expected and fixed up, zero (EQ status) if not
d62 2
d85 1
d114 1
a114 1
        MOVS    r0,r0,LSL #Log2PageSize                  ;address is now ordinary again, and must be non-zero
d118 1
a118 1
        MOV     pc,lr                                    ;r0 is non-zero, NE status
d120 1
a120 1
        MOVS    r0,#0
d122 1
a122 1
        MOV     pc,lr                                    ;r0 is zero, EQ status
@


4.1.3.3.6.6
log
@Further work on Lazy task swapping:
  hooks to give correct mapping info for OS_Memory 0
  same for OS_ReadMemMapEntries
  same for OS_FindMemMapEntries
  Lazy fixup routine no longer assumes an abort in current
  app space must be a truant page. However, work in this
  area not complete (no support yet for abort handler code
  in app space itself, eg. for C trampoline)
Good to know this will be a big performance boost when
our products use one monolithic application (sarcasm).
Ta

Version 5.35, 4.79.2.32. Tagged as 'Kernel-5_35-4_79_2_32'
@
text
@a66 3
        LDR     r1,AMBFlags
        TST     r1,#AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
        BNE     %FT20
d77 2
a78 18
; check/update the MappedIn bitmap
;
        ADR     r2,AMBMappedInRegister
        ADD     r2,r2,r0,LSR #5-2
        BIC     r2,r2,#3                                 ;r2 -> bitmap word affected
        AND     r3,r0,#31
        MOV     r5,#1
        MOV     r5,r5,LSL r3                             ;mask for bit affected in bitmap word
        LDR     r3,[r2]
        TST     r3,r5                                    ;if page already mapped in, not a lazy abort
        BNE     %FT20
        ORR     r3,r3,r5                                 ;ok, mark that we are going to map this page in
        STR     r3,[r2]
        LDR     r2,AMBMappedInNpages                     ;count it
        ADD     r2,r2,#1
        STR     r2,AMBMappedInNpages
;
; now map in the the page that went pop
d91 13
d107 1
a107 1
;
a122 68
; ----------------------------------------------------------------------------------

  [ AMB_LazyMapIn

;
; If page of given logical address (r0) is in current app space, make sure page is
; 'honest' ie. properly mapped in. This is for things like FindMemMapEntries
; that must return sensible info (and presumably their client needs a consistent
; view of app space mapping, so that laziness is transparent)
;
AMB_MakeHonestLA  ROUT
        CMP     r0,#AbsMaxAppSize                        ;quick dismiss if definitely not app address
        MOVHS   pc,lr
        Push    "r1,r12,lr"        
        MOV     r12,#AMBControl_ws
        LDR     r12,[r12]
        CMP     r12,#0
        BEQ     %FT90                                    ;we're dormant!
        SUBS    r14,r0,#ApplicationStart
        BMI     %FT90                                    ;below app space
        MOV     r14,r14,LSR #Log2PageSize                ;pages from ApplicationStart
        LDR     r1,AMBMappedInNode 
        CMP     r1,#0
        BEQ     %FT90                                    ;no node mapped in
        LDR     r1,[r1,#AMBNode_Npages]
        CMP     r1,r14                                   ;HI if log addr is in current app space
        LDRHI   r1, [r0,#0]                              ;make honest if necessary (magic of abort fixups!)
90
        Pull    "r1,r12,pc"


; similar to AMB_MakeHonestLA, but for page of given page number (r0)
;
AMB_MakeHonestPN  ROUT
        Push    "r1-r3,r12,lr"
        MOV     r12,#AMBControl_ws
        LDR     r12,[r12]
        CMP     r12,#0
        BEQ     %FT90                                    ;we're dormant!
        MOV     r14,#0
        LDR     r1,[r14,#MaxCamEntry]
        CMP     r0,r1
        BHI     %FT90                                    ;invalid page number
        LDR     r1,[r14,#CamEntriesPointer]
        LDR     r1,[r1,r0,LSL #3]                        ;logical address from CAM
        LDR     r14,=Nowhere
        TEQ     r1,r14
        BNE     %FT90                                    ;only a page at Nowhere might be dishonest
        LDR     r1,AMBMappedInNode                       ;let's check the current node
        CMP     r1,#0
        BEQ     %FT90                                    ;no node mapped in
        LDR     r14,[r1,#AMBNode_Npages]
        MOV     r14,r14,LSL #Log2PageSize
        ADD     r14,r14,#ApplicationStart                ;top of current app space
        ADD     r1,r1,#AMBNode_pages                     ;[r1] is page number
        MOV     r2,#ApplicationStart                     ;r2 is logical address for page
10
        CMP     r2,r14
        BHS     %FT90
        LDR     r3,[r1],#4                               ;next page number in node
        TEQ     r3,r0                                    ;see if its the one that wants to be honest
        ADDNE   r2,r2,#PageSize                          ;next logical address
        BNE     %BT10
        LDR     r1,[r2,#0]                               ;make honest if necessary (magic of abort fixups!)
90
        Pull    "r1-r3,r12,pc"

  ] ;AMB_LazyMapIn
@


4.1.3.3.6.7
log
@Changes to Lazy task swapping to support abort handler in abort
mode (now that we can count to 32).
LazyFixup now ensures all app pages are mapped in before handing
on a real (no-lazy) abort. This allows an abort handler in app
space itself, without scrambling the details of the original
abort (via a lazy abort in abort mode).
Many a happy minute spent coding and testing for recursive
aborts and fixups, but lets just pretend it doesn't happen
shall we.
Tested with simple popbang code on ARM9 desktop build.

Version 5.35, 4.79.2.36. Tagged as 'Kernel-5_35-4_79_2_36'
@
text
@d66 1
a66 1
        BEQ     %FT90                                    ;not initialised!
d69 4
a72 1
        BNE     %FT90                                    ;not active
d75 1
a75 1
        BEQ     %FT90                                    ;no current node
a76 3
        SUBS    r0,r0,#ApplicationStart
        BMI     %FT20                                    ;abort not in current app space
        MOV     r0,r0,LSR #Log2PageSize                  ;address now in terms of pages from ApplicationStart
d78 1
a78 1
        BLS     %FT20                                    ;abort not in current app space
a121 5
;
; not our abort, but is possible that client abort handler is in app space, so force all
; app space pages in now (so that client abort handler does not cause lazy abort, scribbling over original abort details)
;
        ASSERT  No26bitCode                              ;assumes we have an abort stack! (recursive lazy fixup aborts may occur)
a122 16
        MOV     r1,#ApplicationStart                     ;good old page walk to provoke lazy fixups
        LDR     r2,AMBMappedInNode 
        LDR     r2,[r2,#AMBNode_Npages]
        CMP     r2,#0
        BEQ     %FT90
        MRS     r0,SPSR                                  ;preserve SPSR_abort for original abort details
        MOV     r4,lr                                    ;preserve lr_abort so we can return properly (!)
30
        LDR     r3,[r1]                                  ;bring that page in by the magic of aborts
        SUBS    r2,r2,#1
        ADD     r1,r1,#PageSize
        BNE     %BT30
        MSR     SPSR_all,r0                              ;SPSR for original abort
        MOV     lr,r4                                    ;restore return address
;
90
@


4.1.3.3.6.8
log
@Reimplement enhancements to kernel Dynamic Area support from
Ursula. Quite a hairy code merge really, so let's hope it is
worth it to someone. What you get (back after 2 or 3 years):
- much more efficient for largish numbers of DAs (relevance
  to current build = approx 0)
- fancy reason codes to support fast update of
  Switcher bar display (relevance = 0)
- support for clamped maximum area sizes, to avoid address
  space exhaustion with big memory (relevance = 0)
- better implementation of shrinkable DAs, performance
  wise (if lots of DAs, relevance = approx 0)
- support for 'Sparse' DAs. Holey dynamic areas, Batman!
  (relevance, go on someone use the darned things)
Moderately development tested on HAL/32bit ARM9 desktop.
Note the Switcher should be compiled to use the new
reason codes 6&7, for fabled desktop builds.

Also, during this work, so I could see the wood for the
trees, redid some source code clean up, removing pre-Medusa
stuff (like I did about 3 years ago on Ursula, sigh). That's
why loads of source files have changed. The new DA stuff
is confined pretty much to hdr.KernelWS and s.ChangeDyn.

Ta.

Version 5.35, 4.79.2.38. Tagged as 'Kernel-5_35-4_79_2_38'
@
text
@d140 1
a140 1
        MSR     SPSR_cxsf,r0                             ;SPSR for original abort
@


4.1.3.3.6.9
log
@In the No26bitCode case (ie when abort handlers are entered in ABT32 mode),
if lazy task swapping was enabled and a data abort occurred that was not a
page translation fault, then the code in AMB_LazyFixUp to map in the whole
application slot was being circumvented, leading to problems for abort
handlers in application space because r14_abt was corrupted by any abort
due to accessing the abort handler itself. The test of the FSR (to
compensate for the FAR being unusable for external aborts) which prompted
the circumvention has therefore been moved inside AMB_LazyFixup.
Also now preserves the FSR and FAR across AMB_LazyFixUp, so they are now
visible from application abort handlers if desired.

Version 5.35, 4.79.2.50. Tagged as 'Kernel-5_35-4_79_2_50'
@
text
@a58 2
;        r1 = 1 for prefetch abort, 0 for data abort
;        FSR valid for data aborts, unpredictable for prefetch aborts
a59 1
;        FAR,FSR,SPSR_abt,lr_abt preserved
d67 2
a68 2
        LDR     r2,AMBFlags
        TST     r2,#AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
d70 2
a71 2
        LDR     r2,AMBMappedInNode
        CMP     r2,#0
d73 1
a73 6
        ARM_read_FSR r6                                  ;hang onto FSR in case we have to preserve it
        TEQ     r1,#1                                    ;if data abort
        ANDNE   r3,r6,#&F
        TEQNE   r3,#7                                    ; and not a page translation fault
        BNE     %FT20                                    ; then not a lazy abort (and FAR may be invalid anyway)
        LDR     r2,[r2,#AMBNode_Npages]
d129 1
a129 1
        LDR     r2,AMBMappedInNode
a134 2
        ARM_read_FAR r5                                  ;preserve FAR in case client abort handler wants to read it
                                                         ;preserve FSR (already in r6) similarly
a141 2
        ARM_write_FAR r5                                 ;restore FAR
        ARM_write_FSR r6                                 ;restore FSR
d163 1
a163 1
        Push    "r1,r12,lr"
d171 1
a171 1
        LDR     r1,AMBMappedInNode
d422 1
a422 1
; AMB_SetMemMapEntries:
d432 1
a432 1
        Push    "r0-r4,r7-r11,lr"
d482 1
a482 1
        MOV     r1,r8                            ;number of pages
d519 1
a519 1
        MOV     r1,r8                            ;number of pages
d571 1
a571 1
        ADD     r10,r10,#32*4
d639 1
a639 1
  ]
@


4.1.3.3.6.10
log
@  Commit of kernel as featured in release 5.00.
Detail:
  Lots of changes since last version, at least the following:
  * Updated OS timestamp, removed alpha status
  * Negative INKEY OS version changed to &AA
  * GraphicsV is now alocated vector number &2A
  * ROM moved up to &FC000000
  * Max application slot increased to 512 Mbytes (for now)
  * Max size of RMA increased to 256 Mbytes
  * RMA is now first-created dynamic area (so it gets lowest address after
    top of application slot)
  * OS_Memory 10 reimplemeted
  * New OS_ReadSysInfo 6 values 18-22 added
  * OS_ReadSysInfo 8 gains flag bit to indicate soft power-off
  * Misc internal top-bit-set-address fixes
  * *ChangeDynamicArea can take sizes in megabytes or gigabytes
  * Magic word "&off" in R0 passed to OS_Reset powers down if possible
  * Added acceleration: block copy; CLS; text window scroll up; rectangle
    fill
  * Disabled LED flashing in page mode (liable to crash)
  * Masked sprite plot and VDU 5 text avoids reading the screen if possible
  * Framestore made USR mode accessible
  * Fix for VDU 5,127 bug - now relies on font definitions being in extreme
    quarters of memory, rather than bottom half
  * Allocated 64-bit OS_Convert... SWIs
  * IIC errors use allocated error numbers
  * Looks for Dallas RTC before Philips RTC because we're using a Philips
    NVRAM device with the same ID
  * Fix to bug that meant the oscillator in the Dallas RTC wasn't enabled
  * Default mouse type (USB) changed to allocated number
  * Ram disc max size increased to 128 Mbytes (Ursula merge) and made
    cacheable for StrongARMs (not XScale)
  * Branch through zero handler now works in USR mode, by use of a
    trampoline in the system stack to allow PC-relative register storage
  * Address exception handler changed to not use 0 as workspace
  * OS_Memory 13 extended to allow specification of cacheability and access
    privileges
  * Added OS_Memory 16 to return important memory addresses
  * RISCOS_MapInIO() takes cacheable flag in bit 3, access permissions in
    bits 10 and 11, doubly-mapped flag in bit 20, and access permissions
    specified flag in bit 21
  * Bug fix in last version for application abort handlers didn't quite
    work; register shuffle required
  * "Module is not 32-bit compatible" error now reports the module name
  * Default configured language changed from 10 to 11 (now Desktop again)

Version 5.35, 4.79.2.51. Tagged as 'Kernel-5_35-4_79_2_51'
@
text
@d59 1
a59 1
;        r2 = 1 for prefetch abort, 0 for data abort
d70 2
a71 2
        LDR     r1,AMBFlags
        TST     r1,#AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
d73 2
a74 2
        LDR     r1,AMBMappedInNode
        CMP     r1,#0
d77 1
a77 1
        TEQ     r2,#1                                    ;if data abort
d81 1
a81 1
        LDR     r2,[r1,#AMBNode_Npages]
d83 1
a83 1
        BLO     %FT20                                    ;abort not in current app space
d85 2
a86 2
        CMP     r0,r2
        BHS     %FT20                                    ;abort not in current app space
a90 1
        MOV     r5,#1
d94 1
a96 1
        LDR     r4,AMBMappedInNpages                     ;count it
d101 3
a103 2
        ADD     r4,r4,#1
        STR     r4,AMBMappedInNpages
@


4.1.3.3.6.11
log
@Merge Cortex kernel into HAL branch
Detail:
  This is a full merge of the Cortex kernel back into the HAL branch. Since the Cortex kernel is/was just a superset of the HAL branch, at this point in time both branches are identical.
  Main features the HAL branch gains from this merge:
  - ARMv6/ARMv7 support
  - High processor vectors/zero page relocation support
  - objasm 4 warning fixes
  - Improved HAL related functionality:
    - Support for HAL-driven RTCs instead of kernel-driven IIC based ones
    - Support for arbitrary size machine IDs
    - Support for multiple IIC busses
    - Support for any HAL size, instead of hardcoded 64k size
    - Probably some other stuff I've forgotten
  - Probably a few bug fixes here and there
Admin:
  Tested on BB-xM & Iyonix.
  Was successfully flashed to ROM on an Iyonix to test the Cortex branch implementation of the 2010 RTC bug fix.
  IOMD build untested - but has been known to work in the past.


Version 5.35, 4.79.2.123. Tagged as 'Kernel-5_35-4_79_2_123'
@
text
@d66 1
a66 1
        LDR     r12,=ZeroPage+AMBControl_ws
a109 7

; Calculate the L2PT protection bits in a nice way that won't produce broken code if we change MMU model
; This should match the AP_Full entry from the PPLTrans table that gets used by BangCam (plus C+B bits)
   [ MEMM_Type = "VMSAv6"
        MOV     r3,#(AP_Full*L2X_APMult)+L2_ExtPage+L2_C+L2_B
   |
        ASSERT  (AP_Full*L2_APMult)+L2_SmallPage+L2_C+L2_B = &FFE
d111 1
a111 2
        ORR     r3,r3,#&E
   ]
d122 1
a122 1
        LDR     r5,=ZeroPage
a153 3
      [ MEMM_Type = "VMSAv6"
        myISB   ,r0 ; Not sure if this is necessary or not; do it just in case
      ]
d176 1
a176 1
        LDR     r12,=ZeroPage+AMBControl_ws
d197 1
a197 1
        LDR     r12,=ZeroPage+AMBControl_ws
d201 1
a201 1
        LDR     r14,=ZeroPage
d309 1
a309 1
        LDR     r11,=ZeroPage
d362 1
a362 1
        LDR     r11,=ZeroPage
d466 1
a466 1
        LDR     r7,=ZeroPage
d495 1
a495 1
        LDR     r3,=ZeroPage
d507 1
a507 1
        LDR     r3,=ZeroPage
d520 1
a520 1
        LDR     r3,=ZeroPage
d532 1
a532 1
        LDR     r3,=ZeroPage
d563 2
a564 1
        LDR     r2,=ZeroPage
d566 1
a566 1
        LDR     r7,[r2,#CamEntriesPointer]        ;r7 -> CAM
d568 1
a568 1
        LDR     r1,=DuffEntry                     ;means Nowhere, in CAM
a596 1
        LDR     r2,=ZeroPage
a603 1
        MOV     r2, #0
d613 1
a613 3
        MOV     r2, #0
        STR     r2,[r5,#-4]                       ;zero word of bitmap we've just traversed
        LDR     r2,=ZeroPage
d637 1
a637 1
        LDR     r12,=ZeroPage+AMBControl_ws
d692 1
a692 1
        LDR     r9,=ZeroPage+PhysRamTable
d753 1
a753 1
        LDR     r9,=ZeroPage+PhysRamTable
@


4.1.3.3.6.12
log
@Teach the kernel about different memory attributes
Detail:
  Briefly, this set of changes:
  * Adjusts PhysRamTable so that it retains the flags passed in by the HAL from OS_AddRAM (by storing them in the lower 12 bits of the size field)
  * Sorts the non-VRAM entries of PhysRamTable by speed and DMA capability, to ensure optimal memory allocation during OS startup.
  * Adjust the initial memory allocation logic to allow the cursor/sound chunk and HAL noncacheable workspace to come from DMA capable memory
  * Extends OS_Memory 12 to accept a 'must be DMA capable' flag in bit 8 of R0. This is the same as available in ROL's OS.
  * Extends OS_DynamicArea 0 to allow the creation of dynamic areas that automatically allocate from DMA capable memory. In ROL's OS this was done by setting bit 12 of R4, but we're using bits 12-14 for specifying the cache policy, so instead bit 15 is used.
  * Fixes OS_ReadSysInfo 6 to return the correct DevicesEnd value now that the IRQ/device limit is computed at runtime
  File changes:
  * hdr/OSEntries - Add definitions of the various flags passed to OS_AddRAM by the HAL. Add a new flag, NoDMA, for memory which can't be used for DMA.
  * hdr/KernelWS - Tidy PhysRamTable definition a bit by removing all the DRAM bank definitions except the first - this makes it easier to search for code which is interacting with the table. Remove VRAMFlags, it's redundant now that the flags are kept in the table. Add DMA allocation info to InitWs.
  * s/AMBControl/memmap - Updated to mask out the flags from PhysRamTable when reading RAM block sizes.
  * s/ARM600 - Strip out a lot of IOMD specific pre-HAL code.
  * s/ChangeDyn - Updated to cope with the flags stored in PhysRamTable. Implement support for DMA-capable dynamic areas. Rewrite InitDynamicAreas to insert pages into the free pool in the right order so that the fastest memory will be taken from it first.
  * s/GetAll, s/Middle - Fix OS_ReadSysInfo 6 to return the correct HAL-specific DevicesEnd value
  * s/HAL - Significant rework of initial RAM allocation code to allow the kernel workspace to come from the fastest DMA incapable RAM, while also allowing allocation of DMA capable memory for HAL NCNB workspace & kernel cursor/sound chunks. ClearPhysRAM rewritten as part of this.
  * s/MemInfo - Updated to cope with the flags stored in PhysRamTable. Add support for the new OS_Memory 12 flag. Update OS_Memory 7 to not assume PhysRamTable entries are sorted in address order, and rip out the old pre-HAL IOMD implementation.
  * s/NewReset - Remove GetPagesFromFreePool option, assume TRUE (as this has been the case for the past 10+ years). Revise a few comments and strip dead code. Update to cope with PhysRamTable flags.
  * s/VMSAv6 - Remove a couple of unused definitions
  * s/vdu/vdudriver - Update to cope with PhysRamTable flags
Admin:
  Tested in Kinetic RiscPC ROM softload, Iyonix softload, & OMAP3


Version 5.35, 4.79.2.186. Tagged as 'Kernel-5_35-4_79_2_186'
@
text
@d708 1
a708 2
        MOV     r1,r1,LSR #12
        ADD     r1,r0,r1,LSL #12  ;r0,r1 := lowest addr,highest addr + 1 of chunk
d776 1
a776 1
        MOV     r1,r1,LSR #12
a777 1
        ADD     r1,r0,r1,LSL #12
@


4.1.3.3.6.13
log
@ Re enable compile with kernel built from current tree.

Detail:
Recent kernal changes appear to have enabled lazy task swapping, which brought
up a data alignment abort whilst compiling the source tree using a rom compiled
from this tree. Simple change added
to AMB_MakeHonestLA and PN routines to avoid this.

Admin:
  (highlight level of testing that has taken place)
  (bugfix number if appropriate)


Version 5.35, 4.79.2.240. Tagged as 'Kernel-5_35-4_79_2_240'
@
text
@d199 1
a199 2
        bichi   lr, r0, #3                               ; ensure word aligned
        ldrhi   r1, [lr,#0]                              ;make honest if necessary (magic of abort fixups!)
d236 1
a236 2
        bic     lr, r2, #3                               ;ensure word aligned
        ldr     r1,[lr,#0]                               ;make honest if necessary (magic of abort fixups!)
@


4.1.3.3.6.14
log
@Perform extra TLB maintenance on ARMv6+. Other cache/TLB maintenance tweaks.
Detail:
  s/ARMops - Implement Cache_RangeThreshold for PL310 (helps AMBControl to decide what type of TLB maintenance is best). Fix MMU_ChangingEntry_PL310 doing more work than is necessary; was attempting to flush all ways for a given address tag, when really it should have only been flushing all the lines within a page and letting the cache worry about the tags/indices they correspond to.
  s/ChangeDyn, s/VMSAv6, s/AMBControl/memmap - Do extra TLB maintenance following writes to the page tables, as mandated by the ARMv6+ memory order model. Fixes frequent crashes on Cortex-A9 when running with lazy task swapping disabled (and presumably fixes other crashes too)
  s/MemInfo - Fix OS_Memory cache/uncache so that it does cache/TLB maintenance on a per-page basis instead of a global basis. Vastly improves performance when you have a large cache, but may need tweaking again in future to do a global op if large numbers of pages are being modified.
Admin:
  Tested on Pandaboard


Version 5.35, 4.79.2.255. Tagged as 'Kernel-5_35-4_79_2_255'
@
text
@a288 6
      [ MEMM_Type = "VMSAv6"
        ; DSB + ISB required to ensure effect of page table write is fully
        ; visible (after overwriting a faulting entry)
        myDSB   ,r0
        myISB   ,r0,,y
      ]
a442 11
      [ MEMM_Type = "VMSAv6"
        ; In order to guarantee that the result of a page table write is
        ; visible, the ARMv6+ memory order model requires us to perform TLB
        ; maintenance (equivalent to the MMU_ChangingUncached ARMop) after we've
        ; performed the write. Performing the maintenance beforehand (as we've
        ; done traditionally) will work most of the time, but not always.
        LDR     r0, [sp, #4*4]
        LDR     r1, [sp, #8*4]
        LDR     r2, =ZeroPage
        ARMop   MMU_ChangingUncachedEntries,,,r2
      ]
d531 4
a618 18
     [ MEMM_Type = "VMSAv6"
        ; In order to guarantee that the result of a page table write is
        ; visible, the ARMv6+ memory order model requires us to perform TLB
        ; maintenance (equivalent to the MMU_ChangingUncached ARMop) after we've
        ; performed the write. Performing the maintenance beforehand (as we've
        ; done traditionally) will work most of the time, but not always.
        TEQ     r6, #0
        BNE     %FT15
      [ ZeroPage != 0
        LDR     r2,=ZeroPage
      ]
        MOV     r0,r4
        ARMop   MMU_ChangingUncachedEntry,,,r2
      [ ZeroPage != 0
        MOV     r2,#0
      ]
15
     ]
a632 12
     [ MEMM_Type = "VMSAv6"
        ; In order to guarantee that the result of a page table write is
        ; visible, the ARMv6+ memory order model requires us to perform TLB
        ; maintenance (equivalent to the MMU_ChangingUncached ARMop) after we've
        ; performed the write. Performing the maintenance beforehand (as we've
        ; done traditionally) will work most of the time, but not always.
        TEQ     r6, #0
        BEQ     %FT25
        LDR     r2,=ZeroPage
        ARMop   MMU_ChangingUncached,,,r2
25
     ]       
@


4.1.3.3.6.15
log
@Improve support for VMSAv6 cache policies & memory types. Expose raw ARMops via OS_MMUControl & cache information via OS_PlatformFeatures.
Detail:
  Docs/HAL/ARMop_API - Document two new ARMops: Cache_Examine and IMB_List
  hdr/KernelWS - Shuffle workspace round a bit to allow space for the two new ARMops. IOSystemType now deleted (has been deprecated and fixed at 0 for some time)
  s/ARM600 - Cosmetic changes to BangCam to make it clearer what's going on. Add OS_MMUControl 2 (get ARMop) implementation.
  s/ARMops - Switch out different ARMop implementations and XCB tables depending on MMU model - helps reduce assembler warnings and make it clearer what code paths are and aren't possible. Add implementations of the two new ARMops. Simplify ARM_Analyse_Fancy by removing some tests which we know will have certain results. Use CCSIDR constants in ARMv7 ARMops instead of magic numbers. Update XCB table comments, and add a new table for VMSAv6
  s/ChangeDyn - Define constant for the new NCB 'idempotent' cache policy (VMSAv6 normal, non-cacheable memory)
  s/HAL - Use CCSIDR constants instead of magic numbers. Extend RISCOS_MapInIO to allow the TEX bits to be specified.
  s/Kernel - OS_PlatformFeatures 33 (read cache information) implementation (actually, just calls through to an ARMop)
  s/MemInfo - Modify VMSAv6 OS_Memory 0 cache/uncache implementation to use the XCB table instead of modifying L2_C directly. This allows the cacheability to be changed without affecting the memory type - important for e.g. unaligned accesses to work correctly. Implement cache policy support for OS_Memory 13.
  s/Middle - Remove IOSystemType from OS_ReadSysInfo 6.
  s/VMSAv6 - Make sure BangCam uses the XCB table for working out the attributes of temp-uncacheable pages instead of manipulating L2_C directly. Add OS_MMUControl 2 implementation.
  s/AMBControl/memmap - Update VMSAv6 page table pokeing to use XCB table
  s/PMF/osinit - Remove IOSystemType reference, and switch out some pre-HAL code that was trying to use IOSystemType.
Admin:
  Tested on Iyonix, ARM11, Cortex-A7, -A8, -A9, -A15
  Note that contrary to the comments in the source the default NCB policy currently maps to VMSAv6 Device memory type (as per previous kernel versions). This is just a temporary measure, and it will be switched over to Normal, non-cacheable once appropriate memory barriers have been added to the affected IO code.


Version 5.35, 4.79.2.273. Tagged as 'Kernel-5_35-4_79_2_273'
@
text
@a485 1
        LDR     r7,=ZeroPage
a486 14
  [ MEMM_Type = "VMSAv6"
        ; VMSAv6 is tricky, use XCBTable/PCBTrans
        ASSERT  DynAreaFlags_CPBits = 7*XCB_P :SHL: 10
        ASSERT  DynAreaFlags_NotCacheable = XCB_NC :SHL: 4
        ASSERT  DynAreaFlags_NotBufferable = XCB_NB :SHL: 4
        LDR     r2,[r7,#MMU_PCBTrans]
        TST     r9,#PageFlags_TempUncacheableBits
        AND     r1,r9,#DynAreaFlags_NotCacheable + DynAreaFlags_NotBufferable
        AND     r0,r9,#DynAreaFlags_CPBits
        ORRNE   r1,r1,#XCB_TU<<4            ; if temp uncache, set TU bit
        ORR     r1,r1,r0,LSR #10-4
        LDRB    r1,[r2,r1,LSR #4]           ; convert to X, C and B bits for this CPU
        ORR     r11,r11,r1
  |
a491 1
  ]
d496 1
@


4.1.3.3.6.16
log
@Add initial support for "physical memory pools"
Detail:
  This set of changes adds support for "physical memory pools" (aka PMPs), a new type of dynamic area which allow physical pages to be claimed/allocated without mapping them in to the logical address space. PMPs have full control over which physical pages they use (similar to DAs which request specific physical pages), and also have full control over the logical mapping of their pages (which pages go where, and per-page access/cacheability control).
  Currently the OS makes use of two PMPs: one for the free pool (which now has a logical size of zero - freeing up gigabytes of logical space), and one for the RAM disc (logical size of 1MB, allowing for a physical size limited only by the amount of free memory)
  Implementing these changes has required a number of other changes to be made:
  * The CAM has been expanded from 8 bytes per entry to 16 bytes per entry, in order to allow each RAM page to store information about its PMP association
  * The system heap has been expanded to 32MB in size (from just under 4MB), in order to allow it to be used to store PMP page lists (1 word needed per page, but PMP pages may not always have physical pages assigned to them - so to allow multiple large PMPs to exist we need more than just 1 word per RAM page)
  * The &FA000000-&FBFFFFFF area of fixed kernel workspace has been shuffled around to accomodate the larger CAM, and the system heap is now located just above the RMA.
  * SoftResets code stripped out (unlikely we'll ever want to fix and re-enable it)
  * A couple of FastCDA options are now permanently on
  * Internal page flags shuffled around a bit. PageFlags_Unavailable now publicly exposed so that PMP clients can lock/unlock pages at will.
  * When OS_ChangeDynamicArea is asked to grow or shrink the free pool, it now implicitly converts it into a shrink or grow of application space (which is what would happen anyway). This simplifies the implementation; during a grow, pages (or replacement pages) are always sourced from the free pool, and during a shrink pages are always sent to the free pool.
  File changes:
  - hdr/KernelWS - Extend DANode structure. Describe CAM format. Adjust kernel workspace.
  - hdr/OSRSI6, s/Middle - Add new item to expose the CAM format
  - hdr/Options - Remove SoftResets switch. Add some PMP switches.
  - s/ARM600, s/VMSAv6 - Updated for new CAM format. Note that although the CAM stores PMP information, BangCamUpdate currently doesn't deal with updating that data - it's the caller's responsibility to do so where appropriate.
  - s/ChangeDyn - Lots of changes to implement PMP support, and to cope with the new CAM format.
  - s/HAL - Updated to cope with new CAM format, and lack of logical mapping of free pool.
  - s/MemInfo - Updated to cope with new CAM format. OS_Memory 0 updated to cope with converting PPN to PA for pages which are mapped out. OS_Memory 24 updated to decode the access permissions on a per-page basis for PMPs, and fixed its HWM usage for sparse DAs.
  - s/NewReset - Soft reset code and unused AddCamEntries function removed. Updated to cope with new CAM format, PMP free pool, PMP RAMFS
  - s/AMBControl/allocate - Update comment (RMA hasn't been used for AMBControl nodes for a long time)
  - s/AMBControl/growp, s/AMBControl/memmap, s/AMBControl/shrinkp - Update for new CAM format + PMP free pool
  - s/vdu/vdudriver - Strip out soft reset code.
Admin:
  Tested on Pandaboard
  This is just a first iteration of the PMP feature, with any luck future changes will improve functionality. This means APIs are subject to change as well.


Version 5.35, 4.79.2.284. Tagged as 'Kernel-5_35-4_79_2_284'
@
text
@d127 1
a127 1
        LDR     r5,=L2PT
d132 1
a132 1
        ADD     r5,r5,r6,LSL #CAM_EntrySizeLog2          ;r5 -> CAM entry affected
a134 2
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
d218 1
a218 2
        ASSERT  CAM_LogAddr = 0
        LDR     r1,[r1,r0,LSL #CAM_EntrySizeLog2]       ;logical address from CAM
d306 1
a306 3
        ADD     $reg,r11,$reg,LSL #CAM_EntrySizeLog2 ;r0 -> CAM entry for 1st page
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
a482 1
01
d513 1
a513 1
        ADD     r1,r7,r2,LSL #CAM_EntrySizeLog2 ;r1 -> CAM entry for 1st page
d515 2
a516 2
        LDR     r4,[r1,#CAM_LogAddr]        ;fetch old logical addr. of 1st page from CAM
        LDR     r3,[r1,#CAM_PageFlags]      ;fetch old PPL of 1st page from CAM
d518 1
a518 1
        LDR     r4,[r1,#CAM_LogAddr]        ;fetch old logical addr. of 1st page from CAM
a570 3
        LDR     r3,=DuffEntry
        CMP     r4,r3
        BEQ     %FT50                            ;pages already mapped out - just update CAM for new ownership
d575 1
a576 1
50
a581 18
; ----------------------------------------------------------------------------------
;
; AMB_SetMemMapEntries_MyPPL:
;
; As above, but uses provided PPL when mapping out instead of forcing to AP_Duff
;
; entry:
;   R3 =  no. of pages
;   R4 -> list of page entries (1 word per entry, giving page no.)
;   R5 =  start logical address of mapping (-1 means 'out of the way')
;   R6 =  PPL ('page protection level') for mapping
;
AMB_SetMemMapEntries_MyPPL
        Push    "r0-r4,r7-r11,lr"
        MOVS    r8,r3
        MOV     r9,r6
        BNE     %BT01
        B       AMB_smme_exit
d642 1
a642 3
        ADD     r0,r7,r0,LSL #CAM_EntrySizeLog2   ;r0 -> CAM entry for page
        ASSERT  CAM_LogAddr=0
        ASSERT  CAM_PageFlags=4
@


4.1.3.3.6.17
log
@Misc memory management tweaks & fixes
Detail:
  s/ChangeDyn - Fix OS_DynamicArea 20 to work properly with sparse & PMP DAs. It now checks against the max extent of the area rather than the current size; this matches the logic used for checking fixed system workspace areas. The call only determines the ownership of a logical address, and it's considered the caller's responsibility to check if there's actually a page at the given address.
  s/ChangeDyn - Revise OS_DynamicArea 25 to remove the redundant 'PMP page flags' entry, and to allow pages to be looked up by either PMP page index, phys page number, or DA page index
  s/ChangeDyn - Tidy up InitDynamicAreas by adding the NextFreePage routine to help determine the next page to be added to the free pool.
  s/AMBControl/Workspace, s/AMBControl/main, s/AMBControl/memmap - Fix lazy mapping in of pages to use the correct L2PT flags for the default CB cache policy
  s/AMBControl/allocate - Get rid of magic constant when extracting page flags from DA flags, and make note of the fact that assorted bits of code ignore the flags
  s/AMBControl/growp, s/AMBControl/shrinkp - Reverse the page order when growing/shrinking areas, to match OS_ChangeDynamicArea. This helps both DAs and application space to have pages allocated to them in contiguous physical order - which in turn helps produce shorter, more optimal scatter lists for DMA
Admin:
  Tested on Pandaboard


Version 5.35, 4.79.2.287. Tagged as 'Kernel-5_35-4_79_2_287'
@
text
@d111 9
a119 2
; Get the correct default page flags
        LDR     r3,AMBPageFlags 
@


4.1.3.3.6.18
log
@Cache maintenance fixes
Detail:
  This set of changes tackles two main issues:
  * Before mapping out a cacheable page or making it uncacheable, the OS performs a cache clean+invalidate op. However this leaves a small window where data may be fetched back into the cache, either accidentally (dodgy interrupt handler) or via agressive prefetch (as allowed for by the architecture). This rogue data can then result in coherency issues once the pages are mapped out or made uncacheable a short time later.
    The fix for this is to make the page uncacheable before performing the cache maintenance (although this isn't ideal, as prior to ARMv7 it's implementation defined whether address-based cache maintenance ops affect uncacheable pages or not - and on ARM11 it seems that they don't, so for that CPU we currently force a full cache clean instead)
  * Modern ARMs generally ignore unexpected cache hits, so there's an interrupt hole in the current OS_Memory 0 "make temporarily uncacheable" implementation where the cache is being flushed after the page has been made uncacheable (consider the case of a page that's being used by an interrupt handler, but the page is being made uncacheable so it can also be used by DMA). As well as affecting ARMv7+ devices this was found to affect XScale (and ARM11, although untested for this issue, would have presumably suffered from the "can't clean uncacheable pages" limitation)
    The fix for this is to disable IRQs around the uncache sequence - however FIQs are currently not being dealt with, so there's still a potential issue there.
  File changes:
  - Docs/HAL/ARMop_API, hdr/KernelWS, hdr/OSMisc - Add new Cache_CleanInvalidateRange ARMop
  - s/ARM600, s/VMSAv6 - BangCam updated to make the page uncacheable prior to flushing the cache. Add GetTempUncache macro to help with calculating the page flags required for making pages uncacheable. Fix abort in OS_MMUControl on Raspberry Pi - MCR-based ISB was resetting ZeroPage pointer to 0
  - s/ARMops - Cache_CleanInvalidateRange implementations. PL310 MMU_ChangingEntry/MMU_ChangingEntries refactored to rely on Cache_CleanInvalidateRange_PL310, which should be a more optimal implementation of the cache cleaning code that was previously in MMU_ChangingEntry_PL310.
  - s/ChangeDyn - Rename FastCDA_UpFront to FastCDA_Bulk, since the cache maintenance is no longer performed upfront. CheckCacheabilityR0ByMinusR2 now becomes RemoveCacheabilityR0ByMinusR2. PMP LogOp implementation refactored quite a bit to perform cache/TLB maintenance after making page table changes instead of before. One flaw with this new implementation is that mapping out large areas of cacheable pages will result in multiple full cache cleans while the old implementation would have (generally) only performed one - a two-pass approach over the page list would be needed to solve this.
  - s/GetAll - Change file ordering so GetTempUncache macro is available earlier
  - s/HAL - ROM decompression changed to do full MMU_Changing instead of MMU_ChangingEntries, to make sure earlier cached data is truly gone from the cache. ClearPhysRAM changed to make page uncacheable before flushing cache.
  - s/MemInfo - OS_Memory 0 interrupt hole fix
  - s/AMBControl/memmap - AMB_movepagesout_L2PT now split into cacheable+non-cacheable variants. Sparse map out operation now does two passes through the page list so that they can all be made uncacheable prior to the cache flush + map out.
Admin:
  Tested on StrongARM, XScale, ARM11, Cortex-A7, Cortex-A9, Cortex-A15, Cortex-A53
  Appears to fix the major issues plaguing SATA on IGEPv5


Version 5.35, 4.79.2.306. Tagged as 'Kernel-5_35-4_79_2_306'
@
text
@d409 1
a409 1
;AMB_movecacheablepagesout_L2PT
a413 1
;       r3  =  old page flags
d417 2
a418 9
AMB_movecacheablepagesout_L2PT
        Entry   "r0-r8"

        ; Calculate L2PT flags needed to make the pages uncacheable
        ; Assume all pages will have identical flags (or at least close enough)
        LDR     lr,=ZeroPage
        LDR     lr,[lr, #MMU_PCBTrans]
        GetTempUncache r0, r3, lr, r1
        LDR     r1, =TempUncache_L2PTMask
a422 55
        CMP     r8,#4
        BLT     %FT20
10
        LDMIA   lr,{r2-r5}
        BIC     r2,r2,r1
        BIC     r3,r3,r1
        BIC     r4,r4,r1
        BIC     r5,r5,r1
        ORR     r2,r2,r0
        ORR     r3,r3,r0
        ORR     r4,r4,r0
        ORR     r5,r5,r0
        STMIA   lr!,{r2-r5}
        SUB     r8,r8,#4
        CMP     r8,#4
        BGE     %BT10
20
        CMP     r8,#0
        BEQ     %FT35
30
        LDR     r2,[lr]
        BIC     r2,r2,r1
        ORR     r2,r2,r0
        STR     r2,[lr],#4
        SUBS    r8,r8,#1
        BNE     %BT30
35
        FRAMLDR r0,,r4                           ;address of 1st page
        FRAMLDR r1,,r8                           ;number of pages
        LDR     r3,=ZeroPage
        ARMop   MMU_ChangingUncachedEntries,,,r3 ;flush TLB
        FRAMLDR r0,,r4
        FRAMLDR r1,,r8
        ADD     r1,r0,r1,LSL #Log2PageSize
        ARMop   Cache_CleanInvalidateRange,,,r3  ;flush from cache
        FRAMLDR r4
        FRAMLDR r8
        B       %FT55 ; -> moveuncacheablepagesout_L2PT (avoid pop+push of large stack frame)

; ----------------------------------------------------------------------------------
;
;AMB_moveuncacheablepagesout_L2PT
;
;updates L2PT for old logical page positions, does not update CAM
;
; entry:
;       r4  =  old logical address of 1st page
;       r8  =  number of pages
;
AMB_moveuncacheablepagesout_L2PT
        ALTENTRY
55      ; Enter here from moveuncacheablepagesout
        LDR     lr,=L2PT
        ADD     lr,lr,r4,LSR #(Log2PageSize-2)    ;lr -> L2PT 1st entry

d433 2
a434 2
        BLT     %FT70
60
d438 2
a439 2
        BGE     %BT60
70
d441 2
a442 2
        BEQ     %FT85
80
d445 14
a458 7
        BNE     %BT80
85
        FRAMLDR r0,,r4                           ;address of 1st page
        FRAMLDR r1,,r8                           ;number of pages
        LDR     r3,=ZeroPage
        ARMop   MMU_ChangingUncachedEntries,,,r3 ;no cache worries, hoorah
        EXIT
d513 1
d516 3
d537 4
d542 1
a542 1
        BL      AMB_moveuncacheablepagesout_L2PT ;unmap 'em from where they are
d549 4
a552 1
        BL      AMB_movecacheablepagesout_L2PT
d554 1
d570 2
a571 2
        LDR     lr,=DuffEntry
        CMP     r4,lr
d573 5
a577 1
        BL      AMB_movecacheablepagesout_L2PT
d621 1
a621 10
        Entry   "r0-r11"

;to do this safely we need to do it in two passes
;first pass makes pages uncacheable
;second pass unmaps them
;n.b. like most of the AMB code, this assumes nothing will trigger an abort-based lazy map in operation while we're in the middle of this processing!

;
;pass one: make pages uncacheable (preserve bitmap, CAM)
;
d625 1
d628 4
a631 1
        MOV     r9,#-1                            ;initialised to correct page flags once we find a mapped in page
a632 1
;decide if we want to do TLB coherency as we go
d636 1
d638 1
d646 1
a646 1
;find the sparsely mapped pages, make them uncacheable, doing coherency as we go if enabled
d655 2
a656 6

        CMP     r9,#-1                            ;have page flags yet?
        BNE     %FT14
        LDR     r0,[r10]                          ;page no.
        ADD     r0,r7,r0,LSL #CAM_EntrySizeLog2   ;r0 -> CAM entry for page
        LDR     r0,[r0,#CAM_PageFlags]
d658 2
a659 2
        LDR     r2,[r2,#MMU_PCBTrans]
        GetTempUncache r9,r0,r2,lr
a660 66
        LDR     lr,=L2PT                          ;lr -> L2PT
        LDR     r2,[lr,r4,LSR #(Log2PageSize-2)]  ;L2PT entry for page
        LDR     r0,=TempUncache_L2PTMask
        BIC     r2,r2,r0
        ORR     r2,r2,r9
        STR     r2,[lr,r4,LSR #(Log2PageSize-2)]  ;make uncacheable

        TEQ     r6, #0
        BNE     %FT15
        LDR     r2,=ZeroPage
        MOV     r0,r4
        ARMop   MMU_ChangingUncachedEntry,,,r2    ;flush TLB
        MOV     r0,r4
        ADD     r1,r0,#PageSize
        ARMop   Cache_CleanInvalidateRange,,,r2   ;flush from cache
15
        SUBS    r3,r3,#1
        BEQ     %FT40                             ;done
16
        ADD     r10,r10,#4                        ;next page no.
        ADD     r4,r4,#PageSize                   ;next logical address
        MOVS    r8,r8,LSL #1                      ;if 32 bits processed...
        BNE     %BT12
        B       %BT10

40
        TEQ     r6, #0
        BEQ     %FT45
        LDR     r2,=ZeroPage
        ARMop   MMU_ChangingUncached,,,r2
        ARMop   Cache_CleanInvalidateAll,,,r2
45

;
;pass two: unmap pages (+ clear bitmap + update CAM)
;

        FRAMLDR r3
        FRAMLDR r5
        FRAMLDR r10,,r4                           ;ptr to page list
        LDR     r2,=ZeroPage
        MOV     r9,#AP_Duff                       ;permissions for DuffEntry
        LDR     r7,[r2,#CamEntriesPointer]        ;r7 -> CAM
        MOV     r4,#ApplicationStart              ;log. address of first page
        LDR     r1,=DuffEntry                     ;means Nowhere, in CAM

;decide if we want to do TLB coherency as we go
        MOV     r6, r3, LSR #5 ; r6!=0 if doing global coherency (32 entry TLB)

        B       %FT60

;skip next 32 pages then continue
56
        ADD     r10,r10,#32*4
        ADD     r4,r4,#32*PageSize

;find the sparsely mapped pages, map them out, doing coherency as we go if enabled
60
        MOV     r8,#1                             ;initial bitmap mask for new bitmap word
        LDR     r11,[r5],#4                       ;next word of bitmap
        CMP     r11,#0                            ;if next 32 bits of bitmap clear, skip
        BEQ     %BT56                             ;skip loop must terminate if r3 > 0
62
        TST     r11,r8                            ;page is currently mapped in if bit set
        BEQ     %FT66

d669 6
d676 1
a676 1
        BNE     %FT65
d684 3
a686 2
      ]        
65
d689 2
a690 2
        BEQ     %FT90                             ;done
66
d694 1
a694 1
        BNE     %BT62
d697 2
a698 1
        B       %BT60
d700 7
a706 1
90
d708 1
a708 1
        BEQ     %FT95
d711 4
a714 2
95
        EXIT
@


4.1.3.3.6.10.2.1
log
@Assorted kernel fixes for ARMv6/ARMv7
Detail:
  s/ARMops - Fix IMB_Range_WB_CR7_Lx to clean the correct number of cache lines
  s/HAL - Change CP15 control register flags so unaligned loads are enabled on ARMv6 (to simplify support for ARMv7 where unaligned loads are always enabled, and to match the behaviour expected by the example code in Hdr:CPU.Arch)
  s/AMBControl/memmap - Make AMB_LazyFixUp use the correct L2PT protection flags depending on ARM600/VMSAv6 MMU model. Also guard against problems caused by future L2PT flag changes.
  s/vdu/vdugrafj - Fix previously undiscovered 32bit incompatability in GetSprite (OS_SpriteOp 14/16)
Admin:
  Tested on rev C2 beagleboard


Version 5.35, 4.79.2.98.2.5. Tagged as 'Kernel-5_35-4_79_2_98_2_5'
@
text
@a109 6

; Calculate the L2PT protection bits in a nice way that won't produce broken code if we change MMU model
   [ MEMM_Type = "VMSAv6"
        MOV     r3,#(AP_Full*L2X_APMult)+L2_ExtPage+L2_C+L2_B
   |
        ASSERT  (AP_Full*L2X_APMult)+L2_ExtPage+L2_C+L2_B = &FFE
d111 1
a111 2
        ORR     r3,r3,#&E
   ]
@


4.1.3.3.6.10.2.2
log
@Update Cortex kernel to use correct instruction/memory barriers and to perform branch target predictor maintenance. Plus tweak default CMOS settings.
Detail:
  hdr/Copro15ops - Added myISB, myDSB, myDMB macros to provide barrier functionality on ARMv6+
  s/ARMops, s/HAL, s/VMSAv6, s/AMBControl/memmap - Correct barrier operations are now performed on ARMv6+ following CP15 writes. Branch predictors are now also maintained properly.
  s/NewReset - Change default CMOS settings so number of CDFS drives is 0 in Cortex builds. Fixes rogue CDFS icon on iconbar.
Admin:
  Tested on rev C2 beagleboard


Version 5.35, 4.79.2.98.2.27. Tagged as 'Kernel-5_35-4_79_2_98_2_27'
@
text
@a160 3
      [ MEMM_Type = "VMSAv6"
        myISB ; Not sure if this is necessary or not; do it just in case
      ]
@


4.1.3.3.6.10.2.3
log
@Fix some issues preventing the Cortex kernel from being used on non-Cortex machines
Detail:
  hdr/Options - ARM6support and GetKernelMEMC values are now derived from the value of MEMM_Type
  s/ARMops, s/HAL - Code to detect and handle ARMv7 CPUs is now only enabled when using VMSAv6 MMU model. Saves us from having to deal with lack of myIMB, myDSB, etc. implementations on pre-ARMv6.
  s/HAL - Removed some debug code
  s/NewReset - Fix bug spotted by Tom Walker where R12 wasn't being restored by LookForHALRTC if a non-HAL RTC had already been found
  s/AMBControl/memmap - correct the assert clause that was checking that &FFE are the correct L2PT protection bits for non-VMSAv6 machines
Admin:
  Tested this kernel on a rev C2 beagleboard & Iyonix softload. Also compiled it into an IOMD ROM, but didn't try running it.


Version 5.35, 4.79.2.98.2.32. Tagged as 'Kernel-5_35-4_79_2_98_2_32'
@
text
@a111 1
; This should match the AP_Full entry from the PPLTrans table that gets used by BangCam (plus C+B bits)
d115 1
a115 1
        ASSERT  (AP_Full*L2_APMult)+L2_SmallPage+L2_C+L2_B = &FFE
@


4.1.3.3.6.10.2.4
log
@Add hdr.Variables to the C header export, fix ARMv6 issues
Detail:
  Makefile - Added hdr.Variables to the C header export list
  hdr/ARMops, s/ARMops - Added ARM1176JZF-S to the list of known CPUs
  s/ARMops - Fix unaligned memory access in ARM_PrintProcessorType
  hdr/Copro15ops, s/ARMops, s/HAL, s/VMSAv6, s/AMBControl/memmap - Fixed all myDSB/myISB/etc. macro instances to specify a temp register, so that they work properly when building an ARMv6 version of the kernel
Admin:
  Fixes build errors with the latest Draw module.
  Should also allow the kernel to work properly with the new S3C6410 port.
  ARMv6 version builds OK, but no other builds or runtime tests have been made.


Version 5.35, 4.79.2.98.2.38. Tagged as 'Kernel-5_35-4_79_2_98_2_38'
@
text
@d163 1
a163 1
        myISB   ,r0 ; Not sure if this is necessary or not; do it just in case
@


4.1.3.3.6.10.2.5
log
@Add zero page relocation support
Detail:
  A whole mass of changes to add high processor vectors + zero page relocation support to the Cortex branch of the kernel
  At the moment the code can only cope with two ZeroPage locations, &0 and &FFFF0000. But with a bit more tweaking those restrictions can probably be lifted, allowing ZeroPage to be hidden at almost any address (assuming it's fixed at compile time). If I've done my job right, these restrictions should all be enforced by asserts.
  There's a new option, HiProcVecs, in hdr/Options to control whether high processor vectors are used. When enabling it and building a ROM, remember:
  * FPEmulator needs to be built with the FPEAnchor=High option specified in the components file (not FPEAnchorType=High as my FPEmulator commit comments suggested)
  * ShareFS needs unplugging/removing since it can't cope with it yet
  * Iyonix users will need to use the latest ROOL boot sequence, to ensure the softloaded modules are compatible (OMAP, etc. don't really softload much so they're OK with older sequences)
  * However VProtect also needs patching to fix a nasty bug there - http://www.riscosopen.org/tracker/tickets/294
  The only other notable thing I can think of is that the ProcessTransfer code in s/ARM600 & s/VMSAv6 is disabled if high processor vectors are in use (it's fairly safe to say that code is obsolete in HAL builds anyway?)
  Fun challenge for my successor: Try setting ZeroPage to &FFFF00FF (or similar) so its value can be loaded with MVN instead of LDR. Then use positive/negative address offsets to access the contents.
  File changes:
  - hdr/ARMops - Modified ARMop macro to take the ZeroPage pointer as a parameter instead of 'zero'
  - hdr/Copro15ops - Corrected $quick handling in myISB macro
  - hdr/Options - Added ideal setting for us to use for HiProcVecs
  - s/AMBControl/allocate, s/AMBControl/growp, s/AMBControl/mapslot, s/AMBControl/memmap, s/AMBControl/service, s/AMBControl/shrinkp, s/Arthur2, s/Arthur3, s/ArthurSWIs, s/ChangeDyn, s/ExtraSWIs, s/HAL, s/HeapMan, s/Kernel, s/MemInfo, s/Middle, s/ModHand, s/MoreSWIs, s/MsgCode, s/NewIRQs, s/NewReset, s/Oscli, s/PMF/buffer, s/PMF/IIC, s/PMF/i2cutils, s/PMF/key, s/PMF/mouse, s/PMF/osbyte, s/PMF/oseven, s/PMF/osinit, s/PMF/osword, s/PMF/oswrch, s/SWINaming, s/Super1, s/SysComms, s/TickEvents, s/Utility, s/vdu/vdu23, s/vdu/vdudriver, s/vdu/vdugrafl, s/vdu/vdugrafv, s/vdu/vdupalxx, s/vdu/vdupointer, s/vdu/vduswis, s/vdu/vduwrch - Lots of updates to deal with zero page relocation
  - s/ARM600 - UseProcessTransfer option. Zero page relocation support. Deleted pre-HAL ClearPhysRAM code to tidy the file up a bit.
  - s/ARMops - Zero page relocation support. Set CPUFlag_HiProcVecs when high vectors are in use.
  - s/KbdResPC - Disable compilation of dead code
  - s/VMSAv6 - UseProcessTransfer option. Zero page relocation support.
Admin:
  Tested with OMAP & Iyonix ROM softloads, both with high & low zero page.
  High zero page hasn't had extensive testing, but boot sequence + ROM apps seem to work.


Version 5.35, 4.79.2.98.2.48. Tagged as 'Kernel-5_35-4_79_2_98_2_48'
@
text
@d66 1
a66 1
        LDR     r12,=ZeroPage+AMBControl_ws
d130 1
a130 1
        LDR     r5,=ZeroPage
d187 1
a187 1
        LDR     r12,=ZeroPage+AMBControl_ws
d208 1
a208 1
        LDR     r12,=ZeroPage+AMBControl_ws
d212 1
a212 1
        LDR     r14,=ZeroPage
d320 1
a320 1
        LDR     r11,=ZeroPage
d373 1
a373 1
        LDR     r11,=ZeroPage
d477 1
a477 1
        LDR     r7,=ZeroPage
d506 1
a506 1
        LDR     r3,=ZeroPage
d518 1
a518 1
        LDR     r3,=ZeroPage
d531 1
a531 1
        LDR     r3,=ZeroPage
d543 1
a543 1
        LDR     r3,=ZeroPage
d574 2
a575 1
        LDR     r2,=ZeroPage
d577 1
a577 1
        LDR     r7,[r2,#CamEntriesPointer]        ;r7 -> CAM
d579 1
a579 1
        LDR     r1,=DuffEntry                     ;means Nowhere, in CAM
a607 1
        LDR     r2,=ZeroPage
a614 1
        MOV     r2, #0
d624 1
a624 3
        MOV     r2, #0
        STR     r2,[r5,#-4]                       ;zero word of bitmap we've just traversed
        LDR     r2,=ZeroPage
d648 1
a648 1
        LDR     r12,=ZeroPage+AMBControl_ws
d703 1
a703 1
        LDR     r9,=ZeroPage+PhysRamTable
d764 1
a764 1
        LDR     r9,=ZeroPage+PhysRamTable
@


4.1.3.3.2.1
log
@Added following enhancements:

 - Chocolate screen mapping (section mapped and cached), StrongARM only
   Phoebe h/w (IOMD 2) will have register to assist this, but code currently
   relies on data abort mechanism to keep screen up to date wrt write-back
   data cache.

 - Chocolate AMBControl task switching (lazy page mapping), StrongARM only
   Improves task swapping speed. There appears to be a StrongAEM silicon
   bug rev 2 and 3) which means that LDMIB rn, {regs includind rn} cannot
   be reliably restarted after a data abort. This stuffs Chocolate AMBControl
   (awaiting response from Digital).

Both enhancements need more work to complete for Phoebe. Chocolate AMBControl
may well have to be made dormant because of silicon bug.

Note that this kernel *will* cause problems with task switching on StrongARM,
unless Chocolate task switching is disabled via !Flavour application.
@
text
@d18 2
d21 2
a22 3
; ----------------------------------------------------------------------------------
;
;convert page number in $pnum to L2PT entry (physical address+protection bits),
d25 2
a26 15
;entry: $ptable -> PhysBin table, $pbits = protection bits
;exit:  $temp corrupted
;
        MACRO
        PageNumToL2PT $pnum,$ptable,$pbits,$temp
        BIC     $temp,$pnum,#(3:SHL:(AMBPhysBinShift-2))       ;word alignment for PhysBin lookup
        LDR     $temp,[$ptable,$temp,LSR #(AMBPhysBinShift-2)] ;start physical address of bin
        AND     $pnum,$pnum,#AMBPhysBinMask                    ;no. pages into bin
        ADD     $pnum,$temp,$pnum,LSL #Log2PageSize            ;physical address of page
        ORR     $pnum,$pnum,$pbits                             ;munge in protection bits
        MEND

; ----------------------------------------------------------------------------------
;
;clean+flush a virtual range of StrongARM data cache
a27 25
;addr0 = start of range (inclusive)
;addr1 = end of range (exclusive)
;
;exit: addr0 updated to end of range ( = addr1)
;
    [ SAcleanflushbroken
        MACRO
        AMB_ARMAcleanflushrange $addr0,$addr1
01
        ARMA_clean_DCentry $addr0
        ARMA_flush_DCentry $addr0
        ADD     $addr0,$addr0,#32
        ARMA_clean_DCentry $addr0
        ARMA_flush_DCentry $addr0
        ADD     $addr0,$addr0,#32
        ARMA_clean_DCentry $addr0
        ARMA_flush_DCentry $addr0
        ADD     $addr0,$addr0,#32
        ARMA_clean_DCentry $addr0
        ARMA_flush_DCentry $addr0
        ADD     $addr0,$addr0,#32
        CMP     $addr0,$addr1
        BLO     %BT01
        MEND
    |
d29 6
a34 12
        AMB_ARMAcleanflushrange $addr0,$addr1
01
        ARMA_cleanflush_DCentry $addr0 ;when are Digital going to restore this?
        ADD     $addr0,$addr0,#32
        ARMA_cleanflush_DCentry $addr0
        ADD     $addr0,$addr0,#32
        ARMA_cleanflush_DCentry $addr0
        ADD     $addr0,$addr0,#32
        ARMA_cleanflush_DCentry $addr0
        ADD     $addr0,$addr0,#32
        CMP     $addr0,$addr1
        BLO     %BT01
a35 154
    ]

  [ AMB_LazyMapIn

; ----------------------------------------------------------------------------------
;
;AMB_LazyFixUp
;
; ** StrongARM only **
;
; Routine to be used in abort handlers (in abort32 mode), that checks to see if abort
; is expected, and fixes things up if so, ready to restart instruction.
;
; Fix up consists of mapping in affected page, and updating AMBMappedInRegister. This
; may seem like a lot of work, but remember that the L2PT and CAM updates for each page are
; needed anyway in non-lazy scheme, so there is really only a housekeeping overhead.
;
; There is no cache clean/flush consideration here, since the map is a map in from Nowhere.
; TLB flush consideration is left to main abort handler code - in fact there may not
; be a TLB flush consideration either, if StrongARM TLB can be assumed not to cache an
; entry which is a translation fault.
;
; entry: r0 = aborting address (data address for data abort, instruction address
;        for prefetch abort), r1-r7 trashable, no stack
; exit:  r0 = non-zero if abort was expected and fixed up, zero if not
;
AMB_LazyFixUp ROUT
        MOV     r7,r12
        MOV     r12,#AMBControl_ws
        LDR     r12,[r12]
        CMP     r12,#0
        BEQ     %FT20
        SUBS    r0,r0,#ApplicationStart
        BMI     %FT20
        MOV     r0,r0,LSR #Log2PageSize                  ;adress now in terms of pages from ApplicationStart
        LDR     r1,AMBMappedInNode 
        CMP     r1,#0
        BEQ     %FT20
        LDR     r2,[r1,#AMBNode_Npages]
        CMP     r2,r0
        BLS     %FT20
;
;need not check MappedInRegister first, since if abort has happened in range of current
;AppSpace, then the page can be assumed to be mapped out
;
        ADD     r1,r1,#AMBNode_pages
        ADD     r1,r1,r0,LSL #2                          ;r1 -> page involved, in node page list
        LDR     r2,AMBPhysBin
        MOV     r3,#&FF0
        ORR     r3,r3,#&E                                ;&FFE = L2PT protection bits for ordinary page
        LDR     r4,[r1]
        MOV     r6,r4
        PageNumToL2PT r4,r2,r3,r5
;
;here, r6 = page number of page involved, r4 = new L2PT entry value to map in page
;
        LDR     r2,AMBMappedInNpages                     ;for convenience, update bitmap etc. here...
        ADD     r2,r2,#1
        STR     r2,AMBMappedInNpages
        ADR     r2,AMBMappedInRegister
        ADD     r2,r2,r0,LSR #5-2                        ;r2 -> bitmap word affected
        BIC     r2,r2,#3
        AND     r3,r0,#31
        MOV     r5,#1
        MOV     r5,r5,LSL r3                             ;mask for bit affected in bitmap word
        LDR     r3,[r2]
        ORR     r3,r3,r5
        STR     r3,[r2]

        ADD     r0,r0,#ApplicationStart:SHR:Log2PageSize ;address now in terms of pages from 0
        MOV     r5,#L2PT
        STR     r4,[r5,r0,LSL #2]                        ;update L2PT
        ARMA_drain_WB                                    ;since L2PT for AppSpace is bufferable

        MOV     r5,#0
        LDR     r5,[r5,#CamEntriesPointer]
        ADD     r5,r5,r6,LSL #3                          ;r5 -> CAM entry affected
        MOV     r0,r0,LSL #Log2PageSize                  ;address is now ordinary again, and must be non-zero
        MOV     r1,#0                                    ;0 = AP for ordinary page
        STMIA   r5,{r0,r1}                               ;update CAM entry
        MOV     r12,r7
        MOV     pc,lr                                    ;r0 is non-zero
20
        MOV     r0,#0
        MOV     r12,r7
        MOV     pc,lr

  ] ;AMB_LazyMapIn

  [ {FALSE} ;debug :LAND: debugAMB

; ----------------------------------------------------------------------------------
;
;AMB_ConsistencyCheck
;
; entry: r0 = arbitrary tag (identifies caller in trace)
;
AMB_ConsistencyCheck ROUT
        Push    "r0-r12,lr"
        MOV     r12,#AMBControl_ws
        LDR     r12,[r12]
        CMP     r12,#0
        BEQ     %FT90
        LDR     r1,AMBMappedInNpages
        LDR     r2,AMBMappedInNode
        CMP     r2,#0
        MOVEQ   r3,#0
        LDRNE   r3,[r2,#AMBNode_Npages]
        MOV     r4,#0
        MOV     r5,#0
        Push    "r3"
        ADD     r3,r3,#1
        ADR     r6,AMBMappedInRegister
        LDR     r7,[r6],#4
        MOV     r8,#1
10
        SUBS    r3,r3,#1
        BEQ     %FT20
        TST     r7,r8
        ADDNE   r4,r4,#1
        MOVS    r8,r8,LSL #1
        LDREQ   r7,[r6],#4
        MOVEQ   r8,#1
        B       %BT10
20
        LDR     r3,[sp]
        ADD     r3,r3,#1
        MOV     r6,#L2PT
        ADD     r6,r6,#ApplicationStart:SHR:(Log2PageSize-2)
30
        SUBS    r3,r3,#1
        BEQ     %FT40
        LDR     r7,[r6],#4
        CMP     r7,#0
        ADDNE   r5,r5,#1
        B       %BT30
40
        Pull    "r3"
        CMP     r1,r4
        CMPEQ   r1,r5
        BEQ     %FT90
        CMP     r2,#0
        MOVEQ   r4,#-1
        MOVEQ   r5,#-1

  Debug AMB,"AMB_ConsistencyCheck **OOPS**, tag = ",r0
  Debug AMB,"  MappedInNode, Npages = ",r2,r3
  Debug AMB,"  MappedInNpages value,found in bitmap,found in L2PT = r1,r4,r5

90
        Pull    "r0-r12,pc"


  ] ;debug :LAND: debugAMB
a36 2
; ----------------------------------------------------------------------------------
;
d57 10
a66 10
        LDMIA   r10!,{r0-r7}         ;next 8 page numbers
        PageNumToL2PT r0,lr,r11,r12
        PageNumToL2PT r1,lr,r11,r12
        PageNumToL2PT r2,lr,r11,r12
        PageNumToL2PT r3,lr,r11,r12
        PageNumToL2PT r4,lr,r11,r12
        PageNumToL2PT r5,lr,r11,r12
        PageNumToL2PT r6,lr,r11,r12
        PageNumToL2PT r7,lr,r11,r12
        STMIA   r9!,{r0-r7}          ;write 8 L2PT entries
d75 1
a75 1
        PageNumToL2PT r0,lr,r11,r12
a85 2
; ----------------------------------------------------------------------------------
;
a96 2
; ----------------------------------------------------------------------------------
;
d150 1
a150 2
; ----------------------------------------------------------------------------------
;
d193 1
a193 2
; ----------------------------------------------------------------------------------
;
a237 2
; ----------------------------------------------------------------------------------

d265 2
a266 3
; ----------------------------------------------------------------------------------
;
; AMB_SetMemMapEntries: 
d274 2
d277 1
a277 2

        Push    "r0-r4,r7-r11,lr"  
a328 1

d330 1
a330 1
;nothing mapped in before), but do need to flush TLBs (eg. TLBs will cache
d332 1
a332 11
  [ ARM810support
    ;there is a general macro, should have used this before anyway
        ARM_flush_TLB r0
  |
        ARM_read_ID r0
        AND     r0,r0,#&F000
        CMP     r0,#&A000
        ARM67_flush_TLB NE
        ARMA_flush_TLBs EQ
  ]
        Pull    "r0-r4,r7-r11, pc"
d343 1
a344 18
        BL      AMB_cachecleanflush

AMB_smme_exit
        Pull    "r0-r4,r7-r11, pc"

; ----------------------------------------------------------------------------------
;
; AMB_cachecleanflush
;
; entry:
;   r4 = old logical addr. of 1st page affected by clean/flush
;   r8 = no. of pages affected
;
; action: (clean and) flush cache(s) appropriately, then flush TLB(s)
;
; exit: trashes r0-r4,r7-r11 (assumed protected by client)
;
AMB_cachecleanflush
d350 1
a350 1
        MOVEQ   pc,lr
d355 1
a355 1
        MOVNE   pc,lr
d359 2
d367 5
a371 10
  [ ChocolateScreen
;quick check - if screen cleaner has pending clean required, then we might as well choose
;to do a full clean (forget the threshold check below), since this will save screen cleaner work
;(we're only reading here, so interrupts not a problem)

        MOV     r2,#ARMA_Cleaner_status
        LDR     r2,[r2]
        TST     r2,#ACS_VSCcountdown_MASK
        BNE     AMB_ccf_StrongARM_flushwhole  ;if countdown not zero, screen clean work pending
  ]
d375 1
a375 1
        BLO     AMB_ccf_StrongARM_flushrange
d377 2
a378 10
AMB_ccf_StrongARM_flushwhole
        MOV     r1,pc
        ORR     r2,r1,#I_bit
        TEQP    r2,#0                          ;disable IRQs to mess with ARMA_Cleaner_status
        MOV     r2,#0
        LDR     r3,[r2,#ARMA_Cleaner_status]
        ORR     r3,r3,#ACS_NSCsemaphore        ;set semaphore for non-screen clean
        STR     r3,[r2,#ARMA_Cleaner_status]   ;update status
        TEQP    r1,#0                          ;restore IRQ state
        LDR     r1,[r2,#ARMA_Cleaner_flipflop]
d380 2
a381 2
        STR     r1,[r2,#ARMA_Cleaner_flipflop]
        ARMA_clean_DC r1,r2,r3                 ;effectively, fully clean/flush wrt non-interrupt stuff
d383 36
a418 14
        ARMA_flush_IC WithoutNOPs              ;do *not* flush DC - may be interrupt stuff in it
        ARMA_flush_TLBs                        ;even if paranoid, at least 4 instructions follow IC flush before return
        MOV     r1,pc
        ORR     r2,r1,#I_bit
        TEQP    r2,#0                          ;disable IRQs to mess with ARMA_Cleaner_status and MMUdomain
        MOV     r2,#0
        LDR     r3,[r2,#ARMA_Cleaner_status]
        BIC     r3,r3,#ACS_NSCsemaphore:OR:ACS_VSCcountdown_MASK ;clear semaphore and any pending VSC
        STR     r3,[r2,#ARMA_Cleaner_status]
  [ ChocolateScreen
        TST     r3,#ACS_SCdisable:OR:ACS_SCsuspend
        ARMA_read_MMUdomain r3,EQ
        BICEQ   r3,r3,#&C
        ARMA_write_MMUdomain r3,EQ             ;if SC not disabled or suspended, reset screen (domain 1) to fault
a419 2
        TEQP    r1,#0                          ;restore IRQ state
        MOV     pc,lr
a420 3
AMB_ccf_StrongARM_flushrange

        AMB_ARMAcleanflushrange r0,r1
d427 1
a427 1
        MOV     pc,lr
d429 13
a442 49
  [ AMB_LazyMapIn

; ----------------------------------------------------------------------------------
;
; AMB_SetMemMapEntries_SparseMapOut:
;
; ** StrongARM only **
;
; entry:
;   R3  =  no. of pages currently mapped in (0=none)
;   R4  -> list of page entries (1 word per entry, giving page no.)
;   R5  -> bitmap of pages mapped in (1 bit per page in whole page list)
;   R6  =  total no. of pages in slot
;
AMB_SetMemMapEntries_SparseMapOut ROUT

 [ {FALSE} ;debug :LAND: debugAMB
   Push "r0,lr"
   MOV  r0,#1
   BL   AMB_ConsistencyCheck
   Pull "r0,lr"
 ]
        CMP     r3,#0
        MOVEQ   pc,lr
        Push    "r0-r11,lr"

        MOV     r10,r4                            ;ptr to page list
        MOV     r7,#0
        LDR     r7,[r7,#CamEntriesPointer]        ;r7 -> CAM
        LDR     lr,=L2PT                          ;lr -> L2PT
        MOV     r9,#AP_Duff                       ;permissions for DuffEntry
        LDR     r1,=DuffEntry                     ;means Nowhere, in CAM
        MOV     r4,#ApplicationStart              ;log. address of first page
        MOV     r2,#0                             ;r2 is zero during sparse map out

;if the number of pages mapped in is small enough, we'll do a clean/flush of data
;cache as we go (potentially much cheaper than full clean/flush)
;(Arguably we could check for pending VSync cleans if ChocolateScreen, and always
;choose to do full clean if so. Currently considered best to go for the saving always
;here, with our own - possibly smaller - threshold.)

        CMP     r3,#AMB_ARMA_CleanSparseRange_thresh:SHR:Log2PageSize
        MOVLO   r6,#0                             ;r6 := 0 if we are to clean/flush as we go
        B       %FT10

;skip next 32 pages then continue
06
        ADD     r10,r10,#32*4                     
        ADD     r4,r4,#32*PageSize
d444 1
a444 103
;find the sparsely mapped pages, map them out, and do DC clean/flush as we go if enabled
10
        MOV     r8,#1                             ;initial bitmap mask for new bitmap word
        LDR     r11,[r5],#4                       ;next word of bitmap
        CMP     r11,#0                            ;if next 32 bits of bitmap clear, skip
        BEQ     %BT06                             ;skip loop must terminate if r3 > 0
12
        TST     r11,r8                            ;page is currently mapped in if bit set
        BEQ     %FT16
        LDR     r0,[r10]                          ;page no.
        ADD     r0,r7,r0,LSL #3                   ;r0 -> CAM entry for page
        STMIA   r0,{r1,r9}                        ;CAM entry for page set to DuffEntry,AP_Duff
        STR     r2,[lr,r4,LSR #(Log2PageSize-2)]  ;L2PT entry for page set to 0 (means translation fault)
        CMP     r6,#0
        BNE     %FT14                             ;check for DC clean/flush as we go
        ADD     r0,r4,#PageSize                   ;one page to clean/flush
        AMB_ARMAcleanflushrange r4,r0             ;clean/flush DC wrt this page
        SUB     r4,r4,#PageSize                   ;restore r4
        ARMA_flush_DTLBentry r4                   ;flush DTLB wrt this page
14
        SUBS    r3,r3,#1
        STREQ   r2,[r5,#-4]                       ;make sure we clear last word of bitmap, and...
        BEQ     %FT20                             ;done
16
        ADD     r10,r10,#4                        ;next page no.
        ADD     r4,r4,#PageSize                   ;next logical address
        MOVS    r8,r8,LSL #1                      ;if 32 bits processed...
        BNE     %BT12
        STR     r2,[r5,#-4]                       ;zero word of bitmap we've just traversed (r2 is 0)
        B       %BT10

;if DC clean/flush as we go, then now all we have to do is drain WB, flush IC and ITLB, and exit
20
        CMP     r6,#0
        BNE     %FT40

        ARMA_drain_WB
        ARMA_flush_IC WithoutNOPs
        NOP
        NOP
        ARMA_flush_ITLB
        Pull    "r0-r11,pc"

;do full cache,TLB clean flush and exit
40
        MOV     r4,#ApplicationStart             ;old logical address
        MOV     r8,r6                            ;no. of pages
        BL      AMB_cachecleanflush
        Pull    "r0-r11, pc"


; ----------------------------------------------------------------------------------
;
; AMB_MakeUnsparse
;
; entry: r0 = size of area (at top of current slot) to ensure is not sparsely mapped
;
; action: walk over space involved, to force abort handler fix up to map in any
;         pages not already there
;
AMB_MakeUnsparse ROUT
        Push    "r0-r2,r12,lr"
;  Debug AMB,"AMB_MakeUnsparse r0",r0
        ADD     r0,r0,#PageSize
        SUB     r0,r0,#1
        MOVS    r0,r0,LSR #Log2PageSize
        BEQ     %FT20
        MOV     r12,#AMBControl_ws
        LDR     r12,[r12]
        CMP     r12,#0
        BEQ     %FT20
        LDR     r1,AMBMappedInNode
        CMP     r1,#0
        BEQ     %FT20
        LDR     r2,AMBFlags
        TST     r2,#AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
        BNE     %FT20
  [ AMB_ChocTrace
        LDR     r2,AMBNmakeunsparse
        ADD     r2,r2,#1
        STR     r2,AMBNmakeunsparse
  ] 
        LDR     r2,[r1,#AMBNode_Npages]
; Debug AMB,"AMB_MakeUnsparse pages Npages ",r0,r2
        CMP     r0,r2
        MOVHI   r0,r2
        SUB     lr,r2,r0
        MOV     lr,lr,LSL #Log2PageSize
        ADD     lr,lr,#ApplicationStart
;  Debug AMB,"AMB_MakeUnsparse MappedInNode addr pages ",r1,lr,r0
10
        LDR     r2,[lr]                ;tends to wash data cache a bit, but this should be called rarely
        ADD     lr,lr,#PageSize
        SUBS    r0,r0,#1
        BNE     %BT10
20
        Pull    "r0-r2,r12,pc"


  ] ;AMB_LazyMapIn

; ----------------------------------------------------------------------------------
;
d459 2
a503 2
; ----------------------------------------------------------------------------------
;
@


4.1.3.3.2.2
log
@ 1 Simplify source by removing various long-standing compile flags
   and pre-Medusa h/w support

 2 Fix bug with Pages_Unsafe/Pages_Safe page moving for StrongARM
   (interrupt hole) - also better performance for StrongARM

 3 Improve perfromance of physical memory clear for StrongARM
   (make sure it uses burst write for STM)

 4 Suspend Chocolate task switching for StrongARM if SALDMIBbroken
   is TRUE
@
text
@d45 1
d49 20
a68 1
        ARMA_cleanflush_DCentry $addr0
d79 1
d168 65
d508 1
a508 2
        ADD     r1,r7,r2,LSL #3             ;r1 -> CAM entry for 1st page
  [ AMB_LimpidFreePool
a509 4
        LDR     r3,[r1,#4]                  ;fetch old PPL of 1st page from CAM
  |
        LDR     r4,[r1]                     ;fetch old logical addr. of 1st page from CAM
  ]
d519 3
a521 18
;
  [ AMB_LimpidFreePool
    ;can avoid cache clean/flush for moving pages out from FreePool, since FreePool pages are uncacheable
    ;
        TST     r3, #&20                    ;test NotCacheable bit of PPL of 1st page
        BEQ     AMB_smme_mapnotlimpid       ;if clear, must do full map somewhere with cache clean/flush
    ;
    ;this should be map FreePool -> App Space then
    ;
        MOV     r3,r5
        BL      AMB_movepagesout_L2PT
        BL      AMB_movepagesin_L2PT
        BL      AMB_movepagesin_CAM
        ARM_flush_TLB r0                    ;no cache clean/flush, just TLB flush
        Pull    "r0-r4,r7-r11, pc"
AMB_smme_mapnotlimpid
  ]
;
d536 1
a536 1
;
d540 2
a541 1
;
d543 7
d675 6
@


4.1.3.3.2.3
log
@1) Fixes and tidy ups:
   - mapping of Cur/Sys/Sound area done more elegantly, and soft CAM info
     is now consistent with it
   - cached screen cleaning on VSync performed *after* VSync events
   - comments at top of ARM600 modernised
   - Pages_Unsafe/Safe code fixed to work properly on StrongARM with
     pages that are involved in interrupts (there is no fix for ARM8,
     since that is unlikely to be needed - an ASSERT checks use of ARM8
   - OS_DynamicArea code souped up, to be much more efficient for large
     numbers of dynamic areas (see comments near top of ChangeDyn)
   - cached screen is now suspended on h/w scroll (avoids possible cache
     incoherency)
2) API changes:
   - new OS_Memory reason code (10) allows Wimp to inform kernel of
     Wimp_ClaimFreeMemory, and can control VRAM rescue (see below)
   - new OS_ReadSysInfo reason code (6) allows reading of kernel values
     (reserved for Acorn use, eg. for SoftLoad, ROMPatch)
   - new OS_DynamicArea reason codes (6 and 7) allow for more efficient
     monitoring of dynamic areas by TaskManager (reserved for Acorn use)
3) Changes for Phoebe:
   - kernel runs a VRAM rescue process, which ensures that any VRAM not
     used for the screen is reclaimed if necessary and sinks to the bottom
     of the Free Pool. This is important for Phoebe, where VRAM is slower
     than SDRAM, but does no harm on other platforms.
   - logical copy of physical RAM is removed from memory map. This frees
     up 256M of address space that will later be used for PCI on Phoebe,
     but should do no harm on other platforms (this space is marked
     private in PRMs, so 3rd parties should not use it).
@
text
@d442 2
a443 2
        TST     r3, #DynAreaFlags_NotCacheable  ;test PPL of 1st page for not cacheable bit set
        BEQ     AMB_smme_mapnotlimpid           ;if clear, must do full map somewhere with cache clean/flush
d451 1
a451 1
        ARM_flush_TLB r0                        ;no cache clean/flush, just TLB flush
@


4.1.3.3.2.3.2.1
log
@Changed compile switches, to build Ursula kernel for RPC and A7000(+),
switches now set as follows:
  ARM67Support      TRUE  (for 610,710,7500,7500FE)
  ARMSASupport      TRUE  (for StrongARM)
  ARMSASupport_RevS FALSE (for StrongARMs before rev S)
  IOMD1Support      TRUE  (for old machines)
  IOMD2Support      FALSE (They killed Phoebe!)
Version set to 4.00 (RISC OS 4)
This is the same as my last commit to the Ursula branch
@
text
@d60 1
a60 1
  [ AMB_LazyMapIn :LAND: ARMSASupport
d145 1
a145 1
  ] ;AMB_LazyMapIn :LAND: ARMSASupport
a192 1
  [ :LNOT: ARMSASupport_Only
a196 3
  |
        ARMA_drain_WB            ;because L2PT area for AppSpace will be bufferable
  ]
a350 1
  [ :LNOT: ARMSASupport_Only
a354 3
  |
        ARMA_drain_WB            ;because L2PT area for AppSpace will be bufferable
  ]
d359 26
d456 3
d480 3
a505 1
  [ :LNOT: ARMSASupport_Only
d508 5
a516 3
  ]

  [ ARMSASupport
d526 11
a547 2
        TST     r3,#ACS_MiniDataCache
        BICEQ   r3,r3,#ACS_VSCpending_MASK     ;unless screen uses mini cache, force pending VSC clear since we're about to clean cache
d562 1
a562 1
        BIC     r3,r3,#ACS_NSCsemaphore          ;clear semaphore
a565 2
        BNE     AMB_ccf_nochoc                 ;do nothing if disabled or suspended
        TST     r3,#ACS_HardVIDMRD
d568 1
a568 2
        ARMA_write_MMUdomain r3,EQ             ;if h/w VIDMRD absent, reset screen (domain 1) to fault for VIDMRD emulation
AMB_ccf_nochoc
a583 1
  ] ;ARMSASupport
d585 1
a585 1
  [ AMB_LazyMapIn :LAND: ARMSASupport
d616 3
d728 1
a728 1
  ] ;AMB_LazyMapIn :LAND: ARMSASupport
@


4.1.3.3.2.4
log
@Phoebe aware version of kernel
Source currently builds for Phoebe only. Flipping source switches will
build for Risc PC and/or A7000(+) as well (or instead). Not tested
much on older platforms.
Known issues remaining:
 - on Phoebe, kernel does not always set up the video (new VCO)
   properly. It appears that anything via the display manager is ok,
   old modes are ok before a monitor definition is seen, but mode
   changes via applications in the desktop always/often (?) aren't.
   Most likely area for investigation is whether kernel catches all
   mode change routes for ensuring it programs the new VCO.
 - on Phoebe, kernel does not yet have the hooks to support multiple
   CPU(s) (to park the slaves and allow them to be used later). I
   have a technical note on this, which should be archived as part of
   the Ursula burial work.
 - on older platforms, the areas that need checking most are CMOS
   power on reset (when in ROM) and mode changes by all routes (since
   these areas are bent by Phoebe support)
Note that kernel currently builds for rev S or better StrongARM. The
switch ARMSASupport_RevS should be set false if building for Risc PC.
@
text
@d60 1
a60 1
  [ AMB_LazyMapIn :LAND: ARMSASupport
d145 1
a145 1
  ] ;AMB_LazyMapIn :LAND: ARMSASupport
a192 1
  [ :LNOT: ARMSASupport_Only
a196 3
  |
        ARMA_drain_WB            ;because L2PT area for AppSpace will be bufferable
  ]
a350 1
  [ :LNOT: ARMSASupport_Only
a354 3
  |
        ARMA_drain_WB            ;because L2PT area for AppSpace will be bufferable
  ]
d359 26
d456 3
d480 3
a505 1
  [ :LNOT: ARMSASupport_Only
d508 5
a516 3
  ]

  [ ARMSASupport
d526 11
a547 2
        TST     r3,#ACS_MiniDataCache
        BICEQ   r3,r3,#ACS_VSCpending_MASK     ;unless screen uses mini cache, force pending VSC clear since we're about to clean cache
d562 1
a562 1
        BIC     r3,r3,#ACS_NSCsemaphore          ;clear semaphore
a565 2
        BNE     AMB_ccf_nochoc                 ;do nothing if disabled or suspended
        TST     r3,#ACS_HardVIDMRD
d568 1
a568 2
        ARMA_write_MMUdomain r3,EQ             ;if h/w VIDMRD absent, reset screen (domain 1) to fault for VIDMRD emulation
AMB_ccf_nochoc
a583 1
  ] ;ARMSASupport
d585 1
a585 1
  [ AMB_LazyMapIn :LAND: ARMSASupport
d616 3
d728 1
a728 1
  ] ;AMB_LazyMapIn :LAND: ARMSASupport
@


4.1.3.1.2.1
log
@Merged from 3.71 CD
@
text
@a237 27
  [ ARM810support
    ;Previously supported ARMs all tolerate cache (clean and) flush _after_
    ;remapping - ARMs 6,7 because there is no clean, StrongARM because the cache
    ;writebacks use physical address.
    ;ARM810 does not support clean of writeback cache after remapping, since
    ;writebacks use virtual address. Rather than completely restructure code,
    ;this routine is called before remapping where necessary, and cleans/flushes
    ;if it finds we are running on ARM 810.
    ;
    ;corrupts r3
    ;
AMB_cachecleanflush_ifARM810
        ARM_read_ID r3
        AND     r3,r3,#&F000
        CMP     r3,#&8000
        MOVNE   pc,lr           ;not ARM8
    [ ARM810cleanflushbroken
        Push    "lr"
        ARM8_cleanflush_IDC r3,lr
        Pull    "pc"
    |
        ARM8_cleanflush_IDC r3
        MOV     pc,lr
    ]

  ] ;ARM810support

d239 1
a239 1
; AMB_SetMemMapEntries:
a287 3
  [ ARM810support
        BL      AMB_cachecleanflush_ifARM810
  ]
a295 3
  [ ARM810support
        BL      AMB_cachecleanflush_ifARM810
  ]
a305 3
  [ ARM810support
        BL      AMB_cachecleanflush_ifARM810
  ]
d310 1
a310 1
;(clean and) flush cache(s) appropriately, then flush TLB(s)
a313 5
  [ ARM810support
        CMP     r0,#&8000      ;cache clean/flush done before remapping if ARM810
        ARM8_flush_TLB EQ
        Pull    "r0-r4,r7-r11, pc",EQ
  ]
a391 4
  [ ARM810support
    ;there is a general macro, should have used this before anyway
        ARM_flush_TLB r0
  |
a396 1
  ]
@
