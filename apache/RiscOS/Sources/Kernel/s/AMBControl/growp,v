head	4.5;
access;
symbols
	Kernel-6_14:4.5
	Kernel-6_01-3:4.5
	Kernel-6_13:4.5
	Kernel-6_12:4.5
	Kernel-6_11:4.5
	Kernel-6_10:4.5
	Kernel-6_09:4.5
	Kernel-6_08-4_129_2_10:4.5
	Kernel-6_08-4_129_2_9:4.5
	Kernel-6_08:4.5
	Kernel-6_07:4.5
	Kernel-6_06:4.5
	Kernel-6_05-4_129_2_8:4.5
	Kernel-6_05:4.5
	Kernel-6_04:4.5
	Kernel-6_03:4.5
	Kernel-6_01-2:4.5
	Kernel-6_01-4_146_2_1:4.5
	Kernel-6_02:4.5
	Kernel-6_01-1:4.5
	Kernel-6_01:4.5
	Kernel-6_00:4.5
	Kernel-5_99:4.5
	Kernel-5_98:4.5
	Kernel-5_97-4_129_2_7:4.5
	Kernel-5_97:4.5
	Kernel-5_96:4.5
	Kernel-5_95:4.5
	Kernel-5_94:4.5
	Kernel-5_93:4.5
	Kernel-5_92:4.5
	Kernel-5_91:4.5
	Kernel-5_90:4.5
	Kernel-5_89-4_129_2_6:4.5
	Kernel-5_89:4.5
	Kernel-5_88-4_129_2_5:4.5
	Kernel-5_88-4_129_2_4:4.5
	Kernel-5_88:4.5
	Kernel-5_87:4.5
	Kernel-5_86-4_129_2_3:4.5
	Kernel-5_86-4_129_2_2:4.5
	Kernel-5_86-4_129_2_1:4.5
	Kernel-5_86:4.5
	SMP:4.5.0.2
	SMP_bp:4.5
	Kernel-5_85:4.5
	Kernel-5_54-1:4.3
	Kernel-5_84:4.5
	Kernel-5_83:4.5
	Kernel-5_82:4.5
	Kernel-5_81:4.5
	Kernel-5_80:4.5
	Kernel-5_79:4.5
	Kernel-5_78:4.5
	Kernel-5_77:4.5
	Kernel-5_76:4.5
	Kernel-5_75:4.5
	Kernel-5_74:4.5
	Kernel-5_73:4.5
	Kernel-5_72:4.5
	Kernel-5_71:4.5
	Kernel-5_70:4.5
	Kernel-5_69:4.5
	Kernel-5_68:4.5
	Kernel-5_67:4.5
	Kernel-5_66:4.5
	Kernel-5_65:4.4
	Kernel-5_64:4.4
	Kernel-5_63:4.4
	Kernel-5_62:4.4
	Kernel-5_61:4.4
	Kernel-5_60:4.4
	Kernel-5_59:4.4
	Kernel-5_58:4.4
	Kernel-5_57:4.4
	Kernel-5_56:4.4
	Kernel-5_55:4.4
	Kernel-5_54:4.3
	Kernel-5_53:4.3
	Kernel-5_52:4.3
	Kernel-5_51:4.3
	Kernel-5_50:4.3
	Kernel-5_49:4.3
	HAL_merge:4.2.6.4
	Kernel-5_48:4.3
	Kernel-5_35-4_79_2_327:4.2.6.4
	Kernel-5_35-4_79_2_326:4.2.6.4
	Kernel-5_35-4_79_2_325:4.2.6.4
	Kernel-5_35-4_79_2_324:4.2.6.4
	Kernel-5_35-4_79_2_323:4.2.6.4
	Kernel-5_35-4_79_2_322:4.2.6.4
	Kernel-5_35-4_79_2_321:4.2.6.4
	Kernel-5_35-4_79_2_320:4.2.6.4
	Kernel-5_35-4_79_2_319:4.2.6.4
	Kernel-5_35-4_79_2_318:4.2.6.4
	Kernel-5_35-4_79_2_317:4.2.6.4
	Kernel-5_35-4_79_2_316:4.2.6.4
	Kernel-5_35-4_79_2_315:4.2.6.4
	Kernel-5_35-4_79_2_314:4.2.6.4
	Kernel-5_35-4_79_2_313:4.2.6.4
	Kernel-5_35-4_79_2_312:4.2.6.4
	Kernel-5_35-4_79_2_311:4.2.6.4
	Kernel-5_35-4_79_2_310:4.2.6.4
	Kernel-5_35-4_79_2_309:4.2.6.4
	Kernel-5_35-4_79_2_308:4.2.6.4
	Kernel-5_35-4_79_2_307:4.2.6.4
	Kernel-5_35-4_79_2_306:4.2.6.4
	Kernel-5_35-4_79_2_305:4.2.6.4
	Kernel-5_35-4_79_2_304:4.2.6.4
	Kernel-5_35-4_79_2_303:4.2.6.4
	Kernel-5_35-4_79_2_302:4.2.6.4
	Kernel-5_35-4_79_2_301:4.2.6.4
	Kernel-5_35-4_79_2_300:4.2.6.4
	Kernel-5_35-4_79_2_299:4.2.6.4
	Kernel-5_35-4_79_2_298:4.2.6.4
	Kernel-5_35-4_79_2_297:4.2.6.4
	Kernel-5_35-4_79_2_296:4.2.6.4
	Kernel-5_35-4_79_2_295:4.2.6.4
	Kernel-5_35-4_79_2_294:4.2.6.4
	Kernel-5_35-4_79_2_293:4.2.6.4
	Kernel-5_35-4_79_2_292:4.2.6.4
	Kernel-5_35-4_79_2_291:4.2.6.4
	Kernel-5_35-4_79_2_290:4.2.6.4
	Kernel-5_35-4_79_2_289:4.2.6.4
	Kernel-5_35-4_79_2_288:4.2.6.4
	Kernel-5_35-4_79_2_287:4.2.6.4
	Kernel-5_35-4_79_2_286:4.2.6.3
	Kernel-5_35-4_79_2_285:4.2.6.3
	Kernel-5_35-4_79_2_284:4.2.6.3
	Kernel-5_35-4_79_2_283:4.2.6.2
	Kernel-5_35-4_79_2_282:4.2.6.2
	Kernel-5_35-4_79_2_281:4.2.6.2
	Kernel-5_35-4_79_2_280:4.2.6.2
	Kernel-5_35-4_79_2_279:4.2.6.2
	Kernel-5_35-4_79_2_278:4.2.6.2
	Kernel-5_35-4_79_2_277:4.2.6.2
	Kernel-5_35-4_79_2_276:4.2.6.2
	Kernel-5_35-4_79_2_275:4.2.6.2
	Kernel-5_35-4_79_2_274:4.2.6.2
	Kernel-5_35-4_79_2_273:4.2.6.2
	Kernel-5_35-4_79_2_272:4.2.6.2
	Kernel-5_35-4_79_2_271:4.2.6.2
	Kernel-5_35-4_79_2_270:4.2.6.2
	Kernel-5_35-4_79_2_269:4.2.6.2
	Kernel-5_35-4_79_2_268:4.2.6.2
	Kernel-5_35-4_79_2_267:4.2.6.2
	Kernel-5_35-4_79_2_266:4.2.6.2
	Kernel-5_35-4_79_2_265:4.2.6.2
	Kernel-5_35-4_79_2_264:4.2.6.2
	Kernel-5_35-4_79_2_263:4.2.6.2
	Kernel-5_35-4_79_2_262:4.2.6.2
	Kernel-5_35-4_79_2_261:4.2.6.2
	Kernel-5_35-4_79_2_260:4.2.6.2
	Kernel-5_35-4_79_2_259:4.2.6.2
	Kernel-5_35-4_79_2_258:4.2.6.2
	Kernel-5_35-4_79_2_257:4.2.6.2
	Kernel-5_35-4_79_2_256:4.2.6.2
	Kernel-5_35-4_79_2_255:4.2.6.2
	Kernel-5_35-4_79_2_254:4.2.6.2
	Kernel-5_35-4_79_2_253:4.2.6.2
	Kernel-5_35-4_79_2_252:4.2.6.2
	Kernel-5_35-4_79_2_251:4.2.6.2
	Kernel-5_35-4_79_2_250:4.2.6.2
	Kernel-5_35-4_79_2_249:4.2.6.2
	Kernel-5_35-4_79_2_248:4.2.6.2
	Kernel-5_35-4_79_2_247:4.2.6.2
	Kernel-5_35-4_79_2_246:4.2.6.2
	Kernel-5_35-4_79_2_245:4.2.6.2
	Kernel-5_35-4_79_2_244:4.2.6.2
	Kernel-5_35-4_79_2_243:4.2.6.2
	Kernel-5_35-4_79_2_242:4.2.6.2
	Kernel-5_35-4_79_2_241:4.2.6.2
	Kernel-5_35-4_79_2_240:4.2.6.2
	Kernel-5_35-4_79_2_239:4.2.6.2
	Kernel-5_35-4_79_2_238:4.2.6.2
	Kernel-5_35-4_79_2_237:4.2.6.2
	Kernel-5_35-4_79_2_236:4.2.6.2
	Kernel-5_35-4_79_2_235:4.2.6.2
	Kernel-5_35-4_79_2_234:4.2.6.2
	Kernel-5_35-4_79_2_233:4.2.6.2
	Kernel-5_35-4_79_2_232:4.2.6.2
	Kernel-5_35-4_79_2_231:4.2.6.2
	Kernel-5_35-4_79_2_230:4.2.6.2
	Kernel-5_35-4_79_2_229:4.2.6.2
	Kernel-5_35-4_79_2_228:4.2.6.2
	Kernel-5_35-4_79_2_227:4.2.6.2
	Kernel-5_35-4_79_2_226:4.2.6.2
	Kernel-5_35-4_79_2_225:4.2.6.2
	Kernel-5_35-4_79_2_224:4.2.6.2
	Kernel-5_35-4_79_2_223:4.2.6.2
	Kernel-5_35-4_79_2_222:4.2.6.2
	Kernel-5_35-4_79_2_221:4.2.6.2
	Kernel-5_35-4_79_2_220:4.2.6.2
	Kernel-5_35-4_79_2_219:4.2.6.2
	Kernel-5_35-4_79_2_218:4.2.6.2
	Kernel-5_35-4_79_2_217:4.2.6.2
	Kernel-5_35-4_79_2_216:4.2.6.2
	Kernel-5_35-4_79_2_215:4.2.6.2
	Kernel-5_35-4_79_2_214:4.2.6.2
	Kernel-5_35-4_79_2_213:4.2.6.2
	Kernel-5_35-4_79_2_212:4.2.6.2
	Kernel-5_35-4_79_2_211:4.2.6.2
	Kernel-5_35-4_79_2_210:4.2.6.2
	Kernel-5_35-4_79_2_209:4.2.6.2
	Kernel-5_35-4_79_2_208:4.2.6.2
	Kernel-5_35-4_79_2_207:4.2.6.2
	Kernel-5_35-4_79_2_206:4.2.6.2
	Kernel-5_35-4_79_2_205:4.2.6.2
	Kernel-5_35-4_79_2_204:4.2.6.2
	Kernel-5_35-4_79_2_203:4.2.6.2
	Kernel-5_35-4_79_2_202:4.2.6.2
	Kernel-5_35-4_79_2_201:4.2.6.2
	Kernel-5_35-4_79_2_200:4.2.6.2
	Kernel-5_35-4_79_2_199:4.2.6.2
	Kernel-5_35-4_79_2_198:4.2.6.2
	Kernel-5_35-4_79_2_197:4.2.6.2
	Kernel-5_35-4_79_2_196:4.2.6.2
	Kernel-5_35-4_79_2_195:4.2.6.2
	Kernel-5_35-4_79_2_194:4.2.6.2
	Kernel-5_35-4_79_2_193:4.2.6.2
	Kernel-5_35-4_79_2_192:4.2.6.2
	Kernel-5_35-4_79_2_191:4.2.6.2
	Kernel-5_35-4_79_2_190:4.2.6.2
	Kernel-5_35-4_79_2_189:4.2.6.2
	Kernel-5_35-4_79_2_188:4.2.6.2
	Kernel-5_35-4_79_2_187:4.2.6.2
	Kernel-5_35-4_79_2_186:4.2.6.2
	Kernel-5_35-4_79_2_185:4.2.6.2
	Kernel-5_35-4_79_2_184:4.2.6.2
	Kernel-5_35-4_79_2_183:4.2.6.2
	Kernel-5_35-4_79_2_182:4.2.6.2
	Kernel-5_35-4_79_2_181:4.2.6.2
	Kernel-5_35-4_79_2_180:4.2.6.2
	Kernel-5_35-4_79_2_179:4.2.6.2
	Kernel-5_35-4_79_2_178:4.2.6.2
	Kernel-5_35-4_79_2_177:4.2.6.2
	Kernel-5_35-4_79_2_176:4.2.6.2
	Kernel-5_35-4_79_2_175:4.2.6.2
	Kernel-5_35-4_79_2_174:4.2.6.2
	Kernel-5_35-4_79_2_173:4.2.6.2
	Kernel-5_35-4_79_2_172:4.2.6.2
	Kernel-5_35-4_79_2_171:4.2.6.2
	Kernel-5_35-4_79_2_170:4.2.6.2
	Kernel-5_35-4_79_2_169:4.2.6.2
	Kernel-5_35-4_79_2_168:4.2.6.2
	Kernel-5_35-4_79_2_167:4.2.6.2
	Kernel-5_35-4_79_2_166:4.2.6.2
	Kernel-5_35-4_79_2_165:4.2.6.2
	RPi_merge:4.2.6.2
	Kernel-5_35-4_79_2_147_2_23:4.2.6.2
	Kernel-5_35-4_79_2_147_2_22:4.2.6.2
	Kernel-5_35-4_79_2_147_2_21:4.2.6.2
	Kernel-5_35-4_79_2_147_2_20:4.2.6.2
	Kernel-5_35-4_79_2_147_2_19:4.2.6.2
	Kernel-5_35-4_79_2_147_2_18:4.2.6.2
	Kernel-5_35-4_79_2_164:4.2.6.2
	Kernel-5_35-4_79_2_163:4.2.6.2
	Kernel-5_35-4_79_2_147_2_17:4.2.6.2
	Kernel-5_35-4_79_2_147_2_16:4.2.6.2
	Kernel-5_35-4_79_2_147_2_15:4.2.6.2
	Kernel-5_35-4_79_2_162:4.2.6.2
	Kernel-5_35-4_79_2_161:4.2.6.2
	Kernel-5_35-4_79_2_147_2_14:4.2.6.2
	Kernel-5_35-4_79_2_147_2_13:4.2.6.2
	Kernel-5_35-4_79_2_160:4.2.6.2
	Kernel-5_35-4_79_2_159:4.2.6.2
	Kernel-5_35-4_79_2_158:4.2.6.2
	Kernel-5_35-4_79_2_157:4.2.6.2
	Kernel-5_35-4_79_2_156:4.2.6.2
	Kernel-5_35-4_79_2_147_2_12:4.2.6.2
	Kernel-5_35-4_79_2_147_2_11:4.2.6.2
	Kernel-5_35-4_79_2_155:4.2.6.2
	Kernel-5_35-4_79_2_147_2_10:4.2.6.2
	Kernel-5_35-4_79_2_154:4.2.6.2
	Kernel-5_35-4_79_2_153:4.2.6.2
	Kernel-5_35-4_79_2_147_2_9:4.2.6.2
	Kernel-5_35-4_79_2_152:4.2.6.2
	Kernel-5_35-4_79_2_151:4.2.6.2
	Kernel-5_35-4_79_2_147_2_8:4.2.6.2
	Kernel-5_35-4_79_2_147_2_7:4.2.6.2
	Kernel-5_35-4_79_2_150:4.2.6.2
	Kernel-5_35-4_79_2_147_2_6:4.2.6.2
	Kernel-5_35-4_79_2_147_2_5:4.2.6.2
	Kernel-5_35-4_79_2_149:4.2.6.2
	Kernel-5_35-4_79_2_147_2_4:4.2.6.2
	Kernel-5_35-4_79_2_147_2_3:4.2.6.2
	Kernel-5_35-4_79_2_148:4.2.6.2
	Kernel-5_35-4_79_2_147_2_2:4.2.6.2
	Kernel-5_35-4_79_2_147_2_1:4.2.6.2
	RPi:4.2.6.2.0.2
	RPi_bp:4.2.6.2
	Kernel-5_35-4_79_2_98_2_52_2_1:4.2.6.1.2.1
	alees_Kernel_dev:4.2.6.1.2.1.0.2
	alees_Kernel_dev_bp:4.2.6.1.2.1
	Kernel-5_35-4_79_2_147:4.2.6.2
	Kernel-5_35-4_79_2_146:4.2.6.2
	Kernel-5_35-4_79_2_145:4.2.6.2
	Kernel-5_35-4_79_2_144:4.2.6.2
	Kernel-5_35-4_79_2_143:4.2.6.2
	Kernel-5_35-4_79_2_142:4.2.6.2
	Kernel-5_35-4_79_2_141:4.2.6.2
	Kernel-5_35-4_79_2_140:4.2.6.2
	Kernel-5_35-4_79_2_139:4.2.6.2
	Kernel-5_35-4_79_2_138:4.2.6.2
	Kernel-5_35-4_79_2_137:4.2.6.2
	Kernel-5_35-4_79_2_136:4.2.6.2
	Kernel-5_35-4_79_2_135:4.2.6.2
	Kernel-5_35-4_79_2_134:4.2.6.2
	Kernel-5_35-4_79_2_133:4.2.6.2
	Kernel-5_35-4_79_2_132:4.2.6.2
	Kernel-5_35-4_79_2_131:4.2.6.2
	Kernel-5_35-4_79_2_130:4.2.6.2
	Kernel-5_35-4_79_2_129:4.2.6.2
	Kernel-5_35-4_79_2_128:4.2.6.2
	Kernel-5_35-4_79_2_127:4.2.6.2
	Kernel-5_35-4_79_2_126:4.2.6.2
	Kernel-5_35-4_79_2_125:4.2.6.2
	Kernel-5_35-4_79_2_124:4.2.6.2
	Kernel-5_35-4_79_2_123:4.2.6.2
	Cortex_merge:4.2.6.1.2.1
	Kernel-5_35-4_79_2_122:4.2.6.1
	Kernel-5_35-4_79_2_98_2_54:4.2.6.1.2.1
	Kernel-5_35-4_79_2_98_2_53:4.2.6.1.2.1
	Kernel-5_35-4_79_2_98_2_52:4.2.6.1.2.1
	Kernel-5_35-4_79_2_98_2_51:4.2.6.1.2.1
	Kernel-5_35-4_79_2_98_2_50:4.2.6.1.2.1
	Kernel-5_35-4_79_2_98_2_49:4.2.6.1.2.1
	Kernel-5_35-4_79_2_98_2_48:4.2.6.1.2.1
	Kernel-5_35-4_79_2_121:4.2.6.1
	Kernel-5_35-4_79_2_98_2_47:4.2.6.1
	Kernel-5_35-4_79_2_120:4.2.6.1
	Kernel-5_35-4_79_2_98_2_46:4.2.6.1
	Kernel-5_35-4_79_2_119:4.2.6.1
	Kernel-5_35-4_79_2_98_2_45:4.2.6.1
	Kernel-5_35-4_79_2_98_2_44:4.2.6.1
	Kernel-5_35-4_79_2_118:4.2.6.1
	Kernel-5_35-4_79_2_98_2_43:4.2.6.1
	Kernel-5_35-4_79_2_117:4.2.6.1
	Kernel-5_35-4_79_2_116:4.2.6.1
	Kernel-5_35-4_79_2_98_2_42:4.2.6.1
	Kernel-5_35-4_79_2_115:4.2.6.1
	Kernel-5_35-4_79_2_98_2_41:4.2.6.1
	Kernel-5_35-4_79_2_98_2_40:4.2.6.1
	Kernel-5_35-4_79_2_114:4.2.6.1
	Kernel-5_35-4_79_2_98_2_39:4.2.6.1
	Kernel-5_35-4_79_2_98_2_38:4.2.6.1
	Kernel-5_35-4_79_2_113:4.2.6.1
	Kernel-5_35-4_79_2_112:4.2.6.1
	Kernel-5_35-4_79_2_98_2_37:4.2.6.1
	Kernel-5_35-4_79_2_98_2_36:4.2.6.1
	Kernel-5_35-4_79_2_98_2_35:4.2.6.1
	Kernel-5_35-4_79_2_98_2_34:4.2.6.1
	Kernel-5_35-4_79_2_98_2_33:4.2.6.1
	Kernel-5_35-4_79_2_98_2_32:4.2.6.1
	Kernel-5_35-4_79_2_98_2_31:4.2.6.1
	Kernel-5_35-4_79_2_98_2_30:4.2.6.1
	Kernel-5_35-4_79_2_98_2_29:4.2.6.1
	Kernel-5_35-4_79_2_98_2_28:4.2.6.1
	Kernel-5_35-4_79_2_98_2_27:4.2.6.1
	Kernel-5_35-4_79_2_98_2_26:4.2.6.1
	Kernel-5_35-4_79_2_111:4.2.6.1
	Kernel-5_35-4_79_2_98_2_25:4.2.6.1
	Kernel-5_35-4_79_2_98_2_24:4.2.6.1
	Kernel-5_35-4_79_2_98_2_23:4.2.6.1
	Kernel-5_35-4_79_2_110:4.2.6.1
	Kernel-5_35-4_79_2_98_2_22:4.2.6.1
	Kernel-5_35-4_79_2_109:4.2.6.1
	Kernel-5_35-4_79_2_98_2_21:4.2.6.1
	Kernel-5_35-4_79_2_98_2_20:4.2.6.1
	Kernel-5_35-4_79_2_108:4.2.6.1
	Kernel-5_35-4_79_2_107:4.2.6.1
	Kernel-5_35-4_79_2_98_2_19:4.2.6.1
	Kernel-5_35-4_79_2_98_2_18:4.2.6.1
	Kernel-5_35-4_79_2_98_2_17:4.2.6.1
	Kernel-5_35-4_79_2_98_2_16:4.2.6.1
	Kernel-5_35-4_79_2_98_2_15:4.2.6.1
	Kernel-5_35-4_79_2_106:4.2.6.1
	Kernel-5_35-4_79_2_105:4.2.6.1
	Kernel-5_35-4_79_2_104:4.2.6.1
	Kernel-5_35-4_79_2_98_2_14:4.2.6.1
	Kernel-5_35-4_79_2_98_2_13:4.2.6.1
	Kernel-5_35-4_79_2_98_2_12:4.2.6.1
	Kernel-5_35-4_79_2_98_2_11:4.2.6.1
	Kernel-5_35-4_79_2_98_2_10:4.2.6.1
	Kernel-5_35-4_79_2_98_2_9:4.2.6.1
	Kernel-5_35-4_79_2_103:4.2.6.1
	Kernel-5_35-4_79_2_102:4.2.6.1
	Kernel-5_35-4_79_2_98_2_8:4.2.6.1
	Kernel-5_35-4_79_2_98_2_7:4.2.6.1
	Kernel-5_35-4_79_2_98_2_6:4.2.6.1
	Kernel-5_35-4_79_2_98_2_5:4.2.6.1
	Kernel-5_35-4_79_2_98_2_4:4.2.6.1
	Kernel-5_35-4_79_2_101:4.2.6.1
	Kernel-5_35-4_79_2_100:4.2.6.1
	Kernel-5_35-4_79_2_99:4.2.6.1
	Kernel-5_35-4_79_2_98_2_3:4.2.6.1
	Kernel-5_35-4_79_2_98_2_2:4.2.6.1
	Kernel-5_35-4_79_2_98_2_1:4.2.6.1
	Cortex:4.2.6.1.0.2
	Cortex_bp:4.2.6.1
	Kernel-5_35-4_79_2_98:4.2.6.1
	Kernel-5_35-4_79_2_97:4.2.6.1
	Kernel-5_35-4_79_2_96:4.2.6.1
	Kernel-5_35-4_79_2_95:4.2.6.1
	Kernel-5_35-4_79_2_94:4.2.6.1
	Kernel-5_35-4_79_2_93:4.2.6.1
	Kernel-5_35-4_79_2_92:4.2.6.1
	Kernel-5_35-4_79_2_91:4.2.6.1
	Kernel-5_35-4_79_2_90:4.2.6.1
	Kernel-5_35-4_79_2_89:4.2.6.1
	Kernel-5_35-4_79_2_88:4.2.6.1
	Kernel-5_35-4_79_2_87:4.2.6.1
	Kernel-5_35-4_79_2_86:4.2.6.1
	Kernel-5_35-4_79_2_85:4.2.6.1
	Kernel-5_35-4_79_2_84:4.2.6.1
	Kernel-5_35-4_79_2_83:4.2.6.1
	Kernel-5_35-4_79_2_82:4.2.6.1
	Kernel-5_35-4_79_2_81:4.2.6.1
	Kernel-5_35-4_79_2_80:4.2.6.1
	Kernel-5_35-4_79_2_79:4.2.6.1
	Kernel-5_35-4_79_2_78:4.2.6.1
	Kernel-5_35-4_79_2_77:4.2.6.1
	RO_5_07:4.2.6.1
	Kernel-5_35-4_79_2_76:4.2.6.1
	Kernel-5_35-4_79_2_75:4.2.6.1
	Kernel-5_35-4_79_2_74:4.2.6.1
	Kernel-5_35-4_79_2_73:4.2.6.1
	Kernel-5_35-4_79_2_72:4.2.6.1
	Kernel-5_35-4_79_2_71:4.2.6.1
	Kernel-5_35-4_79_2_70:4.2.6.1
	Kernel-5_35-4_79_2_69:4.2.6.1
	Kernel-5_35-4_79_2_68:4.2.6.1
	Kernel-5_35-4_79_2_67:4.2.6.1
	Kernel-5_35-4_79_2_66:4.2.6.1
	Kernel-5_35-4_79_2_65:4.2.6.1
	Kernel-5_35-4_79_2_64:4.2.6.1
	Kernel-5_35-4_79_2_63:4.2.6.1
	Kernel-5_35-4_79_2_62:4.2.6.1
	Kernel-5_35-4_79_2_61:4.2.6.1
	Kernel-5_35-4_79_2_59:4.2.6.1
	Kernel-5_35-4_79_2_58:4.2.6.1
	Kernel-5_35-4_79_2_57:4.2.6.1
	Kernel-5_35-4_79_2_56:4.2.6.1
	Kernel-5_35-4_79_2_55:4.2.6.1
	Kernel-5_35-4_79_2_54:4.2.6.1
	Kernel-5_35-4_79_2_53:4.2.6.1
	Kernel-5_35-4_79_2_52:4.2.6.1
	Kernel-5_35-4_79_2_51:4.2.6.1
	Kernel-5_35-4_79_2_50:4.2.6.1
	Kernel-5_35-4_79_2_49:4.2.6.1
	Kernel-5_35-4_79_2_48:4.2.6.1
	Kernel-5_47:4.2
	Kernel-5_46-4_90_2_1:4.2
	nbingham_Kernel_FastNC_dev_bp:4.2
	nbingham_Kernel_FastNC_dev:4.2.0.10
	Kernel-5_46:4.2
	Kernel-5_45:4.2
	Kernel-5_35-4_79_2_47:4.2.6.1
	Kernel-5_35-4_79_2_46:4.2.6.1
	Kernel-5_35-4_79_2_45:4.2.6.1
	Kernel-5_35-4_79_2_44:4.2.6.1
	Kernel-5_35-4_79_2_25_2_2:4.2.6.1
	Kernel-5_35-4_79_2_43:4.2.6.1
	Kernel-5_35-4_79_2_42:4.2.6.1
	Kernel-5_35-4_79_2_41:4.2.6.1
	Kernel-5_35-4_79_2_40:4.2.6.1
	Kernel-5_35-4_79_2_39:4.2.6.1
	Kernel-5_35-4_79_2_38:4.2.6.1
	Kernel-5_35-4_79_2_37:4.2.6.1
	Kernel-5_35-4_79_2_36:4.2.6.1
	Kernel-5_35-4_79_2_35:4.2.6.1
	Kernel-5_35-4_79_2_34:4.2.6.1
	Kernel-5_35-4_79_2_33:4.2.6.1
	Kernel-5_35-4_79_2_32:4.2.6.1
	Kernel-5_44:4.2
	Kernel-5_35-4_79_2_25_2_1:4.2.6.1
	Kernel-5_43:4.2
	Kernel-5_35-4_79_2_31:4.2.6.1
	Kernel-5_35-4_79_2_30:4.2.6.1
	Kernel-5_35-4_79_2_29:4.2.6.1
	Kernel-5_35-4_79_2_28:4.2.6.1
	Kernel-5_35-4_79_2_27:4.2.6.1
	Kernel-5_35-4_79_2_26:4.2.6.1
	Kernel-5_42:4.2
	Kernel-5_41:4.2
	Kernel-5_40:4.2
	Kernel-5_35-4_79_2_25:4.2.6.1
	Kernel-5_35-4_79_2_24:4.2.6.1
	Kernel-5_35-4_79_2_23:4.2.6.1
	Kernel-5_35-4_79_2_22:4.2.6.1
	Kernel-5_35-4_79_2_21:4.2.6.1
	Kernel-5_35-4_79_2_20:4.2.6.1
	Kernel-5_35-4_79_2_19:4.2.6.1
	Kernel-5_35-4_79_2_18:4.2.6.1
	Kernel-5_35-4_79_2_17:4.2.6.1
	Kernel-5_35-4_79_2_16:4.2.6.1
	Kernel-5_35-4_79_2_15:4.2.6.1
	Kernel-5_35-4_79_2_14:4.2.6.1
	Kernel-5_39:4.2
	Kernel-5_13-4_52_2_1:4.2
	Bethany:4.2.0.8
	Kernel-5_38:4.2
	Kernel-5_35-4_79_2_13:4.2.6.1
	Kernel-5_35-4_79_2_12:4.2.6.1
	Kernel-5_35-4_79_2_11:4.2
	Kernel-5_37:4.2
	Kernel-5_35-4_79_2_10:4.2
	Kernel-5_35-4_79_2_9:4.2
	Kernel-5_36:4.2
	Kernel-5_35-4_79_2_8:4.2
	Kernel-5_35-4_79_2_7:4.2
	Kernel-5_35-4_79_2_6:4.2
	Kernel-5_35-4_79_2_5:4.2
	Kernel-5_35-4_79_2_4:4.2
	Kernel-5_35-4_79_2_3:4.2
	Kernel-5_35-4_79_2_2:4.2
	dellis_autobuild_BaseSW:4.2
	Kernel-5_35-4_79_2_1:4.2
	HAL:4.2.0.6
	Kernel-5_35:4.2
	Kernel-5_34:4.2
	Kernel-5_33:4.2
	Kernel-5_32:4.2
	Kernel-5_31:4.2
	Kernel-5_30:4.2
	Kernel-5_29:4.2
	Kernel-5_28:4.2
	Kernel-5_27:4.2
	Kernel-5_26:4.2
	Kernel-5_25:4.2
	Kernel-5_24:4.2
	Kernel-5_23:4.2
	Kernel-5_22:4.2
	sbrodie_sedwards_16Mar2000:4.2
	Kernel-5_21:4.2
	Kernel-5_20:4.2
	Kernel-5_19:4.2
	Kernel-5_18:4.2
	Kernel-5_17:4.2
	Kernel-5_16:4.2
	Kernel-5_15:4.2
	Kernel-5_14:4.2
	Kernel-5_13:4.2
	Kernel-5_12:4.2
	Kernel-5_11:4.2
	Kernel-5_10:4.2
	Kernel-5_09:4.2
	Kernel-5_08:4.2
	Kernel-5_07:4.2
	Kernel-5_06:4.2
	Kernel-5_05:4.2
	Kernel-5_04:4.2
	Kernel-5_03:4.2
	Kernel-5_02:4.2
	Kernel-5_01:4.2
	Kernel-5_00:4.2
	Kernel-4_99:4.2
	Kernel-4_98:4.2
	Kernel-4_97:4.2
	Kernel-4_96:4.2
	Kernel-4_95:4.2
	Kernel-4_94:4.2
	Kernel-4_93:4.2
	Kernel-4_92:4.2
	Kernel-4_91:4.2
	Kernel-4_90:4.2
	dcotton_autobuild_BaseSW:4.2
	Kernel-4_89:4.2
	Kernel-4_88:4.2
	Kernel-4_87:4.2
	Kernel-4_86:4.2
	Kernel-4_85:4.2
	sbrodie_UrsulaRiscPC_Kernel_19Aug99:4.2.2.1
	Kernel-4_84:4.2
	sbrodie_UrsulaRiscPC_Kernel_18Aug99:4.2.2.1
	Ursula_RiscPC_bp:4.2.2.1
	Kernel-4_83:4.2
	Kernel-4_82:4.2
	Kernel-4_81:4.2
	Kernel-4_80:4.2
	Kernel-4_79:4.2
	Kernel-4_78:4.2
	Kernel-4_77:4.2
	Kernel-4_76:4.2
	Kernel-4_75:4.2
	Kernel-4_74:4.2
	Kernel-4_73:4.2
	Kernel-4_72:4.2
	Kernel-4_71:4.2
	Kernel-4_70:4.2
	Kernel-4_69:4.2
	Kernel-4_68:4.2
	mstphens_UrsulaRiscPCBuild_20Nov98:4.2.2.1
	Ursula_RiscPC:4.2.2.1.0.2
	Kernel-4_67:4.2
	Kernel-4_66:4.2
	Kernel-4_65:4.2
	Ursula_merge:4.2
	Kernel-4_64:4.2
	mstphens_Kernel-3_81:4.2.2.1
	rthornb_UrsulaBuild-19Aug1998:4.2.2.1
	UrsulaBuild_FinalSoftload:4.2.2.1
	rthornb_UrsulaBuild-12Aug1998:4.2.2.1
	aglover_UrsulaBuild-05Aug1998:4.2.2.1
	rthornb_UrsulaBuild-29Jul1998:4.2.2.1
	rthornb_UrsulaBuild-22Jul1998:4.2.2.1
	rthornb_UrsulaBuild-15Jul1998:4.2.2.1
	rthornb_UrsulaBuild-07Jul1998:4.2.2.1
	rthornb_UrsulaBuild-17Jun1998:4.2.2.1
	rthornb_UrsulaBuild-03Jun1998:4.2.2.1
	rthornb_UrsulaBuild-27May1998:4.2.2.1
	mstphens_Kernel-3_80:4.2.2.1
	rthornb_UrsulaBuild-21May1998:4.2.2.1
	rthornb_UrsulaBuild_01May1998:4.2.2.1
	afrost_NC2_Generic:4.2
	Daytona:4.2.0.4
	Daytona_bp:4.2
	Ursula:4.2.0.2
	Ursula_bp:4.2
	RO_3_71:4.1.3.1
	MergeFiles:4.1.3.1
	RO_3_70:4.1.3.1
	StrongARM:4.1.3;
locks; strict;
comment	@# @;


4.5
date	2016.12.13.16.42.53;	author jlee;	state Exp;
branches;
next	4.4;
commitid	aGog9bB8f4QKlQxz;

4.4
date	2016.08.02.22.10.47;	author jlee;	state Exp;
branches;
next	4.3;
commitid	CnQYuUGzojQfrMgz;

4.3
date	2016.06.30.20.08.12;	author jlee;	state Exp;
branches;
next	4.2;
commitid	IWoXxARWeuLDOwcz;

4.2
date	97.05.14.12.40.00;	author kbracey;	state Exp;
branches
	4.2.2.1
	4.2.6.1;
next	4.1;

4.1
date	96.11.06.02.01.33;	author nturton;	state Exp;
branches
	4.1.3.1;
next	;

4.2.2.1
date	97.05.21.09.30.29;	author mstphens;	state Exp;
branches;
next	;

4.2.6.1
date	2000.11.10.14.41.16;	author mstephen;	state Exp;
branches
	4.2.6.1.2.1;
next	4.2.6.2;

4.2.6.2
date	2011.11.26.21.11.20;	author jlee;	state Exp;
branches;
next	4.2.6.3;
commitid	cI3W0zbtALQG6TIv;

4.2.6.3
date	2015.08.31.19.28.41;	author jlee;	state Exp;
branches;
next	4.2.6.4;
commitid	Ni3KL17bG70fnszy;

4.2.6.4
date	2015.09.06.18.45.17;	author jlee;	state Exp;
branches;
next	;
commitid	9JoJW3FhqXIqWdAy;

4.2.6.1.2.1
date	2011.08.08.23.28.30;	author jlee;	state Exp;
branches;
next	;
commitid	D7rzILnwRRSXoLuv;

4.1.3.1
date	96.11.06.02.01.33;	author nturton;	state Exp;
branches
	4.1.3.1.2.1;
next	;

4.1.3.1.2.1
date	97.05.14.12.38.39;	author kbracey;	state Exp;
branches;
next	;


desc
@@


4.5
log
@Reimplement AMBControl ontop of the PMP system
Detail:
  With this set of changes, each AMB node is now the owner of a fake DANode which is linked to a PMP.
  From a user's perspective the behaviour of AMBControl is the same as before, but rewriting it to use PMPs internally offers the following (potential) benefits:
  * Reduction in the amount of code which messes with the CAM & page tables, simplifying future work/maintenance. Some of the AMB ops (grow, shrink) now just call through to OS_ChangeDynamicArea. However all of the old AMB routines were well-optimised, so to avoid a big performance hit for common operations not all of them have been removed (e.g. mapslot / mapsome). Maybe one day these optimal routines will be made available for use by regular PMP DAs.
  * Removal of the slow Service_MemoryMoved / Service_PagesSafe handlers that had to do page list fixup after the core kernel had reclaimed/moved pages. Since everything is a PMP, the kernel will now deal with this on behalf of AMB.
  * Removal of a couple of other slow code paths (e.g. Do_AMB_MakeUnsparse calls from OS_ChangeDynamicArea)
  * Potential for more flexible mapping of application space in future, e.g. sparse allocation of memory to the wimp slot
  * Simpler transition to an ASID-based task swapping scheme on ARMv6+?
  Other changes of note:
  * AMB_LazyMapIn switch has been fixed up to work correctly (i.e. turning it off now disables lazy task swapping and all associated code instead of producing a build error)
  * The DANode for the current app should be accessed via the GetAppSpaceDANode macro. This will either return the current AMB DANode, or AppSpaceDANode (if e.g. pre-Wimp). However be aware that AppSpaceDANode retains the legacy behaviour of having a base + size relative to &0, while the AMB DANodes (identifiable via the PMP flag) are sane and have their base + size relative to &8000.
  * Mostly-useless DebugAborts switch removed
  * AMBPhysBin (page number -> phys addr lookup table) removed. Didn't seem to give any tangible performance benefit, and was imposing hidden restrictions on memory usage (all phys RAM fragments in PhysRamTable must be multiple of 512k). And if it really was a good optimisation, surely it should have been applied to all areas of the kernel, not just AMB!
  Other potential future improvements:
  * Turn the fake DANodes into real dynamic areas, reducing the amount of special code needed in some places, but allow the DAs to be hidden from OS_DynamicArea 3 so that apps/users won't get too confused
  * Add a generic abort trapping system to PMPs/DAs (lazy task swapping abort handler is still a special case)
  File changes:
  - s/ARM600, s/VMSAv6, s/ExtraSWIs - Remove DebugAborts
  - s/ArthurSWIs - Remove AMB service call handler dispatch
  - s/ChangeDyn - AMB_LazyMapIn switch fixes. Add alternate internal entry points for some PMP ops to allow the DANode to be specified (used by AMB)
  - s/Exceptions - Remove DebugAborts, AMB_LazyMapIn switch fixes
  - s/Kernel - Define GetAppSpaceDANode macro, AMB_LazyMapIn switch fix
  - s/MemInfo - AMB_LazyMapIn switch fixes
  - s/AMBControl/AMB - Update GETs
  - s/AMBControl/Memory - Remove block size quantisation, AMB_BlockResize (page list blocks are now allocated by PMP code)
  - s/AMBControl/Options - Remove PhysBin definitions, AMBMIRegWords (moved to Workspace file), AMB_LimpidFreePool switch. Add AMB_Debug switch.
  - s/AMBControl/Workspace - Update AMBNode to contain an embedded DANode. Move AMBMIRegWords here from Options file.
  - s/AMBControl/allocate - Fake DA node initialisation
  - s/AMBControl/deallocate - Add debug output
  - s/AMBControl/growp, growshrink, mapslot, mapsome, shrinkp - Rewrite to use PMP ops where possible, add debug output
  - s/AMBControl/main - Remove PhysBin initialisation. Update the enumerate/mjs_info call.
  - s/AMBControl/memmap - Low-level memory mapping routines updated or rewritten as appropriate.
  - s/AMBControl/readinfo - Update to cope with DANode
  - s/AMBControl/service - Remove old service call handlers
  - s/AMBControl/handler - DA handler for responding to PMP calls from OS_ChangeDynamicArea; just calls through to growpages/shrinkpages as appropriate.
Admin:
  Tested on pretty much everything currently supported


Version 5.66. Tagged as 'Kernel-5_66'
@
text
@; Copyright 1996 Acorn Computers Ltd
;
; Licensed under the Apache License, Version 2.0 (the "License");
; you may not use this file except in compliance with the License.
; You may obtain a copy of the License at
;
;     http://www.apache.org/licenses/LICENSE-2.0
;
; Unless required by applicable law or agreed to in writing, software
; distributed under the License is distributed on an "AS IS" BASIS,
; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
; See the License for the specific language governing permissions and
; limitations under the License.
;
; > s.growp


;grow slot by taking pages from FreePool; add them to AppSpace


; entry:
;     R1 =  new number of pages for slot (more than current value)
;     R2 -> AMB node
;
; exit: -

growpages ROUT
        Entry   "R0-R7,R10-R11"

        MOV     R6,R1                      ;save entry R1
        LDR     R5,[R2,#AMBNode_DANode+DANode_PMPSize]
        CMP     R5,R6
        EXIT    EQ                         ;done if no. of pages unchanged

      [ AMB_Debug
        DebugReg r1, "growpages to "
        DebugReg r5, "from "
      ]

        ; Grow PMP to right size
        ADD     R10,R2,#AMBNode_DANode
        Push    "R1-R2,R10,R12"
        LDR     R1,[R2,#AMBNode_DANode+DANode_PMPMaxSize]
        SUB     R2,R6,R1
        BL      DynArea_PMP_Resize_WithNode
        Pull    "R1-R2,R10,R12"
        ; Check new size is correct
        LDR     R1,[R2,#AMBNode_DANode+DANode_PMPMaxSize]
      [ AMB_Debug
        DebugReg r1, "Resize result "
      ]
        CMP     R1,R6
        BNE     %FT90                      ;give up if PMP resize failed

        ; Now add pages to the PMP for the new region
        Push    "R5"
        SUB     SP,SP,#64*12               ;Temp page list
        MOV     R7,#-2                     ;-2 = kernel picks physical page
05
        MOV     R3,#0
        LDR     LR,[R10,#DANode_Flags]
        MOV     R2,SP
10
        STMIA   R2!,{R5,R7,LR}             ;Fill page list with increasing PMP page indices, to ensure no gaps in PMP if we hit an error
        ADD     R5,R5,#1
        ADD     R3,R3,#1
        CMP     R5,R6
        CMPNE   R3,#64
        BNE     %BT10
        MOV     R2,SP
        Push    "R10,R12"
        BL      DynArea_PMP_PhysOp_WithNode
        Pull    "R10,R12"
        CMP     R3,#0
        BNE     %FT20 ; Stop if failed to allocate something
        CMP     R5,R6
        BNE     %BT05
20
        ADD     SP,SP,#64*12
        Pull    "R5"

        ; Map in the new pages (if any)
        Push    "R1,R5"
        MOV     R1,R5
        LDR     R5,[R10,#DANode_PMPSize]
        SUBS    R5,R5,R1
        MOV     R7,#0
        BLNE    AMB_SetMemMapEntries_MapIn_Lazy
        Pull    "R1,R5"

        ; Update AplWorkSize, MemLimit
        LDR     R5,[R10,#DANode_PMPSize]
      [ AMB_Debug
        DebugReg r5, "PhysOp result "
      ]
        MOV     R2,#ApplicationStart
        ADD     R5,R2,R5,LSL #Log2PageSize
        LDR     R2,=ZeroPage
        STR     R5,[R2,#AplWorkSize]
        STR     R5,[R2,#MemLimit]

90
        CLRV
        EXIT

    END
@


4.4
log
@Add support for shareable pages and additional access privileges
Detail:
  This set of changes:
  * Refactors page table entry encoding/decoding so that it's (mostly) performed via functions in the MMU files (s.ARM600, s.VMSAv6) rather than on an ad-hoc basis as was the case previously
  * Page table entry encoding/decoding performed during ROM init is also handled via the MMU functions, which resolves some cases where the wrong cache policy was in use on ARMv6+
  * Adds basic support for shareable pages - on non-uniprocessor systems all pages will be marked as shareable (however, we are currently lacking ARMops which broadcast cache maintenance operations to other cores, so safe sharing of cacheable regions isn't possible yet)
  * Adds support for the VMSA XN flag and the "privileged ROM" access permission. These are exposed via RISC OS access privileges 4 and above, taking advantage of the fact that 4 bits have always been reserved for AP values but only 4 values were defined
  * Adds OS_Memory 17 and 18 to convert RWX-style access flags to and from RISC OS access privelege numbers; this allows us to make arbitrary changes to the mappings of AP values 4+ between different OS/hardware versions, and allows software to more easily cope with cases where the most precise AP isn't available (e.g. no XN on <=ARMv5)
  * Extends OS_Memory 24 (CheckMemoryAccess) to return executability information
  * Adds exported OSMem header containing definitions for OS_Memory and OS_DynamicArea
  File changes:
  - Makefile - export C and assembler versions of hdr/OSMem
  - Resources/UK/Messages - Add more text for OS_Memory errors
  - hdr/KernelWS - Correct comment regarding DCacheCleanAddress. Allocate workspace for MMU_PPLTrans and MMU_PPLAccess.
  - hdr/OSMem - New file containing exported OS_Memory and OS_DynamicArea constants, and public page flags
  - hdr/Options - Reduce scope of ARM6support to only cover builds which require ARMv3 support
  - s/AMBControl/Workspace - Clarify AMBNode_PPL usage
  - s/AMBControl/growp, mapslot, mapsome, memmap - Use AreaFlags_ instead of AP_
  - s/AMBControl/main, memmap - Use GetPTE instead of generating page table entry manually
  - s/ARM600 - Remove old coments relating to lack of stack. Update BangCam to use GetPTE. Update PPL tables, removing PPLTransL1 (L1 entries are now derived from L2 table on demand) and adding a separate table for ARM6. Implement the ARM600 versions of the Get*PTE ('get page table entry') and Decode*Entry functions
  - s/ARMops - Add Init_PCBTrans function to allow relevant MMU_PPLTrans/MMU_PCBTrans pointers to be set up during the pre-MMU stage of ROM init. Update ARM_Analyse to set up the pointers that are used post MMU init.
  - s/ChangeDyn - Move a bunch of flags to hdr/OSMem. Rename the AP_ dynamic area flags to AreaFlags_ to avoid name clashes and confusion with the page table AP_ values exported by Hdr:MEMM.ARM600/Hdr:MEMM.VMSAv6. Also generate the relevant flags for OS_Memory 24 so that it can refer to the fixed areas by their name instead of hardcoding the permissions.
  - s/GetAll - GET Hdr:OSMem
  - s/HAL - Change initial page table setup to use DA/page flags and GetPTE instead of building page table entries manually. Simplify AllocateL2PT by removing the requirement for the user to supply the access perimssions that will be used for the area; instead for ARM6 we just assume that cacheable memory is the norm and set L1_U for any L1 entry we create here.
  - s/Kernel - Add GetPTE macro (for easier integration of Get*PTE functions) and GenPPLAccess macro (for easy generation of OS_Memory 24 flags)
  - s/MemInfo - Fixup OS_Memory 0 to not fail on seeing non-executable pages. Implement OS_Memory 17 & 18. Tidy up some error generation. Make OS_Memory 13 use GetPTE. Extend OS_Memory 24 to return (non-) executability information, to use the named CMA_ constants generated by s/ChangeDyn, and to use the Decode*Entry functions when it's necessary to decode page table entries.
  - s/NewReset - Use AreaFlags_ instead of AP_
  - s/VMSAv6 - Remove old comments relating to lack of stack. Update BangCam to use GetPTE. Update PPL tables, removing PPLTransL1 (L1 entries are now derived from L2 table on demand) and adding a separate table for shareable pages. Implement the VMSAv6 versions of the Get*PTE and Decode*Entry functions.
Admin:
  Tested on Raspberry Pi 1, Raspberry Pi 3, Iyonix, RPCEmu (ARM6 & ARM7), comparing before and after CAM and page table dumps to check for any unexpected differences


Version 5.55. Tagged as 'Kernel-5_55'
@
text
@d23 1
a23 1
;     R2 -> AMB node (page table already allocated/grown to size)
d28 1
a28 1
        Push    "R0-R7,LR"
d31 1
a31 1
        LDR     R5,[R2,#AMBNode_Npages]
d33 1
a33 1
        Pull    "R0-R7,PC",EQ              ;done if no. of pages unchanged
d35 28
a62 31
        MOV     R0,R2                      ;R0 -> AMB node

        LDR     R1,=ZeroPage+FreePoolDANode ;R1 := source for pages
        LDR     R2,=ZeroPage+AppSpaceDANode ;R2 := dest for pages

        LDR     R7,[R0,#AMBNode_Npages]    ;R7 := current no. of pages
        SUB     R3,R6,R7                   ;no. of pages required from FreePool
        LDR     R4,[R1,#DANode_PMPSize]    ;no. of pages in FreePool
        CMP     R3,R4
  [ ShrinkableDAs
        BLHI    growp_TryToShrinkShrinkables
  ]
        MOVHI   R3,R4                      ;R3 := no. of pages we will move
        CMP     R3,#0
        BEQ     %FT90                      ;done if can't move any pages
        ADD     R4,R3,R7
        STR     R4,[R0,#AMBNode_Npages]    ;new no. of pages

        ADD     R4,R0,#AMBNode_pages
        ADD     R4,R4,R7,LSL #2            ;R4 -> 1st page table entry for grow
        LDR     R5,[R1,#DANode_PMP]
        LDR     LR,[R1,#DANode_PMPSize]
        ADD     R5,R5,LR,LSL #2            ;current end of FreePool

        ;R3 = no. of pages, R4 -> buffer for page entries,
        ;R5 -> end of free pool PMP
        ; Reverse the order as we copy in order to match OS_ChangeDynamicArea
        ; (helps ensure pages are physically contiguous - to help with any
        ; potential DMA)
        MOV     R2,R3
        MOV     R6,#-1
d64 5
a68 4
        LDR     LR,[R5,#-4]!
        SUBS    R2,R2,#1
        STR     LR,[R4],#4
        STR     R6,[R5]
d70 31
a100 31
        SUB     R4,R4,R3,LSL #2
        LDR     R2,=ZeroPage+AppSpaceDANode ;R2 := dest for pages

  [ AMB_LazyMapIn
        LDR     R5,AMBFlags
        TST     R5,#AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
        ;
        MOVEQ   R5,#-1                     ;map the pages to Nowhere initially
        MOVEQ   R6,#AreaFlags_Duff
        LDRNE   R5,[R0,#AMBNode_startaddr]
        ADDNE   R5,R5,R7,LSL #Log2PageSize ;R5 := first new page new address
        LDRNE   R6,[R0,#AMBNode_PPL]       ;R6 := dest PPL flags
  |
        LDR     R5,[R0,#AMBNode_startaddr]
        ADD     R5,R5,R7,LSL #Log2PageSize ;R5 := first new page new address
        LDR     R6,[R0,#AMBNode_PPL]       ;R6 := dest PPL flags
  ]
        ;
        ;entry: R3 = no. of pages, R4 -> list of page entries,
        ;       R5 = start logical address, R6 = PPL
        BL      AMB_SetMemMapEntries       ;remap

        LDR     R5,[R1,#DANode_PMPSize]
        SUB     R5,R5,R3
        STR     R5,[R1,#DANode_PMPSize]    ;new FreePool size

        LDR     R5,[R2,#DANode_Size]
        ADD     R5,R5,R3,LSL #Log2PageSize
        STR     R5,[R2,#DANode_Size]       ;new AppSpace size
        LDR     R6,=ZeroPage
        STR     R5,[R6,#MemLimit]          ;update MemLimit
a102 1
;;;        STRVS   R0,[SP]
a103 25
        Pull    "R0-R7,PC"

  [ ShrinkableDAs
;
; growp_TryToShrinkShrinkables - try to shrink shrinkable DAs, to grow free pool
;
; entry:
;   R1 -> FreePool DANode
;   R3 = no. of pages required in FreePool
;   R4 = no. of pages in FreePool (must be less than R3)
;
; exit:
;   R4 = new no. of pages in FreePool
;   condition code GT is true if still less than required (ie. R3 > R4 on exit)
;
growp_TryToShrinkShrinkables ROUT
        Entry   "R1-R2,R11,R12"
        MOV     R11,R1
        MOV     R12,#0                          ; dest node irrelevant
        MOV     R1,R3
        MOV     R2,R4
        CLC
        BL      TryToShrinkShrinkables
        MOV     R4,R2
        CMP     R3,R2
a105 2
  ] ;ShrinkableDAs

@


4.3
log
@Merge HAL branch to trunk
Detail:
  This change merges the past 15+ years of HAL branch development back to the trunk.
  This is effectively the end for non-HAL builds of the kernel, as no attempt has been made to maintain it during this merge, and all non-HAL & non-32bit code will soon be removed anyway.
  Rather than list everything that's been added to the HAL branch, it's easier to describe the change in terms of the things that the HAL branch was lacking:
  * Trunk version of Docs/32bit contained updated comments for the SVC stack structure during ErrorV
  * Trunk version of s/HeapMan contained a tweak to try and reduce the number of small free blocks that are created
  * Trunk version of s/Kernel contained a change to only copy 248 bytes of the error string to the error buffer (down from 252 bytes), to take into account the extra 4 bytes needed by the PSR. However this goes against the decision that's been made in the HAL branch that the error buffer should be enlarged to 260 bytes instead (ref: https://www.riscosopen.org/tracker/tickets/201), so the HAL build will retain its current behaviour.
  * Trunk version of s/MsgCode had RMNot32bit error in the list of error messages to count when countmsgusage {TRUE}
  * Trunk version of s/PMF/i2cutils contained support for OS_Memory 5, "read/write value of NVRamWriteSize". Currently the HAL branch doesn't have a use for this (in particular, the correct NVRamWriteSize should be specified by the HAL, so there should be no need for software to change it at runtime), and so this code will remain switched out in the HAL build.
Admin:
  Tested on Raspberry Pi


Version 5.48. Tagged as 'Kernel-5_48'
@
text
@d80 1
a80 1
        MOVEQ   R6,#AP_Duff
@


4.2
log
@Taught AMBControl about shrinkable dynamic areas
@
text
@d37 2
a38 2
        LDR     R1,=FreePoolDANode         ;R1 := source for pages
        LDR     R2,=AppSpaceDANode         ;R2 := dest for pages
d42 1
a42 2
        LDR     R4,[R1,#DANode_Size]
        MOV     R4,R4,LSR #Log2PageSize    ;no. of pages in FreePool
d45 1
a45 1
        BLGT    growp_TryToShrinkShrinkables
d47 1
a47 1
        MOVGT   R3,R4                      ;R3 := no. of pages we will move
d49 1
a49 1
        BEQ     %FT02                      ;done if can't move any pages
d55 30
a84 9
        LDR     R5,[R1,#DANode_Base]
        LDR     LR,[R1,#DANode_Size]
        ADD     R5,R5,LR                   ;current end of FreePool
        SUB     R5,R5,R3,LSL #Log2PageSize ;R5 := first required page address

        ;entry: R3 = no. of pages, R4 -> buffer for page entries,
        ;       R5 := start logical address
        BL      AMB_FindMemMapEntries      ;find page nos.

d88 2
a89 1

d94 3
a96 3
        LDR     R5,[R1,#DANode_Size]
        SUB     R5,R5,R3,LSL #Log2PageSize
        STR     R5,[R1,#DANode_Size]       ;new FreePool size
d101 1
a101 1
        MOV     R6,#0
d104 3
a106 2
02
        STRVS   R0,[SP]
d123 10
a132 30
        Push    "R0-R2,R11,R12,LR"

        MOV     R11,R1                          ; -> FreePool DANode
        MOV     R1,R3,LSL #Log2PageSize         ;amount we need in FreePool
        MOV     R2,R4,LSL #Log2PageSize         ;amount we have in FreePool
        MOV     R10,#DAList
        ASSERT  DANode_Link = 0                 ;because DAList has only link
10
        LDR     R10,[R10,#DANode_Link]          ;and load next
        CMP     R10,#1                          ;any more nodes?
        BCC     %FT99 
        LDR     LR,[R10,#DANode_Flags]          ;check area is shrinkable
        TST     LR,#DynAreaFlags_Shrinkable
        BEQ     %BT10                           ;if not, try next area

        SUBS    R1,R1,R2                        ;amount we still need
        LDR     LR,[R10,#DANode_Size]           ;available size of this area
        CMP     LR,R1
        MOVCC   R1,LR                           ;min(amount we need, size of this area)
        RSB     R1,R1,#0                        ;make negative - it's a shrink
        LDR     R0,[R10,#DANode_Number]
        SWI     XOS_ChangeDynamicArea           ;should not be currently threaded during AMBControl
        MOV     R1,R3,LSL #Log2PageSize         ;original amount we need again
        LDR     R2,[R11,#DANode_Size]           ;get new size of FreePool
        CMP     R2,R1
        BCC     %BT10                           ;if still too small, loop
99
        MOV     R4,R2,LSR #Log2PageSize         ;no. of pages now in FreePool
        CMP     R3,R4
        Pull    "R0-R2,R11,R12,PC"
@


4.2.6.1
log
@reintroduce Ursula AMBControl, recoded with generic ARMop style, not debugged yet

Version 5.35, 4.79.2.12. Tagged as 'Kernel-5_35-4_79_2_12'
@
text
@a64 10
  [ AMB_LazyMapIn
        LDR     R5,AMBFlags
        TST     R5,#AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
        ;
        MOVEQ   R5,#-1                     ;map the pages to Nowhere initially
        MOVEQ   R6,#AP_Duff
        LDRNE   R5,[R0,#AMBNode_startaddr]
        ADDNE   R5,R5,R7,LSL #Log2PageSize ;R5 := first new page new address
        LDRNE   R6,[R0,#AMBNode_PPL]       ;R6 := dest PPL flags
  |
d68 1
a68 2
  ]
        ;
d84 1
a84 2
;;;        STRVS   R0,[SP]
        CLRV
@


4.2.6.2
log
@Merge Cortex kernel into HAL branch
Detail:
  This is a full merge of the Cortex kernel back into the HAL branch. Since the Cortex kernel is/was just a superset of the HAL branch, at this point in time both branches are identical.
  Main features the HAL branch gains from this merge:
  - ARMv6/ARMv7 support
  - High processor vectors/zero page relocation support
  - objasm 4 warning fixes
  - Improved HAL related functionality:
    - Support for HAL-driven RTCs instead of kernel-driven IIC based ones
    - Support for arbitrary size machine IDs
    - Support for multiple IIC busses
    - Support for any HAL size, instead of hardcoded 64k size
    - Probably some other stuff I've forgotten
  - Probably a few bug fixes here and there
Admin:
  Tested on BB-xM & Iyonix.
  Was successfully flashed to ROM on an Iyonix to test the Cortex branch implementation of the 2010 RTC bug fix.
  IOMD build untested - but has been known to work in the past.


Version 5.35, 4.79.2.123. Tagged as 'Kernel-5_35-4_79_2_123'
@
text
@d37 2
a38 2
        LDR     R1,=ZeroPage+FreePoolDANode ;R1 := source for pages
        LDR     R2,=ZeroPage+AppSpaceDANode ;R2 := dest for pages
d91 1
a91 1
        LDR     R6,=ZeroPage
d118 1
a118 1
        LDR     R10,=ZeroPage+DAList
@


4.2.6.3
log
@Add initial support for "physical memory pools"
Detail:
  This set of changes adds support for "physical memory pools" (aka PMPs), a new type of dynamic area which allow physical pages to be claimed/allocated without mapping them in to the logical address space. PMPs have full control over which physical pages they use (similar to DAs which request specific physical pages), and also have full control over the logical mapping of their pages (which pages go where, and per-page access/cacheability control).
  Currently the OS makes use of two PMPs: one for the free pool (which now has a logical size of zero - freeing up gigabytes of logical space), and one for the RAM disc (logical size of 1MB, allowing for a physical size limited only by the amount of free memory)
  Implementing these changes has required a number of other changes to be made:
  * The CAM has been expanded from 8 bytes per entry to 16 bytes per entry, in order to allow each RAM page to store information about its PMP association
  * The system heap has been expanded to 32MB in size (from just under 4MB), in order to allow it to be used to store PMP page lists (1 word needed per page, but PMP pages may not always have physical pages assigned to them - so to allow multiple large PMPs to exist we need more than just 1 word per RAM page)
  * The &FA000000-&FBFFFFFF area of fixed kernel workspace has been shuffled around to accomodate the larger CAM, and the system heap is now located just above the RMA.
  * SoftResets code stripped out (unlikely we'll ever want to fix and re-enable it)
  * A couple of FastCDA options are now permanently on
  * Internal page flags shuffled around a bit. PageFlags_Unavailable now publicly exposed so that PMP clients can lock/unlock pages at will.
  * When OS_ChangeDynamicArea is asked to grow or shrink the free pool, it now implicitly converts it into a shrink or grow of application space (which is what would happen anyway). This simplifies the implementation; during a grow, pages (or replacement pages) are always sourced from the free pool, and during a shrink pages are always sent to the free pool.
  File changes:
  - hdr/KernelWS - Extend DANode structure. Describe CAM format. Adjust kernel workspace.
  - hdr/OSRSI6, s/Middle - Add new item to expose the CAM format
  - hdr/Options - Remove SoftResets switch. Add some PMP switches.
  - s/ARM600, s/VMSAv6 - Updated for new CAM format. Note that although the CAM stores PMP information, BangCamUpdate currently doesn't deal with updating that data - it's the caller's responsibility to do so where appropriate.
  - s/ChangeDyn - Lots of changes to implement PMP support, and to cope with the new CAM format.
  - s/HAL - Updated to cope with new CAM format, and lack of logical mapping of free pool.
  - s/MemInfo - Updated to cope with new CAM format. OS_Memory 0 updated to cope with converting PPN to PA for pages which are mapped out. OS_Memory 24 updated to decode the access permissions on a per-page basis for PMPs, and fixed its HWM usage for sparse DAs.
  - s/NewReset - Soft reset code and unused AddCamEntries function removed. Updated to cope with new CAM format, PMP free pool, PMP RAMFS
  - s/AMBControl/allocate - Update comment (RMA hasn't been used for AMBControl nodes for a long time)
  - s/AMBControl/growp, s/AMBControl/memmap, s/AMBControl/shrinkp - Update for new CAM format + PMP free pool
  - s/vdu/vdudriver - Strip out soft reset code.
Admin:
  Tested on Pandaboard
  This is just a first iteration of the PMP feature, with any luck future changes will improve functionality. This means APIs are subject to change as well.


Version 5.35, 4.79.2.284. Tagged as 'Kernel-5_35-4_79_2_284'
@
text
@d42 2
a43 1
        LDR     R4,[R1,#DANode_PMPSize]    ;no. of pages in FreePool
d46 1
a46 1
        BLHI    growp_TryToShrinkShrinkables
d48 1
a48 1
        MOVHI   R3,R4                      ;R3 := no. of pages we will move
d50 1
a50 1
        BEQ     %FT90                      ;done if can't move any pages
d56 8
a63 17
        LDR     R5,[R1,#DANode_PMP]
        LDR     LR,[R1,#DANode_PMPSize]
        ADD     R5,R5,LR,LSL #2            ;current end of FreePool
        SUB     R5,R5,R3,LSL #2            ;R5 := first required page in PMP

        ;R3 = no. of pages, R4 -> buffer for page entries,
        ;R5 := start page in PMP
        MOV     R2,R3
        MOV     R6,#-1
10
        LDR     LR,[R5]
        SUBS    R2,R2,#1
        STR     LR,[R4],#4
        STR     R6,[R5],#4
        BNE     %BT10
        SUB     R4,R4,R3,LSL #2
        LDR     R2,=ZeroPage+AppSpaceDANode ;R2 := dest for pages
d84 3
a86 3
        LDR     R5,[R1,#DANode_PMPSize]
        SUB     R5,R5,R3
        STR     R5,[R1,#DANode_PMPSize]    ;new FreePool size
d94 1
a94 1
90
d113 30
a142 10
        Entry   "R1-R2,R11,R12"
        MOV     R11,R1
        MOV     R12,#0                          ; dest node irrelevant
        MOV     R1,R3
        MOV     R2,R4
        CLC
        BL      TryToShrinkShrinkables
        MOV     R4,R2
        CMP     R3,R2
        EXIT
@


4.2.6.4
log
@Misc memory management tweaks & fixes
Detail:
  s/ChangeDyn - Fix OS_DynamicArea 20 to work properly with sparse & PMP DAs. It now checks against the max extent of the area rather than the current size; this matches the logic used for checking fixed system workspace areas. The call only determines the ownership of a logical address, and it's considered the caller's responsibility to check if there's actually a page at the given address.
  s/ChangeDyn - Revise OS_DynamicArea 25 to remove the redundant 'PMP page flags' entry, and to allow pages to be looked up by either PMP page index, phys page number, or DA page index
  s/ChangeDyn - Tidy up InitDynamicAreas by adding the NextFreePage routine to help determine the next page to be added to the free pool.
  s/AMBControl/Workspace, s/AMBControl/main, s/AMBControl/memmap - Fix lazy mapping in of pages to use the correct L2PT flags for the default CB cache policy
  s/AMBControl/allocate - Get rid of magic constant when extracting page flags from DA flags, and make note of the fact that assorted bits of code ignore the flags
  s/AMBControl/growp, s/AMBControl/shrinkp - Reverse the page order when growing/shrinking areas, to match OS_ChangeDynamicArea. This helps both DAs and application space to have pages allocated to them in contiguous physical order - which in turn helps produce shorter, more optimal scatter lists for DMA
Admin:
  Tested on Pandaboard


Version 5.35, 4.79.2.287. Tagged as 'Kernel-5_35-4_79_2_287'
@
text
@d58 1
d61 1
a61 4
        ;R5 -> end of free pool PMP
        ; Reverse the order as we copy in order to match OS_ChangeDynamicArea
        ; (helps ensure pages are physically contiguous - to help with any
        ; potential DMA)
d65 1
a65 1
        LDR     LR,[R5,#-4]!
d68 1
a68 1
        STR     R6,[R5]
@


4.2.6.1.2.1
log
@Add zero page relocation support
Detail:
  A whole mass of changes to add high processor vectors + zero page relocation support to the Cortex branch of the kernel
  At the moment the code can only cope with two ZeroPage locations, &0 and &FFFF0000. But with a bit more tweaking those restrictions can probably be lifted, allowing ZeroPage to be hidden at almost any address (assuming it's fixed at compile time). If I've done my job right, these restrictions should all be enforced by asserts.
  There's a new option, HiProcVecs, in hdr/Options to control whether high processor vectors are used. When enabling it and building a ROM, remember:
  * FPEmulator needs to be built with the FPEAnchor=High option specified in the components file (not FPEAnchorType=High as my FPEmulator commit comments suggested)
  * ShareFS needs unplugging/removing since it can't cope with it yet
  * Iyonix users will need to use the latest ROOL boot sequence, to ensure the softloaded modules are compatible (OMAP, etc. don't really softload much so they're OK with older sequences)
  * However VProtect also needs patching to fix a nasty bug there - http://www.riscosopen.org/tracker/tickets/294
  The only other notable thing I can think of is that the ProcessTransfer code in s/ARM600 & s/VMSAv6 is disabled if high processor vectors are in use (it's fairly safe to say that code is obsolete in HAL builds anyway?)
  Fun challenge for my successor: Try setting ZeroPage to &FFFF00FF (or similar) so its value can be loaded with MVN instead of LDR. Then use positive/negative address offsets to access the contents.
  File changes:
  - hdr/ARMops - Modified ARMop macro to take the ZeroPage pointer as a parameter instead of 'zero'
  - hdr/Copro15ops - Corrected $quick handling in myISB macro
  - hdr/Options - Added ideal setting for us to use for HiProcVecs
  - s/AMBControl/allocate, s/AMBControl/growp, s/AMBControl/mapslot, s/AMBControl/memmap, s/AMBControl/service, s/AMBControl/shrinkp, s/Arthur2, s/Arthur3, s/ArthurSWIs, s/ChangeDyn, s/ExtraSWIs, s/HAL, s/HeapMan, s/Kernel, s/MemInfo, s/Middle, s/ModHand, s/MoreSWIs, s/MsgCode, s/NewIRQs, s/NewReset, s/Oscli, s/PMF/buffer, s/PMF/IIC, s/PMF/i2cutils, s/PMF/key, s/PMF/mouse, s/PMF/osbyte, s/PMF/oseven, s/PMF/osinit, s/PMF/osword, s/PMF/oswrch, s/SWINaming, s/Super1, s/SysComms, s/TickEvents, s/Utility, s/vdu/vdu23, s/vdu/vdudriver, s/vdu/vdugrafl, s/vdu/vdugrafv, s/vdu/vdupalxx, s/vdu/vdupointer, s/vdu/vduswis, s/vdu/vduwrch - Lots of updates to deal with zero page relocation
  - s/ARM600 - UseProcessTransfer option. Zero page relocation support. Deleted pre-HAL ClearPhysRAM code to tidy the file up a bit.
  - s/ARMops - Zero page relocation support. Set CPUFlag_HiProcVecs when high vectors are in use.
  - s/KbdResPC - Disable compilation of dead code
  - s/VMSAv6 - UseProcessTransfer option. Zero page relocation support.
Admin:
  Tested with OMAP & Iyonix ROM softloads, both with high & low zero page.
  High zero page hasn't had extensive testing, but boot sequence + ROM apps seem to work.


Version 5.35, 4.79.2.98.2.48. Tagged as 'Kernel-5_35-4_79_2_98_2_48'
@
text
@d37 2
a38 2
        LDR     R1,=ZeroPage+FreePoolDANode ;R1 := source for pages
        LDR     R2,=ZeroPage+AppSpaceDANode ;R2 := dest for pages
d91 1
a91 1
        LDR     R6,=ZeroPage
d118 1
a118 1
        LDR     R10,=ZeroPage+DAList
@


4.2.2.1
log
@Added following enhancements:

 - Chocolate screen mapping (section mapped and cached), StrongARM only
   Phoebe h/w (IOMD 2) will have register to assist this, but code currently
   relies on data abort mechanism to keep screen up to date wrt write-back
   data cache.

 - Chocolate AMBControl task switching (lazy page mapping), StrongARM only
   Improves task swapping speed. There appears to be a StrongAEM silicon
   bug rev 2 and 3) which means that LDMIB rn, {regs includind rn} cannot
   be reliably restarted after a data abort. This stuffs Chocolate AMBControl
   (awaiting response from Digital).

Both enhancements need more work to complete for Phoebe. Chocolate AMBControl
may well have to be made dormant because of silicon bug.

Note that this kernel *will* cause problems with task switching on StrongARM,
unless Chocolate task switching is disabled via !Flavour application.
@
text
@a64 10
  [ AMB_LazyMapIn
        LDR     R5,AMBFlags
        TST     R5,#AMBFlag_LazyMapIn_disable :OR: AMBFlag_LazyMapIn_suspend
        ;
        MOVEQ   R5,#-1                     ;map the pages to Nowhere initially
        MOVEQ   R6,#AP_Duff
        LDRNE   R5,[R0,#AMBNode_startaddr]
        ADDNE   R5,R5,R7,LSL #Log2PageSize ;R5 := first new page new address
        LDRNE   R6,[R0,#AMBNode_PPL]       ;R6 := dest PPL flags
  |
d68 1
a68 2
  ]
        ;
d84 1
a84 2
;;;        STRVS   R0,[SP]
        CLRV
@


4.1
log
@Initial revision
@
text
@d27 1
a27 1
growpages
d45 3
d86 47
@


4.1.3.1
log
@Import from cleaned 370 CD
@
text
@@


4.1.3.1.2.1
log
@Taught AMBControl about shrinkable dynamic areas
@
text
@d27 1
a27 1
growpages ROUT
a44 3
  [ ShrinkableDAs
        BLGT    growp_TryToShrinkShrinkables
  ]
a82 47

  [ ShrinkableDAs
;
; growp_TryToShrinkShrinkables - try to shrink shrinkable DAs, to grow free pool
;
; entry:
;   R1 -> FreePool DANode
;   R3 = no. of pages required in FreePool
;   R4 = no. of pages in FreePool (must be less than R3)
;
; exit:
;   R4 = new no. of pages in FreePool
;   condition code GT is true if still less than required (ie. R3 > R4 on exit)
;
growp_TryToShrinkShrinkables ROUT
        Push    "R0-R2,R11,R12,LR"

        MOV     R11,R1                          ; -> FreePool DANode
        MOV     R1,R3,LSL #Log2PageSize         ;amount we need in FreePool
        MOV     R2,R4,LSL #Log2PageSize         ;amount we have in FreePool
        MOV     R10,#DAList
        ASSERT  DANode_Link = 0                 ;because DAList has only link
10
        LDR     R10,[R10,#DANode_Link]          ;and load next
        CMP     R10,#1                          ;any more nodes?
        BCC     %FT99 
        LDR     LR,[R10,#DANode_Flags]          ;check area is shrinkable
        TST     LR,#DynAreaFlags_Shrinkable
        BEQ     %BT10                           ;if not, try next area

        SUBS    R1,R1,R2                        ;amount we still need
        LDR     LR,[R10,#DANode_Size]           ;available size of this area
        CMP     LR,R1
        MOVCC   R1,LR                           ;min(amount we need, size of this area)
        RSB     R1,R1,#0                        ;make negative - it's a shrink
        LDR     R0,[R10,#DANode_Number]
        SWI     XOS_ChangeDynamicArea           ;should not be currently threaded during AMBControl
        MOV     R1,R3,LSL #Log2PageSize         ;original amount we need again
        LDR     R2,[R11,#DANode_Size]           ;get new size of FreePool
        CMP     R2,R1
        BCC     %BT10                           ;if still too small, loop
99
        MOV     R4,R2,LSR #Log2PageSize         ;no. of pages now in FreePool
        CMP     R3,R4
        Pull    "R0-R2,R11,R12,PC"

  ] ;ShrinkableDAs
@
