head	4.3;
access;
symbols
	Kernel-6_15:4.3
	Kernel-6_14:4.3
	Kernel-6_01-3:4.3
	Kernel-6_13:4.3
	Kernel-6_12:4.3
	Kernel-6_11:4.3
	Kernel-6_10:4.3
	Kernel-6_09:4.3
	Kernel-6_08-4_129_2_10:4.3
	Kernel-6_08-4_129_2_9:4.3
	Kernel-6_08:4.3
	Kernel-6_07:4.3
	Kernel-6_06:4.3
	Kernel-6_05-4_129_2_8:4.3
	Kernel-6_05:4.3
	Kernel-6_04:4.3
	Kernel-6_03:4.3
	Kernel-6_01-2:4.3
	Kernel-6_01-4_146_2_1:4.3
	Kernel-6_02:4.3
	Kernel-6_01-1:4.3
	Kernel-6_01:4.3
	Kernel-6_00:4.3
	Kernel-5_99:4.3
	Kernel-5_98:4.3
	Kernel-5_97-4_129_2_7:4.3
	Kernel-5_97:4.3
	Kernel-5_96:4.3
	Kernel-5_95:4.3
	Kernel-5_94:4.3
	Kernel-5_93:4.3
	Kernel-5_92:4.3
	Kernel-5_91:4.3
	Kernel-5_90:4.3
	Kernel-5_89-4_129_2_6:4.3
	Kernel-5_89:4.3
	Kernel-5_88-4_129_2_5:4.3
	Kernel-5_88-4_129_2_4:4.3
	Kernel-5_88:4.3
	Kernel-5_87:4.3
	Kernel-5_86-4_129_2_3:4.3
	Kernel-5_86-4_129_2_2:4.3
	Kernel-5_86-4_129_2_1:4.3
	Kernel-5_86:4.3
	SMP:4.3.0.2
	SMP_bp:4.3
	Kernel-5_85:4.3
	Kernel-5_54-1:4.2
	Kernel-5_84:4.3
	Kernel-5_83:4.3
	Kernel-5_82:4.3
	Kernel-5_81:4.3
	Kernel-5_80:4.3
	Kernel-5_79:4.3
	Kernel-5_78:4.3
	Kernel-5_77:4.3
	Kernel-5_76:4.3
	Kernel-5_75:4.3
	Kernel-5_74:4.3
	Kernel-5_73:4.3
	Kernel-5_72:4.3
	Kernel-5_71:4.3
	Kernel-5_70:4.3
	Kernel-5_69:4.3
	Kernel-5_68:4.3
	Kernel-5_67:4.3
	Kernel-5_66:4.3
	Kernel-5_65:4.2
	Kernel-5_64:4.2
	Kernel-5_63:4.2
	Kernel-5_62:4.2
	Kernel-5_61:4.2
	Kernel-5_60:4.2
	Kernel-5_59:4.2
	Kernel-5_58:4.2
	Kernel-5_57:4.2
	Kernel-5_56:4.2
	Kernel-5_55:4.2
	Kernel-5_54:4.2
	Kernel-5_53:4.2
	Kernel-5_52:4.2
	Kernel-5_51:4.2
	Kernel-5_50:4.2
	Kernel-5_49:4.2
	HAL_merge:4.1.3.1.8.5
	Kernel-5_48:4.2
	Kernel-5_35-4_79_2_327:4.1.3.1.8.5
	Kernel-5_35-4_79_2_326:4.1.3.1.8.5
	Kernel-5_35-4_79_2_325:4.1.3.1.8.5
	Kernel-5_35-4_79_2_324:4.1.3.1.8.5
	Kernel-5_35-4_79_2_323:4.1.3.1.8.4
	Kernel-5_35-4_79_2_322:4.1.3.1.8.4
	Kernel-5_35-4_79_2_321:4.1.3.1.8.4
	Kernel-5_35-4_79_2_320:4.1.3.1.8.4
	Kernel-5_35-4_79_2_319:4.1.3.1.8.4
	Kernel-5_35-4_79_2_318:4.1.3.1.8.4
	Kernel-5_35-4_79_2_317:4.1.3.1.8.4
	Kernel-5_35-4_79_2_316:4.1.3.1.8.4
	Kernel-5_35-4_79_2_315:4.1.3.1.8.4
	Kernel-5_35-4_79_2_314:4.1.3.1.8.4
	Kernel-5_35-4_79_2_313:4.1.3.1.8.4
	Kernel-5_35-4_79_2_312:4.1.3.1.8.4
	Kernel-5_35-4_79_2_311:4.1.3.1.8.4
	Kernel-5_35-4_79_2_310:4.1.3.1.8.4
	Kernel-5_35-4_79_2_309:4.1.3.1.8.4
	Kernel-5_35-4_79_2_308:4.1.3.1.8.4
	Kernel-5_35-4_79_2_307:4.1.3.1.8.4
	Kernel-5_35-4_79_2_306:4.1.3.1.8.4
	Kernel-5_35-4_79_2_305:4.1.3.1.8.4
	Kernel-5_35-4_79_2_304:4.1.3.1.8.4
	Kernel-5_35-4_79_2_303:4.1.3.1.8.4
	Kernel-5_35-4_79_2_302:4.1.3.1.8.4
	Kernel-5_35-4_79_2_301:4.1.3.1.8.4
	Kernel-5_35-4_79_2_300:4.1.3.1.8.4
	Kernel-5_35-4_79_2_299:4.1.3.1.8.4
	Kernel-5_35-4_79_2_298:4.1.3.1.8.4
	Kernel-5_35-4_79_2_297:4.1.3.1.8.4
	Kernel-5_35-4_79_2_296:4.1.3.1.8.4
	Kernel-5_35-4_79_2_295:4.1.3.1.8.4
	Kernel-5_35-4_79_2_294:4.1.3.1.8.4
	Kernel-5_35-4_79_2_293:4.1.3.1.8.4
	Kernel-5_35-4_79_2_292:4.1.3.1.8.4
	Kernel-5_35-4_79_2_291:4.1.3.1.8.4
	Kernel-5_35-4_79_2_290:4.1.3.1.8.4
	Kernel-5_35-4_79_2_289:4.1.3.1.8.4
	Kernel-5_35-4_79_2_288:4.1.3.1.8.4
	Kernel-5_35-4_79_2_287:4.1.3.1.8.4
	Kernel-5_35-4_79_2_286:4.1.3.1.8.3
	Kernel-5_35-4_79_2_285:4.1.3.1.8.3
	Kernel-5_35-4_79_2_284:4.1.3.1.8.3
	Kernel-5_35-4_79_2_283:4.1.3.1.8.2
	Kernel-5_35-4_79_2_282:4.1.3.1.8.2
	Kernel-5_35-4_79_2_281:4.1.3.1.8.2
	Kernel-5_35-4_79_2_280:4.1.3.1.8.2
	Kernel-5_35-4_79_2_279:4.1.3.1.8.2
	Kernel-5_35-4_79_2_278:4.1.3.1.8.2
	Kernel-5_35-4_79_2_277:4.1.3.1.8.2
	Kernel-5_35-4_79_2_276:4.1.3.1.8.2
	Kernel-5_35-4_79_2_275:4.1.3.1.8.2
	Kernel-5_35-4_79_2_274:4.1.3.1.8.2
	Kernel-5_35-4_79_2_273:4.1.3.1.8.2
	Kernel-5_35-4_79_2_272:4.1.3.1.8.2
	Kernel-5_35-4_79_2_271:4.1.3.1.8.2
	Kernel-5_35-4_79_2_270:4.1.3.1.8.2
	Kernel-5_35-4_79_2_269:4.1.3.1.8.2
	Kernel-5_35-4_79_2_268:4.1.3.1.8.2
	Kernel-5_35-4_79_2_267:4.1.3.1.8.2
	Kernel-5_35-4_79_2_266:4.1.3.1.8.2
	Kernel-5_35-4_79_2_265:4.1.3.1.8.2
	Kernel-5_35-4_79_2_264:4.1.3.1.8.2
	Kernel-5_35-4_79_2_263:4.1.3.1.8.2
	Kernel-5_35-4_79_2_262:4.1.3.1.8.2
	Kernel-5_35-4_79_2_261:4.1.3.1.8.2
	Kernel-5_35-4_79_2_260:4.1.3.1.8.2
	Kernel-5_35-4_79_2_259:4.1.3.1.8.2
	Kernel-5_35-4_79_2_258:4.1.3.1.8.2
	Kernel-5_35-4_79_2_257:4.1.3.1.8.2
	Kernel-5_35-4_79_2_256:4.1.3.1.8.2
	Kernel-5_35-4_79_2_255:4.1.3.1.8.2
	Kernel-5_35-4_79_2_254:4.1.3.1.8.2
	Kernel-5_35-4_79_2_253:4.1.3.1.8.2
	Kernel-5_35-4_79_2_252:4.1.3.1.8.2
	Kernel-5_35-4_79_2_251:4.1.3.1.8.2
	Kernel-5_35-4_79_2_250:4.1.3.1.8.2
	Kernel-5_35-4_79_2_249:4.1.3.1.8.2
	Kernel-5_35-4_79_2_248:4.1.3.1.8.2
	Kernel-5_35-4_79_2_247:4.1.3.1.8.2
	Kernel-5_35-4_79_2_246:4.1.3.1.8.2
	Kernel-5_35-4_79_2_245:4.1.3.1.8.2
	Kernel-5_35-4_79_2_244:4.1.3.1.8.2
	Kernel-5_35-4_79_2_243:4.1.3.1.8.2
	Kernel-5_35-4_79_2_242:4.1.3.1.8.2
	Kernel-5_35-4_79_2_241:4.1.3.1.8.2
	Kernel-5_35-4_79_2_240:4.1.3.1.8.2
	Kernel-5_35-4_79_2_239:4.1.3.1.8.2
	Kernel-5_35-4_79_2_238:4.1.3.1.8.2
	Kernel-5_35-4_79_2_237:4.1.3.1.8.2
	Kernel-5_35-4_79_2_236:4.1.3.1.8.2
	Kernel-5_35-4_79_2_235:4.1.3.1.8.2
	Kernel-5_35-4_79_2_234:4.1.3.1.8.2
	Kernel-5_35-4_79_2_233:4.1.3.1.8.2
	Kernel-5_35-4_79_2_232:4.1.3.1.8.2
	Kernel-5_35-4_79_2_231:4.1.3.1.8.2
	Kernel-5_35-4_79_2_230:4.1.3.1.8.2
	Kernel-5_35-4_79_2_229:4.1.3.1.8.2
	Kernel-5_35-4_79_2_228:4.1.3.1.8.2
	Kernel-5_35-4_79_2_227:4.1.3.1.8.2
	Kernel-5_35-4_79_2_226:4.1.3.1.8.2
	Kernel-5_35-4_79_2_225:4.1.3.1.8.2
	Kernel-5_35-4_79_2_224:4.1.3.1.8.2
	Kernel-5_35-4_79_2_223:4.1.3.1.8.2
	Kernel-5_35-4_79_2_222:4.1.3.1.8.2
	Kernel-5_35-4_79_2_221:4.1.3.1.8.2
	Kernel-5_35-4_79_2_220:4.1.3.1.8.2
	Kernel-5_35-4_79_2_219:4.1.3.1.8.2
	Kernel-5_35-4_79_2_218:4.1.3.1.8.2
	Kernel-5_35-4_79_2_217:4.1.3.1.8.2
	Kernel-5_35-4_79_2_216:4.1.3.1.8.2
	Kernel-5_35-4_79_2_215:4.1.3.1.8.2
	Kernel-5_35-4_79_2_214:4.1.3.1.8.2
	Kernel-5_35-4_79_2_213:4.1.3.1.8.2
	Kernel-5_35-4_79_2_212:4.1.3.1.8.2
	Kernel-5_35-4_79_2_211:4.1.3.1.8.2
	Kernel-5_35-4_79_2_210:4.1.3.1.8.2
	Kernel-5_35-4_79_2_209:4.1.3.1.8.2
	Kernel-5_35-4_79_2_208:4.1.3.1.8.2
	Kernel-5_35-4_79_2_207:4.1.3.1.8.2
	Kernel-5_35-4_79_2_206:4.1.3.1.8.2
	Kernel-5_35-4_79_2_205:4.1.3.1.8.2
	Kernel-5_35-4_79_2_204:4.1.3.1.8.2
	Kernel-5_35-4_79_2_203:4.1.3.1.8.2
	Kernel-5_35-4_79_2_202:4.1.3.1.8.2
	Kernel-5_35-4_79_2_201:4.1.3.1.8.2
	Kernel-5_35-4_79_2_200:4.1.3.1.8.2
	Kernel-5_35-4_79_2_199:4.1.3.1.8.2
	Kernel-5_35-4_79_2_198:4.1.3.1.8.2
	Kernel-5_35-4_79_2_197:4.1.3.1.8.2
	Kernel-5_35-4_79_2_196:4.1.3.1.8.2
	Kernel-5_35-4_79_2_195:4.1.3.1.8.2
	Kernel-5_35-4_79_2_194:4.1.3.1.8.2
	Kernel-5_35-4_79_2_193:4.1.3.1.8.2
	Kernel-5_35-4_79_2_192:4.1.3.1.8.2
	Kernel-5_35-4_79_2_191:4.1.3.1.8.2
	Kernel-5_35-4_79_2_190:4.1.3.1.8.2
	Kernel-5_35-4_79_2_189:4.1.3.1.8.2
	Kernel-5_35-4_79_2_188:4.1.3.1.8.2
	Kernel-5_35-4_79_2_187:4.1.3.1.8.2
	Kernel-5_35-4_79_2_186:4.1.3.1.8.2
	Kernel-5_35-4_79_2_185:4.1.3.1.8.2
	Kernel-5_35-4_79_2_184:4.1.3.1.8.2
	Kernel-5_35-4_79_2_183:4.1.3.1.8.2
	Kernel-5_35-4_79_2_182:4.1.3.1.8.2
	Kernel-5_35-4_79_2_181:4.1.3.1.8.2
	Kernel-5_35-4_79_2_180:4.1.3.1.8.2
	Kernel-5_35-4_79_2_179:4.1.3.1.8.2
	Kernel-5_35-4_79_2_178:4.1.3.1.8.2
	Kernel-5_35-4_79_2_177:4.1.3.1.8.2
	Kernel-5_35-4_79_2_176:4.1.3.1.8.2
	Kernel-5_35-4_79_2_175:4.1.3.1.8.2
	Kernel-5_35-4_79_2_174:4.1.3.1.8.2
	Kernel-5_35-4_79_2_173:4.1.3.1.8.2
	Kernel-5_35-4_79_2_172:4.1.3.1.8.2
	Kernel-5_35-4_79_2_171:4.1.3.1.8.2
	Kernel-5_35-4_79_2_170:4.1.3.1.8.2
	Kernel-5_35-4_79_2_169:4.1.3.1.8.2
	Kernel-5_35-4_79_2_168:4.1.3.1.8.2
	Kernel-5_35-4_79_2_167:4.1.3.1.8.2
	Kernel-5_35-4_79_2_166:4.1.3.1.8.2
	Kernel-5_35-4_79_2_165:4.1.3.1.8.2
	RPi_merge:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_23:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_22:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_21:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_20:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_19:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_18:4.1.3.1.8.2
	Kernel-5_35-4_79_2_164:4.1.3.1.8.2
	Kernel-5_35-4_79_2_163:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_17:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_16:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_15:4.1.3.1.8.2
	Kernel-5_35-4_79_2_162:4.1.3.1.8.2
	Kernel-5_35-4_79_2_161:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_14:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_13:4.1.3.1.8.2
	Kernel-5_35-4_79_2_160:4.1.3.1.8.2
	Kernel-5_35-4_79_2_159:4.1.3.1.8.2
	Kernel-5_35-4_79_2_158:4.1.3.1.8.2
	Kernel-5_35-4_79_2_157:4.1.3.1.8.2
	Kernel-5_35-4_79_2_156:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_12:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_11:4.1.3.1.8.2
	Kernel-5_35-4_79_2_155:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_10:4.1.3.1.8.2
	Kernel-5_35-4_79_2_154:4.1.3.1.8.2
	Kernel-5_35-4_79_2_153:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_9:4.1.3.1.8.2
	Kernel-5_35-4_79_2_152:4.1.3.1.8.2
	Kernel-5_35-4_79_2_151:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_8:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_7:4.1.3.1.8.2
	Kernel-5_35-4_79_2_150:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_6:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_5:4.1.3.1.8.2
	Kernel-5_35-4_79_2_149:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_4:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_3:4.1.3.1.8.2
	Kernel-5_35-4_79_2_148:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_2:4.1.3.1.8.2
	Kernel-5_35-4_79_2_147_2_1:4.1.3.1.8.2
	RPi:4.1.3.1.8.2.0.2
	RPi_bp:4.1.3.1.8.2
	Kernel-5_35-4_79_2_98_2_52_2_1:4.1.3.1.8.1.2.1
	alees_Kernel_dev:4.1.3.1.8.1.2.1.0.2
	alees_Kernel_dev_bp:4.1.3.1.8.1.2.1
	Kernel-5_35-4_79_2_147:4.1.3.1.8.2
	Kernel-5_35-4_79_2_146:4.1.3.1.8.2
	Kernel-5_35-4_79_2_145:4.1.3.1.8.2
	Kernel-5_35-4_79_2_144:4.1.3.1.8.2
	Kernel-5_35-4_79_2_143:4.1.3.1.8.2
	Kernel-5_35-4_79_2_142:4.1.3.1.8.2
	Kernel-5_35-4_79_2_141:4.1.3.1.8.2
	Kernel-5_35-4_79_2_140:4.1.3.1.8.2
	Kernel-5_35-4_79_2_139:4.1.3.1.8.2
	Kernel-5_35-4_79_2_138:4.1.3.1.8.2
	Kernel-5_35-4_79_2_137:4.1.3.1.8.2
	Kernel-5_35-4_79_2_136:4.1.3.1.8.2
	Kernel-5_35-4_79_2_135:4.1.3.1.8.2
	Kernel-5_35-4_79_2_134:4.1.3.1.8.2
	Kernel-5_35-4_79_2_133:4.1.3.1.8.2
	Kernel-5_35-4_79_2_132:4.1.3.1.8.2
	Kernel-5_35-4_79_2_131:4.1.3.1.8.2
	Kernel-5_35-4_79_2_130:4.1.3.1.8.2
	Kernel-5_35-4_79_2_129:4.1.3.1.8.2
	Kernel-5_35-4_79_2_128:4.1.3.1.8.2
	Kernel-5_35-4_79_2_127:4.1.3.1.8.2
	Kernel-5_35-4_79_2_126:4.1.3.1.8.2
	Kernel-5_35-4_79_2_125:4.1.3.1.8.2
	Kernel-5_35-4_79_2_124:4.1.3.1.8.2
	Kernel-5_35-4_79_2_123:4.1.3.1.8.2
	Cortex_merge:4.1.3.1.8.1.2.1
	Kernel-5_35-4_79_2_122:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_54:4.1.3.1.8.1.2.1
	Kernel-5_35-4_79_2_98_2_53:4.1.3.1.8.1.2.1
	Kernel-5_35-4_79_2_98_2_52:4.1.3.1.8.1.2.1
	Kernel-5_35-4_79_2_98_2_51:4.1.3.1.8.1.2.1
	Kernel-5_35-4_79_2_98_2_50:4.1.3.1.8.1.2.1
	Kernel-5_35-4_79_2_98_2_49:4.1.3.1.8.1.2.1
	Kernel-5_35-4_79_2_98_2_48:4.1.3.1.8.1.2.1
	Kernel-5_35-4_79_2_121:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_47:4.1.3.1.8.1
	Kernel-5_35-4_79_2_120:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_46:4.1.3.1.8.1
	Kernel-5_35-4_79_2_119:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_45:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_44:4.1.3.1.8.1
	Kernel-5_35-4_79_2_118:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_43:4.1.3.1.8.1
	Kernel-5_35-4_79_2_117:4.1.3.1.8.1
	Kernel-5_35-4_79_2_116:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_42:4.1.3.1.8.1
	Kernel-5_35-4_79_2_115:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_41:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_40:4.1.3.1.8.1
	Kernel-5_35-4_79_2_114:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_39:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_38:4.1.3.1.8.1
	Kernel-5_35-4_79_2_113:4.1.3.1.8.1
	Kernel-5_35-4_79_2_112:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_37:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_36:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_35:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_34:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_33:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_32:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_31:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_30:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_29:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_28:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_27:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_26:4.1.3.1.8.1
	Kernel-5_35-4_79_2_111:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_25:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_24:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_23:4.1.3.1.8.1
	Kernel-5_35-4_79_2_110:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_22:4.1.3.1.8.1
	Kernel-5_35-4_79_2_109:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_21:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_20:4.1.3.1.8.1
	Kernel-5_35-4_79_2_108:4.1.3.1.8.1
	Kernel-5_35-4_79_2_107:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_19:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_18:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_17:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_16:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_15:4.1.3.1.8.1
	Kernel-5_35-4_79_2_106:4.1.3.1.8.1
	Kernel-5_35-4_79_2_105:4.1.3.1.8.1
	Kernel-5_35-4_79_2_104:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_14:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_13:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_12:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_11:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_10:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_9:4.1.3.1.8.1
	Kernel-5_35-4_79_2_103:4.1.3.1.8.1
	Kernel-5_35-4_79_2_102:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_8:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_7:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_6:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_5:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_4:4.1.3.1.8.1
	Kernel-5_35-4_79_2_101:4.1.3.1.8.1
	Kernel-5_35-4_79_2_100:4.1.3.1.8.1
	Kernel-5_35-4_79_2_99:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_3:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_2:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98_2_1:4.1.3.1.8.1
	Cortex:4.1.3.1.8.1.0.2
	Cortex_bp:4.1.3.1.8.1
	Kernel-5_35-4_79_2_98:4.1.3.1.8.1
	Kernel-5_35-4_79_2_97:4.1.3.1.8.1
	Kernel-5_35-4_79_2_96:4.1.3.1.8.1
	Kernel-5_35-4_79_2_95:4.1.3.1.8.1
	Kernel-5_35-4_79_2_94:4.1.3.1.8.1
	Kernel-5_35-4_79_2_93:4.1.3.1.8.1
	Kernel-5_35-4_79_2_92:4.1.3.1.8.1
	Kernel-5_35-4_79_2_91:4.1.3.1.8.1
	Kernel-5_35-4_79_2_90:4.1.3.1.8.1
	Kernel-5_35-4_79_2_89:4.1.3.1.8.1
	Kernel-5_35-4_79_2_88:4.1.3.1.8.1
	Kernel-5_35-4_79_2_87:4.1.3.1.8.1
	Kernel-5_35-4_79_2_86:4.1.3.1.8.1
	Kernel-5_35-4_79_2_85:4.1.3.1.8.1
	Kernel-5_35-4_79_2_84:4.1.3.1.8.1
	Kernel-5_35-4_79_2_83:4.1.3.1.8.1
	Kernel-5_35-4_79_2_82:4.1.3.1.8.1
	Kernel-5_35-4_79_2_81:4.1.3.1.8.1
	Kernel-5_35-4_79_2_80:4.1.3.1.8.1
	Kernel-5_35-4_79_2_79:4.1.3.1.8.1
	Kernel-5_35-4_79_2_78:4.1.3.1.8.1
	Kernel-5_35-4_79_2_77:4.1.3.1.8.1
	RO_5_07:4.1.3.1.8.1
	Kernel-5_35-4_79_2_76:4.1.3.1.8.1
	Kernel-5_35-4_79_2_75:4.1.3.1.8.1
	Kernel-5_35-4_79_2_74:4.1.3.1.8.1
	Kernel-5_35-4_79_2_73:4.1.3.1.8.1
	Kernel-5_35-4_79_2_72:4.1.3.1.8.1
	Kernel-5_35-4_79_2_71:4.1.3.1.8.1
	Kernel-5_35-4_79_2_70:4.1.3.1.8.1
	Kernel-5_35-4_79_2_69:4.1.3.1.8.1
	Kernel-5_35-4_79_2_68:4.1.3.1.8.1
	Kernel-5_35-4_79_2_67:4.1.3.1.8.1
	Kernel-5_35-4_79_2_66:4.1.3.1.8.1
	Kernel-5_35-4_79_2_65:4.1.3.1.8.1
	Kernel-5_35-4_79_2_64:4.1.3.1.8.1
	Kernel-5_35-4_79_2_63:4.1.3.1.8.1
	Kernel-5_35-4_79_2_62:4.1.3.1.8.1
	Kernel-5_35-4_79_2_61:4.1.3.1.8.1
	Kernel-5_35-4_79_2_59:4.1.3.1.8.1
	Kernel-5_35-4_79_2_58:4.1.3.1.8.1
	Kernel-5_35-4_79_2_57:4.1.3.1.8.1
	Kernel-5_35-4_79_2_56:4.1.3.1.8.1
	Kernel-5_35-4_79_2_55:4.1.3.1.8.1
	Kernel-5_35-4_79_2_54:4.1.3.1.8.1
	Kernel-5_35-4_79_2_53:4.1.3.1.8.1
	Kernel-5_35-4_79_2_52:4.1.3.1.8.1
	Kernel-5_35-4_79_2_51:4.1.3.1.8.1
	Kernel-5_35-4_79_2_50:4.1.3.1.8.1
	Kernel-5_35-4_79_2_49:4.1.3.1.8.1
	Kernel-5_35-4_79_2_48:4.1.3.1.8.1
	Kernel-5_47:4.1.3.1
	Kernel-5_46-4_90_2_1:4.1.3.1
	nbingham_Kernel_FastNC_dev_bp:4.1.3.1
	nbingham_Kernel_FastNC_dev:4.1.3.1.0.12
	Kernel-5_46:4.1.3.1
	Kernel-5_45:4.1.3.1
	Kernel-5_35-4_79_2_47:4.1.3.1.8.1
	Kernel-5_35-4_79_2_46:4.1.3.1.8.1
	Kernel-5_35-4_79_2_45:4.1.3.1.8.1
	Kernel-5_35-4_79_2_44:4.1.3.1.8.1
	Kernel-5_35-4_79_2_25_2_2:4.1.3.1.8.1
	Kernel-5_35-4_79_2_43:4.1.3.1.8.1
	Kernel-5_35-4_79_2_42:4.1.3.1.8.1
	Kernel-5_35-4_79_2_41:4.1.3.1.8.1
	Kernel-5_35-4_79_2_40:4.1.3.1.8.1
	Kernel-5_35-4_79_2_39:4.1.3.1.8.1
	Kernel-5_35-4_79_2_38:4.1.3.1.8.1
	Kernel-5_35-4_79_2_37:4.1.3.1.8.1
	Kernel-5_35-4_79_2_36:4.1.3.1.8.1
	Kernel-5_35-4_79_2_35:4.1.3.1.8.1
	Kernel-5_35-4_79_2_34:4.1.3.1.8.1
	Kernel-5_35-4_79_2_33:4.1.3.1.8.1
	Kernel-5_35-4_79_2_32:4.1.3.1.8.1
	Kernel-5_44:4.1.3.1
	Kernel-5_35-4_79_2_25_2_1:4.1.3.1.8.1
	Kernel-5_43:4.1.3.1
	Kernel-5_35-4_79_2_31:4.1.3.1.8.1
	Kernel-5_35-4_79_2_30:4.1.3.1.8.1
	Kernel-5_35-4_79_2_29:4.1.3.1.8.1
	Kernel-5_35-4_79_2_28:4.1.3.1.8.1
	Kernel-5_35-4_79_2_27:4.1.3.1.8.1
	Kernel-5_35-4_79_2_26:4.1.3.1.8.1
	Kernel-5_42:4.1.3.1
	Kernel-5_41:4.1.3.1
	Kernel-5_40:4.1.3.1
	Kernel-5_35-4_79_2_25:4.1.3.1.8.1
	Kernel-5_35-4_79_2_24:4.1.3.1.8.1
	Kernel-5_35-4_79_2_23:4.1.3.1.8.1
	Kernel-5_35-4_79_2_22:4.1.3.1.8.1
	Kernel-5_35-4_79_2_21:4.1.3.1.8.1
	Kernel-5_35-4_79_2_20:4.1.3.1.8.1
	Kernel-5_35-4_79_2_19:4.1.3.1.8.1
	Kernel-5_35-4_79_2_18:4.1.3.1.8.1
	Kernel-5_35-4_79_2_17:4.1.3.1.8.1
	Kernel-5_35-4_79_2_16:4.1.3.1.8.1
	Kernel-5_35-4_79_2_15:4.1.3.1.8.1
	Kernel-5_35-4_79_2_14:4.1.3.1.8.1
	Kernel-5_39:4.1.3.1
	Kernel-5_13-4_52_2_1:4.1.3.1
	Bethany:4.1.3.1.0.10
	Kernel-5_38:4.1.3.1
	Kernel-5_35-4_79_2_13:4.1.3.1.8.1
	Kernel-5_35-4_79_2_12:4.1.3.1.8.1
	Kernel-5_35-4_79_2_11:4.1.3.1
	Kernel-5_37:4.1.3.1
	Kernel-5_35-4_79_2_10:4.1.3.1
	Kernel-5_35-4_79_2_9:4.1.3.1
	Kernel-5_36:4.1.3.1
	Kernel-5_35-4_79_2_8:4.1.3.1
	Kernel-5_35-4_79_2_7:4.1.3.1
	Kernel-5_35-4_79_2_6:4.1.3.1
	Kernel-5_35-4_79_2_5:4.1.3.1
	Kernel-5_35-4_79_2_4:4.1.3.1
	Kernel-5_35-4_79_2_3:4.1.3.1
	Kernel-5_35-4_79_2_2:4.1.3.1
	dellis_autobuild_BaseSW:4.1.3.1
	Kernel-5_35-4_79_2_1:4.1.3.1
	HAL:4.1.3.1.0.8
	Kernel-5_35:4.1.3.1
	Kernel-5_34:4.1.3.1
	Kernel-5_33:4.1.3.1
	Kernel-5_32:4.1.3.1
	Kernel-5_31:4.1.3.1
	Kernel-5_30:4.1.3.1
	Kernel-5_29:4.1.3.1
	Kernel-5_28:4.1.3.1
	Kernel-5_27:4.1.3.1
	Kernel-5_26:4.1.3.1
	Kernel-5_25:4.1.3.1
	Kernel-5_24:4.1.3.1
	Kernel-5_23:4.1.3.1
	Kernel-5_22:4.1.3.1
	sbrodie_sedwards_16Mar2000:4.1.3.1
	Kernel-5_21:4.1.3.1
	Kernel-5_20:4.1.3.1
	Kernel-5_19:4.1.3.1
	Kernel-5_18:4.1.3.1
	Kernel-5_17:4.1.3.1
	Kernel-5_16:4.1.3.1
	Kernel-5_15:4.1.3.1
	Kernel-5_14:4.1.3.1
	Kernel-5_13:4.1.3.1
	Kernel-5_12:4.1.3.1
	Kernel-5_11:4.1.3.1
	Kernel-5_10:4.1.3.1
	Kernel-5_09:4.1.3.1
	Kernel-5_08:4.1.3.1
	Kernel-5_07:4.1.3.1
	Kernel-5_06:4.1.3.1
	Kernel-5_05:4.1.3.1
	Kernel-5_04:4.1.3.1
	Kernel-5_03:4.1.3.1
	Kernel-5_02:4.1.3.1
	Kernel-5_01:4.1.3.1
	Kernel-5_00:4.1.3.1
	Kernel-4_99:4.1.3.1
	Kernel-4_98:4.1.3.1
	Kernel-4_97:4.1.3.1
	Kernel-4_96:4.1.3.1
	Kernel-4_95:4.1.3.1
	Kernel-4_94:4.1.3.1
	Kernel-4_93:4.1.3.1
	Kernel-4_92:4.1.3.1
	Kernel-4_91:4.1.3.1
	Kernel-4_90:4.1.3.1
	dcotton_autobuild_BaseSW:4.1.3.1
	Kernel-4_89:4.1.3.1
	Kernel-4_88:4.1.3.1
	Kernel-4_87:4.1.3.1
	Kernel-4_86:4.1.3.1
	Kernel-4_85:4.1.3.1
	sbrodie_UrsulaRiscPC_Kernel_19Aug99:4.1.3.1.2.3
	Kernel-4_84:4.1.3.1
	sbrodie_UrsulaRiscPC_Kernel_18Aug99:4.1.3.1.2.3
	Ursula_RiscPC_bp:4.1.3.1.2.3
	Kernel-4_83:4.1.3.1
	Kernel-4_82:4.1.3.1
	Kernel-4_81:4.1.3.1
	Kernel-4_80:4.1.3.1
	Kernel-4_79:4.1.3.1
	Kernel-4_78:4.1.3.1
	Kernel-4_77:4.1.3.1
	Kernel-4_76:4.1.3.1
	Kernel-4_75:4.1.3.1
	Kernel-4_74:4.1.3.1
	Kernel-4_73:4.1.3.1
	Kernel-4_72:4.1.3.1
	Kernel-4_71:4.1.3.1
	Kernel-4_70:4.1.3.1
	Kernel-4_69:4.1.3.1
	Kernel-4_68:4.1.3.1
	mstphens_UrsulaRiscPCBuild_20Nov98:4.1.3.1.2.3
	Ursula_RiscPC:4.1.3.1.2.3.0.2
	Kernel-4_67:4.1.3.1
	Kernel-4_66:4.1.3.1
	Kernel-4_65:4.1.3.1
	Ursula_merge:4.1.3.1
	Kernel-4_64:4.1.3.1
	mstphens_Kernel-3_81:4.1.3.1.2.3
	rthornb_UrsulaBuild-19Aug1998:4.1.3.1.2.3
	UrsulaBuild_FinalSoftload:4.1.3.1.2.3
	rthornb_UrsulaBuild-12Aug1998:4.1.3.1.2.3
	aglover_UrsulaBuild-05Aug1998:4.1.3.1.2.3
	rthornb_UrsulaBuild-29Jul1998:4.1.3.1.2.3
	rthornb_UrsulaBuild-22Jul1998:4.1.3.1.2.3
	rthornb_UrsulaBuild-15Jul1998:4.1.3.1.2.3
	rthornb_UrsulaBuild-07Jul1998:4.1.3.1.2.3
	rthornb_UrsulaBuild-17Jun1998:4.1.3.1.2.3
	rthornb_UrsulaBuild-03Jun1998:4.1.3.1.2.3
	rthornb_UrsulaBuild-27May1998:4.1.3.1.2.3
	mstphens_Kernel-3_80:4.1.3.1.2.3
	rthornb_UrsulaBuild-21May1998:4.1.3.1.2.3
	rthornb_UrsulaBuild_01May1998:4.1.3.1.2.3
	afrost_NC2_Generic:4.1.3.1
	Daytona:4.1.3.1.0.6
	Daytona_bp:4.1.3.1
	Ursula_bp:4.1.3.1
	Ursula:4.1.3.1.0.2
	RO_3_71:4.1.3.1
	MergeFiles:4.1.3.1
	RO_3_70:4.1.3.1
	StrongARM:4.1.3;
locks; strict;
comment	@# @;


4.3
date	2016.12.13.16.42.53;	author jlee;	state Exp;
branches;
next	4.2;
commitid	aGog9bB8f4QKlQxz;

4.2
date	2016.06.30.20.08.12;	author jlee;	state Exp;
branches;
next	4.1;
commitid	IWoXxARWeuLDOwcz;

4.1
date	96.11.06.02.01.32;	author nturton;	state Exp;
branches
	4.1.3.1;
next	;

4.1.3.1
date	96.11.06.02.01.32;	author nturton;	state Exp;
branches
	4.1.3.1.2.1
	4.1.3.1.8.1;
next	;

4.1.3.1.2.1
date	97.05.21.09.30.26;	author mstphens;	state Exp;
branches;
next	4.1.3.1.2.2;

4.1.3.1.2.2
date	97.09.09.13.33.52;	author mstphens;	state Exp;
branches;
next	4.1.3.1.2.3;

4.1.3.1.2.3
date	98.03.26.11.26.11;	author mstphens;	state Exp;
branches;
next	;

4.1.3.1.8.1
date	2000.11.10.14.41.16;	author mstephen;	state Exp;
branches
	4.1.3.1.8.1.2.1;
next	4.1.3.1.8.2;

4.1.3.1.8.2
date	2011.11.26.21.11.20;	author jlee;	state Exp;
branches;
next	4.1.3.1.8.3;
commitid	cI3W0zbtALQG6TIv;

4.1.3.1.8.3
date	2015.08.31.19.28.41;	author jlee;	state Exp;
branches;
next	4.1.3.1.8.4;
commitid	Ni3KL17bG70fnszy;

4.1.3.1.8.4
date	2015.09.06.18.45.17;	author jlee;	state Exp;
branches;
next	4.1.3.1.8.5;
commitid	9JoJW3FhqXIqWdAy;

4.1.3.1.8.5
date	2016.05.23.22.02.39;	author jlee;	state Exp;
branches;
next	;
commitid	vs9giyyGBlq1GE7z;

4.1.3.1.8.1.2.1
date	2011.08.08.23.28.30;	author jlee;	state Exp;
branches;
next	;
commitid	D7rzILnwRRSXoLuv;


desc
@@


4.3
log
@Reimplement AMBControl ontop of the PMP system
Detail:
  With this set of changes, each AMB node is now the owner of a fake DANode which is linked to a PMP.
  From a user's perspective the behaviour of AMBControl is the same as before, but rewriting it to use PMPs internally offers the following (potential) benefits:
  * Reduction in the amount of code which messes with the CAM & page tables, simplifying future work/maintenance. Some of the AMB ops (grow, shrink) now just call through to OS_ChangeDynamicArea. However all of the old AMB routines were well-optimised, so to avoid a big performance hit for common operations not all of them have been removed (e.g. mapslot / mapsome). Maybe one day these optimal routines will be made available for use by regular PMP DAs.
  * Removal of the slow Service_MemoryMoved / Service_PagesSafe handlers that had to do page list fixup after the core kernel had reclaimed/moved pages. Since everything is a PMP, the kernel will now deal with this on behalf of AMB.
  * Removal of a couple of other slow code paths (e.g. Do_AMB_MakeUnsparse calls from OS_ChangeDynamicArea)
  * Potential for more flexible mapping of application space in future, e.g. sparse allocation of memory to the wimp slot
  * Simpler transition to an ASID-based task swapping scheme on ARMv6+?
  Other changes of note:
  * AMB_LazyMapIn switch has been fixed up to work correctly (i.e. turning it off now disables lazy task swapping and all associated code instead of producing a build error)
  * The DANode for the current app should be accessed via the GetAppSpaceDANode macro. This will either return the current AMB DANode, or AppSpaceDANode (if e.g. pre-Wimp). However be aware that AppSpaceDANode retains the legacy behaviour of having a base + size relative to &0, while the AMB DANodes (identifiable via the PMP flag) are sane and have their base + size relative to &8000.
  * Mostly-useless DebugAborts switch removed
  * AMBPhysBin (page number -> phys addr lookup table) removed. Didn't seem to give any tangible performance benefit, and was imposing hidden restrictions on memory usage (all phys RAM fragments in PhysRamTable must be multiple of 512k). And if it really was a good optimisation, surely it should have been applied to all areas of the kernel, not just AMB!
  Other potential future improvements:
  * Turn the fake DANodes into real dynamic areas, reducing the amount of special code needed in some places, but allow the DAs to be hidden from OS_DynamicArea 3 so that apps/users won't get too confused
  * Add a generic abort trapping system to PMPs/DAs (lazy task swapping abort handler is still a special case)
  File changes:
  - s/ARM600, s/VMSAv6, s/ExtraSWIs - Remove DebugAborts
  - s/ArthurSWIs - Remove AMB service call handler dispatch
  - s/ChangeDyn - AMB_LazyMapIn switch fixes. Add alternate internal entry points for some PMP ops to allow the DANode to be specified (used by AMB)
  - s/Exceptions - Remove DebugAborts, AMB_LazyMapIn switch fixes
  - s/Kernel - Define GetAppSpaceDANode macro, AMB_LazyMapIn switch fix
  - s/MemInfo - AMB_LazyMapIn switch fixes
  - s/AMBControl/AMB - Update GETs
  - s/AMBControl/Memory - Remove block size quantisation, AMB_BlockResize (page list blocks are now allocated by PMP code)
  - s/AMBControl/Options - Remove PhysBin definitions, AMBMIRegWords (moved to Workspace file), AMB_LimpidFreePool switch. Add AMB_Debug switch.
  - s/AMBControl/Workspace - Update AMBNode to contain an embedded DANode. Move AMBMIRegWords here from Options file.
  - s/AMBControl/allocate - Fake DA node initialisation
  - s/AMBControl/deallocate - Add debug output
  - s/AMBControl/growp, growshrink, mapslot, mapsome, shrinkp - Rewrite to use PMP ops where possible, add debug output
  - s/AMBControl/main - Remove PhysBin initialisation. Update the enumerate/mjs_info call.
  - s/AMBControl/memmap - Low-level memory mapping routines updated or rewritten as appropriate.
  - s/AMBControl/readinfo - Update to cope with DANode
  - s/AMBControl/service - Remove old service call handlers
  - s/AMBControl/handler - DA handler for responding to PMP calls from OS_ChangeDynamicArea; just calls through to growpages/shrinkpages as appropriate.
Admin:
  Tested on pretty much everything currently supported


Version 5.66. Tagged as 'Kernel-5_66'
@
text
@; Copyright 1996 Acorn Computers Ltd
;
; Licensed under the Apache License, Version 2.0 (the "License");
; you may not use this file except in compliance with the License.
; You may obtain a copy of the License at
;
;     http://www.apache.org/licenses/LICENSE-2.0
;
; Unless required by applicable law or agreed to in writing, software
; distributed under the License is distributed on an "AS IS" BASIS,
; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
; See the License for the specific language governing permissions and
; limitations under the License.
;
; > s.allocate

; handle allocate reason code

; entry:
;     R0 = 0 (reason code 0)
;     R1 = number of pages
;
; exit:
;     R1 = no. of pages actually allocated
;     R2 = handle for allocation, 0 if pages were requested but none could be claimed

allocate
        Push    "R0,R3,R4,LR"

      [ AMB_Debug
        DebugReg r1, "allocate "
      ]

        LDR     R3,=AbsMaxAppSize-ApplicationStart
        MOV     R3,R3,LSR #Log2PageSize   ;R3 = absolute max app pages

        CMP     R1,R3
        MOVGT   R1,R3

;get handle for node
        LDR     R0,AMBNodeHandles
        LDR     R4,[R0]
        CMP     R4,#0            ;any handles available?
        BNE     %FT01

  ; give up
        Pull    "R0,R3,R4,LR"
        ADR     R0,err_nomorehandles
        B       SLVK_SetV

01
;get memory for node - from system heap
        MOV     R3,#AMBNode_HdrSize
        BL      AMB_BlockClaim
        BVS     alloc_done

;remember handle in node
        STR     R4,[R2,#AMBNode_handle]

;init fields of new node
        LDR     R4,=AMBMagicNodeID
        STR     R4,[R2,#AMBNode_id]        ;magic id
        MOV     R4,#0
        STR     R4,[R2,#AMBNode_DANode+DANode_PMPSize] ;number of pages = 0 (so far)
        LDR     R4,=ZeroPage+AppSpaceDANode
        LDR     R4,[R4,#DANode_Flags]      ;Get the page flags from the DA.
        LDR     LR,=DynAreaFlags_AccessMask
        AND     R4,R4,LR
        ORR     R4,R4,#DynAreaFlags_PMP
        ORR     R4,R4,#DynAreaFlags_NeedsSpecificPages
        STR     R4,[R2,#AMBNode_DANode+DANode_Flags]
;fill in other DANode bits
        MOV     R4,#0
        STR     R4,[R2,#AMBNode_DANode+DANode_Link]
        STR     R4,[R2,#AMBNode_DANode+DANode_Size]
        STR     R4,[R2,#AMBNode_DANode+DANode_SubLink]
        STR     R4,[R2,#AMBNode_DANode+DANode_SparseHWM]
        STR     R4,[R2,#AMBNode_DANode+DANode_SortLink]
        STR     R4,[R2,#AMBNode_DANode+DANode_PMP]
        STR     R4,[R2,#AMBNode_DANode+DANode_PMPMaxSize]
        STR     R2,[R2,#AMBNode_DANode+DANode_Workspace]
        ADR     R4,AMBDAHandler
        STR     R4,[R2,#AMBNode_DANode+DANode_Handler]
        ADR     R4,AMBDAName
        STR     R4,[R2,#AMBNode_DANode+DANode_Title]
        LDR     R4,=AbsMaxAppSize-ApplicationStart
        STR     R4,[R2,#AMBNode_DANode+DANode_MaxSize]
        MOV     R4,#ChangeDyn_AplSpace
        STR     R4,[R2,#AMBNode_DANode+DANode_Number]
        MOV     R4,#ApplicationStart
        STR     R4,[R2,#AMBNode_DANode+DANode_Base]

;do the actual MMU page allocation (grow from 0), for R1 pages, using node R2
        BL      growpages              
        BVS     alloc_done

        CMP     R1,#0                    ;EQ status if we were asked for 0 pages
        LDR     R1,[R2,#AMBNode_DANode+DANode_PMPSize] ;actual no. of pages we achieved
        BEQ     alloc_ok                 ;if asked for 0, regard as ok

        CMP     R1,#0
        BEQ     alloc_zeropages          ;achieving 0 pages is not ok

;ok, so remove handle from free list
alloc_ok
        LDR     R0,AMBNodeHandles
        LDR     R4,[R2,#AMBNode_handle]
        LDR     R3,[R0,R4,LSL #2]  ;next free handle
        STR     R3,[R0]            ;store as new first free handle
        STR     R2,[R0,R4,LSL #2]  ;and remember node address in handle array

;R2 -> new node - put it on front of list
        ADR     R3,AMBAnchorNode        ; R3 -> ank_node
        LDR     R4,[R3,#AMBNode_next]   ; R4 -> old_node (old front)
        STR     R4,[R2,#AMBNode_next]   ; new_next := old_node
        STR     R2,[R3,#AMBNode_next]   ; ank_next := new_node
        STR     R3,[R2,#AMBNode_prev]   ; new_prev := ank_node
        STR     R2,[R4,#AMBNode_prev]   ; old_prev := new_node

        LDR     R4,AMBNtasks
        ADD     R4,R4,#1
        STR     R4,AMBNtasks

        STR     R2,AMBMappedInNode       ;allocated node is also mapped in
        LDR     R2,[R2,#AMBNode_handle]  ;change address to handle
        CLRV

alloc_done
      [ AMB_Debug
        DebugReg r1,"<alloc "
        DebugReg r2
      ]
        STRVS   R0,[SP]
        Pull    "R0,R3,R4,LR"
        B       SLVK_TestV

;free page table space and return 0 handle
alloc_zeropages
        BL      AMB_BlockFree
        MOV     R1,#0
        MOV     R2,#0
        B       alloc_done

AMBDAName
        = "AMBControl DANode", 0
        ALIGN

        LTORG

    END
@


4.2
log
@Merge HAL branch to trunk
Detail:
  This change merges the past 15+ years of HAL branch development back to the trunk.
  This is effectively the end for non-HAL builds of the kernel, as no attempt has been made to maintain it during this merge, and all non-HAL & non-32bit code will soon be removed anyway.
  Rather than list everything that's been added to the HAL branch, it's easier to describe the change in terms of the things that the HAL branch was lacking:
  * Trunk version of Docs/32bit contained updated comments for the SVC stack structure during ErrorV
  * Trunk version of s/HeapMan contained a tweak to try and reduce the number of small free blocks that are created
  * Trunk version of s/Kernel contained a change to only copy 248 bytes of the error string to the error buffer (down from 252 bytes), to take into account the extra 4 bytes needed by the PSR. However this goes against the decision that's been made in the HAL branch that the error buffer should be enlarged to 260 bytes instead (ref: https://www.riscosopen.org/tracker/tickets/201), so the HAL build will retain its current behaviour.
  * Trunk version of s/MsgCode had RMNot32bit error in the list of error messages to count when countmsgusage {TRUE}
  * Trunk version of s/PMF/i2cutils contained support for OS_Memory 5, "read/write value of NVRamWriteSize". Currently the HAL branch doesn't have a use for this (in particular, the correct NVRamWriteSize should be specified by the HAL, so there should be no need for software to change it at runtime), and so this code will remain switched out in the HAL build.
Admin:
  Tested on Raspberry Pi


Version 5.48. Tagged as 'Kernel-5_48'
@
text
@d30 3
a32 1
;  Debug AMB,"allocate ",r0,r1
d34 1
a34 2
        MOV     R3,#AbsMaxAppSize
        SUB     R3,R3,#ApplicationStart
d53 1
a53 2
        MOV     R3,#(AMBNode_pages - AMBNode_id) ;size excluding page list
        ADD     R3,R3,R1,LSL #2                  ;plus one word per page
d64 1
a64 3
        STR     R4,[R2,#AMBNode_Npages]    ;number of pages = 0 (so far)
        MOV     R4,#ApplicationStart
        STR     R4,[R2,#AMBNode_startaddr]
d67 25
a91 3
        LDR     LR,=DynAreaFlags_AccessMask;Note that this is rather academic
        AND     R4,R4,LR                   ;because various bits of code ignore
        STR     R4,[R2,#AMBNode_PPL]       ;or overwrite these flags!
d98 1
a98 1
        LDR     R1,[R2,#AMBNode_Npages]  ;actual no. of pages we achieved
d129 4
a132 1
;  Debug AMB,"<alloc ",r1,r2
d144 4
@


4.1
log
@Initial revision
@
text
@d25 1
a25 1
;     R2 = handle for allocation
d30 2
d45 1
a45 1
  [ {TRUE} ; give up
a48 27
  ]

  [ {FALSE} ; oh no we don't, we're not a module now
;try to extend handles array
        MOV     R2,R0
        MOV     R0,#ModHandReason_ExtendBlock
        MOV     R3,#(GrowMaxNodes:SHL:2)
        SWI     XOS_Module
        BVS     alloc_done
        MOV     R0,R2
        STR     R0,AMBNodeHandles   ;in case block has moved
        LDR     R3,AMBNhandles
        STR     R3,[R0]             ;first free handle now
        ADD     LR,R3,#GrowMaxNodes
        STR     LR,AMBNhandles
        ADD     R4,R0,R3,LSL #2
        ADD     R3,R3,#1
;put new handles on free list
00
        STR     R3,[R4],#4
        ADD     R3,R3,#1
        CMP     R3,LR
        BNE     %BT00
        MOV     R3,#0
        STR     R3,[R4]          ;0 = end of list
        LDR     R4,[R0]          ;now we have a useable handle
  ]
d51 1
a51 1
;get memory for node - from RMA
d54 1
a54 2
        MOV     R0,#ModHandReason_Claim
        BL      XROS_Module
d67 5
a71 4
        LDR     R4,=AppSpaceDANode
        LDR     R4,[R4,#DANode_Flags]
        AND     R4,R4,#&7F
        STR     R4,[R2,#AMBNode_PPL]       ;PPL from bottom 8 bits of DA flags
d106 1
d109 1
d116 1
a116 2
        MOV     R0,#ModHandReason_Free
        BL      XROS_Module
@


4.1.3.1
log
@Import from cleaned 370 CD
@
text
@@


4.1.3.1.8.1
log
@reintroduce Ursula AMBControl, recoded with generic ARMop style, not debugged yet

Version 5.35, 4.79.2.12. Tagged as 'Kernel-5_35-4_79_2_12'
@
text
@a29 2
;  Debug AMB,"allocate ",r0,r1

d43 1
a43 1
  ; give up
d47 27
d79 2
a80 1
        BL      AMB_BlockClaim
a130 1
        CLRV
a132 1
;  Debug AMB,"<alloc ",r1,r2
d139 2
a140 1
        BL      AMB_BlockFree
@


4.1.3.1.8.2
log
@Merge Cortex kernel into HAL branch
Detail:
  This is a full merge of the Cortex kernel back into the HAL branch. Since the Cortex kernel is/was just a superset of the HAL branch, at this point in time both branches are identical.
  Main features the HAL branch gains from this merge:
  - ARMv6/ARMv7 support
  - High processor vectors/zero page relocation support
  - objasm 4 warning fixes
  - Improved HAL related functionality:
    - Support for HAL-driven RTCs instead of kernel-driven IIC based ones
    - Support for arbitrary size machine IDs
    - Support for multiple IIC busses
    - Support for any HAL size, instead of hardcoded 64k size
    - Probably some other stuff I've forgotten
  - Probably a few bug fixes here and there
Admin:
  Tested on BB-xM & Iyonix.
  Was successfully flashed to ROM on an Iyonix to test the Cortex branch implementation of the 2010 RTC bug fix.
  IOMD build untested - but has been known to work in the past.


Version 5.35, 4.79.2.123. Tagged as 'Kernel-5_35-4_79_2_123'
@
text
@d67 1
a67 1
        LDR     R4,=ZeroPage+AppSpaceDANode
@


4.1.3.1.8.3
log
@Add initial support for "physical memory pools"
Detail:
  This set of changes adds support for "physical memory pools" (aka PMPs), a new type of dynamic area which allow physical pages to be claimed/allocated without mapping them in to the logical address space. PMPs have full control over which physical pages they use (similar to DAs which request specific physical pages), and also have full control over the logical mapping of their pages (which pages go where, and per-page access/cacheability control).
  Currently the OS makes use of two PMPs: one for the free pool (which now has a logical size of zero - freeing up gigabytes of logical space), and one for the RAM disc (logical size of 1MB, allowing for a physical size limited only by the amount of free memory)
  Implementing these changes has required a number of other changes to be made:
  * The CAM has been expanded from 8 bytes per entry to 16 bytes per entry, in order to allow each RAM page to store information about its PMP association
  * The system heap has been expanded to 32MB in size (from just under 4MB), in order to allow it to be used to store PMP page lists (1 word needed per page, but PMP pages may not always have physical pages assigned to them - so to allow multiple large PMPs to exist we need more than just 1 word per RAM page)
  * The &FA000000-&FBFFFFFF area of fixed kernel workspace has been shuffled around to accomodate the larger CAM, and the system heap is now located just above the RMA.
  * SoftResets code stripped out (unlikely we'll ever want to fix and re-enable it)
  * A couple of FastCDA options are now permanently on
  * Internal page flags shuffled around a bit. PageFlags_Unavailable now publicly exposed so that PMP clients can lock/unlock pages at will.
  * When OS_ChangeDynamicArea is asked to grow or shrink the free pool, it now implicitly converts it into a shrink or grow of application space (which is what would happen anyway). This simplifies the implementation; during a grow, pages (or replacement pages) are always sourced from the free pool, and during a shrink pages are always sent to the free pool.
  File changes:
  - hdr/KernelWS - Extend DANode structure. Describe CAM format. Adjust kernel workspace.
  - hdr/OSRSI6, s/Middle - Add new item to expose the CAM format
  - hdr/Options - Remove SoftResets switch. Add some PMP switches.
  - s/ARM600, s/VMSAv6 - Updated for new CAM format. Note that although the CAM stores PMP information, BangCamUpdate currently doesn't deal with updating that data - it's the caller's responsibility to do so where appropriate.
  - s/ChangeDyn - Lots of changes to implement PMP support, and to cope with the new CAM format.
  - s/HAL - Updated to cope with new CAM format, and lack of logical mapping of free pool.
  - s/MemInfo - Updated to cope with new CAM format. OS_Memory 0 updated to cope with converting PPN to PA for pages which are mapped out. OS_Memory 24 updated to decode the access permissions on a per-page basis for PMPs, and fixed its HWM usage for sparse DAs.
  - s/NewReset - Soft reset code and unused AddCamEntries function removed. Updated to cope with new CAM format, PMP free pool, PMP RAMFS
  - s/AMBControl/allocate - Update comment (RMA hasn't been used for AMBControl nodes for a long time)
  - s/AMBControl/growp, s/AMBControl/memmap, s/AMBControl/shrinkp - Update for new CAM format + PMP free pool
  - s/vdu/vdudriver - Strip out soft reset code.
Admin:
  Tested on Pandaboard
  This is just a first iteration of the PMP feature, with any luck future changes will improve functionality. This means APIs are subject to change as well.


Version 5.35, 4.79.2.284. Tagged as 'Kernel-5_35-4_79_2_284'
@
text
@d51 1
a51 1
;get memory for node - from system heap
@


4.1.3.1.8.4
log
@Misc memory management tweaks & fixes
Detail:
  s/ChangeDyn - Fix OS_DynamicArea 20 to work properly with sparse & PMP DAs. It now checks against the max extent of the area rather than the current size; this matches the logic used for checking fixed system workspace areas. The call only determines the ownership of a logical address, and it's considered the caller's responsibility to check if there's actually a page at the given address.
  s/ChangeDyn - Revise OS_DynamicArea 25 to remove the redundant 'PMP page flags' entry, and to allow pages to be looked up by either PMP page index, phys page number, or DA page index
  s/ChangeDyn - Tidy up InitDynamicAreas by adding the NextFreePage routine to help determine the next page to be added to the free pool.
  s/AMBControl/Workspace, s/AMBControl/main, s/AMBControl/memmap - Fix lazy mapping in of pages to use the correct L2PT flags for the default CB cache policy
  s/AMBControl/allocate - Get rid of magic constant when extracting page flags from DA flags, and make note of the fact that assorted bits of code ignore the flags
  s/AMBControl/growp, s/AMBControl/shrinkp - Reverse the page order when growing/shrinking areas, to match OS_ChangeDynamicArea. This helps both DAs and application space to have pages allocated to them in contiguous physical order - which in turn helps produce shorter, more optimal scatter lists for DMA
Admin:
  Tested on Pandaboard


Version 5.35, 4.79.2.287. Tagged as 'Kernel-5_35-4_79_2_287'
@
text
@d68 3
a70 4
        LDR     R4,[R4,#DANode_Flags]      ;Get the page flags from the DA.
        LDR     LR,=DynAreaFlags_AccessMask;Note that this is rather academic
        AND     R4,R4,LR                   ;because various bits of code ignore
        STR     R4,[R2,#AMBNode_PPL]       ;or overwrite these flags!
@


4.1.3.1.8.5
log
@Fix PMP corruption caused by early errors generated by OS_DynamicArea 21
Detail:
  s/ChangeDyn - When DynArea_PMP_PhysOp generates an error during the initial page list scan, make sure r12 is initialised to the (new) PMP size, as expected by PMPMemoryMoved.
  s/AMBControl/allocate, s/AMBControl/growshrink - Document some extra exit conditions for the AMB allocate & grow/shrink routines
Admin:
  Tested on BB-xM
  Fixes RAM disc PMP becoming corrupt when attempting to grow it (e.g. via *ChangeDynamicArea) by an amount larger than the amount of free memory in the system


Version 5.35, 4.79.2.324. Tagged as 'Kernel-5_35-4_79_2_324'
@
text
@d25 1
a25 1
;     R2 = handle for allocation, 0 if pages were requested but none could be claimed
@


4.1.3.1.8.1.2.1
log
@Add zero page relocation support
Detail:
  A whole mass of changes to add high processor vectors + zero page relocation support to the Cortex branch of the kernel
  At the moment the code can only cope with two ZeroPage locations, &0 and &FFFF0000. But with a bit more tweaking those restrictions can probably be lifted, allowing ZeroPage to be hidden at almost any address (assuming it's fixed at compile time). If I've done my job right, these restrictions should all be enforced by asserts.
  There's a new option, HiProcVecs, in hdr/Options to control whether high processor vectors are used. When enabling it and building a ROM, remember:
  * FPEmulator needs to be built with the FPEAnchor=High option specified in the components file (not FPEAnchorType=High as my FPEmulator commit comments suggested)
  * ShareFS needs unplugging/removing since it can't cope with it yet
  * Iyonix users will need to use the latest ROOL boot sequence, to ensure the softloaded modules are compatible (OMAP, etc. don't really softload much so they're OK with older sequences)
  * However VProtect also needs patching to fix a nasty bug there - http://www.riscosopen.org/tracker/tickets/294
  The only other notable thing I can think of is that the ProcessTransfer code in s/ARM600 & s/VMSAv6 is disabled if high processor vectors are in use (it's fairly safe to say that code is obsolete in HAL builds anyway?)
  Fun challenge for my successor: Try setting ZeroPage to &FFFF00FF (or similar) so its value can be loaded with MVN instead of LDR. Then use positive/negative address offsets to access the contents.
  File changes:
  - hdr/ARMops - Modified ARMop macro to take the ZeroPage pointer as a parameter instead of 'zero'
  - hdr/Copro15ops - Corrected $quick handling in myISB macro
  - hdr/Options - Added ideal setting for us to use for HiProcVecs
  - s/AMBControl/allocate, s/AMBControl/growp, s/AMBControl/mapslot, s/AMBControl/memmap, s/AMBControl/service, s/AMBControl/shrinkp, s/Arthur2, s/Arthur3, s/ArthurSWIs, s/ChangeDyn, s/ExtraSWIs, s/HAL, s/HeapMan, s/Kernel, s/MemInfo, s/Middle, s/ModHand, s/MoreSWIs, s/MsgCode, s/NewIRQs, s/NewReset, s/Oscli, s/PMF/buffer, s/PMF/IIC, s/PMF/i2cutils, s/PMF/key, s/PMF/mouse, s/PMF/osbyte, s/PMF/oseven, s/PMF/osinit, s/PMF/osword, s/PMF/oswrch, s/SWINaming, s/Super1, s/SysComms, s/TickEvents, s/Utility, s/vdu/vdu23, s/vdu/vdudriver, s/vdu/vdugrafl, s/vdu/vdugrafv, s/vdu/vdupalxx, s/vdu/vdupointer, s/vdu/vduswis, s/vdu/vduwrch - Lots of updates to deal with zero page relocation
  - s/ARM600 - UseProcessTransfer option. Zero page relocation support. Deleted pre-HAL ClearPhysRAM code to tidy the file up a bit.
  - s/ARMops - Zero page relocation support. Set CPUFlag_HiProcVecs when high vectors are in use.
  - s/KbdResPC - Disable compilation of dead code
  - s/VMSAv6 - UseProcessTransfer option. Zero page relocation support.
Admin:
  Tested with OMAP & Iyonix ROM softloads, both with high & low zero page.
  High zero page hasn't had extensive testing, but boot sequence + ROM apps seem to work.


Version 5.35, 4.79.2.98.2.48. Tagged as 'Kernel-5_35-4_79_2_98_2_48'
@
text
@d67 1
a67 1
        LDR     R4,=ZeroPage+AppSpaceDANode
@


4.1.3.1.2.1
log
@Added following enhancements:

 - Chocolate screen mapping (section mapped and cached), StrongARM only
   Phoebe h/w (IOMD 2) will have register to assist this, but code currently
   relies on data abort mechanism to keep screen up to date wrt write-back
   data cache.

 - Chocolate AMBControl task switching (lazy page mapping), StrongARM only
   Improves task swapping speed. There appears to be a StrongAEM silicon
   bug rev 2 and 3) which means that LDMIB rn, {regs includind rn} cannot
   be reliably restarted after a data abort. This stuffs Chocolate AMBControl
   (awaiting response from Digital).

Both enhancements need more work to complete for Phoebe. Chocolate AMBControl
may well have to be made dormant because of silicon bug.

Note that this kernel *will* cause problems with task switching on StrongARM,
unless Chocolate task switching is disabled via !Flavour application.
@
text
@a29 2
;  Debug AMB,"allocate ",r0,r1

a130 1
        CLRV
a132 1
;  Debug AMB,"<alloc ",r1,r2
@


4.1.3.1.2.2
log
@ 1 Simplify source by removing various long-standing compile flags
   and pre-Medusa h/w support

 2 Fix bug with Pages_Unsafe/Pages_Safe page moving for StrongARM
   (interrupt hole) - also better performance for StrongARM

 3 Improve perfromance of physical memory clear for StrongARM
   (make sure it uses burst write for STM)

 4 Suspend Chocolate task switching for StrongARM if SALDMIBbroken
   is TRUE
@
text
@d45 1
a45 1
  ; give up
d49 27
@


4.1.3.1.2.3
log
@added support for Sparse dynamic areas
fixed performance disaster caused by naff API for Shrinkable areas
implemented clamps for dynamic areas max size
configured kernel to not own or create RAMFS area (needs new RAMFS)
AMBControl now uses system heap for space, not RMA
AMBControl enables Lazy task swapping if running on rev T or better SA
kernel now assumes there could be code above 64M
SWIS for limited 32 bit user code support implemented
Long command lines implemented (1k instead of 256)
Fast service call distribution implemented (uses Ursula module format)
*fx,*key etc now allow missing space before first parameter
*configure is reinstated (bug fix)
@
text
@d54 2
a55 1
        BL      AMB_BlockClaim
d116 2
a117 1
        BL      AMB_BlockFree
@
