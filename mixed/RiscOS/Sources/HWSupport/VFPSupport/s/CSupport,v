head	1.1;
access;
symbols
	VFPSupport-0_13:1.1
	VFPSupport-0_12:1.1
	VFPSupport-0_11:1.1
	VFPSupport-0_10:1.1
	VFPSupport-0_09:1.1
	VFPSupport-0_08:1.1
	VFPSupport-0_07:1.1
	VFPSupport-0_06:1.1;
locks; strict;
comment	@# @;


1.1
date	2014.02.08.15.27.44;	author jlee;	state Exp;
branches;
next	;
commitid	x9VDVmUDNhCwkjox;


desc
@@


1.1
log
@Add VFPv2 support code
Detail:
  This update adds the support code necessary to allow the VFPv2 coprocessor in the Raspberry Pi to be used in its full IEEE-compliant mode, and to add support for the generation of errors on VFP match exceptions (division by zero, etc.)
  SoftFloat Release 2b (http://www.jhauser.us/arithmetic/SoftFloat.html) is used to perform the floating point calculations in software, ensuring their accuracy.
  As with FPEmulator, the support code will only be included on machines which require it; at the moment this decision is handled by the makefile, based around the target machine type.
  Note that the current version of the support code does not implement default NaN or flush-to-zero mode, so it is not a fully accurate emulation of the hardware.
  Also added a new SWI, VFPSupport_ExceptionDump, for creating and reading a VFP context exception dump, and a new reason code to VFPSupport_Features to query which exception enable bits are supported
  File changes:
  - Makefile - Rewritten to use the CModule fragment, and to add the necessary rules for (optionally) building the support code.
  - Licences - File summarising the different licences used by different portions of the code
  - Test/test2c,ffb, Test/test3,ffb, Test/test4,ffb, Test/test5,ffb, Test/test6src,ffb - Several test programs for validating behaviour of the support code, mainly focused around unusual causes of exceptions and validating instruction decoding and short vector support
  - actions/ARMv7_VFP, arctions/common, c/head, cache/classify - decgen files for building the instruction decoder. This decoder is only used for synchronous exceptions, to determine whether the instruction is or isn't a valid VFP instruction. Asynchronous exceptions utilise a simpler, hand-crafted decoder in the assembler sources.
  - h/classify - Header for the decgen decoder
  - hdr/shared - Header with some definitions shared between the C and assembler sources (will be Hdr2H'd to generate the C header)
  - hdr/VFPSupport - Updated with new error numbers, SWI definitions
  - s/CSupport - Math support functions from the C library sources, required by SoftFloat
  - s/Errors - Added new error definitions
  - s/GetAll - Include support code if necessary. Add debug switch, plus optimisation switch for machines with 16 D registers
  - s/Instructions - Core data processing instruction emulation code. This file is included twice, once for single precision and once for double precision. The code calls through to the SoftFloat routines to perform the calculations.
  - s/Module - Adjust handling of undefined instruction vector to allow the support code to be installed instead of OldHandler if necessary. Initialise the support code as necessary. Add new SWIs & reason codes.
  - s/SupportCode - Undefined instruction handler which controls the rest of the support code. Also raises RISC OS errors as necessary (division by zero, etc.)
  - softfloat/* - The SoftFloat library sources. It's been tweaked in a few places for integration with the assembler support code, but otherwise no changes to the core logic were necessary.
  - Resources/UK/Messages - Updated with new error text
Admin:
  Tested on Raspberry Pi & BB-xM
  Support code tested using the supplied test routines and the TestFloat tool (http://www.jhauser.us/arithmetic/TestFloat.html)


Version 0.06. Tagged as 'VFPSupport-0_06'
@
text
@; Copyright 2014 Castle Technology Ltd
;
; Licensed under the Apache License, Version 2.0 (the "License");
; you may not use this file except in compliance with the License.
; You may obtain a copy of the License at
;
;     http://www.apache.org/licenses/LICENSE-2.0
;
; Unless required by applicable law or agreed to in writing, software
; distributed under the License is distributed on an "AS IS" BASIS,
; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
; See the License for the specific language governing permissions and
; limitations under the License.
;

; Math functions taken from SharedCLibrary - just the bare minimum needed by
; softfloat

        EXPORT  __rt_udiv
        EXPORT  _ll_mul
        EXPORT  _ll_udiv
        EXPORT  _ll_shift_l
        EXPORT  _ll_ushift_r
        EXPORT  _ll_cmpu

; Redefine CLib macros to the general ones

        MACRO
        FunctionEntry $UsesSb, $SaveList, $MakeFrame
        ASSERT  "$UsesSb"=""
        ASSERT  "$MakeFrame"=""
        LCLS    Temps
        LCLS    TempsC
Temps   SETS    "$SaveList"
TempsC  SETS    ""
 [ Temps <> ""
TempsC SETS Temps :CC: ","
 ]
        Push    "$TempsC.lr"
        MEND

        MACRO
        Return $UsesSb, $ReloadList, $Base, $CC
        ASSERT  "$UsesSb"=""
        LCLS    Temps
        LCLS    TempsC
Temps   SETS    "$ReloadList"
TempsC  SETS    ""
 [ Temps <> ""
TempsC SETS Temps :CC: ","
 ]
   [ "$Base" = "LinkNotStacked" :LAND: "$ReloadList"=""
        MOV$CC  pc, lr
   |
        ASSERT  "$Base"=""
        Pull    "$TempsC.pc", $CC
   ]
        MEND

        MACRO
        Pop     $Regs,$Caret,$CC,$Base
        ASSERT  "$Caret"=""
        ASSERT  "$Base"=""
        Pull    "$Regs", $CC
        MEND

|__rt_udiv|
; Unsigned divide of a2 by a1: returns quotient in a1, remainder in a2
; Destroys a3 and ip

        MOV     a3, #0
        RSBS    ip, a1, a2, LSR #3
        BCC     u_sh2
        RSBS    ip, a1, a2, LSR #8
        BCC     u_sh7
        MOV     a1, a1, LSL #8
        ORR     a3, a3, #&FF000000
        RSBS    ip, a1, a2, LSR #4
        BCC     u_sh3
        RSBS    ip, a1, a2, LSR #8
        BCC     u_sh7
        MOV     a1, a1, LSL #8
        ORR     a3, a3, #&00FF0000
        RSBS    ip, a1, a2, LSR #8
        MOVCS   a1, a1, LSL #8
        ORRCS   a3, a3, #&0000FF00
        RSBS    ip, a1, a2, LSR #4
        BCC     u_sh3
        RSBS    ip, a1, #0
        BCS     dividebyzero
u_loop  MOVCS   a1, a1, LSR #8
u_sh7   RSBS    ip, a1, a2, LSR #7
        SUBCS   a2, a2, a1, LSL #7
        ADC     a3, a3, a3
u_sh6   RSBS    ip, a1, a2, LSR #6
        SUBCS   a2, a2, a1, LSL #6
        ADC     a3, a3, a3
u_sh5   RSBS    ip, a1, a2, LSR #5
        SUBCS   a2, a2, a1, LSL #5
        ADC     a3, a3, a3
u_sh4   RSBS    ip, a1, a2, LSR #4
        SUBCS   a2, a2, a1, LSL #4
        ADC     a3, a3, a3
u_sh3   RSBS    ip, a1, a2, LSR #3
        SUBCS   a2, a2, a1, LSL #3
        ADC     a3, a3, a3
u_sh2   RSBS    ip, a1, a2, LSR #2
        SUBCS   a2, a2, a1, LSL #2
        ADC     a3, a3, a3
u_sh1   RSBS    ip, a1, a2, LSR #1
        SUBCS   a2, a2, a1, LSL #1
        ADC     a3, a3, a3
u_sh0   RSBS    ip, a1, a2
        SUBCS   a2, a2, a1
        ADCS    a3, a3, a3
        BCS     u_loop
        MOV     a1, a3
        MOV     pc, lr


; Assume ARMv5+, but keep these switches in place so that any updates to the
; SCL code can easily be copied over to here

                GBLL    HaveCLZ
HaveCLZ         SETL    {TRUE}

                GBLL    HaveMULL
HaveMULL        SETL    {TRUE}

                GBLL    RuntimeArch
RuntimeArch     SETL    {FALSE}

        ; Multiply two 64-bit numbers
        ; In:  (a1,a2),(a3,a4)
        ; Out: (a1,a2)
_ll_mul         ROUT
        FunctionEntry
      [ RuntimeArch
        CPUArch ip, lr
        CMP     ip, #CPUArch_v4
        BCC     mul_hardway
      ]
      [ RuntimeArch :LOR: HaveMULL
        ; Have UMULL instruction
        MOV     ip, a1
        UMULL   a1, lr, a3, a1
        MLA     lr, ip, a4, lr
        MLA     a2, a3, a2, lr
        Return
      ]
      [ RuntimeArch :LOR: :LNOT: HaveMULL
mul_hardway     ROUT
        ; No UMULL instruction
        ; Break the operation down thus:
        ;              aaaaaaaa bbbb cccc
        ;            * dddddddd eeee ffff
        ;              ------------------
        ;                     cccc * ffff
        ;                bbbb * ffff
        ;                cccc * eeee
        ;           bbbb * eeee
        ;   aaaaaaaa * eeeeffff
        ; + dddddddd * bbbbcccc
        MUL     a2, a3, a2          ; msw starts as aaaaaaaa * eeeeffff
        MLA     a2, a4, a1, a2      ; msw += dddddddd * bbbbcccc

        MOV     lr, a3, LSR #16     ; lr = eeee from now on
        MOV     ip, a1, LSR #16     ; ip = bbbb from now on
        SUB     a4, a3, lr, LSL #16 ; a4 = ffff
        SUB     a3, a1, ip, LSL #16 ; a3 = cccc
        MUL     a1, a3, a4          ; lsw starts as cccc * ffff

        MUL     a4, ip, a4
        MUL     a3, lr, a3
        ADDS    a3, a4, a3          ; a3 = (bbbb * ffff + cccc * eeee) [0:31]
        MOV     a4, a3, RRX         ; a4 = (bbbb * ffff + cccc * eeee) [1:32]

        ADDS    a1, a1, a3, LSL #16 ; lsw now complete
        ADC     a2, a2, a4, LSR #15
        MLA     a2, ip, lr, a2      ; msw completed by adding bbbb * eeee
        Return
      ]
      
        ; Divide a uint64_t by another, returning both quotient and remainder
        ; In:  dividend (a1,a2), divisor (a3,a4)
        ; Out: quotient (a1,a2), remainder (a3,a4)
_ll_udiv        ROUT
        FunctionEntry , "a1-v6,sl,fp"
        ; Register usage:
        ; v1,v2 = quotient (initially 0)
        ; v3,v4 = remainder (initially dividend)
        ; v5,v6 = divisor
        ; sl = CPU architecture
        ; fp used as a scratch register
        ; note none of our callees use sl or fp in their usual sense
        Pop     "v3-v6"
_ll_udiv_lateentry  ROUT
        MOV     v1, #0
        MOV     v2, #0

        ; Calculate a floating point underestimate of the
        ; reciprocal of the divisor. The representation used is
        ;   mantissa: 16 bits
        ;   exponent: number of binary places below integers of lsb of mantissa
        ; The way the mantissa and exponent are calculated
        ; depends upon the number of leading zeros in the divisor.
     [ RuntimeArch
        CPUArch sl, lr
        CMP     sl, #CPUArch_v5T
        CLZCS   a1, v6
        MOVCC   a1, v6
        BLCC    soft_clz
     |
      [ HaveCLZ
        CLZ     a1, v6
      |
        MOV     a1, v6
        BL      soft_clz
      ]
     ]
        MOV     fp, a1 ; fp = leading zeros in divisor
        CMP     fp, #16
        BCS     %FT10
        ; Divisor has 0..15 leading zeros.
        MOV     a2, v6, LSL fp
        MOVS    a1, v5
        MOVEQS  a1, a2, LSL #16
        MOVNE   a1, #1 ; round up to account for loss of accuracy
        ADD     a1, a1, a2, LSR #16 ; divisor for calculating mantissa
        B       %FT40

10      CMP     v6, #0
        BEQ     %FT20
        ; Divisor has 16..31 leading zeros.
        SUB     a2, fp, #16
        RSB     a3, fp, #48
        MOVS    a1, v5, LSL a2
        MOVNE   a1, #1 ; round up to account for loss of accuracy
        ADD     a1, a1, v6, LSL a2
        ADD     a1, a1, v5, LSR a3 ; divisor for calculating mantissa
        B       %FT40

20
     [ RuntimeArch
        CMP     sl, #CPUArch_v5T
        CLZCS   a1, v5
        MOVCC   a1, v5
        BLCC    soft_clz
     |
      [ HaveCLZ
        CLZ     a1, v5
      |
        MOV     a1, v5
        BL      soft_clz
      ]
     ]
        ADD     fp, a1, #32 ; fp = leading zeros in divisor
        CMP     fp, #48
        BCS     %FT30
        ; Divisor has 32..47 leading zeros.
        MOV     a2, v5, LSL a1
        MOVS    a1, a2, LSL #16
        MOVNE   a1, #1 ; round up to account for loss of accuracy
        ADD     a1, a1, a2, LSR #16 ; divisor for calculating mantissa
        B       %FT40

30      CMP     v5, #0
        BEQ     %FT99
        ; Divisor has 48..63 leading zeros.
        SUB     a2, a1, #16
        MOV     a1, v5, LSL a2 ; divisor for calculating mantissa
        ; drop through

40      MOV     a2, #&80000000 ; dividend for calculating mantissa
        BL      __rt_udiv ; a1 = mantissa &8000..&10000
        RSB     a2, fp, #15+64 ; a2 = exponent
        TST     a1, #&10000
        MOVNE   a1, #&8000 ; force any &10000 mantissas into 16 bits
        SUBNE   a2, a2, #1

50      ; Main iteration loop:
        ; each time round loop, calculate a close underestimate of
        ; the quotient by multiplying through the "remainder" by the
        ; approximate reciprocal of the divisor.
        ; a1 = mantissa
        ; a2 = exponent

        ; Perform 16 (a1) * 64 (v3,v4) -> 80 (a3,a4,lr) multiply
      [ RuntimeArch
        CMP     sl, #CPUArch_v4
        BCC     %FT51
      ]

      [ RuntimeArch :LOR: HaveMULL
        ; Have UMULL instruction
        UMULL   a3, ip, v3, a1
        UMULL   a4, lr, v4, a1
        ADDS    a4, ip, a4
        ADC     lr, lr, #0
      ]
      [ RuntimeArch
        B       %FT60

51
      ]
      [ RuntimeArch :LOR: :LNOT: HaveMULL
        ; No UMULL instruction
        ;        aaaa bbbb cccc dddd
        ;      *                eeee
        ;        -------------------
        ;                dddd * eeee
        ;           cccc * eeee
        ;      bbbb * eeee
        ; aaaa * eeee
        MOV     ip, v4, LSR #16
        MOV     fp, v3, LSR #16
        SUB     a4, v4, ip, LSL #16
        SUB     a3, v3, fp, LSL #16
        MUL     ip, a1, ip
        MUL     fp, a1, fp
        MUL     a4, a1, a4
        MUL     a3, a1, a3
        MOV     lr, ip, LSR #16
        MOV     ip, ip, LSL #16
        ORR     ip, ip, fp, LSR #16
        MOV     fp, fp, LSL #16
        ADDS    a3, a3, fp
        ADCS    a4, a4, ip
        ADC     lr, lr, #0
      ]
60      ; Shift down by exponent
        ; First a word at a time, if necessary:
        SUBS    ip, a2, #32
        BCC     %FT62
61      MOV     a3, a4
        MOV     a4, lr
        MOV     lr, #0
        SUBS    ip, ip, #32
        BCS     %BT61
62      ; Then by bits, if necessary:
        ADDS    ip, ip, #32
        BEQ     %FT70
        RSB     fp, ip, #32
        MOV     a3, a3, LSR ip
        ORR     a3, a3, a4, LSL fp
        MOV     a4, a4, LSR ip
        ORR     a4, a4, lr, LSL fp

70      ; Now (a3,a4) contains an underestimate of the quotient.
        ; Add it to the running total for the quotient, then
        ; multiply through by divisor and subtract from the remainder.

        ; Sometimes (a3,a4) = 0, in which case this step can be skipped.
        ORRS    lr, a3, a4
        BEQ     %FT80

        ADDS    v1, v1, a3
        ADC     v2, v2, a4

      [ RuntimeArch
        CMP     sl, #CPUArch_v4
        MOVCS   lr, a3
        UMULLCS a3, ip, v5, lr
        MLACS   a4, v5, a4, ip
        MLACS   a4, v6, lr, a4
        BCS     %FT75
      ]
      [ :LNOT: RuntimeArch :LAND: HaveMULL
        MOV     lr, a3
        UMULL   a3, ip, v5, lr
        MLA     a4, v5, a4, ip
        MLA     a4, v6, lr, a4
      ]
      [ RuntimeArch :LOR: :LNOT: HaveMULL
        ; No UMULL instruction
        ; Proceeed as for mul_hardway
        MUL     a4, v5, a4
        MLA     a4, v6, a3, a4

        MOV     ip, a3, LSR #16
        MOV     lr, v5, LSR #16
        SUB     fp, a3, ip, LSL #16
        SUB     lr, v5, lr, LSL #16
        MUL     a3, fp, lr
        Push    "ip"

        MUL     ip, lr, ip
        MOV     lr, v5, LSR #16
        MUL     fp, lr, fp
        ADDS    fp, ip, fp
        MOV     ip, fp, RRX

        ADDS    a3, a3, fp, LSL #16
        ADC     a4, a4, ip, LSR #15
        Pop     "ip"
        MLA     a4, ip, lr, a4
      ]
75      SUBS    v3, v3, a3
        SBC     v4, v4, a4

80      ; Termination condition for iteration loop is
        ;   remainder < divisor
        ; OR
        ;   quotient increment == 0
        CMP     v3, v5
        SBCS    lr, v4, v6
        TEQCC   lr, lr     ; set Z if r < d (and preserve C)
        ORRCSS  lr, a3, a4 ; else Z = a3 and a4 both 0
        BNE     %BT50

        ; The final multiple of the divisor can get lost in rounding
        ; so subtract one more divisor if necessary
        CMP     v3, v5
        SBCS    lr, v4, v6
        BCC     %FT85
        ADDS    v1, v1, #1
        ADC     v2, v2, #0
        SUBS    v3, v3, v5
        SBC     v4, v4, v6
85
        Push    "v1-v4"
        Return  , "a1-v6,sl,fp"

99      ; Division by zero
        Pop     "v1-v6,sl,fp,lr"
        B       __rt_div0

        ; Shift a 64-bit number left
        ; In:  (a1,a2),a3
        ; Out: (a1,a2)
_ll_shift_l     ROUT
        RSBS    ip, a3, #32
        MOVHI   a2, a2, LSL a3
        ORRHI   a2, a2, a1, LSR ip
        MOVHI   a1, a1, LSL a3
        Return  ,, LinkNotStacked, HI
        SUB     ip, a3, #32
        MOV     a2, a1, LSL ip
        MOV     a1, #0
        Return  ,, LinkNotStacked

        ; Logical-shift a 64-bit number right
        ; In:  (a1,a2),a3
        ; Out: (a1,a2)
_ll_ushift_r    ROUT
        RSBS    ip, a3, #32
        MOVHI   a1, a1, LSR a3
        ORRHI   a1, a1, a2, LSL ip
        MOVHI   a2, a2, LSR a3
        Return  ,, LinkNotStacked, HI
        SUB     ip, a3, #32
        MOV     a1, a2, LSR ip
        MOV     a2, #0
        Return  ,, LinkNotStacked

        ; Compare two uint64_t numbers, or test two int64_t numbers for equality
        ; In:  (a1,a2),(a3,a4)
        ; Out: Z set if equal, Z clear if different
        ;      C set if unsigned higher or same, C clear if unsigned lower
        ;      all registers preserved
_ll_cmpu        ROUT
        CMP     a2, a4
        CMPEQ   a1, a3
        MOV     pc, lr ; irrespective of calling standard

        END
@
